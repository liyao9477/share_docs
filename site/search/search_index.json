{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . data synchronization Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Copying branch and pushing to GitHub Trashes \u4f60\u7684\u7f16\u7a0b\u80fd\u529b\u4ec0\u4e48\u65f6\u5019\u5f00\u59cb\u7a81\u98de\u731b\u8fdb\uff1f Roadmap data scientist-roadmap","title":"Index"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org . data synchronization","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. mkdocs gh-deploy - Copying branch and pushing to GitHub","title":"Commands"},{"location":"#trashes","text":"\u4f60\u7684\u7f16\u7a0b\u80fd\u529b\u4ec0\u4e48\u65f6\u5019\u5f00\u59cb\u7a81\u98de\u731b\u8fdb\uff1f","title":"Trashes"},{"location":"#roadmap","text":"data scientist-roadmap","title":"Roadmap"},{"location":"21m_mysql/","text":"21\u5206\u949fMySQL\u57fa\u7840\u5165\u95e8 \u4e3a\u4ec0\u4e48\u53ea\u9700\u898121\u5206\u949f\u5462\uff1f\u56e0\u4e3a\u5728\u6211\u4eec\u5927\u5929\u671d\u6709\u53e5\u8bdd\u53eb\u505a\u4e09\u4e03\u4e8c\u5341\u4e00\uff0c\u4f60\u53ef\u4ee5\u4e0d\u7ba1\u4e09\u4e03\u4e8c\u5341\u4e00\u5f00\u59cb\u4f7f\u7528 MySQL \u53ca\u5feb\u901f\u7684\u65b9\u5f0f\u5165\u95e8 MySQL \u3002\u5176\u5b9e21\u5206\u949f\u628a\u4e0b\u9762\u8bed\u53e5\u6267\u884c\u4e00\u904d\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u8981\u7406\u89e3\u7684\u8bdd\u4f30\u8ba1\u4e0d\u6b6221\u5206\u949f\uff0c\u5bf9\u4e8e\u521d\u5b66\u8005\u6765\u8bf4\u53ea\u9700\u6ee1\u8db3\u81ea\u5df1\u9700\u6c42\u53ef\u4ee5\u589e\u5220\u6539\u67e5\u7b49\u7b80\u6613\u7684\u7ef4\u62a4\u5373\u53ef\u3002 \u76ee\u5f55 \u5f00\u59cb\u4f7f\u7528 \u767b\u5f55MySQL \u521b\u5efa\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u8868 \u589e\u5220\u6539\u67e5 SELECT UPDATE INSERT DELETE WHERE AND \u548c OR AND OR ORDER BY IN NOT UNION AS JOIN SQL \u51fd\u6570 COUNT MAX \u89e6\u53d1\u5668 \u6dfb\u52a0\u7d22\u5f15 \u666e\u901a\u7d22\u5f15(INDEX) \u4e3b\u952e\u7d22\u5f15(PRIMARY key) \u552f\u4e00\u7d22\u5f15(UNIQUE) \u5168\u6587\u7d22\u5f15(FULLTEXT) \u6dfb\u52a0\u591a\u5217\u7d22\u5f15 \u5efa\u7acb\u7d22\u5f15\u7684\u65f6\u673a \u521b\u5efa\u540e\u8868\u7684\u4fee\u6539 \u6dfb\u52a0\u5217 \u4fee\u6539\u5217 \u5220\u9664\u5217 \u91cd\u547d\u540d\u8868 \u6e05\u7a7a\u8868\u6570\u636e \u5220\u9664\u6574\u5f20\u8868 \u5220\u9664\u6574\u4e2a\u6570\u636e\u5e93 \u5176\u5b83\u5b9e\u4f8b SQL\u5220\u9664\u91cd\u590d\u8bb0\u5f55 \u53c2\u8003\u624b\u518c \u5f00\u59cb\u4f7f\u7528 \u6211\u4e0b\u9762\u6240\u6709\u7684SQL\u8bed\u53e5\u662f\u57fa\u4e8eMySQL 5.6+\u8fd0\u884c\u3002 MySQL \u4e3a\u5173\u7cfb\u578b\u6570\u636e\u5e93(Relational Database Management System)\uff0c\u4e00\u4e2a\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7531\u4e00\u4e2a\u6216\u6570\u4e2a\u8868\u683c\u7ec4\u6210, \u5982\u56fe\u6240\u793a\u7684\u4e00\u4e2a\u8868\u683c\uff1a \u8868\u5934(header) : \u6bcf\u4e00\u5217\u7684\u540d\u79f0; \u5217(col) : \u5177\u6709\u76f8\u540c\u6570\u636e\u7c7b\u578b\u7684\u6570\u636e\u7684\u96c6\u5408; \u884c(row) : \u6bcf\u4e00\u884c\u7528\u6765\u63cf\u8ff0\u67d0\u4e2a\u4eba/\u7269\u7684\u5177\u4f53\u4fe1\u606f; \u503c(value) : \u884c\u7684\u5177\u4f53\u4fe1\u606f, \u6bcf\u4e2a\u503c\u5fc5\u987b\u4e0e\u8be5\u5217\u7684\u6570\u636e\u7c7b\u578b\u76f8\u540c; \u952e(key) : \u8868\u4e2d\u7528\u6765\u8bc6\u522b\u67d0\u4e2a\u7279\u5b9a\u7684\u4eba\\\u7269\u7684\u65b9\u6cd5, \u952e\u7684\u503c\u5728\u5f53\u524d\u5217\u4e2d\u5177\u6709\u552f\u4e00\u6027\u3002 \u767b\u5f55MySQL mysql -h 127.0.0.1 -u \u7528\u6237\u540d -p mysql -D \u6240\u9009\u62e9\u7684\u6570\u636e\u5e93\u540d -h \u4e3b\u673a\u540d -u \u7528\u6237\u540d -p mysql exit # \u9000\u51fa \u4f7f\u7528 \u201cquit;\u201d \u6216 \u201c\\q;\u201d \u4e00\u6837\u7684\u6548\u679c mysql status; # \u663e\u793a\u5f53\u524dmysql\u7684version\u7684\u5404\u79cd\u4fe1\u606f mysql select version(); # \u663e\u793a\u5f53\u524dmysql\u7684version\u4fe1\u606f mysql show global variables like 'port'; # \u67e5\u770bMySQL\u7aef\u53e3\u53f7 \u521b\u5efa\u6570\u636e\u5e93 \u5bf9\u4e8e\u8868\u7684\u64cd\u4f5c\u9700\u8981\u5148\u8fdb\u5165\u5e93 use \u5e93\u540d; -- \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a samp_db \u7684\u6570\u636e\u5e93\uff0c\u6570\u636e\u5e93\u5b57\u7b26\u7f16\u7801\u6307\u5b9a\u4e3a gbk create database samp_db character set gbk; drop database samp_db; -- \u5220\u9664 \u5e93\u540d\u4e3asamp_db\u7684\u5e93 show databases; -- \u663e\u793a\u6570\u636e\u5e93\u5217\u8868\u3002 use samp_db; -- \u9009\u62e9\u521b\u5efa\u7684\u6570\u636e\u5e93samp_db show tables; -- \u663e\u793asamp_db\u4e0b\u9762\u6240\u6709\u7684\u8868\u540d\u5b57 describe \u8868\u540d; -- \u663e\u793a\u6570\u636e\u8868\u7684\u7ed3\u6784 delete from \u8868\u540d; -- \u6e05\u7a7a\u8868\u4e2d\u8bb0\u5f55 \u521b\u5efa\u6570\u636e\u5e93\u8868 \u4f7f\u7528 create table \u8bed\u53e5\u53ef\u5b8c\u6210\u5bf9\u8868\u7684\u521b\u5efa, create table \u7684\u5e38\u89c1\u5f62\u5f0f: \u8bed\u6cd5\uff1acreate table \u8868\u540d\u79f0(\u5217\u58f0\u660e); -- \u5982\u679c\u6570\u636e\u5e93\u4e2d\u5b58\u5728user_accounts\u8868\uff0c\u5c31\u628a\u5b83\u4ece\u6570\u636e\u5e93\u4e2ddrop\u6389 DROP TABLE IF EXISTS `user_accounts`; CREATE TABLE `user_accounts` ( `id` int(100) unsigned NOT NULL AUTO_INCREMENT primary key, `password` varchar(32) NOT NULL DEFAULT '' COMMENT '\u7528\u6237\u5bc6\u7801', `reset_password` tinyint(32) NOT NULL DEFAULT 0 COMMENT '\u7528\u6237\u7c7b\u578b\uff1a0\uff0d\u4e0d\u9700\u8981\u91cd\u7f6e\u5bc6\u7801\uff1b1-\u9700\u8981\u91cd\u7f6e\u5bc6\u7801', `mobile` varchar(20) NOT NULL DEFAULT '' COMMENT '\u624b\u673a', `create_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6), `update_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), -- \u521b\u5efa\u552f\u4e00\u7d22\u5f15\uff0c\u4e0d\u5141\u8bb8\u91cd\u590d UNIQUE INDEX idx_user_mobile(`mobile`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='\u7528\u6237\u8868\u4fe1\u606f'; \u6570\u636e\u7c7b\u578b\u7684\u5c5e\u6027\u89e3\u91ca NULL \uff1a\u6570\u636e\u5217\u53ef\u5305\u542bNULL\u503c\uff1b NOT NULL \uff1a\u6570\u636e\u5217\u4e0d\u5141\u8bb8\u5305\u542bNULL\u503c\uff1b DEFAULT \uff1a\u9ed8\u8ba4\u503c\uff1b PRIMARY KEY \uff1a\u4e3b\u952e\uff1b AUTO_INCREMENT \uff1a\u81ea\u52a8\u9012\u589e\uff0c\u9002\u7528\u4e8e\u6574\u6570\u7c7b\u578b\uff1b UNSIGNED \uff1a\u662f\u6307\u6570\u503c\u7c7b\u578b\u53ea\u80fd\u4e3a\u6b63\u6570\uff1b CHARACTER SET name \uff1a\u6307\u5b9a\u4e00\u4e2a\u5b57\u7b26\u96c6\uff1b COMMENT \uff1a\u5bf9\u8868\u6216\u8005\u5b57\u6bb5\u8bf4\u660e\uff1b \u589e\u5220\u6539\u67e5 SELECT SELECT \u8bed\u53e5\u7528\u4e8e\u4ece\u8868\u4e2d\u9009\u53d6\u6570\u636e\u3002 \u8bed\u6cd5\uff1a SELECT \u5217\u540d\u79f0 FROM \u8868\u540d\u79f0 \u8bed\u6cd5\uff1a SELECT * FROM \u8868\u540d\u79f0 -- \u8868station\u53d6\u4e2a\u522b\u540d\u53ebs\uff0c\u8868station\u4e2d\u4e0d\u5305\u542b \u5b57\u6bb5id=13\u6216\u800514 \u7684\uff0c\u5e76\u4e14id\u4e0d\u7b49\u4e8e4\u7684 \u67e5\u8be2\u51fa\u6765\uff0c\u53ea\u663e\u793aid SELECT s.id from station s WHERE id in (13,14) and id not in (4); -- \u4ece\u8868 Persons \u9009\u53d6 LastName \u5217\u7684\u6570\u636e SELECT LastName FROM Persons -- \u4ece\u8868 users \u9009\u53d6 id=3 \u7684\u6570\u636e\uff0c\u5e76\u53ea\u62c9\u4e00\u6761\u6570\u636e(\u636e\u8bf4\u80fd\u4f18\u5316\u6027\u80fd) SELECT * FROM users where id=3 limit 1 -- \u7ed3\u679c\u96c6\u4e2d\u4f1a\u81ea\u52a8\u53bb\u91cd\u590d\u6570\u636e SELECT DISTINCT Company FROM Orders -- \u8868 Persons \u5b57\u6bb5 Id_P \u7b49\u4e8e Orders \u5b57\u6bb5 Id_P \u7684\u503c\uff0c -- \u7ed3\u679c\u96c6\u663e\u793a Persons\u8868\u7684 LastName\u3001FirstName\u5b57\u6bb5\uff0cOrders\u8868\u7684OrderNo\u5b57\u6bb5 SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p, Orders o WHERE p.Id_P = o.Id_P -- gbk \u548c utf8 \u4e2d\u82f1\u6587\u6df7\u5408\u6392\u5e8f\u6700\u7b80\u5355\u7684\u529e\u6cd5 -- ci\u662f case insensitive, \u5373 \u201c\u5927\u5c0f\u5199\u4e0d\u654f\u611f\u201d SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using gbk) collate gbk_chinese_ci; SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using utf8) collate utf8_unicode_ci; UPDATE Update \u8bed\u53e5\u7528\u4e8e\u4fee\u6539\u8868\u4e2d\u7684\u6570\u636e\u3002 \u8bed\u6cd5\uff1a UPDATE \u8868\u540d\u79f0 SET \u5217\u540d\u79f0 = \u65b0\u503c WHERE \u5217\u540d\u79f0 = \u67d0\u503c -- update\u8bed\u53e5\u8bbe\u7f6e\u5b57\u6bb5\u503c\u4e3a\u53e6\u4e00\u4e2a\u7ed3\u679c\u53d6\u51fa\u6765\u7684\u5b57\u6bb5 update user set name = (select name from user1 where user1 .id = 1 ) where id = (select id from user2 where user2 .name='\u5c0f\u82cf'); -- \u66f4\u65b0\u8868 orders \u4e2d id=1 \u7684\u90a3\u4e00\u884c\u6570\u636e\u66f4\u65b0\u5b83\u7684 title \u5b57\u6bb5 UPDATE `orders` set title='\u8fd9\u91cc\u662f\u6807\u9898' WHERE id=1; INSERT INSERT INTO \u8bed\u53e5\u7528\u4e8e\u5411\u8868\u683c\u4e2d\u63d2\u5165\u65b0\u7684\u884c\u3002 \u8bed\u6cd5\uff1a INSERT INTO \u8868\u540d\u79f0 VALUES (\u503c1, \u503c2,....) \u8bed\u6cd5\uff1a INSERT INTO \u8868\u540d\u79f0 (\u52171, \u52172,...) VALUES (\u503c1, \u503c2,....) -- \u5411\u8868 Persons \u63d2\u5165\u4e00\u6761\u5b57\u6bb5 LastName = JSLite \u5b57\u6bb5 Address = shanghai INSERT INTO Persons (LastName, Address) VALUES ('JSLite', 'shanghai'); -- \u5411\u8868 meeting \u63d2\u5165 \u5b57\u6bb5 a=1 \u548c\u5b57\u6bb5 b=2 INSERT INTO meeting SET a=1,b=2; -- -- SQL\u5b9e\u73b0\u5c06\u4e00\u4e2a\u8868\u7684\u6570\u636e\u63d2\u5165\u5230\u53e6\u5916\u4e00\u4e2a\u8868\u7684\u4ee3\u7801 -- \u5982\u679c\u53ea\u5e0c\u671b\u5bfc\u5165\u6307\u5b9a\u5b57\u6bb5\uff0c\u53ef\u4ee5\u7528\u8fd9\u79cd\u65b9\u6cd5\uff1a -- INSERT INTO \u76ee\u6807\u8868 (\u5b57\u6bb51, \u5b57\u6bb52, ...) SELECT \u5b57\u6bb51, \u5b57\u6bb52, ... FROM \u6765\u6e90\u8868; INSERT INTO orders (user_account_id, title) SELECT m.user_id, m.title FROM meeting m where m.id=1; -- \u5411\u8868 charger \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u5df2\u5b58\u5728\u5c31\u5bf9\u8868 charger \u66f4\u65b0 `type`,`update_at` \u5b57\u6bb5\uff1b INSERT INTO `charger` (`id`,`type`,`create_at`,`update_at`) VALUES (3,2,'2017-05-18 11:06:17','2017-05-18 11:06:17') ON DUPLICATE KEY UPDATE `id`=VALUES(`id`), `type`=VALUES(`type`), `update_at`=VALUES(`update_at`); DELETE DELETE \u8bed\u53e5\u7528\u4e8e\u5220\u9664\u8868\u4e2d\u7684\u884c\u3002 \u8bed\u6cd5\uff1a DELETE FROM \u8868\u540d\u79f0 WHERE \u5217\u540d\u79f0 = \u503c -- \u5728\u4e0d\u5220\u9664table_name\u8868\u7684\u60c5\u51b5\u4e0b\u5220\u9664\u6240\u6709\u7684\u884c\uff0c\u6e05\u7a7a\u8868\u3002 DELETE FROM table_name -- \u6216\u8005 DELETE * FROM table_name -- \u5220\u9664 Person\u8868\u5b57\u6bb5 LastName = 'JSLite' DELETE FROM Person WHERE LastName = 'JSLite' -- \u5220\u9664 \u8868meeting id \u4e3a2\u548c3\u7684\u4e24\u6761\u6570\u636e DELETE from meeting where id in (2,3); WHERE WHERE \u5b50\u53e5\u7528\u4e8e\u89c4\u5b9a\u9009\u62e9\u7684\u6807\u51c6\u3002 \u8bed\u6cd5\uff1a SELECT \u5217\u540d\u79f0 FROM \u8868\u540d\u79f0 WHERE \u5217 \u8fd0\u7b97\u7b26 \u503c -- \u4ece\u8868 Persons \u4e2d\u9009\u51fa Year \u5b57\u6bb5\u5927\u4e8e 1965 \u7684\u6570\u636e SELECT * FROM Persons WHERE Year 1965 AND \u548c OR AND - \u5982\u679c\u7b2c\u4e00\u4e2a\u6761\u4ef6\u548c\u7b2c\u4e8c\u4e2a\u6761\u4ef6\u90fd\u6210\u7acb\uff1b OR - \u5982\u679c\u7b2c\u4e00\u4e2a\u6761\u4ef6\u548c\u7b2c\u4e8c\u4e2a\u6761\u4ef6\u4e2d\u53ea\u8981\u6709\u4e00\u4e2a\u6210\u7acb\uff1b AND -- \u5220\u9664 meeting \u8868\u5b57\u6bb5 -- id=2 \u5e76\u4e14 user_id=5 \u7684\u6570\u636e \u548c -- id=3 \u5e76\u4e14 user_id=6 \u7684\u6570\u636e DELETE from meeting where id in (2,3) and user_id in (5,6); -- \u4f7f\u7528 AND \u6765\u663e\u793a\u6240\u6709\u59d3\u4e3a Carter \u5e76\u4e14\u540d\u4e3a Thomas \u7684\u4eba\uff1a SELECT * FROM Persons WHERE FirstName='Thomas' AND LastName='Carter'; OR -- \u4f7f\u7528 OR \u6765\u663e\u793a\u6240\u6709\u59d3\u4e3a Carter \u6216\u8005\u540d\u4e3a Thomas \u7684\u4eba\uff1a SELECT * FROM Persons WHERE firstname='Thomas' OR lastname='Carter' ORDER BY \u8bed\u53e5\u9ed8\u8ba4\u6309\u7167\u5347\u5e8f\u5bf9\u8bb0\u5f55\u8fdb\u884c\u6392\u5e8f\u3002 ORDER BY - \u8bed\u53e5\u7528\u4e8e\u6839\u636e\u6307\u5b9a\u7684\u5217\u5bf9\u7ed3\u679c\u96c6\u8fdb\u884c\u6392\u5e8f\u3002 DESC - \u6309\u7167\u964d\u5e8f\u5bf9\u8bb0\u5f55\u8fdb\u884c\u6392\u5e8f\u3002 ASC - \u6309\u7167\u987a\u5e8f\u5bf9\u8bb0\u5f55\u8fdb\u884c\u6392\u5e8f\u3002 -- Company\u5728\u8868Orders\u4e2d\u4e3a\u5b57\u6bcd\uff0c\u5219\u4f1a\u4ee5\u5b57\u6bcd\u987a\u5e8f\u663e\u793a\u516c\u53f8\u540d\u79f0 SELECT Company, OrderNumber FROM Orders ORDER BY Company -- \u540e\u9762\u8ddf\u4e0a DESC \u5219\u4e3a\u964d\u5e8f\u663e\u793a SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC -- Company\u4ee5\u964d\u5e8f\u663e\u793a\u516c\u53f8\u540d\u79f0\uff0c\u5e76OrderNumber\u4ee5\u987a\u5e8f\u663e\u793a SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC IN IN - \u64cd\u4f5c\u7b26\u5141\u8bb8\u6211\u4eec\u5728 WHERE \u5b50\u53e5\u4e2d\u89c4\u5b9a\u591a\u4e2a\u503c\u3002 IN - \u64cd\u4f5c\u7b26\u7528\u6765\u6307\u5b9a\u8303\u56f4\uff0c\u8303\u56f4\u4e2d\u7684\u6bcf\u4e00\u6761\uff0c\u90fd\u8fdb\u884c\u5339\u914d\u3002IN\u53d6\u503c\u89c4\u5f8b\uff0c\u7531\u9017\u53f7\u5206\u5272\uff0c\u5168\u90e8\u653e\u7f6e\u62ec\u53f7\u4e2d\u3002 \u8bed\u6cd5\uff1a SELECT \"\u5b57\u6bb5\u540d\"FROM \"\u8868\u683c\u540d\"WHERE \"\u5b57\u6bb5\u540d\" IN ('\u503c\u4e00', '\u503c\u4e8c', ...); -- \u4ece\u8868 Persons \u9009\u53d6 \u5b57\u6bb5 LastName \u7b49\u4e8e Adams\u3001Carter SELECT * FROM Persons WHERE LastName IN ('Adams','Carter') NOT NOT - \u64cd\u4f5c\u7b26\u603b\u662f\u4e0e\u5176\u4ed6\u64cd\u4f5c\u7b26\u4e00\u8d77\u4f7f\u7528\uff0c\u7528\u5728\u8981\u8fc7\u6ee4\u7684\u524d\u9762\u3002 SELECT vend_id, prod_name FROM Products WHERE NOT vend_id = 'DLL01' ORDER BY prod_name; UNION UNION - \u64cd\u4f5c\u7b26\u7528\u4e8e\u5408\u5e76\u4e24\u4e2a\u6216\u591a\u4e2a SELECT \u8bed\u53e5\u7684\u7ed3\u679c\u96c6\u3002 -- \u5217\u51fa\u6240\u6709\u5728\u4e2d\u56fd\u8868\uff08Employees_China\uff09\u548c\u7f8e\u56fd\uff08Employees_USA\uff09\u7684\u4e0d\u540c\u7684\u96c7\u5458\u540d SELECT E_Name FROM Employees_China UNION SELECT E_Name FROM Employees_USA -- \u5217\u51fa meeting \u8868\u4e2d\u7684 pic_url\uff0c -- station \u8868\u4e2d\u7684 number_station \u522b\u540d\u8bbe\u7f6e\u6210 pic_url \u907f\u514d\u5b57\u6bb5\u4e0d\u4e00\u6837\u62a5\u9519 -- \u6309\u66f4\u65b0\u65f6\u95f4\u6392\u5e8f SELECT id,pic_url FROM meeting UNION ALL SELECT id,number_station AS pic_url FROM station ORDER BY update_at; -- \u901a\u8fc7 UNION \u8bed\u6cd5\u540c\u65f6\u67e5\u8be2\u4e86 products \u8868 \u548c comments \u8868\u7684\u603b\u8bb0\u5f55\u6570\uff0c\u5e76\u4e14\u6309\u7167 count \u6392\u5e8f SELECT 'product' AS type, count(*) as count FROM `products` union select 'comment' as type, count(*) as count FROM `comments` order by count; AS as - \u53ef\u7406\u89e3\u4e3a\uff1a\u7528\u4f5c\u3001\u5f53\u6210\uff0c\u4f5c\u4e3a\uff1b\u522b\u540d \u4e00\u822c\u662f\u91cd\u547d\u540d\u5217\u540d\u6216\u8005\u8868\u540d\u3002 \u8bed\u6cd5\uff1a select column_1 as \u52171,column_2 as \u52172 from table as \u8868 SELECT * FROM Employee AS emp -- \u8fd9\u53e5\u610f\u601d\u662f\u67e5\u627e\u6240\u6709Employee \u8868\u91cc\u9762\u7684\u6570\u636e\uff0c\u5e76\u628aEmployee\u8868\u683c\u547d\u540d\u4e3a emp\u3002 -- \u5f53\u4f60\u547d\u540d\u4e00\u4e2a\u8868\u4e4b\u540e\uff0c\u4f60\u53ef\u4ee5\u5728\u4e0b\u9762\u7528 emp \u4ee3\u66ff Employee. -- \u4f8b\u5982 SELECT * FROM emp. SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders -- \u5217\u51fa\u8868 Orders \u5b57\u6bb5 OrderPrice \u5217\u6700\u5927\u503c\uff0c -- \u7ed3\u679c\u96c6\u5217\u4e0d\u663e\u793a OrderPrice \u663e\u793a LargestOrderPrice -- \u663e\u793a\u8868 users_profile \u4e2d\u7684 name \u5217 SELECT t.name from (SELECT * from users_profile a) AS t; -- \u8868 user_accounts \u547d\u540d\u522b\u540d ua\uff0c\u8868 users_profile \u547d\u540d\u522b\u540d up -- \u6ee1\u8db3\u6761\u4ef6 \u8868 user_accounts \u5b57\u6bb5 id \u7b49\u4e8e \u8868 users_profile \u5b57\u6bb5 user_id -- \u7ed3\u679c\u96c6\u53ea\u663e\u793amobile\u3001name\u4e24\u5217 SELECT ua.mobile,up.name FROM user_accounts as ua INNER JOIN users_profile as up ON ua.id = up.user_id; JOIN \u7528\u4e8e\u6839\u636e\u4e24\u4e2a\u6216\u591a\u4e2a\u8868\u4e2d\u7684\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ece\u8fd9\u4e9b\u8868\u4e2d\u67e5\u8be2\u6570\u636e\u3002 JOIN : \u5982\u679c\u8868\u4e2d\u6709\u81f3\u5c11\u4e00\u4e2a\u5339\u914d\uff0c\u5219\u8fd4\u56de\u884c INNER JOIN :\u5728\u8868\u4e2d\u5b58\u5728\u81f3\u5c11\u4e00\u4e2a\u5339\u914d\u65f6\uff0cINNER JOIN \u5173\u952e\u5b57\u8fd4\u56de\u884c\u3002 LEFT JOIN : \u5373\u4f7f\u53f3\u8868\u4e2d\u6ca1\u6709\u5339\u914d\uff0c\u4e5f\u4ece\u5de6\u8868\u8fd4\u56de\u6240\u6709\u7684\u884c RIGHT JOIN : \u5373\u4f7f\u5de6\u8868\u4e2d\u6ca1\u6709\u5339\u914d\uff0c\u4e5f\u4ece\u53f3\u8868\u8fd4\u56de\u6240\u6709\u7684\u884c FULL JOIN : \u53ea\u8981\u5176\u4e2d\u4e00\u4e2a\u8868\u4e2d\u5b58\u5728\u5339\u914d\uff0c\u5c31\u8fd4\u56de\u884c(MySQL \u662f\u4e0d\u652f\u6301\u7684\uff0c\u901a\u8fc7 LEFT JOIN + UNION + RIGHT JOIN \u7684\u65b9\u5f0f \u6765\u5b9e\u73b0) SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo FROM Persons INNER JOIN Orders ON Persons.Id_P = Orders.Id_P ORDER BY Persons.LastName; SQL \u51fd\u6570 COUNT COUNT \u8ba9\u6211\u4eec\u80fd\u591f\u6570\u51fa\u5728\u8868\u683c\u4e2d\u6709\u591a\u5c11\u7b14\u8d44\u6599\u88ab\u9009\u51fa\u6765\u3002 \u8bed\u6cd5\uff1a SELECT COUNT(\"\u5b57\u6bb5\u540d\") FROM \"\u8868\u683c\u540d\"; -- \u8868 Store_Information \u6709\u51e0\u7b14 store_name \u680f\u4e0d\u662f\u7a7a\u767d\u7684\u8d44\u6599\u3002 -- IS NOT NULL \u662f \u8fd9\u4e2a\u680f\u4f4d\u4e0d\u662f\u7a7a\u767d \u7684\u610f\u601d\u3002 SELECT COUNT (Store_Name) FROM Store_Information WHERE Store_Name IS NOT NULL; -- \u83b7\u53d6 Persons \u8868\u7684\u603b\u6570 SELECT COUNT(1) AS totals FROM Persons; -- \u83b7\u53d6\u8868 station \u5b57\u6bb5 user_id \u76f8\u540c\u7684\u603b\u6570 select user_id, count(*) as totals from station group by user_id; MAX MAX \u51fd\u6570\u8fd4\u56de\u4e00\u5217\u4e2d\u7684\u6700\u5927\u503c\u3002NULL \u503c\u4e0d\u5305\u62ec\u5728\u8ba1\u7b97\u4e2d\u3002 \u8bed\u6cd5\uff1a SELECT MAX(\"\u5b57\u6bb5\u540d\") FROM \"\u8868\u683c\u540d\" -- \u5217\u51fa\u8868 Orders \u5b57\u6bb5 OrderPrice \u5217\u6700\u5927\u503c\uff0c -- \u7ed3\u679c\u96c6\u5217\u4e0d\u663e\u793a OrderPrice \u663e\u793a LargestOrderPrice SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders \u89e6\u53d1\u5668 \u8bed\u6cd5\uff1a create trigger \u89e6\u53d1\u5668\u540d\u79f0 { before | after} # \u4e4b\u524d\u6216\u8005\u4e4b\u540e\u51fa\u53d1 insert | update | delete # \u6307\u660e\u4e86\u6fc0\u6d3b\u89e6\u53d1\u7a0b\u5e8f\u7684\u8bed\u53e5\u7684\u7c7b\u578b on \u8868\u540d # \u64cd\u4f5c\u54ea\u5f20\u8868 for each row # \u89e6\u53d1\u5668\u7684\u6267\u884c\u95f4\u9694\uff0cfor each row \u901a\u77e5\u89e6\u53d1\u5668\u6bcf\u9694\u4e00\u884c\u6267\u884c\u4e00\u6b21\u52a8\u4f5c\uff0c\u800c\u4e0d\u662f\u5bf9\u6574\u4e2a\u8868\u6267\u884c\u4e00\u6b21\u3002 \u89e6\u53d1\u5668SQL\u8bed\u53e5 delimiter $ CREATE TRIGGER set_userdate BEFORE INSERT on `message` for EACH ROW BEGIN set @statu = new.status; -- \u58f0\u660e\u590d\u5236\u53d8\u91cf statu if @statu = 0 then -- \u5224\u65ad statu \u662f\u5426\u7b49\u4e8e 0 UPDATE `user_accounts` SET status=1 WHERE openid=NEW.openid; end if; END $ DELIMITER ; -- \u6062\u590d\u7ed3\u675f\u7b26\u53f7 OLD\u548cNEW\u4e0d\u533a\u5206\u5927\u5c0f\u5199 - NEW \u7528NEW.col_name\uff0c\u6ca1\u6709\u65e7\u884c\u3002\u5728DELETE\u89e6\u53d1\u7a0b\u5e8f\u4e2d\uff0c\u4ec5\u80fd\u4f7f\u7528OLD.col_name\uff0c\u6ca1\u6709\u65b0\u884c\u3002 - OLD \u7528OLD.col_name\u6765\u5f15\u7528\u66f4\u65b0\u524d\u7684\u67d0\u4e00\u884c\u7684\u5217 \u6dfb\u52a0\u7d22\u5f15 \u666e\u901a\u7d22\u5f15(INDEX) \u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD INDEX \u7d22\u5f15\u540d\u5b57 ( \u5b57\u6bb5\u540d\u5b57 ) -- \u2013\u76f4\u63a5\u521b\u5efa\u7d22\u5f15 CREATE INDEX index_user ON user(title) -- \u2013\u4fee\u6539\u8868\u7ed3\u6784\u7684\u65b9\u5f0f\u6dfb\u52a0\u7d22\u5f15 ALTER TABLE table_name ADD INDEX index_name ON (column(length)) -- \u7ed9 user \u8868\u4e2d\u7684 name \u5b57\u6bb5 \u6dfb\u52a0\u666e\u901a\u7d22\u5f15(INDEX) ALTER TABLE `user` ADD INDEX index_name (name) -- \u2013\u521b\u5efa\u8868\u7684\u65f6\u5019\u540c\u65f6\u521b\u5efa\u7d22\u5f15 CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL , `content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (title(length)) ) -- \u2013\u5220\u9664\u7d22\u5f15 DROP INDEX index_name ON table \u4e3b\u952e\u7d22\u5f15(PRIMARY key) \u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD PRIMARY KEY ( \u5b57\u6bb5\u540d\u5b57 ) -- \u7ed9 user \u8868\u4e2d\u7684 id\u5b57\u6bb5 \u6dfb\u52a0\u4e3b\u952e\u7d22\u5f15(PRIMARY key) ALTER TABLE `user` ADD PRIMARY key (id); \u552f\u4e00\u7d22\u5f15(UNIQUE) \u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD UNIQUE ( \u5b57\u6bb5\u540d\u5b57 ) -- \u7ed9 user \u8868\u4e2d\u7684 creattime \u5b57\u6bb5\u6dfb\u52a0\u552f\u4e00\u7d22\u5f15(UNIQUE) ALTER TABLE `user` ADD UNIQUE (creattime); \u5168\u6587\u7d22\u5f15(FULLTEXT) \u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD FULLTEXT ( \u5b57\u6bb5\u540d\u5b57 ) -- \u7ed9 user \u8868\u4e2d\u7684 description \u5b57\u6bb5\u6dfb\u52a0\u5168\u6587\u7d22\u5f15(FULLTEXT) ALTER TABLE `user` ADD FULLTEXT (description); \u6dfb\u52a0\u591a\u5217\u7d22\u5f15 \u8bed\u6cd5\uff1a ALTER TABLE table_name ADD INDEX index_name ( column1 , column2 , column3 ) -- \u7ed9 user \u8868\u4e2d\u7684 name\u3001city\u3001age \u5b57\u6bb5\u6dfb\u52a0\u540d\u5b57\u4e3aname_city_age\u7684\u666e\u901a\u7d22\u5f15(INDEX) ALTER TABLE user ADD INDEX name_city_age (name(10),city,age); \u5efa\u7acb\u7d22\u5f15\u7684\u65f6\u673a \u5728 WHERE \u548c JOIN \u4e2d\u51fa\u73b0\u7684\u5217\u9700\u8981\u5efa\u7acb\u7d22\u5f15\uff0c\u4f46\u4e5f\u4e0d\u5b8c\u5168\u5982\u6b64\uff1a MySQL\u53ea\u5bf9 \uff0c = \uff0c = \uff0c \uff0c = \uff0c BETWEEN \uff0c IN \u4f7f\u7528\u7d22\u5f15 \u67d0\u4e9b\u65f6\u5019\u7684 LIKE \u4e5f\u4f1a\u4f7f\u7528\u7d22\u5f15\u3002 \u5728 LIKE \u4ee5\u901a\u914d\u7b26%\u548c_\u5f00\u5934\u4f5c\u67e5\u8be2\u65f6\uff0cMySQL\u4e0d\u4f1a\u4f7f\u7528\u7d22\u5f15\u3002 -- \u6b64\u65f6\u5c31\u9700\u8981\u5bf9city\u548cage\u5efa\u7acb\u7d22\u5f15\uff0c -- \u7531\u4e8emytable\u8868\u7684userame\u4e5f\u51fa\u73b0\u5728\u4e86JOIN\u5b50\u53e5\u4e2d\uff0c\u4e5f\u6709\u5bf9\u5b83\u5efa\u7acb\u7d22\u5f15\u7684\u5fc5\u8981\u3002 SELECT t.Name FROM mytable t LEFT JOIN mytable m ON t.Name=m.username WHERE m.age=20 AND m.city='\u4e0a\u6d77'; SELECT * FROM mytable WHERE username like'admin%'; -- \u800c\u4e0b\u53e5\u5c31\u4e0d\u4f1a\u4f7f\u7528\uff1a SELECT * FROM mytable WHERE Name like'%admin'; -- \u56e0\u6b64\uff0c\u5728\u4f7f\u7528LIKE\u65f6\u5e94\u6ce8\u610f\u4ee5\u4e0a\u7684\u533a\u522b\u3002 \u7d22\u5f15\u7684\u6ce8\u610f\u4e8b\u9879 \u7d22\u5f15\u4e0d\u4f1a\u5305\u542b\u6709NULL\u503c\u7684\u5217 \u4f7f\u7528\u77ed\u7d22\u5f15 \u4e0d\u8981\u5728\u5217\u4e0a\u8fdb\u884c\u8fd0\u7b97 \u7d22\u5f15\u4f1a\u5931\u6548 \u521b\u5efa\u540e\u8868\u7684\u4fee\u6539 \u6dfb\u52a0\u5217 \u8bed\u6cd5\uff1a alter table \u8868\u540d add \u5217\u540d \u5217\u6570\u636e\u7c7b\u578b [after \u63d2\u5165\u4f4d\u7f6e]; \u793a\u4f8b: -- \u5728\u8868students\u7684\u6700\u540e\u8ffd\u52a0\u5217 address: alter table students add address char(60); -- \u5728\u540d\u4e3a age \u7684\u5217\u540e\u63d2\u5165\u5217 birthday: alter table students add birthday date after age; -- \u5728\u540d\u4e3a number_people \u7684\u5217\u540e\u63d2\u5165\u5217 weeks: alter table students add column `weeks` varchar(5) not null default after `number_people`; \u4fee\u6539\u5217 \u8bed\u6cd5\uff1a alter table \u8868\u540d change \u5217\u540d\u79f0 \u5217\u65b0\u540d\u79f0 \u65b0\u6570\u636e\u7c7b\u578b; -- \u5c06\u8868 tel \u5217\u6539\u540d\u4e3a telphone: alter table students change tel telphone char(13) default - ; -- \u5c06 name \u5217\u7684\u6570\u636e\u7c7b\u578b\u6539\u4e3a char(16): alter table students change name name char(16) not null; -- \u4fee\u6539 COMMENT \u524d\u9762\u5fc5\u987b\u5f97\u6709\u7c7b\u578b\u5c5e\u6027 alter table students change name name char(16) COMMENT '\u8fd9\u91cc\u662f\u540d\u5b57'; -- \u4fee\u6539\u5217\u5c5e\u6027\u7684\u65f6\u5019 \u5efa\u8bae\u4f7f\u7528modify,\u4e0d\u9700\u8981\u91cd\u5efa\u8868 -- change\u7528\u4e8e\u4fee\u6539\u5217\u540d\u5b57\uff0c\u8fd9\u4e2a\u9700\u8981\u91cd\u5efa\u8868 alter table meeting modify `weeks` varchar(20) NOT NULL DEFAULT '' COMMENT '\u5f00\u653e\u65e5\u671f \u5468\u4e00\u5230\u5468\u65e5\uff1a0~6\uff0c\u95f4\u9694\u7528\u82f1\u6587\u9017\u53f7\u9694\u5f00'; -- `user`\u8868\u7684`id`\u5217\uff0c\u4fee\u6539\u6210\u5b57\u7b26\u4e32\u7c7b\u578b\u957f\u5ea650\uff0c\u4e0d\u80fd\u4e3a\u7a7a\uff0c`FIRST`\u653e\u5728\u7b2c\u4e00\u5217\u7684\u4f4d\u7f6e alter table `user` modify COLUMN `id` varchar(50) NOT NULL FIRST ; \u5220\u9664\u5217 \u8bed\u6cd5\uff1a alter table \u8868\u540d drop \u5217\u540d\u79f0; -- \u5220\u9664\u8868students\u4e2d\u7684 birthday \u5217: alter table students drop birthday; \u91cd\u547d\u540d\u8868 \u8bed\u6cd5\uff1a alter table \u8868\u540d rename \u65b0\u8868\u540d; -- \u91cd\u547d\u540d students \u8868\u4e3a workmates: alter table students rename workmates; \u6e05\u7a7a\u8868\u6570\u636e \u65b9\u6cd5\u4e00\uff1a delete from \u8868\u540d; \u65b9\u6cd5\u4e8c\uff1a truncate table \"\u8868\u540d\"; DELETE: 1. DML\u8bed\u8a00;2. \u53ef\u4ee5\u56de\u9000;3. \u53ef\u4ee5\u6709\u6761\u4ef6\u7684\u5220\u9664; TRUNCATE: 1. DDL\u8bed\u8a00;2. \u65e0\u6cd5\u56de\u9000;3. \u9ed8\u8ba4\u6240\u6709\u7684\u8868\u5185\u5bb9\u90fd\u5220\u9664;4. \u5220\u9664\u901f\u5ea6\u6bd4delete\u5feb\u3002 -- \u6e05\u7a7a\u8868\u4e3a workmates \u91cc\u9762\u7684\u6570\u636e\uff0c\u4e0d\u5220\u9664\u8868\u3002 delete from workmates; -- \u5220\u9664workmates\u8868\u4e2d\u7684\u6240\u6709\u6570\u636e\uff0c\u4e14\u65e0\u6cd5\u6062\u590d truncate table workmates; \u5220\u9664\u6574\u5f20\u8868 \u8bed\u6cd5\uff1a drop table \u8868\u540d; -- \u5220\u9664 workmates \u8868: drop table workmates; \u5220\u9664\u6574\u4e2a\u6570\u636e\u5e93 \u8bed\u6cd5\uff1a drop database \u6570\u636e\u5e93\u540d; -- \u5220\u9664 samp_db \u6570\u636e\u5e93: drop database samp_db; \u5176\u5b83\u5b9e\u4f8b SQL\u5220\u9664\u91cd\u590d\u8bb0\u5f55 \u8f6c\u8f7d -- \u67e5\u627e\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff0c\u91cd\u590d\u8bb0\u5f55\u662f\u6839\u636e\u5355\u4e2a\u5b57\u6bb5\uff08peopleId\uff09\u6765\u5224\u65ad select * from people where peopleId in (select peopleId from people group by peopleId having count(peopleId) 1) -- \u5220\u9664\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff0c\u91cd\u590d\u8bb0\u5f55\u662f\u6839\u636e\u5355\u4e2a\u5b57\u6bb5\uff08peopleId\uff09\u6765\u5224\u65ad\uff0c\u53ea\u7559\u6709rowid\u6700\u5c0f\u7684\u8bb0\u5f55 delete from people where peopleId in (select peopleId from people group by peopleId having count(peopleId) 1) and rowid not in (select min(rowid) from people group by peopleId having count(peopleId ) 1) -- \u67e5\u627e\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff08\u591a\u4e2a\u5b57\u6bb5\uff09 select * from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) 1) -- \u5220\u9664\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff08\u591a\u4e2a\u5b57\u6bb5\uff09\uff0c\u53ea\u7559\u6709rowid\u6700\u5c0f\u7684\u8bb0\u5f55 delete from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) 1) and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*) 1) -- \u67e5\u627e\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff08\u591a\u4e2a\u5b57\u6bb5\uff09\uff0c\u4e0d\u5305\u542browid\u6700\u5c0f\u7684\u8bb0\u5f55 select * from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) 1) and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*) 1) \u53c2\u8003\u624b\u518c http://www.w3school.com.cn/sql/index.asp http://www.1keydata.com/cn/sql/sql-count.php","title":"Mysql_21m"},{"location":"21m_mysql/#21mysql","text":"\u4e3a\u4ec0\u4e48\u53ea\u9700\u898121\u5206\u949f\u5462\uff1f\u56e0\u4e3a\u5728\u6211\u4eec\u5927\u5929\u671d\u6709\u53e5\u8bdd\u53eb\u505a\u4e09\u4e03\u4e8c\u5341\u4e00\uff0c\u4f60\u53ef\u4ee5\u4e0d\u7ba1\u4e09\u4e03\u4e8c\u5341\u4e00\u5f00\u59cb\u4f7f\u7528 MySQL \u53ca\u5feb\u901f\u7684\u65b9\u5f0f\u5165\u95e8 MySQL \u3002\u5176\u5b9e21\u5206\u949f\u628a\u4e0b\u9762\u8bed\u53e5\u6267\u884c\u4e00\u904d\u662f\u6ca1\u6709\u95ee\u9898\u7684\uff0c\u8981\u7406\u89e3\u7684\u8bdd\u4f30\u8ba1\u4e0d\u6b6221\u5206\u949f\uff0c\u5bf9\u4e8e\u521d\u5b66\u8005\u6765\u8bf4\u53ea\u9700\u6ee1\u8db3\u81ea\u5df1\u9700\u6c42\u53ef\u4ee5\u589e\u5220\u6539\u67e5\u7b49\u7b80\u6613\u7684\u7ef4\u62a4\u5373\u53ef\u3002","title":"21\u5206\u949fMySQL\u57fa\u7840\u5165\u95e8"},{"location":"21m_mysql/#_1","text":"\u5f00\u59cb\u4f7f\u7528 \u767b\u5f55MySQL \u521b\u5efa\u6570\u636e\u5e93 \u521b\u5efa\u6570\u636e\u5e93\u8868 \u589e\u5220\u6539\u67e5 SELECT UPDATE INSERT DELETE WHERE AND \u548c OR AND OR ORDER BY IN NOT UNION AS JOIN SQL \u51fd\u6570 COUNT MAX \u89e6\u53d1\u5668 \u6dfb\u52a0\u7d22\u5f15 \u666e\u901a\u7d22\u5f15(INDEX) \u4e3b\u952e\u7d22\u5f15(PRIMARY key) \u552f\u4e00\u7d22\u5f15(UNIQUE) \u5168\u6587\u7d22\u5f15(FULLTEXT) \u6dfb\u52a0\u591a\u5217\u7d22\u5f15 \u5efa\u7acb\u7d22\u5f15\u7684\u65f6\u673a \u521b\u5efa\u540e\u8868\u7684\u4fee\u6539 \u6dfb\u52a0\u5217 \u4fee\u6539\u5217 \u5220\u9664\u5217 \u91cd\u547d\u540d\u8868 \u6e05\u7a7a\u8868\u6570\u636e \u5220\u9664\u6574\u5f20\u8868 \u5220\u9664\u6574\u4e2a\u6570\u636e\u5e93 \u5176\u5b83\u5b9e\u4f8b SQL\u5220\u9664\u91cd\u590d\u8bb0\u5f55 \u53c2\u8003\u624b\u518c","title":"\u76ee\u5f55"},{"location":"21m_mysql/#_2","text":"\u6211\u4e0b\u9762\u6240\u6709\u7684SQL\u8bed\u53e5\u662f\u57fa\u4e8eMySQL 5.6+\u8fd0\u884c\u3002 MySQL \u4e3a\u5173\u7cfb\u578b\u6570\u636e\u5e93(Relational Database Management System)\uff0c\u4e00\u4e2a\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7531\u4e00\u4e2a\u6216\u6570\u4e2a\u8868\u683c\u7ec4\u6210, \u5982\u56fe\u6240\u793a\u7684\u4e00\u4e2a\u8868\u683c\uff1a \u8868\u5934(header) : \u6bcf\u4e00\u5217\u7684\u540d\u79f0; \u5217(col) : \u5177\u6709\u76f8\u540c\u6570\u636e\u7c7b\u578b\u7684\u6570\u636e\u7684\u96c6\u5408; \u884c(row) : \u6bcf\u4e00\u884c\u7528\u6765\u63cf\u8ff0\u67d0\u4e2a\u4eba/\u7269\u7684\u5177\u4f53\u4fe1\u606f; \u503c(value) : \u884c\u7684\u5177\u4f53\u4fe1\u606f, \u6bcf\u4e2a\u503c\u5fc5\u987b\u4e0e\u8be5\u5217\u7684\u6570\u636e\u7c7b\u578b\u76f8\u540c; \u952e(key) : \u8868\u4e2d\u7528\u6765\u8bc6\u522b\u67d0\u4e2a\u7279\u5b9a\u7684\u4eba\\\u7269\u7684\u65b9\u6cd5, \u952e\u7684\u503c\u5728\u5f53\u524d\u5217\u4e2d\u5177\u6709\u552f\u4e00\u6027\u3002","title":"\u5f00\u59cb\u4f7f\u7528"},{"location":"21m_mysql/#mysql","text":"mysql -h 127.0.0.1 -u \u7528\u6237\u540d -p mysql -D \u6240\u9009\u62e9\u7684\u6570\u636e\u5e93\u540d -h \u4e3b\u673a\u540d -u \u7528\u6237\u540d -p mysql exit # \u9000\u51fa \u4f7f\u7528 \u201cquit;\u201d \u6216 \u201c\\q;\u201d \u4e00\u6837\u7684\u6548\u679c mysql status; # \u663e\u793a\u5f53\u524dmysql\u7684version\u7684\u5404\u79cd\u4fe1\u606f mysql select version(); # \u663e\u793a\u5f53\u524dmysql\u7684version\u4fe1\u606f mysql show global variables like 'port'; # \u67e5\u770bMySQL\u7aef\u53e3\u53f7","title":"\u767b\u5f55MySQL"},{"location":"21m_mysql/#_3","text":"\u5bf9\u4e8e\u8868\u7684\u64cd\u4f5c\u9700\u8981\u5148\u8fdb\u5165\u5e93 use \u5e93\u540d; -- \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a samp_db \u7684\u6570\u636e\u5e93\uff0c\u6570\u636e\u5e93\u5b57\u7b26\u7f16\u7801\u6307\u5b9a\u4e3a gbk create database samp_db character set gbk; drop database samp_db; -- \u5220\u9664 \u5e93\u540d\u4e3asamp_db\u7684\u5e93 show databases; -- \u663e\u793a\u6570\u636e\u5e93\u5217\u8868\u3002 use samp_db; -- \u9009\u62e9\u521b\u5efa\u7684\u6570\u636e\u5e93samp_db show tables; -- \u663e\u793asamp_db\u4e0b\u9762\u6240\u6709\u7684\u8868\u540d\u5b57 describe \u8868\u540d; -- \u663e\u793a\u6570\u636e\u8868\u7684\u7ed3\u6784 delete from \u8868\u540d; -- \u6e05\u7a7a\u8868\u4e2d\u8bb0\u5f55","title":"\u521b\u5efa\u6570\u636e\u5e93"},{"location":"21m_mysql/#_4","text":"\u4f7f\u7528 create table \u8bed\u53e5\u53ef\u5b8c\u6210\u5bf9\u8868\u7684\u521b\u5efa, create table \u7684\u5e38\u89c1\u5f62\u5f0f: \u8bed\u6cd5\uff1acreate table \u8868\u540d\u79f0(\u5217\u58f0\u660e); -- \u5982\u679c\u6570\u636e\u5e93\u4e2d\u5b58\u5728user_accounts\u8868\uff0c\u5c31\u628a\u5b83\u4ece\u6570\u636e\u5e93\u4e2ddrop\u6389 DROP TABLE IF EXISTS `user_accounts`; CREATE TABLE `user_accounts` ( `id` int(100) unsigned NOT NULL AUTO_INCREMENT primary key, `password` varchar(32) NOT NULL DEFAULT '' COMMENT '\u7528\u6237\u5bc6\u7801', `reset_password` tinyint(32) NOT NULL DEFAULT 0 COMMENT '\u7528\u6237\u7c7b\u578b\uff1a0\uff0d\u4e0d\u9700\u8981\u91cd\u7f6e\u5bc6\u7801\uff1b1-\u9700\u8981\u91cd\u7f6e\u5bc6\u7801', `mobile` varchar(20) NOT NULL DEFAULT '' COMMENT '\u624b\u673a', `create_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6), `update_at` timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), -- \u521b\u5efa\u552f\u4e00\u7d22\u5f15\uff0c\u4e0d\u5141\u8bb8\u91cd\u590d UNIQUE INDEX idx_user_mobile(`mobile`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='\u7528\u6237\u8868\u4fe1\u606f'; \u6570\u636e\u7c7b\u578b\u7684\u5c5e\u6027\u89e3\u91ca NULL \uff1a\u6570\u636e\u5217\u53ef\u5305\u542bNULL\u503c\uff1b NOT NULL \uff1a\u6570\u636e\u5217\u4e0d\u5141\u8bb8\u5305\u542bNULL\u503c\uff1b DEFAULT \uff1a\u9ed8\u8ba4\u503c\uff1b PRIMARY KEY \uff1a\u4e3b\u952e\uff1b AUTO_INCREMENT \uff1a\u81ea\u52a8\u9012\u589e\uff0c\u9002\u7528\u4e8e\u6574\u6570\u7c7b\u578b\uff1b UNSIGNED \uff1a\u662f\u6307\u6570\u503c\u7c7b\u578b\u53ea\u80fd\u4e3a\u6b63\u6570\uff1b CHARACTER SET name \uff1a\u6307\u5b9a\u4e00\u4e2a\u5b57\u7b26\u96c6\uff1b COMMENT \uff1a\u5bf9\u8868\u6216\u8005\u5b57\u6bb5\u8bf4\u660e\uff1b","title":"\u521b\u5efa\u6570\u636e\u5e93\u8868"},{"location":"21m_mysql/#_5","text":"","title":"\u589e\u5220\u6539\u67e5"},{"location":"21m_mysql/#select","text":"SELECT \u8bed\u53e5\u7528\u4e8e\u4ece\u8868\u4e2d\u9009\u53d6\u6570\u636e\u3002 \u8bed\u6cd5\uff1a SELECT \u5217\u540d\u79f0 FROM \u8868\u540d\u79f0 \u8bed\u6cd5\uff1a SELECT * FROM \u8868\u540d\u79f0 -- \u8868station\u53d6\u4e2a\u522b\u540d\u53ebs\uff0c\u8868station\u4e2d\u4e0d\u5305\u542b \u5b57\u6bb5id=13\u6216\u800514 \u7684\uff0c\u5e76\u4e14id\u4e0d\u7b49\u4e8e4\u7684 \u67e5\u8be2\u51fa\u6765\uff0c\u53ea\u663e\u793aid SELECT s.id from station s WHERE id in (13,14) and id not in (4); -- \u4ece\u8868 Persons \u9009\u53d6 LastName \u5217\u7684\u6570\u636e SELECT LastName FROM Persons -- \u4ece\u8868 users \u9009\u53d6 id=3 \u7684\u6570\u636e\uff0c\u5e76\u53ea\u62c9\u4e00\u6761\u6570\u636e(\u636e\u8bf4\u80fd\u4f18\u5316\u6027\u80fd) SELECT * FROM users where id=3 limit 1 -- \u7ed3\u679c\u96c6\u4e2d\u4f1a\u81ea\u52a8\u53bb\u91cd\u590d\u6570\u636e SELECT DISTINCT Company FROM Orders -- \u8868 Persons \u5b57\u6bb5 Id_P \u7b49\u4e8e Orders \u5b57\u6bb5 Id_P \u7684\u503c\uff0c -- \u7ed3\u679c\u96c6\u663e\u793a Persons\u8868\u7684 LastName\u3001FirstName\u5b57\u6bb5\uff0cOrders\u8868\u7684OrderNo\u5b57\u6bb5 SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p, Orders o WHERE p.Id_P = o.Id_P -- gbk \u548c utf8 \u4e2d\u82f1\u6587\u6df7\u5408\u6392\u5e8f\u6700\u7b80\u5355\u7684\u529e\u6cd5 -- ci\u662f case insensitive, \u5373 \u201c\u5927\u5c0f\u5199\u4e0d\u654f\u611f\u201d SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using gbk) collate gbk_chinese_ci; SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using utf8) collate utf8_unicode_ci;","title":"SELECT"},{"location":"21m_mysql/#update","text":"Update \u8bed\u53e5\u7528\u4e8e\u4fee\u6539\u8868\u4e2d\u7684\u6570\u636e\u3002 \u8bed\u6cd5\uff1a UPDATE \u8868\u540d\u79f0 SET \u5217\u540d\u79f0 = \u65b0\u503c WHERE \u5217\u540d\u79f0 = \u67d0\u503c -- update\u8bed\u53e5\u8bbe\u7f6e\u5b57\u6bb5\u503c\u4e3a\u53e6\u4e00\u4e2a\u7ed3\u679c\u53d6\u51fa\u6765\u7684\u5b57\u6bb5 update user set name = (select name from user1 where user1 .id = 1 ) where id = (select id from user2 where user2 .name='\u5c0f\u82cf'); -- \u66f4\u65b0\u8868 orders \u4e2d id=1 \u7684\u90a3\u4e00\u884c\u6570\u636e\u66f4\u65b0\u5b83\u7684 title \u5b57\u6bb5 UPDATE `orders` set title='\u8fd9\u91cc\u662f\u6807\u9898' WHERE id=1;","title":"UPDATE"},{"location":"21m_mysql/#insert","text":"INSERT INTO \u8bed\u53e5\u7528\u4e8e\u5411\u8868\u683c\u4e2d\u63d2\u5165\u65b0\u7684\u884c\u3002 \u8bed\u6cd5\uff1a INSERT INTO \u8868\u540d\u79f0 VALUES (\u503c1, \u503c2,....) \u8bed\u6cd5\uff1a INSERT INTO \u8868\u540d\u79f0 (\u52171, \u52172,...) VALUES (\u503c1, \u503c2,....) -- \u5411\u8868 Persons \u63d2\u5165\u4e00\u6761\u5b57\u6bb5 LastName = JSLite \u5b57\u6bb5 Address = shanghai INSERT INTO Persons (LastName, Address) VALUES ('JSLite', 'shanghai'); -- \u5411\u8868 meeting \u63d2\u5165 \u5b57\u6bb5 a=1 \u548c\u5b57\u6bb5 b=2 INSERT INTO meeting SET a=1,b=2; -- -- SQL\u5b9e\u73b0\u5c06\u4e00\u4e2a\u8868\u7684\u6570\u636e\u63d2\u5165\u5230\u53e6\u5916\u4e00\u4e2a\u8868\u7684\u4ee3\u7801 -- \u5982\u679c\u53ea\u5e0c\u671b\u5bfc\u5165\u6307\u5b9a\u5b57\u6bb5\uff0c\u53ef\u4ee5\u7528\u8fd9\u79cd\u65b9\u6cd5\uff1a -- INSERT INTO \u76ee\u6807\u8868 (\u5b57\u6bb51, \u5b57\u6bb52, ...) SELECT \u5b57\u6bb51, \u5b57\u6bb52, ... FROM \u6765\u6e90\u8868; INSERT INTO orders (user_account_id, title) SELECT m.user_id, m.title FROM meeting m where m.id=1; -- \u5411\u8868 charger \u63d2\u5165\u4e00\u6761\u6570\u636e\uff0c\u5df2\u5b58\u5728\u5c31\u5bf9\u8868 charger \u66f4\u65b0 `type`,`update_at` \u5b57\u6bb5\uff1b INSERT INTO `charger` (`id`,`type`,`create_at`,`update_at`) VALUES (3,2,'2017-05-18 11:06:17','2017-05-18 11:06:17') ON DUPLICATE KEY UPDATE `id`=VALUES(`id`), `type`=VALUES(`type`), `update_at`=VALUES(`update_at`);","title":"INSERT"},{"location":"21m_mysql/#delete","text":"DELETE \u8bed\u53e5\u7528\u4e8e\u5220\u9664\u8868\u4e2d\u7684\u884c\u3002 \u8bed\u6cd5\uff1a DELETE FROM \u8868\u540d\u79f0 WHERE \u5217\u540d\u79f0 = \u503c -- \u5728\u4e0d\u5220\u9664table_name\u8868\u7684\u60c5\u51b5\u4e0b\u5220\u9664\u6240\u6709\u7684\u884c\uff0c\u6e05\u7a7a\u8868\u3002 DELETE FROM table_name -- \u6216\u8005 DELETE * FROM table_name -- \u5220\u9664 Person\u8868\u5b57\u6bb5 LastName = 'JSLite' DELETE FROM Person WHERE LastName = 'JSLite' -- \u5220\u9664 \u8868meeting id \u4e3a2\u548c3\u7684\u4e24\u6761\u6570\u636e DELETE from meeting where id in (2,3);","title":"DELETE"},{"location":"21m_mysql/#where","text":"WHERE \u5b50\u53e5\u7528\u4e8e\u89c4\u5b9a\u9009\u62e9\u7684\u6807\u51c6\u3002 \u8bed\u6cd5\uff1a SELECT \u5217\u540d\u79f0 FROM \u8868\u540d\u79f0 WHERE \u5217 \u8fd0\u7b97\u7b26 \u503c -- \u4ece\u8868 Persons \u4e2d\u9009\u51fa Year \u5b57\u6bb5\u5927\u4e8e 1965 \u7684\u6570\u636e SELECT * FROM Persons WHERE Year 1965","title":"WHERE"},{"location":"21m_mysql/#and-or","text":"AND - \u5982\u679c\u7b2c\u4e00\u4e2a\u6761\u4ef6\u548c\u7b2c\u4e8c\u4e2a\u6761\u4ef6\u90fd\u6210\u7acb\uff1b OR - \u5982\u679c\u7b2c\u4e00\u4e2a\u6761\u4ef6\u548c\u7b2c\u4e8c\u4e2a\u6761\u4ef6\u4e2d\u53ea\u8981\u6709\u4e00\u4e2a\u6210\u7acb\uff1b","title":"AND \u548c OR"},{"location":"21m_mysql/#and","text":"-- \u5220\u9664 meeting \u8868\u5b57\u6bb5 -- id=2 \u5e76\u4e14 user_id=5 \u7684\u6570\u636e \u548c -- id=3 \u5e76\u4e14 user_id=6 \u7684\u6570\u636e DELETE from meeting where id in (2,3) and user_id in (5,6); -- \u4f7f\u7528 AND \u6765\u663e\u793a\u6240\u6709\u59d3\u4e3a Carter \u5e76\u4e14\u540d\u4e3a Thomas \u7684\u4eba\uff1a SELECT * FROM Persons WHERE FirstName='Thomas' AND LastName='Carter';","title":"AND"},{"location":"21m_mysql/#or","text":"-- \u4f7f\u7528 OR \u6765\u663e\u793a\u6240\u6709\u59d3\u4e3a Carter \u6216\u8005\u540d\u4e3a Thomas \u7684\u4eba\uff1a SELECT * FROM Persons WHERE firstname='Thomas' OR lastname='Carter'","title":"OR"},{"location":"21m_mysql/#order-by","text":"\u8bed\u53e5\u9ed8\u8ba4\u6309\u7167\u5347\u5e8f\u5bf9\u8bb0\u5f55\u8fdb\u884c\u6392\u5e8f\u3002 ORDER BY - \u8bed\u53e5\u7528\u4e8e\u6839\u636e\u6307\u5b9a\u7684\u5217\u5bf9\u7ed3\u679c\u96c6\u8fdb\u884c\u6392\u5e8f\u3002 DESC - \u6309\u7167\u964d\u5e8f\u5bf9\u8bb0\u5f55\u8fdb\u884c\u6392\u5e8f\u3002 ASC - \u6309\u7167\u987a\u5e8f\u5bf9\u8bb0\u5f55\u8fdb\u884c\u6392\u5e8f\u3002 -- Company\u5728\u8868Orders\u4e2d\u4e3a\u5b57\u6bcd\uff0c\u5219\u4f1a\u4ee5\u5b57\u6bcd\u987a\u5e8f\u663e\u793a\u516c\u53f8\u540d\u79f0 SELECT Company, OrderNumber FROM Orders ORDER BY Company -- \u540e\u9762\u8ddf\u4e0a DESC \u5219\u4e3a\u964d\u5e8f\u663e\u793a SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC -- Company\u4ee5\u964d\u5e8f\u663e\u793a\u516c\u53f8\u540d\u79f0\uff0c\u5e76OrderNumber\u4ee5\u987a\u5e8f\u663e\u793a SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC","title":"ORDER BY"},{"location":"21m_mysql/#in","text":"IN - \u64cd\u4f5c\u7b26\u5141\u8bb8\u6211\u4eec\u5728 WHERE \u5b50\u53e5\u4e2d\u89c4\u5b9a\u591a\u4e2a\u503c\u3002 IN - \u64cd\u4f5c\u7b26\u7528\u6765\u6307\u5b9a\u8303\u56f4\uff0c\u8303\u56f4\u4e2d\u7684\u6bcf\u4e00\u6761\uff0c\u90fd\u8fdb\u884c\u5339\u914d\u3002IN\u53d6\u503c\u89c4\u5f8b\uff0c\u7531\u9017\u53f7\u5206\u5272\uff0c\u5168\u90e8\u653e\u7f6e\u62ec\u53f7\u4e2d\u3002 \u8bed\u6cd5\uff1a SELECT \"\u5b57\u6bb5\u540d\"FROM \"\u8868\u683c\u540d\"WHERE \"\u5b57\u6bb5\u540d\" IN ('\u503c\u4e00', '\u503c\u4e8c', ...); -- \u4ece\u8868 Persons \u9009\u53d6 \u5b57\u6bb5 LastName \u7b49\u4e8e Adams\u3001Carter SELECT * FROM Persons WHERE LastName IN ('Adams','Carter')","title":"IN"},{"location":"21m_mysql/#not","text":"NOT - \u64cd\u4f5c\u7b26\u603b\u662f\u4e0e\u5176\u4ed6\u64cd\u4f5c\u7b26\u4e00\u8d77\u4f7f\u7528\uff0c\u7528\u5728\u8981\u8fc7\u6ee4\u7684\u524d\u9762\u3002 SELECT vend_id, prod_name FROM Products WHERE NOT vend_id = 'DLL01' ORDER BY prod_name;","title":"NOT"},{"location":"21m_mysql/#union","text":"UNION - \u64cd\u4f5c\u7b26\u7528\u4e8e\u5408\u5e76\u4e24\u4e2a\u6216\u591a\u4e2a SELECT \u8bed\u53e5\u7684\u7ed3\u679c\u96c6\u3002 -- \u5217\u51fa\u6240\u6709\u5728\u4e2d\u56fd\u8868\uff08Employees_China\uff09\u548c\u7f8e\u56fd\uff08Employees_USA\uff09\u7684\u4e0d\u540c\u7684\u96c7\u5458\u540d SELECT E_Name FROM Employees_China UNION SELECT E_Name FROM Employees_USA -- \u5217\u51fa meeting \u8868\u4e2d\u7684 pic_url\uff0c -- station \u8868\u4e2d\u7684 number_station \u522b\u540d\u8bbe\u7f6e\u6210 pic_url \u907f\u514d\u5b57\u6bb5\u4e0d\u4e00\u6837\u62a5\u9519 -- \u6309\u66f4\u65b0\u65f6\u95f4\u6392\u5e8f SELECT id,pic_url FROM meeting UNION ALL SELECT id,number_station AS pic_url FROM station ORDER BY update_at; -- \u901a\u8fc7 UNION \u8bed\u6cd5\u540c\u65f6\u67e5\u8be2\u4e86 products \u8868 \u548c comments \u8868\u7684\u603b\u8bb0\u5f55\u6570\uff0c\u5e76\u4e14\u6309\u7167 count \u6392\u5e8f SELECT 'product' AS type, count(*) as count FROM `products` union select 'comment' as type, count(*) as count FROM `comments` order by count;","title":"UNION"},{"location":"21m_mysql/#as","text":"as - \u53ef\u7406\u89e3\u4e3a\uff1a\u7528\u4f5c\u3001\u5f53\u6210\uff0c\u4f5c\u4e3a\uff1b\u522b\u540d \u4e00\u822c\u662f\u91cd\u547d\u540d\u5217\u540d\u6216\u8005\u8868\u540d\u3002 \u8bed\u6cd5\uff1a select column_1 as \u52171,column_2 as \u52172 from table as \u8868 SELECT * FROM Employee AS emp -- \u8fd9\u53e5\u610f\u601d\u662f\u67e5\u627e\u6240\u6709Employee \u8868\u91cc\u9762\u7684\u6570\u636e\uff0c\u5e76\u628aEmployee\u8868\u683c\u547d\u540d\u4e3a emp\u3002 -- \u5f53\u4f60\u547d\u540d\u4e00\u4e2a\u8868\u4e4b\u540e\uff0c\u4f60\u53ef\u4ee5\u5728\u4e0b\u9762\u7528 emp \u4ee3\u66ff Employee. -- \u4f8b\u5982 SELECT * FROM emp. SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders -- \u5217\u51fa\u8868 Orders \u5b57\u6bb5 OrderPrice \u5217\u6700\u5927\u503c\uff0c -- \u7ed3\u679c\u96c6\u5217\u4e0d\u663e\u793a OrderPrice \u663e\u793a LargestOrderPrice -- \u663e\u793a\u8868 users_profile \u4e2d\u7684 name \u5217 SELECT t.name from (SELECT * from users_profile a) AS t; -- \u8868 user_accounts \u547d\u540d\u522b\u540d ua\uff0c\u8868 users_profile \u547d\u540d\u522b\u540d up -- \u6ee1\u8db3\u6761\u4ef6 \u8868 user_accounts \u5b57\u6bb5 id \u7b49\u4e8e \u8868 users_profile \u5b57\u6bb5 user_id -- \u7ed3\u679c\u96c6\u53ea\u663e\u793amobile\u3001name\u4e24\u5217 SELECT ua.mobile,up.name FROM user_accounts as ua INNER JOIN users_profile as up ON ua.id = up.user_id;","title":"AS"},{"location":"21m_mysql/#join","text":"\u7528\u4e8e\u6839\u636e\u4e24\u4e2a\u6216\u591a\u4e2a\u8868\u4e2d\u7684\u5217\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ece\u8fd9\u4e9b\u8868\u4e2d\u67e5\u8be2\u6570\u636e\u3002 JOIN : \u5982\u679c\u8868\u4e2d\u6709\u81f3\u5c11\u4e00\u4e2a\u5339\u914d\uff0c\u5219\u8fd4\u56de\u884c INNER JOIN :\u5728\u8868\u4e2d\u5b58\u5728\u81f3\u5c11\u4e00\u4e2a\u5339\u914d\u65f6\uff0cINNER JOIN \u5173\u952e\u5b57\u8fd4\u56de\u884c\u3002 LEFT JOIN : \u5373\u4f7f\u53f3\u8868\u4e2d\u6ca1\u6709\u5339\u914d\uff0c\u4e5f\u4ece\u5de6\u8868\u8fd4\u56de\u6240\u6709\u7684\u884c RIGHT JOIN : \u5373\u4f7f\u5de6\u8868\u4e2d\u6ca1\u6709\u5339\u914d\uff0c\u4e5f\u4ece\u53f3\u8868\u8fd4\u56de\u6240\u6709\u7684\u884c FULL JOIN : \u53ea\u8981\u5176\u4e2d\u4e00\u4e2a\u8868\u4e2d\u5b58\u5728\u5339\u914d\uff0c\u5c31\u8fd4\u56de\u884c(MySQL \u662f\u4e0d\u652f\u6301\u7684\uff0c\u901a\u8fc7 LEFT JOIN + UNION + RIGHT JOIN \u7684\u65b9\u5f0f \u6765\u5b9e\u73b0) SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo FROM Persons INNER JOIN Orders ON Persons.Id_P = Orders.Id_P ORDER BY Persons.LastName;","title":"JOIN"},{"location":"21m_mysql/#sql","text":"","title":"SQL \u51fd\u6570"},{"location":"21m_mysql/#count","text":"COUNT \u8ba9\u6211\u4eec\u80fd\u591f\u6570\u51fa\u5728\u8868\u683c\u4e2d\u6709\u591a\u5c11\u7b14\u8d44\u6599\u88ab\u9009\u51fa\u6765\u3002 \u8bed\u6cd5\uff1a SELECT COUNT(\"\u5b57\u6bb5\u540d\") FROM \"\u8868\u683c\u540d\"; -- \u8868 Store_Information \u6709\u51e0\u7b14 store_name \u680f\u4e0d\u662f\u7a7a\u767d\u7684\u8d44\u6599\u3002 -- IS NOT NULL \u662f \u8fd9\u4e2a\u680f\u4f4d\u4e0d\u662f\u7a7a\u767d \u7684\u610f\u601d\u3002 SELECT COUNT (Store_Name) FROM Store_Information WHERE Store_Name IS NOT NULL; -- \u83b7\u53d6 Persons \u8868\u7684\u603b\u6570 SELECT COUNT(1) AS totals FROM Persons; -- \u83b7\u53d6\u8868 station \u5b57\u6bb5 user_id \u76f8\u540c\u7684\u603b\u6570 select user_id, count(*) as totals from station group by user_id;","title":"COUNT"},{"location":"21m_mysql/#max","text":"MAX \u51fd\u6570\u8fd4\u56de\u4e00\u5217\u4e2d\u7684\u6700\u5927\u503c\u3002NULL \u503c\u4e0d\u5305\u62ec\u5728\u8ba1\u7b97\u4e2d\u3002 \u8bed\u6cd5\uff1a SELECT MAX(\"\u5b57\u6bb5\u540d\") FROM \"\u8868\u683c\u540d\" -- \u5217\u51fa\u8868 Orders \u5b57\u6bb5 OrderPrice \u5217\u6700\u5927\u503c\uff0c -- \u7ed3\u679c\u96c6\u5217\u4e0d\u663e\u793a OrderPrice \u663e\u793a LargestOrderPrice SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders","title":"MAX"},{"location":"21m_mysql/#_6","text":"\u8bed\u6cd5\uff1a create trigger \u89e6\u53d1\u5668\u540d\u79f0 { before | after} # \u4e4b\u524d\u6216\u8005\u4e4b\u540e\u51fa\u53d1 insert | update | delete # \u6307\u660e\u4e86\u6fc0\u6d3b\u89e6\u53d1\u7a0b\u5e8f\u7684\u8bed\u53e5\u7684\u7c7b\u578b on \u8868\u540d # \u64cd\u4f5c\u54ea\u5f20\u8868 for each row # \u89e6\u53d1\u5668\u7684\u6267\u884c\u95f4\u9694\uff0cfor each row \u901a\u77e5\u89e6\u53d1\u5668\u6bcf\u9694\u4e00\u884c\u6267\u884c\u4e00\u6b21\u52a8\u4f5c\uff0c\u800c\u4e0d\u662f\u5bf9\u6574\u4e2a\u8868\u6267\u884c\u4e00\u6b21\u3002 \u89e6\u53d1\u5668SQL\u8bed\u53e5 delimiter $ CREATE TRIGGER set_userdate BEFORE INSERT on `message` for EACH ROW BEGIN set @statu = new.status; -- \u58f0\u660e\u590d\u5236\u53d8\u91cf statu if @statu = 0 then -- \u5224\u65ad statu \u662f\u5426\u7b49\u4e8e 0 UPDATE `user_accounts` SET status=1 WHERE openid=NEW.openid; end if; END $ DELIMITER ; -- \u6062\u590d\u7ed3\u675f\u7b26\u53f7 OLD\u548cNEW\u4e0d\u533a\u5206\u5927\u5c0f\u5199 - NEW \u7528NEW.col_name\uff0c\u6ca1\u6709\u65e7\u884c\u3002\u5728DELETE\u89e6\u53d1\u7a0b\u5e8f\u4e2d\uff0c\u4ec5\u80fd\u4f7f\u7528OLD.col_name\uff0c\u6ca1\u6709\u65b0\u884c\u3002 - OLD \u7528OLD.col_name\u6765\u5f15\u7528\u66f4\u65b0\u524d\u7684\u67d0\u4e00\u884c\u7684\u5217","title":"\u89e6\u53d1\u5668"},{"location":"21m_mysql/#_7","text":"","title":"\u6dfb\u52a0\u7d22\u5f15"},{"location":"21m_mysql/#index","text":"\u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD INDEX \u7d22\u5f15\u540d\u5b57 ( \u5b57\u6bb5\u540d\u5b57 ) -- \u2013\u76f4\u63a5\u521b\u5efa\u7d22\u5f15 CREATE INDEX index_user ON user(title) -- \u2013\u4fee\u6539\u8868\u7ed3\u6784\u7684\u65b9\u5f0f\u6dfb\u52a0\u7d22\u5f15 ALTER TABLE table_name ADD INDEX index_name ON (column(length)) -- \u7ed9 user \u8868\u4e2d\u7684 name \u5b57\u6bb5 \u6dfb\u52a0\u666e\u901a\u7d22\u5f15(INDEX) ALTER TABLE `user` ADD INDEX index_name (name) -- \u2013\u521b\u5efa\u8868\u7684\u65f6\u5019\u540c\u65f6\u521b\u5efa\u7d22\u5f15 CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL , `content` text CHARACTER SET utf8 COLLATE utf8_general_ci NULL , `time` int(10) NULL DEFAULT NULL , PRIMARY KEY (`id`), INDEX index_name (title(length)) ) -- \u2013\u5220\u9664\u7d22\u5f15 DROP INDEX index_name ON table","title":"\u666e\u901a\u7d22\u5f15(INDEX)"},{"location":"21m_mysql/#primary-key","text":"\u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD PRIMARY KEY ( \u5b57\u6bb5\u540d\u5b57 ) -- \u7ed9 user \u8868\u4e2d\u7684 id\u5b57\u6bb5 \u6dfb\u52a0\u4e3b\u952e\u7d22\u5f15(PRIMARY key) ALTER TABLE `user` ADD PRIMARY key (id);","title":"\u4e3b\u952e\u7d22\u5f15(PRIMARY key)"},{"location":"21m_mysql/#unique","text":"\u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD UNIQUE ( \u5b57\u6bb5\u540d\u5b57 ) -- \u7ed9 user \u8868\u4e2d\u7684 creattime \u5b57\u6bb5\u6dfb\u52a0\u552f\u4e00\u7d22\u5f15(UNIQUE) ALTER TABLE `user` ADD UNIQUE (creattime);","title":"\u552f\u4e00\u7d22\u5f15(UNIQUE)"},{"location":"21m_mysql/#fulltext","text":"\u8bed\u6cd5\uff1aALTER TABLE \u8868\u540d\u5b57 ADD FULLTEXT ( \u5b57\u6bb5\u540d\u5b57 ) -- \u7ed9 user \u8868\u4e2d\u7684 description \u5b57\u6bb5\u6dfb\u52a0\u5168\u6587\u7d22\u5f15(FULLTEXT) ALTER TABLE `user` ADD FULLTEXT (description);","title":"\u5168\u6587\u7d22\u5f15(FULLTEXT)"},{"location":"21m_mysql/#_8","text":"\u8bed\u6cd5\uff1a ALTER TABLE table_name ADD INDEX index_name ( column1 , column2 , column3 ) -- \u7ed9 user \u8868\u4e2d\u7684 name\u3001city\u3001age \u5b57\u6bb5\u6dfb\u52a0\u540d\u5b57\u4e3aname_city_age\u7684\u666e\u901a\u7d22\u5f15(INDEX) ALTER TABLE user ADD INDEX name_city_age (name(10),city,age);","title":"\u6dfb\u52a0\u591a\u5217\u7d22\u5f15"},{"location":"21m_mysql/#_9","text":"\u5728 WHERE \u548c JOIN \u4e2d\u51fa\u73b0\u7684\u5217\u9700\u8981\u5efa\u7acb\u7d22\u5f15\uff0c\u4f46\u4e5f\u4e0d\u5b8c\u5168\u5982\u6b64\uff1a MySQL\u53ea\u5bf9 \uff0c = \uff0c = \uff0c \uff0c = \uff0c BETWEEN \uff0c IN \u4f7f\u7528\u7d22\u5f15 \u67d0\u4e9b\u65f6\u5019\u7684 LIKE \u4e5f\u4f1a\u4f7f\u7528\u7d22\u5f15\u3002 \u5728 LIKE \u4ee5\u901a\u914d\u7b26%\u548c_\u5f00\u5934\u4f5c\u67e5\u8be2\u65f6\uff0cMySQL\u4e0d\u4f1a\u4f7f\u7528\u7d22\u5f15\u3002 -- \u6b64\u65f6\u5c31\u9700\u8981\u5bf9city\u548cage\u5efa\u7acb\u7d22\u5f15\uff0c -- \u7531\u4e8emytable\u8868\u7684userame\u4e5f\u51fa\u73b0\u5728\u4e86JOIN\u5b50\u53e5\u4e2d\uff0c\u4e5f\u6709\u5bf9\u5b83\u5efa\u7acb\u7d22\u5f15\u7684\u5fc5\u8981\u3002 SELECT t.Name FROM mytable t LEFT JOIN mytable m ON t.Name=m.username WHERE m.age=20 AND m.city='\u4e0a\u6d77'; SELECT * FROM mytable WHERE username like'admin%'; -- \u800c\u4e0b\u53e5\u5c31\u4e0d\u4f1a\u4f7f\u7528\uff1a SELECT * FROM mytable WHERE Name like'%admin'; -- \u56e0\u6b64\uff0c\u5728\u4f7f\u7528LIKE\u65f6\u5e94\u6ce8\u610f\u4ee5\u4e0a\u7684\u533a\u522b\u3002 \u7d22\u5f15\u7684\u6ce8\u610f\u4e8b\u9879 \u7d22\u5f15\u4e0d\u4f1a\u5305\u542b\u6709NULL\u503c\u7684\u5217 \u4f7f\u7528\u77ed\u7d22\u5f15 \u4e0d\u8981\u5728\u5217\u4e0a\u8fdb\u884c\u8fd0\u7b97 \u7d22\u5f15\u4f1a\u5931\u6548","title":"\u5efa\u7acb\u7d22\u5f15\u7684\u65f6\u673a"},{"location":"21m_mysql/#_10","text":"","title":"\u521b\u5efa\u540e\u8868\u7684\u4fee\u6539"},{"location":"21m_mysql/#_11","text":"\u8bed\u6cd5\uff1a alter table \u8868\u540d add \u5217\u540d \u5217\u6570\u636e\u7c7b\u578b [after \u63d2\u5165\u4f4d\u7f6e]; \u793a\u4f8b: -- \u5728\u8868students\u7684\u6700\u540e\u8ffd\u52a0\u5217 address: alter table students add address char(60); -- \u5728\u540d\u4e3a age \u7684\u5217\u540e\u63d2\u5165\u5217 birthday: alter table students add birthday date after age; -- \u5728\u540d\u4e3a number_people \u7684\u5217\u540e\u63d2\u5165\u5217 weeks: alter table students add column `weeks` varchar(5) not null default after `number_people`;","title":"\u6dfb\u52a0\u5217"},{"location":"21m_mysql/#_12","text":"\u8bed\u6cd5\uff1a alter table \u8868\u540d change \u5217\u540d\u79f0 \u5217\u65b0\u540d\u79f0 \u65b0\u6570\u636e\u7c7b\u578b; -- \u5c06\u8868 tel \u5217\u6539\u540d\u4e3a telphone: alter table students change tel telphone char(13) default - ; -- \u5c06 name \u5217\u7684\u6570\u636e\u7c7b\u578b\u6539\u4e3a char(16): alter table students change name name char(16) not null; -- \u4fee\u6539 COMMENT \u524d\u9762\u5fc5\u987b\u5f97\u6709\u7c7b\u578b\u5c5e\u6027 alter table students change name name char(16) COMMENT '\u8fd9\u91cc\u662f\u540d\u5b57'; -- \u4fee\u6539\u5217\u5c5e\u6027\u7684\u65f6\u5019 \u5efa\u8bae\u4f7f\u7528modify,\u4e0d\u9700\u8981\u91cd\u5efa\u8868 -- change\u7528\u4e8e\u4fee\u6539\u5217\u540d\u5b57\uff0c\u8fd9\u4e2a\u9700\u8981\u91cd\u5efa\u8868 alter table meeting modify `weeks` varchar(20) NOT NULL DEFAULT '' COMMENT '\u5f00\u653e\u65e5\u671f \u5468\u4e00\u5230\u5468\u65e5\uff1a0~6\uff0c\u95f4\u9694\u7528\u82f1\u6587\u9017\u53f7\u9694\u5f00'; -- `user`\u8868\u7684`id`\u5217\uff0c\u4fee\u6539\u6210\u5b57\u7b26\u4e32\u7c7b\u578b\u957f\u5ea650\uff0c\u4e0d\u80fd\u4e3a\u7a7a\uff0c`FIRST`\u653e\u5728\u7b2c\u4e00\u5217\u7684\u4f4d\u7f6e alter table `user` modify COLUMN `id` varchar(50) NOT NULL FIRST ;","title":"\u4fee\u6539\u5217"},{"location":"21m_mysql/#_13","text":"\u8bed\u6cd5\uff1a alter table \u8868\u540d drop \u5217\u540d\u79f0; -- \u5220\u9664\u8868students\u4e2d\u7684 birthday \u5217: alter table students drop birthday;","title":"\u5220\u9664\u5217"},{"location":"21m_mysql/#_14","text":"\u8bed\u6cd5\uff1a alter table \u8868\u540d rename \u65b0\u8868\u540d; -- \u91cd\u547d\u540d students \u8868\u4e3a workmates: alter table students rename workmates;","title":"\u91cd\u547d\u540d\u8868"},{"location":"21m_mysql/#_15","text":"\u65b9\u6cd5\u4e00\uff1a delete from \u8868\u540d; \u65b9\u6cd5\u4e8c\uff1a truncate table \"\u8868\u540d\"; DELETE: 1. DML\u8bed\u8a00;2. \u53ef\u4ee5\u56de\u9000;3. \u53ef\u4ee5\u6709\u6761\u4ef6\u7684\u5220\u9664; TRUNCATE: 1. DDL\u8bed\u8a00;2. \u65e0\u6cd5\u56de\u9000;3. \u9ed8\u8ba4\u6240\u6709\u7684\u8868\u5185\u5bb9\u90fd\u5220\u9664;4. \u5220\u9664\u901f\u5ea6\u6bd4delete\u5feb\u3002 -- \u6e05\u7a7a\u8868\u4e3a workmates \u91cc\u9762\u7684\u6570\u636e\uff0c\u4e0d\u5220\u9664\u8868\u3002 delete from workmates; -- \u5220\u9664workmates\u8868\u4e2d\u7684\u6240\u6709\u6570\u636e\uff0c\u4e14\u65e0\u6cd5\u6062\u590d truncate table workmates;","title":"\u6e05\u7a7a\u8868\u6570\u636e"},{"location":"21m_mysql/#_16","text":"\u8bed\u6cd5\uff1a drop table \u8868\u540d; -- \u5220\u9664 workmates \u8868: drop table workmates;","title":"\u5220\u9664\u6574\u5f20\u8868"},{"location":"21m_mysql/#_17","text":"\u8bed\u6cd5\uff1a drop database \u6570\u636e\u5e93\u540d; -- \u5220\u9664 samp_db \u6570\u636e\u5e93: drop database samp_db;","title":"\u5220\u9664\u6574\u4e2a\u6570\u636e\u5e93"},{"location":"21m_mysql/#_18","text":"","title":"\u5176\u5b83\u5b9e\u4f8b"},{"location":"21m_mysql/#sql_1","text":"\u8f6c\u8f7d -- \u67e5\u627e\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff0c\u91cd\u590d\u8bb0\u5f55\u662f\u6839\u636e\u5355\u4e2a\u5b57\u6bb5\uff08peopleId\uff09\u6765\u5224\u65ad select * from people where peopleId in (select peopleId from people group by peopleId having count(peopleId) 1) -- \u5220\u9664\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff0c\u91cd\u590d\u8bb0\u5f55\u662f\u6839\u636e\u5355\u4e2a\u5b57\u6bb5\uff08peopleId\uff09\u6765\u5224\u65ad\uff0c\u53ea\u7559\u6709rowid\u6700\u5c0f\u7684\u8bb0\u5f55 delete from people where peopleId in (select peopleId from people group by peopleId having count(peopleId) 1) and rowid not in (select min(rowid) from people group by peopleId having count(peopleId ) 1) -- \u67e5\u627e\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff08\u591a\u4e2a\u5b57\u6bb5\uff09 select * from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) 1) -- \u5220\u9664\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff08\u591a\u4e2a\u5b57\u6bb5\uff09\uff0c\u53ea\u7559\u6709rowid\u6700\u5c0f\u7684\u8bb0\u5f55 delete from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) 1) and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*) 1) -- \u67e5\u627e\u8868\u4e2d\u591a\u4f59\u7684\u91cd\u590d\u8bb0\u5f55\uff08\u591a\u4e2a\u5b57\u6bb5\uff09\uff0c\u4e0d\u5305\u542browid\u6700\u5c0f\u7684\u8bb0\u5f55 select * from vitae a where (a.peopleId,a.seq) in (select peopleId,seq from vitae group by peopleId,seq having count(*) 1) and rowid not in (select min(rowid) from vitae group by peopleId,seq having count(*) 1)","title":"SQL\u5220\u9664\u91cd\u590d\u8bb0\u5f55"},{"location":"21m_mysql/#_19","text":"http://www.w3school.com.cn/sql/index.asp http://www.1keydata.com/cn/sql/sql-count.php","title":"\u53c2\u8003\u624b\u518c"},{"location":"SUMMARY/","text":"Summary \u7b80\u4ecb \u6570\u636e\u6316\u6398\u5bfc\u8bba\u548c\u4fe1\u8d37\u6a21\u578b \u56de\u5f52\u6a21\u578b\u548c\u623f\u4ef7\u9884\u6d4b \u611f\u77e5\u673a\u548c\u903b\u8f91\u56de\u5f52 \u51b3\u7b56\u6811\u548c\u96c6\u6210\u5b66\u4e60 \u7279\u5f81\u5de5\u7a0b \u53c2\u6570\u8c03\u4f18 \u65e0\u76d1\u7763\u5b66\u4e60 \u6587\u672c\u6316\u6398 \u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60","title":"Summary"},{"location":"SUMMARY/#summary","text":"\u7b80\u4ecb \u6570\u636e\u6316\u6398\u5bfc\u8bba\u548c\u4fe1\u8d37\u6a21\u578b \u56de\u5f52\u6a21\u578b\u548c\u623f\u4ef7\u9884\u6d4b \u611f\u77e5\u673a\u548c\u903b\u8f91\u56de\u5f52 \u51b3\u7b56\u6811\u548c\u96c6\u6210\u5b66\u4e60 \u7279\u5f81\u5de5\u7a0b \u53c2\u6570\u8c03\u4f18 \u65e0\u76d1\u7763\u5b66\u4e60 \u6587\u672c\u6316\u6398 \u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60","title":"Summary"},{"location":"about/","text":"Dedit et unda scitarier Suos regnumque Lorem markdownum gerunt, Elin nunc mille rettulit Oceani: lebetes, cadendum bracchiaque quae? Pro longo; in exercet eris rescindere undas , soporem pete. Subiectos questuque pictis, accipe desistere partimque in dicere tempus! Cum mundo pariterque vocem, et protinus nactus maturuit catulo: illa. Qui auctor potentem vultumque unum Non Denique Nat in dicere littera ut partis versus Peleus multaque herbas Par olim et deo et novat Et Aegides moderamine deos. Fixa fovebat nec poscebat vero dicebat est poscebatur atque, per . Lyra Lotis Cyllare sceleratus proles frigusque Lapithas te silices stetit. Toto forte nec, ante non tigridis hoc totoque et ignibus ille venit animas hiemem parvum. Ultima habenas septemplicis rates immaduisse tantis Cancri has pariter tacito, quae Amphrysi quibus! Non fluminibus Concilio tantis poscimur sepulcri quaerens mento , quoque quicquid diemque relinque dentibus corpora pendebat Priameia vobis, ut. Sit obscenas comas dextra; sunt ab, et rictu odisse, laceras di movit. Notitiamque deae notissima luctus. Solet hunc tunc tyrannus illis Lyaeumque passu? Ab sint nomen Nobilitate salutifer induit potentior auguris deo reliquit Nivea permaturuit praelata tetendi carinae dixit quo Sed cumque ibi Dianae tumidaeque a ventis, dilataque, est agat plaudat poscit. Atque siquis inplerunt decem. Oppida prior Niveae perque ferrum; ira Styga pandis sati equam nostros necem. Petit tyranni tetigit illic ad almus sedem, Iovis nec potest sensi Vulcani vero! Factos gens peritura quem, dat eduxit nulla; auctor nec fide Athamanta Ulixis adsuetaque, coercuit . Cuius dederis: et albo ignes ex sensisse se partes tumefactum commune; cui regnum cecidit!","title":"Dedit et unda scitarier"},{"location":"about/#dedit-et-unda-scitarier","text":"","title":"Dedit et unda scitarier"},{"location":"about/#suos-regnumque","text":"Lorem markdownum gerunt, Elin nunc mille rettulit Oceani: lebetes, cadendum bracchiaque quae? Pro longo; in exercet eris rescindere undas , soporem pete. Subiectos questuque pictis, accipe desistere partimque in dicere tempus! Cum mundo pariterque vocem, et protinus nactus maturuit catulo: illa. Qui auctor potentem vultumque unum Non Denique Nat in dicere littera ut partis versus","title":"Suos regnumque"},{"location":"about/#peleus-multaque-herbas","text":"Par olim et deo et novat Et Aegides moderamine deos. Fixa fovebat nec poscebat vero dicebat est poscebatur atque, per . Lyra Lotis Cyllare sceleratus proles frigusque Lapithas te silices stetit. Toto forte nec, ante non tigridis hoc totoque et ignibus ille venit animas hiemem parvum. Ultima habenas septemplicis rates immaduisse tantis Cancri has pariter tacito, quae Amphrysi quibus!","title":"Peleus multaque herbas"},{"location":"about/#non-fluminibus","text":"Concilio tantis poscimur sepulcri quaerens mento , quoque quicquid diemque relinque dentibus corpora pendebat Priameia vobis, ut. Sit obscenas comas dextra; sunt ab, et rictu odisse, laceras di movit. Notitiamque deae notissima luctus. Solet hunc tunc tyrannus illis Lyaeumque passu? Ab sint nomen Nobilitate salutifer induit potentior auguris deo reliquit Nivea permaturuit praelata tetendi carinae dixit quo","title":"Non fluminibus"},{"location":"about/#sed-cumque-ibi","text":"Dianae tumidaeque a ventis, dilataque, est agat plaudat poscit. Atque siquis inplerunt decem.","title":"Sed cumque ibi"},{"location":"about/#oppida-prior","text":"Niveae perque ferrum; ira Styga pandis sati equam nostros necem. Petit tyranni tetigit illic ad almus sedem, Iovis nec potest sensi Vulcani vero! Factos gens peritura quem, dat eduxit nulla; auctor nec fide Athamanta Ulixis adsuetaque, coercuit . Cuius dederis: et albo ignes ex sensisse se partes tumefactum commune; cui regnum cecidit!","title":"Oppida prior"},{"location":"ajenti/","text":"Ajenti http://ajenti.org/ \u4e2d\u6587 url Ajenti is a Linux BSD web admin panel. Feature highlights Easy installation Ajenti is installed through your system's package manager. Installation only takes a minute. Existing configuration Picks up your current configuration and works on your existing system as-is, without any preparation. Caring Does not overwrite your config files, options and comments. All changes are non-destructive. Batteries included Includes lots of plugins for system and software configuration, monitoring and management. Extensible Ajenti is easily extensible using Python. Plugin development is a quick and pleasant with Ajenti APIs. Modern Pleasant to look at, satisfying to click and accessible anywhere from tablets and mobile. Lightweight Small memory footprint and CPU usage. Runs on low-end machines, wall plugs, routers and so on. Listening We listen to your feedback and add features in the fast-paces weekly release cycle. See http://ajenti.org for more information","title":"ajenti"},{"location":"ajenti/#ajenti","text":"http://ajenti.org/ \u4e2d\u6587 url Ajenti is a Linux BSD web admin panel.","title":"Ajenti"},{"location":"ajenti/#feature-highlights","text":"","title":"Feature highlights"},{"location":"ajenti/#easy-installation","text":"Ajenti is installed through your system's package manager. Installation only takes a minute.","title":"Easy installation"},{"location":"ajenti/#existing-configuration","text":"Picks up your current configuration and works on your existing system as-is, without any preparation.","title":"Existing configuration"},{"location":"ajenti/#caring","text":"Does not overwrite your config files, options and comments. All changes are non-destructive.","title":"Caring"},{"location":"ajenti/#batteries-included","text":"Includes lots of plugins for system and software configuration, monitoring and management.","title":"Batteries included"},{"location":"ajenti/#extensible","text":"Ajenti is easily extensible using Python. Plugin development is a quick and pleasant with Ajenti APIs.","title":"Extensible"},{"location":"ajenti/#modern","text":"Pleasant to look at, satisfying to click and accessible anywhere from tablets and mobile.","title":"Modern"},{"location":"ajenti/#lightweight","text":"Small memory footprint and CPU usage. Runs on low-end machines, wall plugs, routers and so on.","title":"Lightweight"},{"location":"ajenti/#listening","text":"We listen to your feedback and add features in the fast-paces weekly release cycle. See http://ajenti.org for more information","title":"Listening"},{"location":"awe_mysql/","text":"MySQL \u8d44\u6e90\u5927\u5168\u4e2d\u6587\u7248 \u6211\u60f3\u5f88\u591a\u7a0b\u5e8f\u5458\u5e94\u8be5\u8bb0\u5f97 GitHub \u4e0a\u6709\u4e00\u4e2a Awesome - XXX \u7cfb\u5217\u7684\u8d44\u6e90\u6574\u7406\u3002 awesome-mysql \u5c31\u662f shlomi-noach \u53d1\u8d77\u7ef4\u62a4\u7684 MySQL \u8d44\u6e90\u5217\u8868\uff0c\u5185\u5bb9\u5305\u62ec\uff1a\u5206\u6790\u5de5\u5177\u3001\u5907\u4efd\u3001\u6027\u80fd\u6d4b\u8bd5\u3001\u914d\u7f6e\u3001\u90e8\u7f72\u3001GUI \u7b49\u3002 Awesome \u7cfb\u5217\u867d\u7136\u633a\u5168\uff0c\u4f46\u57fa\u672c\u53ea\u5bf9\u6536\u5f55\u7684\u8d44\u6e90\u505a\u4e86\u6781\u4e3a\u7b80\u8981\u7684\u4ecb\u7ecd\uff0c\u5982\u679c\u6709\u66f4\u8be6\u7ec6\u7684\u4e2d\u6587\u4ecb\u7ecd\uff0c\u5bf9\u76f8\u5e94\u5f00\u53d1\u8005\u7684\u5e2e\u52a9\u4f1a\u66f4\u5927\u3002\u8fd9\u4e5f\u662f\u6211\u4eec\u53d1\u8d77\u8fd9\u4e2a\u5f00\u6e90\u9879\u76ee\u7684\u521d\u8877\u3002 \u6211\u4eec\u8981\u505a\u4ec0\u4e48\uff1f \u57fa\u4e8e awesome-sysadmin \u8d44\u6e90\u5217\u8868\uff0c\u6211\u4eec\u5c06\u5bf9\u5404\u4e2a\u8d44\u6e90\u9879\u8fdb\u884c\u7f16\u8bd1\u6574\u7406\u3002 \u6574\u7406\u540e\u7684\u5185\u5bb9\uff0c\u5c06\u6536\u5f55\u5728 \u4f2f\u4e50\u5728\u7ebf\u8d44\u6e90\u9891\u9053 \u3002\u53ef\u53c2\u8003\u5df2\u6574\u7406\u7684\u5185\u5bb9\uff1a \u300a Logstash\uff1a\u65e5\u5fd7\u6587\u4ef6\u7ba1\u7406\u5de5\u5177 \u300b \u300a MyCli\uff1a\u652f\u6301\u81ea\u52a8\u8865\u5168\u548c\u8bed\u6cd5\u9ad8\u4eae\u7684 MySQL \u5ba2\u6237\u7aef \u300b \u5982\u4f55\u53c2\u4e0e\u672c\u9879\u76ee\uff1f \u672c\u9879\u76ee\u7684\u53c2\u4e0e\u8005 \u7ef4\u62a4\u8005\uff1a tangyouhua \u8d21\u732e\u8005\uff1a \u590f\u4e86\u590f\u5929 \u3001 lovecn \u3001You \u6ce8\uff1a\u540d\u5355\u4e0d\u5206\u6392\u540d\uff0c\u4e0d\u5b9a\u671f\u8865\u5145\u66f4\u65b0 \u76ee\u5f55 \u5206\u6790\u5de5\u5177 \u5907\u4efd \u6027\u80fd\u6d4b\u8bd5 \u804a\u5929\u5e94\u7528 \u914d\u7f6e \u8fde\u63a5\u5668 \u90e8\u7f72 \u5f00\u53d1 GUI HA \u4ee3\u7406 \u590d\u5236 \u6a21\u5f0f \u670d\u52a1\u5668 \u5206\u7247 \u5de5\u5177\u5305 \u8d44\u6e90 \u6587\u6863 \u7535\u5b50\u4e66 \u5a92\u4f53 \u7b80\u8baf \u5206\u6790\u5de5\u5177 \u6027\u80fd\uff0c\u7ed3\u6784\u548c\u6570\u636e\u5206\u6790\u5de5\u5177 Anemometer - \u4e00\u4e2a SQL \u6162\u67e5\u8be2\u76d1\u63a7\u5668\u3002 innodb-ruby - \u4e00\u4e2a\u5bf9 InooDB \u683c\u5f0f\u6587\u4ef6\u7684\u89e3\u6790\u5668\uff0c\u7528\u4e8e Ruby \u8bed\u8a00\u3002 innotop - \u4e00\u4e2a\u5177\u5907\u591a\u79cd\u7279\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684 MySQL \u7248 'top' \u5de5\u5177\u3002 pstop - \u4e00\u4e2a\u9488\u5bf9 MySQL \u7684\u7c7b top \u7a0b\u5e8f\uff0c\u7528\u4e8e\u6536\u96c6\uff0c\u6c47\u603b\u4ee5\u53ca\u5c55\u793a\u6765\u81ea performance_schema \u7684\u4fe1\u606f\u3002 mysql-statsd - \u4e00\u4e2a\u6536\u96c6 MySQL \u4fe1\u606f\u7684 Python \u5b88\u62a4\u8fdb\u7a0b\uff0c\u5e76\u901a\u8fc7 StatsD \u53d1\u9001\u5230 Graphite\u3002 \u5907\u4efd \u5907\u4efd/\u5b58\u50a8/\u6062\u590d \u5de5\u5177 MyDumper - \u903b\u8f91\u7684\uff0c\u5e76\u884c\u7684 MySQL \u5907\u4efd/\u8f6c\u50a8\u5de5\u5177\u3002 MySQLDumper - \u57fa\u4e8e web \u7684\u5f00\u6e90\u5907\u4efd\u5de5\u5177-\u5bf9\u4e8e\u5171\u4eab\u865a\u62df\u4e3b\u673a\u975e\u5e38\u6709\u7528\u3002 mysqldump-secure - \u5c06\u52a0\u5bc6\uff0c\u538b\u7f29\uff0c\u65e5\u5fd7\uff0c\u9ed1\u540d\u5355\u548c Nagios \u76d1\u63a7\u4e00\u4f53\u5316\u7684 mysqldump \u5b89\u5168\u811a\u672c\u3002 Percona Xtrabackup - \u9488\u5bf9 MySQL \u7684\u4e00\u4e2a\u5f00\u6e90\u70ed\u5907\u4efd\u5b9e\u7528\u7a0b\u5e8f\u2014\u2014\u5728\u670d\u52a1\u5668\u7684\u5907\u4efd\u671f\u95f4\u4e0d\u4f1a\u9501\u5b9a\u4f60\u7684\u6570\u636e\u5e93\u3002 \u6027\u80fd\u6d4b\u8bd5 \u7ed9\u4f60\u7684\u670d\u52a1\u5668\u8fdb\u884c\u538b\u6d4b\u7684\u5de5\u5177 iibench-mysql -\u57fa\u4e8e Java \u7684 MySQL/Percona/MariaDB \u7d22\u5f15\u8fdb\u884c\u63d2\u5165\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\u3002 Sysbench - \u4e00\u4e2a\u6a21\u5757\u5316\uff0c\u8de8\u5e73\u53f0\u4ee5\u53ca\u591a\u7ebf\u7a0b\u7684\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\u3002 \u804a\u5929\u5e94\u7528 \u96c6\u6210\u8fdb\u804a\u5929\u5ba4\u7684\u811a\u672c Hubot MySQL ChatOps \u914d\u7f6e MySQL \u914d\u7f6e\u5b9e\u4f8b\u53ca\u6307\u5bfc mysql-compatibility-config - \u4f7f MySQL \u914d\u7f6e\u8d77\u6765\u66f4\u50cf\u65b0\u7684\uff08\u6216\u5148\u524d\uff09\u7684 MySQL \u7248\u672c\u3002 \u8fde\u63a5\u5668 \u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684 MySQL \u8fde\u63a5\u5668 Connector/Python - \u4e00\u4e2a\u5bf9\u4e8e Python \u5e73\u53f0\u548c\u5f00\u53d1\u7684\u6807\u51c6\u5316\u6570\u636e\u5e93\u9a71\u52a8\u7a0b\u5e8f\u3002 go-sql-driver - \u4e00\u4e2a Go \u8bed\u8a00\u7684\u8f7b\u91cf\u7ea7\u3001\u6781\u901f\u7684 MySQL \u9a71\u52a8\u7a0b\u5e8f\u3002 libAttachSQL - libAttachSQL \u662f MySQL \u670d\u52a1\u5668\u7684\u4e00\u4e2a\u8f7b\u91cf\u7ea7\uff0c\u975e\u963b\u585e\u7684 C \u8bed\u8a00 API\u3002 MariaDB Java Client - \u9488\u5bf9 Java \u5e94\u7528\u4e14\u7ecf\u8fc7 LGPL \u8bb8\u53ef\u7684 MariaDB \u5ba2\u6237\u7aef\u5e93\u3002 MySQL-Python - \u4e00\u4e2a Python \u8bed\u8a00\u7684 MySQL \u6570\u636e\u5e93\u8fde\u63a5\u5668\u3002 PHP mysqlnd - \u9488\u5bf9 MySQL \u7684 MySQL \u672c\u5730\u9a71\u52a8\uff0c\u5f03\u7528\u8fc7\u65f6\u7684 libmysql \u57fa\u7840\u9a71\u52a8\u3002 \u5f00\u53d1 \u652f\u6301 MySQL \u76f8\u5173\u5f00\u53d1\u7684\u5de5\u5177 Flywaydb - \u6570\u636e\u5e93\u8fc1\u79fb;\u4efb\u610f\u60c5\u51b5\u4e0b\u8f7b\u677e\u53ef\u9760\u5730\u6f14\u53d8\u4f60\u7684\u6570\u636e\u5e93\u7248\u672c\u3002 Liquibase - \u5bf9\u4f60\u7684\u6570\u636e\u5e93\u8fdb\u884c\u6e90\u4ee3\u7801\u63a7\u5236\u3002 Propagator - \u96c6\u4e2d\u6a21\u5f0f\u548c\u6570\u636e\u90e8\u7f72\u5728\u4e00\u4e2a\u591a\u7ef4\u62d3\u6251\u4e0a\u3002 GUI \u524d\u7aef\u548c\u5e94\u7528\u7684 GUI Adminer - \u4e00\u4e2a PHP \u7f16\u5199\u7684\u6570\u636e\u5e93\u7ba1\u7406\u5de5\u5177\u3002 HeidiSQL - Windows \u4e0b\u7684 MySQL \u56fe\u5f62\u5316\u7ba1\u7406\u5de5\u5177\u3002 MySQL Workbench - \u63d0\u4f9b\u7ed9\u6570\u636e\u5e93\u7ba1\u7406\u5458\u548c\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u5efa\u6a21\u7684\u96c6\u6210\u5de5\u5177\u73af\u5883;SQL \u5f00\u53d1;\u6570\u636e\u5e93\u7ba1\u7406\u3002 phpMyAdmin - \u4e00\u4e2a PHP \u5199\u6210\u7684\u5f00\u6e90\u8f6f\u4ef6\uff0c\u610f\u56fe\u5bf9 web \u4e0a\u7684 MySQL \u8fdb\u884c\u7ba1\u7406\u3002 SequelPro - \u4e00\u4e2a mac \u4e0b\u8fd0\u884c MySQL \u7684\u6570\u636e\u5e93\u7ba1\u7406\u5e94\u7528\u7a0b\u5e8f\u3002 mycli - \u4e00\u4e2a\u5e26\u81ea\u52a8\u8865\u5168\u548c\u8bed\u6cd5\u9ad8\u4eae\u7684\u7ec8\u7aef\u7248 MySQL \u5ba2\u6237\u7aef HA \u9ad8\u53ef\u7528\u89e3\u51b3\u65b9\u6848 Galera Cluster - \u4e00\u4e2a\u57fa\u4e8e\u540c\u6b65\u590d\u5236\u7684\u591a\u4e3b\u673a\u96c6\u7fa4\u65b9\u6848\u3002 MHA - \u9488\u5bf9 MySQL \u7684\u4f18\u79c0\u9ad8\u53ef\u7528\u7ba1\u7406\u5668\u53ca\u5de5\u5177 MySQL Fabric - \u4e00\u4e2a\u7528\u4e8e\u7ba1\u7406 MySQL \u670d\u52a1\u5668\u573a\uff08Server Farms\uff09\u7684\u53ef\u6269\u5c55\u6846\u67b6\u3002 Percona Replication Manager - \u9488\u5bf9 MySQL \u7684\u5f02\u6b65\u590d\u5236\u7ba1\u7406\u4ee3\u7406\u3002\u652f\u6301\u4ee5\u6587\u4ef6\u548c GTID \u4e3a\u57fa\u7840\u7684\u590d\u5236\uff0c\u4f7f\u7528 booth \u5b9e\u73b0\u7684\u5730\u7406\u5206\u5e03\u5f0f\u96c6\u7fa4\u3002 \u4ee3\u7406 MySQL \u4ee3\u7406 MaxScale - \u5f00\u6e90\uff0c\u4ee5\u6570\u636e\u5e93\u4e3a\u4e2d\u5fc3\u7684\u4ee3\u7406\u3002 Mixer - Go \u5b9e\u73b0\u7684\u4e00\u4e2a MySQL \u4ee3\u7406\uff0c\u76ee\u7684\u4e3a MySQL \u5206\u7247\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\u3002 MySQL Proxy - \u4e00\u4e2a\u5904\u4e8e\u4f60\u7684\u5ba2\u6237\u7aef\u548c MySQL \u670d\u52a1\u7aef\u4e4b\u95f4\u7684\u7b80\u5355\u7a0b\u5e8f\uff0c\u5b83\u53ef\u4ee5\u68c0\u6d4b\u3001\u5206\u6790\u6216\u8005\u6539\u53d8\u5b83\u4eec\u7684\u901a\u4fe1\u3002 ProxySQL - \u9ad8\u6027\u80fd\u7684 MySQL \u4ee3\u7406\u3002 \u590d\u5236 \u590d\u5236\u76f8\u5173\u7684\u8f6f\u4ef6 orchestrator - \u5bf9 MySQL \u590d\u5236\u62d3\u6251\u7ba1\u7406\u5e76\u53ef\u89c6\u5316\u7684\u5de5\u5177\u3002 Tungsten Replicator - MySQL \u7684\u4e00\u4e2a\u9ad8\u6027\u80fd\u3001\u5f00\u6e90\u3001\u6570\u636e\u590d\u5236\u5f15\u64ce\u3002 \u6a21\u5f0f \u9644\u52a0\u6a21\u5f0f common_schema - MySQL DBA \u7684\u6846\u67b6\uff0c \u63d0\u4f9b\u4e00\u4e2a\u5177\u6709\u51fd\u6570\u5e93\u3001\u89c6\u56fe\u5e93\u548c\u67e5\u8be2\u811a\u672c\u7684\u89e3\u91ca\u5668\u3002 sys - \u4e00\u4e2a\u89c6\u56fe\u3001\u51fd\u6570\u548c\u8fc7\u7a0b\u7684\u96c6\u5408\uff0c\u4ee5\u5e2e\u52a9 MySQL \u7ba1\u7406\u4eba\u5458\u66f4\u52a0\u6df1\u5165\u7406\u89e3 MySQL \u6570\u636e\u5e93\u7684\u4f7f\u7528\u3002 \u670d\u52a1\u5668 MySQL server flavors MariaDB - MySQL server \u7684\u4e00\u4e2a\u7531\u793e\u533a\u5f00\u53d1\u7684\u5206\u652f\u3002 MySQL Server MySQL Cluster - Oracle \u5b98\u65b9\u7684 MySQL server \u548c MySQL \u96c6\u7fa4\u5206\u5e03\u3002 Percona Server - \u4e00\u4e2a\u52a0\u5f3a\u7248\u7684 MySQL \u66ff\u4ee3\u54c1 WebScaleSQL - WebScaleSQL\uff0c5.6 \u7248\u672c\uff0c\u57fa\u4e8e MySQL 5.6 \u793e\u533a\u7248\u672c\u3002 \u5206\u7247 \u5206\u7247\u89e3\u51b3\u65b9\u6848/\u6846\u67b6 vitess - \u5bf9\u4e8e\u5927\u89c4\u6a21\u7684 web \u670d\u52a1\uff0cvitess \u63d0\u4f9b\u670d\u52a1\u548c\u5de5\u5177\u4ee5\u4fbf\u4e8e MySQL \u6570\u636e\u5e93\u7684\u7f29\u653e\u3002 jetpants - \u4e00\u4e2a\u81ea\u52a8\u5316\u5957\u4ef6\uff0c\u7528\u4e8e\u7ba1\u7406\u5927\u89c4\u6a21\u5206\u7247\u96c6\u7fa4\uff0c\u7531 Tumblr \u5f00\u53d1\u3002 \u5de5\u5177\u5305 \u5de5\u5177\u5305\uff0c\u901a\u7528\u811a\u672c go-mysql - \u4e00\u4e2a\u7eaf go \u7684\u5e93\uff0c\u7528\u4e8e\u5904\u7406 MySQL \u7684\u7f51\u7edc\u534f\u8bae\u548c\u590d\u5236\u3002 MySQL Utilities - \u4e00\u4e2a\u547d\u4ee4\u884c\u5b9e\u7528\u7a0b\u5e8f\u7684\u96c6\u5408\uff0cPython \u8bed\u8a00\u7f16\u5199\uff0c\u7528\u4e8e\u7ef4\u62a4\u548c\u7ba1\u7406\u5355\u4e00\u6216\u591a\u5c42\u7684 MySQL\u3002 Percona Toolkit - \u4e00\u4e2a\u5148\u8fdb\u7684\u547d\u4ee4\u884c\u5de5\u5177\u96c6\uff0c\u7528\u4e8e\u6267\u884c\u5bf9\u4e8e MySQL \u670d\u52a1\u5668\u548c\u7cfb\u7edf\u8fc7\u4e8e\u56f0\u96be\u6216\u590d\u6742\u7684\u4efb\u52a1\u3002 openark kit - \u4e00\u7ec4\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u89e3\u51b3\u65e5\u5e38\u7684\u7ef4\u62a4\u5de5\u4f5c\uff0c\u5305\u62ec\u4e00\u4e9b\u590d\u6742\u7684\u6216\u9700\u5f92\u624b\u64cd\u4f5c\u7684\uff0c\u7528 Python \u8bed\u8a00\u7f16\u5199\u3002 UnDROP - \u4e00\u4e2a\u7528\u6765\u6062\u590d\u5220\u9664\u6216\u635f\u574f\u7684 InnoDB \u8868\u4e2d\u6570\u636e\u7684\u5de5\u5177\u3002 \u8d44\u6e90 \u5728\u8fd9\u4e2a\u9636\u6bb5\uff0c\u201c\u8d44\u6e90\u201d\u4e0d\u5305\u62ec\u7f51\u7ad9\uff0c\u535a\u5ba2\uff0c\u5e7b\u706f\u7247\uff0c\u6f14\u793a\u89c6\u9891\u7b49\u3002\u8fd9\u4e9b\u8d44\u6e90\u5217\u8868\u7684\u5927\u5c0f\u4ee4\u4eba\u6050\u60e7\u3002 \u4f1a\u8bae \u56f4\u7ed5 MySQL \u8fdb\u884c\u516c\u5f00\uff0c\u7ecf\u5e38\u6027\u7684\u5927\u4f1a\u3002 FOSDEM - \u4e00\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\u89c1\u9762\u3001\u4ea4\u6d41\u601d\u60f3\u4e0e\u534f\u4f5c\u7684\u6d3b\u52a8\u3002\u6bcf\u5e74\u5728 Brussels \u4e3e\u884c\u3002\u63d0\u4f9b \u201cMySQL friends\u201d \u5f00\u53d1\u5de5\u4f5c\u5ba4\u3002 MySQL Central - Oracle \u5e74\u5ea6 MySQL \u5927\u4f1a\uff0c\u662f Oracle Open World \u7684\u4e00\u90e8\u5206\u3002 Percona Live - MySQL \u548c OpenStack \u7684\u91cd\u8981\u4f1a\u8bae\u3002 SCALE - \u4e00\u4e2a\u6bcf\u5e74\u5728\u5357\u52a0\u5dde\u4e3e\u529e\uff0c\u7531\u793e\u533a\u7ec4\u7ec7\u7684 Linux \u548c \u5f00\u6e90\u8f6f\u4ef6\u5927\u4f1a\u3002\u7531\u5f53\u5730 MySQL\u793e\u533a\u7ec4\u7ec7\u5e76\u4ee5MySQL\u793e\u533a\u65e5\u7684\u540d\u4e49\u4e3e\u529e\u3002 \u7535\u5b50\u4e66 MySQL \u7535\u5b50\u4e66\u4ee5\u53ca\u76f8\u5173\u6750\u6599\u3002 SQL-exercise - \u5305\u542b\u51e0\u4e2a SQL \u7ec3\u4e60\uff0c\u5305\u62ec\u6a21\u5f0f\u63cf\u8ff0\uff0c\u7528 SQL \u8bed\u53e5\u53bb\u5efa\u7acb\u6a21\u5f0f\uff0cSQL \u7684\u95ee\u9898\u53ca\u89e3\u51b3\u65b9\u6848\u3002\u4ee5 wikibook SQL \u7ec3\u4e60\u4e3a\u57fa\u7840\u3002 sqlfiddle - \u5728\u7ebf\u6267\u884c sql \u6d4b\u8bd5\u3002 \u5a92\u4f53 \u516c\u5f00\uff0c\u6301\u7eed\u7684\u89c6\u9891\u548c\u97f3\u9891\u8f6c\u64ad\u3002\u8fd9\u4e0d\u5305\u62ec\u4f1a\u8bae\u6f14\u8bb2\u90a3\u4ee4\u4eba\u6050\u60e7\u7684\u8d44\u6e90\u5217\u8868\u5927\u5c0f\u3002 DBHangOps - \u4e24\u5468\u4e00\u6b21\u7531\u5404\u79cd\u5404\u6837\u7684 MySQL \u793e\u533a\u4eba\u5458\u53c2\u52a0\u7684 google \u805a\u96c6\u5927\u4f1a\uff0c\u5927\u4f1a\u7684\u65e5\u5e38\u5c31\u662f\u8c08\u8bba\u4e00\u5207\u5173\u4e8e MySQL \u7684\u4e1c\u897f\u3002 OurSQL Podcast - MySQL \u6570\u636e\u5e93\u793e\u533a\u64ad\u5ba2\u3002 \u65b0\u95fb\u5468\u520a \u987e\u540d\u601d\u4e49\uff0c\u65b0\u95fb\u5468\u520a_\u9700\u8981\u4e00\u4e2a email \u5730\u5740\u3002\u4e0b\u9762\u5217\u51fa\u5468\u520a\u53ea\u9700\u8981\u4e00\u4e2a email \u5730\u5740\u3002 Weekly MySQL News - \u975e\u5b98\u65b9\u7684 MySQL\u65b0\u95fb\u6458\u8981\uff0c\u5305\u542b\u5173\u4e8eMySQL\u7684\u5404\u7c7b\u4fe1\u606f\u3002 \u5fae\u4fe1\u516c\u4f17\u53f7 \u6570\u636e\u5206\u6790\u4e0e\u5f00\u53d1\uff1a\u4e13\u6ce8\u5206\u4eab\u6570\u636e\u5e93\u76f8\u5173\u5185\u5bb9\uff0c\u5305\u62ec\uff1a\u5404\u79cd\u4e3b\u6d41 DB \u7684\u6700\u4f73\u5b9e\u8df5\u3001\u6570\u636e\u5e93\u57fa\u7840\u77e5\u8bc6\u3001\u6027\u80fd\u4f18\u5316\u3001\u6570\u636e\u5b89\u5168\u7b49\u3002 Linux\u7231\u597d\u8005\uff1a\u4e13\u6ce8\u5206\u4eab Linux/Unix \u76f8\u5173\u5185\u5bb9\uff0c\u5305\u62ec\uff1a\u5de5\u5177\u8d44\u6e90\u3001\u4f7f\u7528\u6280\u5de7\u3001\u8bfe\u7a0b\u3001\u4e66\u7c4d\u7b49\u3002","title":"MySQL \u8d44\u6e90\u5927\u5168\u4e2d\u6587\u7248"},{"location":"awe_mysql/#mysql","text":"\u6211\u60f3\u5f88\u591a\u7a0b\u5e8f\u5458\u5e94\u8be5\u8bb0\u5f97 GitHub \u4e0a\u6709\u4e00\u4e2a Awesome - XXX \u7cfb\u5217\u7684\u8d44\u6e90\u6574\u7406\u3002 awesome-mysql \u5c31\u662f shlomi-noach \u53d1\u8d77\u7ef4\u62a4\u7684 MySQL \u8d44\u6e90\u5217\u8868\uff0c\u5185\u5bb9\u5305\u62ec\uff1a\u5206\u6790\u5de5\u5177\u3001\u5907\u4efd\u3001\u6027\u80fd\u6d4b\u8bd5\u3001\u914d\u7f6e\u3001\u90e8\u7f72\u3001GUI \u7b49\u3002 Awesome \u7cfb\u5217\u867d\u7136\u633a\u5168\uff0c\u4f46\u57fa\u672c\u53ea\u5bf9\u6536\u5f55\u7684\u8d44\u6e90\u505a\u4e86\u6781\u4e3a\u7b80\u8981\u7684\u4ecb\u7ecd\uff0c\u5982\u679c\u6709\u66f4\u8be6\u7ec6\u7684\u4e2d\u6587\u4ecb\u7ecd\uff0c\u5bf9\u76f8\u5e94\u5f00\u53d1\u8005\u7684\u5e2e\u52a9\u4f1a\u66f4\u5927\u3002\u8fd9\u4e5f\u662f\u6211\u4eec\u53d1\u8d77\u8fd9\u4e2a\u5f00\u6e90\u9879\u76ee\u7684\u521d\u8877\u3002","title":"MySQL \u8d44\u6e90\u5927\u5168\u4e2d\u6587\u7248"},{"location":"awe_mysql/#_1","text":"\u57fa\u4e8e awesome-sysadmin \u8d44\u6e90\u5217\u8868\uff0c\u6211\u4eec\u5c06\u5bf9\u5404\u4e2a\u8d44\u6e90\u9879\u8fdb\u884c\u7f16\u8bd1\u6574\u7406\u3002 \u6574\u7406\u540e\u7684\u5185\u5bb9\uff0c\u5c06\u6536\u5f55\u5728 \u4f2f\u4e50\u5728\u7ebf\u8d44\u6e90\u9891\u9053 \u3002\u53ef\u53c2\u8003\u5df2\u6574\u7406\u7684\u5185\u5bb9\uff1a \u300a Logstash\uff1a\u65e5\u5fd7\u6587\u4ef6\u7ba1\u7406\u5de5\u5177 \u300b \u300a MyCli\uff1a\u652f\u6301\u81ea\u52a8\u8865\u5168\u548c\u8bed\u6cd5\u9ad8\u4eae\u7684 MySQL \u5ba2\u6237\u7aef \u300b","title":"\u6211\u4eec\u8981\u505a\u4ec0\u4e48\uff1f"},{"location":"awe_mysql/#_2","text":"","title":"\u5982\u4f55\u53c2\u4e0e\u672c\u9879\u76ee\uff1f"},{"location":"awe_mysql/#_3","text":"\u7ef4\u62a4\u8005\uff1a tangyouhua \u8d21\u732e\u8005\uff1a \u590f\u4e86\u590f\u5929 \u3001 lovecn \u3001You \u6ce8\uff1a\u540d\u5355\u4e0d\u5206\u6392\u540d\uff0c\u4e0d\u5b9a\u671f\u8865\u5145\u66f4\u65b0","title":"\u672c\u9879\u76ee\u7684\u53c2\u4e0e\u8005"},{"location":"awe_mysql/#_4","text":"\u5206\u6790\u5de5\u5177 \u5907\u4efd \u6027\u80fd\u6d4b\u8bd5 \u804a\u5929\u5e94\u7528 \u914d\u7f6e \u8fde\u63a5\u5668 \u90e8\u7f72 \u5f00\u53d1 GUI HA \u4ee3\u7406 \u590d\u5236 \u6a21\u5f0f \u670d\u52a1\u5668 \u5206\u7247 \u5de5\u5177\u5305 \u8d44\u6e90 \u6587\u6863 \u7535\u5b50\u4e66 \u5a92\u4f53 \u7b80\u8baf","title":"\u76ee\u5f55"},{"location":"awe_mysql/#_5","text":"\u6027\u80fd\uff0c\u7ed3\u6784\u548c\u6570\u636e\u5206\u6790\u5de5\u5177 Anemometer - \u4e00\u4e2a SQL \u6162\u67e5\u8be2\u76d1\u63a7\u5668\u3002 innodb-ruby - \u4e00\u4e2a\u5bf9 InooDB \u683c\u5f0f\u6587\u4ef6\u7684\u89e3\u6790\u5668\uff0c\u7528\u4e8e Ruby \u8bed\u8a00\u3002 innotop - \u4e00\u4e2a\u5177\u5907\u591a\u79cd\u7279\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684 MySQL \u7248 'top' \u5de5\u5177\u3002 pstop - \u4e00\u4e2a\u9488\u5bf9 MySQL \u7684\u7c7b top \u7a0b\u5e8f\uff0c\u7528\u4e8e\u6536\u96c6\uff0c\u6c47\u603b\u4ee5\u53ca\u5c55\u793a\u6765\u81ea performance_schema \u7684\u4fe1\u606f\u3002 mysql-statsd - \u4e00\u4e2a\u6536\u96c6 MySQL \u4fe1\u606f\u7684 Python \u5b88\u62a4\u8fdb\u7a0b\uff0c\u5e76\u901a\u8fc7 StatsD \u53d1\u9001\u5230 Graphite\u3002","title":"\u5206\u6790\u5de5\u5177"},{"location":"awe_mysql/#_6","text":"\u5907\u4efd/\u5b58\u50a8/\u6062\u590d \u5de5\u5177 MyDumper - \u903b\u8f91\u7684\uff0c\u5e76\u884c\u7684 MySQL \u5907\u4efd/\u8f6c\u50a8\u5de5\u5177\u3002 MySQLDumper - \u57fa\u4e8e web \u7684\u5f00\u6e90\u5907\u4efd\u5de5\u5177-\u5bf9\u4e8e\u5171\u4eab\u865a\u62df\u4e3b\u673a\u975e\u5e38\u6709\u7528\u3002 mysqldump-secure - \u5c06\u52a0\u5bc6\uff0c\u538b\u7f29\uff0c\u65e5\u5fd7\uff0c\u9ed1\u540d\u5355\u548c Nagios \u76d1\u63a7\u4e00\u4f53\u5316\u7684 mysqldump \u5b89\u5168\u811a\u672c\u3002 Percona Xtrabackup - \u9488\u5bf9 MySQL \u7684\u4e00\u4e2a\u5f00\u6e90\u70ed\u5907\u4efd\u5b9e\u7528\u7a0b\u5e8f\u2014\u2014\u5728\u670d\u52a1\u5668\u7684\u5907\u4efd\u671f\u95f4\u4e0d\u4f1a\u9501\u5b9a\u4f60\u7684\u6570\u636e\u5e93\u3002","title":"\u5907\u4efd"},{"location":"awe_mysql/#_7","text":"\u7ed9\u4f60\u7684\u670d\u52a1\u5668\u8fdb\u884c\u538b\u6d4b\u7684\u5de5\u5177 iibench-mysql -\u57fa\u4e8e Java \u7684 MySQL/Percona/MariaDB \u7d22\u5f15\u8fdb\u884c\u63d2\u5165\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\u3002 Sysbench - \u4e00\u4e2a\u6a21\u5757\u5316\uff0c\u8de8\u5e73\u53f0\u4ee5\u53ca\u591a\u7ebf\u7a0b\u7684\u6027\u80fd\u6d4b\u8bd5\u5de5\u5177\u3002","title":"\u6027\u80fd\u6d4b\u8bd5"},{"location":"awe_mysql/#_8","text":"\u96c6\u6210\u8fdb\u804a\u5929\u5ba4\u7684\u811a\u672c Hubot MySQL ChatOps","title":"\u804a\u5929\u5e94\u7528"},{"location":"awe_mysql/#_9","text":"MySQL \u914d\u7f6e\u5b9e\u4f8b\u53ca\u6307\u5bfc mysql-compatibility-config - \u4f7f MySQL \u914d\u7f6e\u8d77\u6765\u66f4\u50cf\u65b0\u7684\uff08\u6216\u5148\u524d\uff09\u7684 MySQL \u7248\u672c\u3002","title":"\u914d\u7f6e"},{"location":"awe_mysql/#_10","text":"\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u7684 MySQL \u8fde\u63a5\u5668 Connector/Python - \u4e00\u4e2a\u5bf9\u4e8e Python \u5e73\u53f0\u548c\u5f00\u53d1\u7684\u6807\u51c6\u5316\u6570\u636e\u5e93\u9a71\u52a8\u7a0b\u5e8f\u3002 go-sql-driver - \u4e00\u4e2a Go \u8bed\u8a00\u7684\u8f7b\u91cf\u7ea7\u3001\u6781\u901f\u7684 MySQL \u9a71\u52a8\u7a0b\u5e8f\u3002 libAttachSQL - libAttachSQL \u662f MySQL \u670d\u52a1\u5668\u7684\u4e00\u4e2a\u8f7b\u91cf\u7ea7\uff0c\u975e\u963b\u585e\u7684 C \u8bed\u8a00 API\u3002 MariaDB Java Client - \u9488\u5bf9 Java \u5e94\u7528\u4e14\u7ecf\u8fc7 LGPL \u8bb8\u53ef\u7684 MariaDB \u5ba2\u6237\u7aef\u5e93\u3002 MySQL-Python - \u4e00\u4e2a Python \u8bed\u8a00\u7684 MySQL \u6570\u636e\u5e93\u8fde\u63a5\u5668\u3002 PHP mysqlnd - \u9488\u5bf9 MySQL \u7684 MySQL \u672c\u5730\u9a71\u52a8\uff0c\u5f03\u7528\u8fc7\u65f6\u7684 libmysql \u57fa\u7840\u9a71\u52a8\u3002","title":"\u8fde\u63a5\u5668"},{"location":"awe_mysql/#_11","text":"\u652f\u6301 MySQL \u76f8\u5173\u5f00\u53d1\u7684\u5de5\u5177 Flywaydb - \u6570\u636e\u5e93\u8fc1\u79fb;\u4efb\u610f\u60c5\u51b5\u4e0b\u8f7b\u677e\u53ef\u9760\u5730\u6f14\u53d8\u4f60\u7684\u6570\u636e\u5e93\u7248\u672c\u3002 Liquibase - \u5bf9\u4f60\u7684\u6570\u636e\u5e93\u8fdb\u884c\u6e90\u4ee3\u7801\u63a7\u5236\u3002 Propagator - \u96c6\u4e2d\u6a21\u5f0f\u548c\u6570\u636e\u90e8\u7f72\u5728\u4e00\u4e2a\u591a\u7ef4\u62d3\u6251\u4e0a\u3002","title":"\u5f00\u53d1"},{"location":"awe_mysql/#gui","text":"\u524d\u7aef\u548c\u5e94\u7528\u7684 GUI Adminer - \u4e00\u4e2a PHP \u7f16\u5199\u7684\u6570\u636e\u5e93\u7ba1\u7406\u5de5\u5177\u3002 HeidiSQL - Windows \u4e0b\u7684 MySQL \u56fe\u5f62\u5316\u7ba1\u7406\u5de5\u5177\u3002 MySQL Workbench - \u63d0\u4f9b\u7ed9\u6570\u636e\u5e93\u7ba1\u7406\u5458\u548c\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u5efa\u6a21\u7684\u96c6\u6210\u5de5\u5177\u73af\u5883;SQL \u5f00\u53d1;\u6570\u636e\u5e93\u7ba1\u7406\u3002 phpMyAdmin - \u4e00\u4e2a PHP \u5199\u6210\u7684\u5f00\u6e90\u8f6f\u4ef6\uff0c\u610f\u56fe\u5bf9 web \u4e0a\u7684 MySQL \u8fdb\u884c\u7ba1\u7406\u3002 SequelPro - \u4e00\u4e2a mac \u4e0b\u8fd0\u884c MySQL \u7684\u6570\u636e\u5e93\u7ba1\u7406\u5e94\u7528\u7a0b\u5e8f\u3002 mycli - \u4e00\u4e2a\u5e26\u81ea\u52a8\u8865\u5168\u548c\u8bed\u6cd5\u9ad8\u4eae\u7684\u7ec8\u7aef\u7248 MySQL \u5ba2\u6237\u7aef","title":"GUI"},{"location":"awe_mysql/#ha","text":"\u9ad8\u53ef\u7528\u89e3\u51b3\u65b9\u6848 Galera Cluster - \u4e00\u4e2a\u57fa\u4e8e\u540c\u6b65\u590d\u5236\u7684\u591a\u4e3b\u673a\u96c6\u7fa4\u65b9\u6848\u3002 MHA - \u9488\u5bf9 MySQL \u7684\u4f18\u79c0\u9ad8\u53ef\u7528\u7ba1\u7406\u5668\u53ca\u5de5\u5177 MySQL Fabric - \u4e00\u4e2a\u7528\u4e8e\u7ba1\u7406 MySQL \u670d\u52a1\u5668\u573a\uff08Server Farms\uff09\u7684\u53ef\u6269\u5c55\u6846\u67b6\u3002 Percona Replication Manager - \u9488\u5bf9 MySQL \u7684\u5f02\u6b65\u590d\u5236\u7ba1\u7406\u4ee3\u7406\u3002\u652f\u6301\u4ee5\u6587\u4ef6\u548c GTID \u4e3a\u57fa\u7840\u7684\u590d\u5236\uff0c\u4f7f\u7528 booth \u5b9e\u73b0\u7684\u5730\u7406\u5206\u5e03\u5f0f\u96c6\u7fa4\u3002","title":"HA"},{"location":"awe_mysql/#_12","text":"MySQL \u4ee3\u7406 MaxScale - \u5f00\u6e90\uff0c\u4ee5\u6570\u636e\u5e93\u4e3a\u4e2d\u5fc3\u7684\u4ee3\u7406\u3002 Mixer - Go \u5b9e\u73b0\u7684\u4e00\u4e2a MySQL \u4ee3\u7406\uff0c\u76ee\u7684\u4e3a MySQL \u5206\u7247\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u7684\u89e3\u51b3\u65b9\u6848\u3002 MySQL Proxy - \u4e00\u4e2a\u5904\u4e8e\u4f60\u7684\u5ba2\u6237\u7aef\u548c MySQL \u670d\u52a1\u7aef\u4e4b\u95f4\u7684\u7b80\u5355\u7a0b\u5e8f\uff0c\u5b83\u53ef\u4ee5\u68c0\u6d4b\u3001\u5206\u6790\u6216\u8005\u6539\u53d8\u5b83\u4eec\u7684\u901a\u4fe1\u3002 ProxySQL - \u9ad8\u6027\u80fd\u7684 MySQL \u4ee3\u7406\u3002","title":"\u4ee3\u7406"},{"location":"awe_mysql/#_13","text":"\u590d\u5236\u76f8\u5173\u7684\u8f6f\u4ef6 orchestrator - \u5bf9 MySQL \u590d\u5236\u62d3\u6251\u7ba1\u7406\u5e76\u53ef\u89c6\u5316\u7684\u5de5\u5177\u3002 Tungsten Replicator - MySQL \u7684\u4e00\u4e2a\u9ad8\u6027\u80fd\u3001\u5f00\u6e90\u3001\u6570\u636e\u590d\u5236\u5f15\u64ce\u3002","title":"\u590d\u5236"},{"location":"awe_mysql/#_14","text":"\u9644\u52a0\u6a21\u5f0f common_schema - MySQL DBA \u7684\u6846\u67b6\uff0c \u63d0\u4f9b\u4e00\u4e2a\u5177\u6709\u51fd\u6570\u5e93\u3001\u89c6\u56fe\u5e93\u548c\u67e5\u8be2\u811a\u672c\u7684\u89e3\u91ca\u5668\u3002 sys - \u4e00\u4e2a\u89c6\u56fe\u3001\u51fd\u6570\u548c\u8fc7\u7a0b\u7684\u96c6\u5408\uff0c\u4ee5\u5e2e\u52a9 MySQL \u7ba1\u7406\u4eba\u5458\u66f4\u52a0\u6df1\u5165\u7406\u89e3 MySQL \u6570\u636e\u5e93\u7684\u4f7f\u7528\u3002","title":"\u6a21\u5f0f"},{"location":"awe_mysql/#_15","text":"MySQL server flavors MariaDB - MySQL server \u7684\u4e00\u4e2a\u7531\u793e\u533a\u5f00\u53d1\u7684\u5206\u652f\u3002 MySQL Server MySQL Cluster - Oracle \u5b98\u65b9\u7684 MySQL server \u548c MySQL \u96c6\u7fa4\u5206\u5e03\u3002 Percona Server - \u4e00\u4e2a\u52a0\u5f3a\u7248\u7684 MySQL \u66ff\u4ee3\u54c1 WebScaleSQL - WebScaleSQL\uff0c5.6 \u7248\u672c\uff0c\u57fa\u4e8e MySQL 5.6 \u793e\u533a\u7248\u672c\u3002","title":"\u670d\u52a1\u5668"},{"location":"awe_mysql/#_16","text":"\u5206\u7247\u89e3\u51b3\u65b9\u6848/\u6846\u67b6 vitess - \u5bf9\u4e8e\u5927\u89c4\u6a21\u7684 web \u670d\u52a1\uff0cvitess \u63d0\u4f9b\u670d\u52a1\u548c\u5de5\u5177\u4ee5\u4fbf\u4e8e MySQL \u6570\u636e\u5e93\u7684\u7f29\u653e\u3002 jetpants - \u4e00\u4e2a\u81ea\u52a8\u5316\u5957\u4ef6\uff0c\u7528\u4e8e\u7ba1\u7406\u5927\u89c4\u6a21\u5206\u7247\u96c6\u7fa4\uff0c\u7531 Tumblr \u5f00\u53d1\u3002","title":"\u5206\u7247"},{"location":"awe_mysql/#_17","text":"\u5de5\u5177\u5305\uff0c\u901a\u7528\u811a\u672c go-mysql - \u4e00\u4e2a\u7eaf go \u7684\u5e93\uff0c\u7528\u4e8e\u5904\u7406 MySQL \u7684\u7f51\u7edc\u534f\u8bae\u548c\u590d\u5236\u3002 MySQL Utilities - \u4e00\u4e2a\u547d\u4ee4\u884c\u5b9e\u7528\u7a0b\u5e8f\u7684\u96c6\u5408\uff0cPython \u8bed\u8a00\u7f16\u5199\uff0c\u7528\u4e8e\u7ef4\u62a4\u548c\u7ba1\u7406\u5355\u4e00\u6216\u591a\u5c42\u7684 MySQL\u3002 Percona Toolkit - \u4e00\u4e2a\u5148\u8fdb\u7684\u547d\u4ee4\u884c\u5de5\u5177\u96c6\uff0c\u7528\u4e8e\u6267\u884c\u5bf9\u4e8e MySQL \u670d\u52a1\u5668\u548c\u7cfb\u7edf\u8fc7\u4e8e\u56f0\u96be\u6216\u590d\u6742\u7684\u4efb\u52a1\u3002 openark kit - \u4e00\u7ec4\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u89e3\u51b3\u65e5\u5e38\u7684\u7ef4\u62a4\u5de5\u4f5c\uff0c\u5305\u62ec\u4e00\u4e9b\u590d\u6742\u7684\u6216\u9700\u5f92\u624b\u64cd\u4f5c\u7684\uff0c\u7528 Python \u8bed\u8a00\u7f16\u5199\u3002 UnDROP - \u4e00\u4e2a\u7528\u6765\u6062\u590d\u5220\u9664\u6216\u635f\u574f\u7684 InnoDB \u8868\u4e2d\u6570\u636e\u7684\u5de5\u5177\u3002","title":"\u5de5\u5177\u5305"},{"location":"awe_mysql/#_18","text":"\u5728\u8fd9\u4e2a\u9636\u6bb5\uff0c\u201c\u8d44\u6e90\u201d\u4e0d\u5305\u62ec\u7f51\u7ad9\uff0c\u535a\u5ba2\uff0c\u5e7b\u706f\u7247\uff0c\u6f14\u793a\u89c6\u9891\u7b49\u3002\u8fd9\u4e9b\u8d44\u6e90\u5217\u8868\u7684\u5927\u5c0f\u4ee4\u4eba\u6050\u60e7\u3002","title":"\u8d44\u6e90"},{"location":"awe_mysql/#_19","text":"\u56f4\u7ed5 MySQL \u8fdb\u884c\u516c\u5f00\uff0c\u7ecf\u5e38\u6027\u7684\u5927\u4f1a\u3002 FOSDEM - \u4e00\u4e2a\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\u89c1\u9762\u3001\u4ea4\u6d41\u601d\u60f3\u4e0e\u534f\u4f5c\u7684\u6d3b\u52a8\u3002\u6bcf\u5e74\u5728 Brussels \u4e3e\u884c\u3002\u63d0\u4f9b \u201cMySQL friends\u201d \u5f00\u53d1\u5de5\u4f5c\u5ba4\u3002 MySQL Central - Oracle \u5e74\u5ea6 MySQL \u5927\u4f1a\uff0c\u662f Oracle Open World \u7684\u4e00\u90e8\u5206\u3002 Percona Live - MySQL \u548c OpenStack \u7684\u91cd\u8981\u4f1a\u8bae\u3002 SCALE - \u4e00\u4e2a\u6bcf\u5e74\u5728\u5357\u52a0\u5dde\u4e3e\u529e\uff0c\u7531\u793e\u533a\u7ec4\u7ec7\u7684 Linux \u548c \u5f00\u6e90\u8f6f\u4ef6\u5927\u4f1a\u3002\u7531\u5f53\u5730 MySQL\u793e\u533a\u7ec4\u7ec7\u5e76\u4ee5MySQL\u793e\u533a\u65e5\u7684\u540d\u4e49\u4e3e\u529e\u3002","title":"\u4f1a\u8bae"},{"location":"awe_mysql/#_20","text":"MySQL \u7535\u5b50\u4e66\u4ee5\u53ca\u76f8\u5173\u6750\u6599\u3002 SQL-exercise - \u5305\u542b\u51e0\u4e2a SQL \u7ec3\u4e60\uff0c\u5305\u62ec\u6a21\u5f0f\u63cf\u8ff0\uff0c\u7528 SQL \u8bed\u53e5\u53bb\u5efa\u7acb\u6a21\u5f0f\uff0cSQL \u7684\u95ee\u9898\u53ca\u89e3\u51b3\u65b9\u6848\u3002\u4ee5 wikibook SQL \u7ec3\u4e60\u4e3a\u57fa\u7840\u3002 sqlfiddle - \u5728\u7ebf\u6267\u884c sql \u6d4b\u8bd5\u3002","title":"\u7535\u5b50\u4e66"},{"location":"awe_mysql/#_21","text":"\u516c\u5f00\uff0c\u6301\u7eed\u7684\u89c6\u9891\u548c\u97f3\u9891\u8f6c\u64ad\u3002\u8fd9\u4e0d\u5305\u62ec\u4f1a\u8bae\u6f14\u8bb2\u90a3\u4ee4\u4eba\u6050\u60e7\u7684\u8d44\u6e90\u5217\u8868\u5927\u5c0f\u3002 DBHangOps - \u4e24\u5468\u4e00\u6b21\u7531\u5404\u79cd\u5404\u6837\u7684 MySQL \u793e\u533a\u4eba\u5458\u53c2\u52a0\u7684 google \u805a\u96c6\u5927\u4f1a\uff0c\u5927\u4f1a\u7684\u65e5\u5e38\u5c31\u662f\u8c08\u8bba\u4e00\u5207\u5173\u4e8e MySQL \u7684\u4e1c\u897f\u3002 OurSQL Podcast - MySQL \u6570\u636e\u5e93\u793e\u533a\u64ad\u5ba2\u3002","title":"\u5a92\u4f53"},{"location":"awe_mysql/#_22","text":"\u987e\u540d\u601d\u4e49\uff0c\u65b0\u95fb\u5468\u520a_\u9700\u8981\u4e00\u4e2a email \u5730\u5740\u3002\u4e0b\u9762\u5217\u51fa\u5468\u520a\u53ea\u9700\u8981\u4e00\u4e2a email \u5730\u5740\u3002 Weekly MySQL News - \u975e\u5b98\u65b9\u7684 MySQL\u65b0\u95fb\u6458\u8981\uff0c\u5305\u542b\u5173\u4e8eMySQL\u7684\u5404\u7c7b\u4fe1\u606f\u3002","title":"\u65b0\u95fb\u5468\u520a"},{"location":"awe_python/","text":"Awesome Python A curated list of awesome Python frameworks, libraries, software and resources. Inspired by awesome-php . Awesome Python Admin Panels Algorithms and Design Patterns ASGI Servers Asynchronous Programming Audio Authentication Build Tools Built-in Classes Enhancement Caching ChatOps Tools CMS Code Analysis Command-line Interface Development Command-line Tools Compatibility Computer Vision Concurrency and Parallelism Configuration Cryptography Data Analysis Data Validation Data Visualization Database Drivers Database Date and Time Debugging Tools Deep Learning DevOps Tools Distributed Computing Distribution Documentation Downloader E-commerce Editor Plugins and IDEs Email Environment Management Files Foreign Function Interface Forms Functional Programming Game Development Geolocation GUI Development Hardware HTML Manipulation HTTP Clients Image Processing Implementations Interactive Interpreter Internationalization Job Scheduler Logging Machine Learning Miscellaneous Natural Language Processing Network Virtualization News Feed ORM Package Management Package Repositories Permissions Processes Recommender Systems Refactoring RESTful API Robotics RPC Servers Science Search Serialization Serverless Frameworks Specific Formats Processing Static Site Generator Tagging Task Queues Template Engine Testing Text Processing Third-party APIs URL Manipulation Video Web Asset Management Web Content Extracting Web Crawling Web Frameworks WebSocket WSGI Servers Resources Podcasts Twitter Websites Weekly Contributing Admin Panels Libraries for administrative interfaces. ajenti - The admin panel your servers deserve. django-grappelli - A jazzy skin for the Django Admin-Interface. django-jet - Modern responsive template for the Django admin interface with improved functionality. django-suit - Alternative Django Admin-Interface (free only for Non-commercial use). django-xadmin - Drop-in replacement of Django admin comes with lots of goodies. jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django) flask-admin - Simple and extensible administrative interface framework for Flask. flower - Real-time monitor and web admin for Celery. wooey - A Django app which creates automatic web UIs for Python scripts. Algorithms and Design Patterns Python implementation of algorithms and design patterns. algorithms - Minimal examples of data structures and algorithms in Python. PyPattyrn - A simple yet effective library for implementing common design patterns. python-ds - Clean and simple collection of data structure and algorithms in Python for coding interviews. python-patterns - A collection of design patterns in Python. sortedcontainers - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types. transitions - A lightweight, object-oriented finite state machine implementation in Python. ASGI Servers ASGI-compatible web servers. uvicorn - Uvicorn is a lightning-fast ASGI server implementation, using uvloop and httptools. Asynchronous Programming asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks. awesome-asyncio uvloop - Ultra fast asyncio event loop. Twisted - An event-driven networking engine. Audio Libraries for manipulating audio and its metadata. Audio audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding. dejavu - Audio fingerprinting and recognition. matchering - A library for automated reference audio mastering. mingus - An advanced music theory and notation package with MIDI file and playback support. pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications. pydub - Manipulate audio with a simple and easy high level interface. TimeSide - Open web audio processing framework. Metadata beets - A music library manager and MusicBrainz tagger. eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata. mutagen - A Python module to handle audio metadata. tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files. Authentication Libraries for implementing authentications schemes. OAuth authlib - JavaScript Object Signing and Encryption draft implementation. django-allauth - Authentication app for Django that \"just works.\" django-oauth-toolkit - OAuth 2 goodies for Django. oauthlib - A generic and thorough implementation of the OAuth request-signing logic. python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers. python-social-auth - An easy-to-setup social authentication mechanism. JWT pyjwt - JSON Web Token implementation in Python. python-jose - A JOSE implementation in Python. python-jwt - A module for generating and verifying JSON Web Tokens. Build Tools Compile software from source code. BitBake - A make-like build tool for embedded Linux. buildout - A build system for creating, assembling and deploying applications from multiple parts. PlatformIO - A console tool to build code with different development platforms. pybuilder - A continuous build tool written in pure Python. SCons - A software construction tool. Built-in Classes Enhancement Libraries for enhancing Python built-in classes. dataclasses - (Python standard library) Data classes. attrs - Replacement for __init__ , __eq__ , __repr__ , etc. boilerplate in class definitions. bidict - Efficient, Pythonic bidirectional map data structures and related functionality.. Box - Python dictionaries with advanced dot notation access. DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation. CMS Content Management Systems. wagtail - A Django content management system. django-cms - An Open source enterprise CMS based on the Django. feincms - One of the most advanced Content Management Systems built on Django. indico - A feature-rich event management system, made @ CERN . Kotti - A high-level, Pythonic web application framework built on Pyramid. mezzanine - A powerful, consistent, and flexible content management platform. plone - A CMS built on top of the open source application server Zope. quokka - Flexible, extensible, small CMS powered by Flask and MongoDB. Caching Libraries for caching data. beaker - A WSGI middleware for sessions and caching. django-cache-machine - Automatic caching and invalidation for Django models. django-cacheops - A slick ORM cache with automatic granular event-driven invalidation. dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors. HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention. pylibmc - A Python wrapper around the libmemcached interface. python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis. ChatOps Tools Libraries for chatbot development. errbot - The easiest and most popular chatbot to implement ChatOps. Code Analysis Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis . Code Analysis coala - Language independent and easily extendable code analysis application. code2flow - Turn your Python and JavaScript code into DOT flowcharts. prospector - A tool to analyse Python code. pycallgraph - A library that visualises the flow (call graph) of your Python application. vulture - A tool for finding and analysing dead Python code. Code Linters flake8 - A wrapper around pycodestyle , pyflakes and McCabe. awesome-flake8-extensions pylint - A fully customizable source code analyzer. pylama - A code audit tool for Python and JavaScript. wemake-python-styleguide - The strictest and most opinionated python linter ever. Code Formatters black - The uncompromising Python code formatter. yapf - Yet another Python code formatter from Google. Static Type Checkers, also see awesome-python-typing mypy - Check variable types during compile time. typeshed - Collection of library stubs for Python, with static types. pyre-check - Performant type checking. Static Type Annotations Generators MonkeyType - A system for Python that generates static type annotations by collecting runtime types Command-line Interface Development Libraries for building command-line applications. Command-line Application Development cement - CLI Application Framework for Python. click - A package for creating beautiful command line interfaces in a composable way. cliff - A framework for creating command-line programs with multi-level commands. docopt - Pythonic command line arguments parser. python-fire - A library for creating command line interfaces from absolutely any Python object. python-prompt-toolkit - A library for building powerful interactive command lines. Terminal Rendering asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations). bashplotlib - Making basic plots in the terminal. colorama - Cross-platform colored terminal text. rich - Python library for rich text and beautiful formatting in the terminal. Also provides a great RichHandler log handler. tqdm - Fast, extensible progress bar for loops and CLI. Command-line Tools Useful CLI-based tools for productivity. Productivity Tools cookiecutter - A command-line utility that creates projects from cookiecutters (project templates). doitlive - A tool for live presentations in the terminal. howdoi - Instant coding answers via the command line. Invoke - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. PathPicker - Select files out of bash output. percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX. thefuck - Correcting your previous console command. tmuxp - A tmux session manager. try - A dead simple CLI to try out python packages - it's never been easier. CLI Enhancements httpie - A command line HTTP client, a user-friendly cURL replacement. kube-shell - An integrated shell for working with the Kubernetes CLI. mycli - MySQL CLI with autocompletion and syntax highlighting. pgcli - PostgreSQL CLI with autocompletion and syntax highlighting. iredis - Redis CLI with autocompletion and syntax highlighting. litecli - SQLite CLI with autocompletion and syntax highlighting. saws - A Supercharged aws-cli . Compatibility Libraries for migrating from Python 2 to 3. python-future - The missing compatibility layer between Python 2 and Python 3. python-modernize - Modernizes Python code for eventual Python 3 migration. six - Python 2 and 3 compatibility utilities. Computer Vision Libraries for Computer Vision. Kornia - Open Source Differentiable Computer Vision Library for PyTorch. OpenCV - Open Source Computer Vision Library. pytesseract - Another wrapper for Google Tesseract OCR . tesserocr - A simple, Pillow-friendly, wrapper around the tesseract-ocr API for OCR. SimpleCV - An open source framework for building computer vision applications. Concurrency and Parallelism Libraries for concurrent and parallel execution. Also see awesome-asyncio . concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables. multiprocessing - (Python standard library) Process-based parallelism. eventlet - Asynchronous framework with WSGI support. gevent - A coroutine-based Python networking library that uses greenlet . uvloop - Ultra fast implementation of asyncio event loop on top of libuv . scoop - Scalable Concurrent Operations in Python. Configuration Libraries for storing and parsing configuration options. configobj - INI file parser with validation. configparser - (Python standard library) INI file parser. profig - Config from multiple formats with value conversion. python-decouple - Strict separation of settings from code. Cryptography cryptography - A package designed to expose cryptographic primitives and recipes to Python developers. paramiko - The leading native Python SSHv2 protocol library. passlib - Secure password storage/hashing library, very high level. pynacl - Python binding to the Networking and Cryptography (NaCl) library. Data Analysis Libraries for data analyzing. Blaze - NumPy and Pandas interface to Big Data. Open Mining - Business Intelligence (BI) in Pandas interface. Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts. Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools. Optimus - Agile Data Science Workflows made easy with PySpark. Data Validation Libraries for validating data. Used for forms in many cases. Cerberus - A lightweight and extensible data validation library. colander - Validating and deserializing data obtained via XML, JSON, an HTML form post. jsonschema - An implementation of JSON Schema for Python. schema - A library for validating Python data structures. Schematics - Data Structure Validation. valideer - Lightweight extensible data validation and adaptation library. voluptuous - A Python data validation library. Data Visualization Libraries for visualizing data. Also see awesome-javascript . Altair - Declarative statistical visualization library for Python. Bokeh - Interactive Web Plotting for Python. bqplot - Interactive Plotting Library for the Jupyter Notebook Dash - Built on top of Flask, React and Plotly aimed at analytical web applications. awesome-dash diagrams - Diagram as Code. plotnine - A grammar of graphics for Python based on ggplot2. Matplotlib - A Python 2D plotting library. Pygal - A Python SVG Charts Creator. PyGraphviz - Python interface to Graphviz . PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets. Seaborn - Statistical data visualization using Matplotlib. VisPy - High-performance scientific visualization based on OpenGL. Database Databases implemented in Python. pickleDB - A simple and lightweight key-value store for Python. tinydb - A tiny, document-oriented database. ZODB - A native object database for Python. A key-value and object graph database. Database Drivers Libraries for connecting and operating databases. MySQL - awesome-mysql mysqlclient - MySQL connector with Python 3 support ( mysql-python fork). PyMySQL - A pure Python MySQL driver compatible to mysql-python. PostgreSQL - awesome-postgres psycopg2 - The most popular PostgreSQL adapter for Python. queries - A wrapper of the psycopg2 library for interacting with PostgreSQL. Other Relational Databases pymssql - A simple database interface to Microsoft SQL Server. SuperSQLite - A supercharged SQLite library built on top of apsw . NoSQL Databases cassandra-driver - The Python Driver for Apache Cassandra. happybase - A developer-friendly library for Apache HBase. kafka-python - The Python client for Apache Kafka. py2neo - A client library and toolkit for working with Neo4j. pymongo - The official Python client for MongoDB. redis-py - The Python client for Redis. Asynchronous Clients motor - The async Python driver for MongoDB. Date and Time Libraries for working with dates and times. Arrow - A Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. Chronyk - A Python 3 library for parsing human-written times and dates. dateutil - Extensions to the standard Python datetime module. delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes. moment - A Python library for dealing with dates/times. Inspired by Moment.js . Pendulum - Python datetimes made easy. PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string. pytz - World timezone definitions, modern and historical. Brings the tz database into Python. when.py - Providing user-friendly functions to help perform common date and time actions. maya - Datetimes for Humans. Debugging Tools Libraries for debugging code. pdb-like Debugger ipdb - IPython-enabled pdb . pdb++ - Another drop-in replacement for pdb. pudb - A full-screen, console-based Python debugger. wdb - An improbable web debugger through WebSockets. Tracing lptrace - strace for Python programs. manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt. pyringe - Debugger capable of attaching to and injecting code into Python processes. python-hunter - A flexible code tracing toolkit. Profiler line_profiler - Line-by-line profiling. memory_profiler - Monitor Memory usage of Python code. profiling - An interactive Python profiler. py-spy - A sampling profiler for Python programs. Written in Rust. pyflame - A ptracing profiler For Python. vprof - Visual Python profiler. Others icecream - Inspect variables, expressions, and program execution with a single, simple function call. django-debug-toolbar - Display various debug information for Django. django-devserver - A drop-in replacement for Django's runserver. flask-debugtoolbar - A port of the django-debug-toolbar to flask. pyelftools - Parsing and analyzing ELF files and DWARF debugging information. Deep Learning Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning . caffe - A fast open framework for deep learning.. keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano. mxnet - A deep learning framework designed for both efficiency and flexibility. pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration. SerpentAI - Game agent framework. Use any video game as a deep learning sandbox. tensorflow - The most popular Deep Learning framework created by Google. Theano - A library for fast numerical computation. DevOps Tools Software and libraries for DevOps. ansible - A radically simple IT automation platform. cloudinit - A multi-distribution package that handles early initialization of a cloud instance. cuisine - Chef-like functionality for Fabric. docker-compose - Fast, isolated development environments using Docker . fabric - A simple, Pythonic tool for remote execution and deployment. fabtools - Tools for writing awesome Fabric files. honcho - A Python clone of Foreman , for managing Procfile-based applications. OpenStack - Open source software for building private and public clouds. pexpect - Controlling interactive programs in a pseudo-terminal like GNU expect. psutil - A cross-platform process and system utilities module. saltstack - Infrastructure automation and management system. supervisor - Supervisor process control system for UNIX. Distributed Computing Frameworks and libraries for Distributed Computing. Batch Processing PySpark - Apache Spark Python API. dask - A flexible parallel computing library for analytic computing. luigi - A module that helps you build complex pipelines of batch jobs. mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services. Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem. Stream Processing faust - A stream processing library, porting the ideas from Kafka Streams to Python. streamparse - Run Python code against real-time streams of data via Apache Storm . Distribution Libraries to create packaged executables for release distribution. dh-virtualenv - Build and distribute a virtualenv as a Debian package. Nuitka - Compile scripts, modules, packages to an executable or extension module. py2app - Freezes Python scripts (Mac OS X). py2exe - Freezes Python scripts (Windows). PyInstaller - Converts Python programs into stand-alone executables (cross-platform). pynsist - A tool to build Windows installers, installers bundle Python itself. pyarmor - A tool used to obfuscate python scripts, bind obfuscated scripts to fixed machine or expire obfuscated scripts. shiv - A command line utility for building fully self-contained zipapps (PEP 441), but with all their dependencies included. Documentation Libraries for generating project documentation. sphinx - Python Documentation generator. awesome-sphinxdoc pdoc - Epydoc replacement to auto generate API documentation for Python libraries. pycco - The literate-programming-style documentation generator. Downloader Libraries for downloading. s3cmd - A command line tool for managing Amazon S3 and CloudFront. s4cmd - Super S3 command line tool, good for higher performance. you-get - A YouTube/Youku/Niconico video downloader written in Python 3. youtube-dl - A small command-line program to download videos from YouTube. akshare - A financial data interface library, built for human beings! E-commerce Frameworks and libraries for e-commerce and payments. alipay - Unofficial Alipay API for Python. Cartridge - A shopping cart app built using the Mezzanine. django-oscar - An open-source e-commerce framework for Django. django-shop - A Django based shop system. merchant - A Django app to accept payments from various payment processors. money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange. python-currencies - Display money format and its filthy currencies. forex-python - Foreign exchange rates, Bitcoin price index and currency conversion. saleor - An e-commerce storefront for Django. shoop - An open source E-Commerce platform based on Django. Editor Plugins and IDEs Emacs elpy - Emacs Python Development Environment. Sublime Text anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE. SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi. Vim jedi-vim - Vim bindings for the Jedi auto-completion library for Python. python-mode - An all in one plugin for turning Vim into a Python IDE. YouCompleteMe - Includes Jedi -based completion engine for Python. Visual Studio PTVS - Python Tools for Visual Studio. Visual Studio Code Python - The official VSCode extension with rich support for Python. IDE PyCharm - Commercial Python IDE by JetBrains. Has free community edition available. spyder - Open Source Python IDE. Email Libraries for sending and parsing email. Mail Servers modoboa - A mail hosting and management platform including a modern Web UI. salmon - A Python Mail Server. Clients imbox - Python IMAP for Humans. yagmail - Yet another Gmail/SMTP client. Others flanker - An email address and Mime parsing library. mailer - High-performance extensible mail delivery framework. Environment Management Libraries for Python version and virtual environment management. pyenv - Simple Python version management. virtualenv - A tool to create isolated Python environments. Files Libraries for file manipulation and MIME type detection. mimetypes - (Python standard library) Map filenames to MIME types. path.py - A module wrapper for os.path . pathlib - (Python standard library) An cross-platform, object-oriented path library. PyFilesystem2 - Python's filesystem abstraction layer. python-magic - A Python interface to the libmagic file type identification library. Unipath - An object-oriented approach to file/directory operations. watchdog - API and shell utilities to monitor file system events. Foreign Function Interface Libraries for providing foreign function interface. cffi - Foreign Function Interface for Python calling C code. ctypes - (Python standard library) Foreign Function Interface for Python calling C code. PyCUDA - A Python wrapper for Nvidia's CUDA API. SWIG - Simplified Wrapper and Interface Generator. Forms Libraries for working with forms. Deform - Python HTML form generation library influenced by the formish form generation library. django-bootstrap3 - Bootstrap 3 integration with Django. django-bootstrap4 - Bootstrap 4 integration with Django. django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way. django-remote-forms - A platform independent Django form serializer. WTForms - A flexible forms validation and rendering library. Functional Programming Functional Programming with Python. Coconut - A variant of Python built for simple, elegant, Pythonic functional programming. fn.py - Functional programming in Python: implementation of missing features to enjoy FP. funcy - A fancy and practical functional tools. more-itertools - More routines for operating on iterables, beyond itertools . returns - A set of type-safe monads, tranformers, and composition utilities. Toolz - A collection of functional utilities for iterators, functions, and dictionaries. CyToolz - Cython implementation of Toolz : High performance functional utilities. GUI Development Libraries for working with graphical user interface applications. curses - Built-in wrapper for ncurses used to create terminal GUI applications. Eel - A library for making simple Electron-like offline HTML/JS GUI apps. enaml - Creating beautiful user-interfaces with Declarative Syntax like QML. Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering. Gooey - Turn command line programs into a full GUI application with one line. kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS. pyglet - A cross-platform windowing and multimedia library for Python. PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3). PyQt - Python bindings for the Qt cross-platform application and UI framework. PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi. pywebview - A lightweight cross-platform native wrapper around a webview component. Tkinter - Tkinter is Python's de-facto standard GUI package. Toga - A Python native, OS native GUI toolkit. urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc. wxPython - A blending of the wxWidgets C++ class library with the Python. GraphQL Libraries for working with GraphQL. tartiflette - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio. tartiflette-aiohttp - An aiohttp -based wrapper for Tartiflette to expose GraphQL APIs over HTTP. tartiflette-asgi - ASGI support for the Tartiflette GraphQL engine. Game Development Awesome game development libraries. Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. Harfang3D - Python framework for 3D, VR and game development. Panda3D - 3D game engine developed by Disney. Pygame - Pygame is a set of Python modules designed for writing games. PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D. PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs. PySDL2 - A ctypes based wrapper for the SDL2 library. RenPy - A Visual Novel engine. Geolocation Libraries for geocoding addresses and working with latitudes and longitudes. django-countries - A Django app that provides a country field for models and forms. GeoDjango - A world-class geographic web framework. GeoIP - Python API for MaxMind GeoIP Legacy Database. geojson - Python bindings and utilities for GeoJSON. geopy - Python Geocoding Toolbox. pygeoip - Pure Python GeoIP API. HTML Manipulation Libraries for working with HTML and XML. BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML. bleach - A whitelist-based HTML sanitization and text linkification library. cssutils - A CSS library for Python. html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments. lxml - A very fast, easy-to-use and versatile library for handling HTML and XML. MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python. pyquery - A jQuery-like library for parsing HTML. untangle - Converts XML documents to Python objects for easy access. WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF. xmldataset - Simple XML Parsing. xmltodict - Working with XML feel like you are working with JSON. HTTP Clients Libraries for working with HTTP. grequests - requests + gevent for asynchronous HTTP requests. httplib2 - Comprehensive HTTP client library. httpx - A next generation HTTP client for Python. requests - HTTP Requests for Humans. treq - Python requests like API built on top of Twisted's HTTP client. urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly. Hardware Libraries for programming with hardware. ino - Command line toolkit for working with Arduino . keyboard - Hook and simulate global keyboard events on Windows and Linux. mouse - Hook and simulate global mouse events on Windows and Linux. Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc. PyUserInput - A module for cross-platform control of the mouse and keyboard. scapy - A brilliant packet manipulation library. wifi - A Python library and command line tool for working with WiFi on Linux. Image Processing Libraries for manipulating images. hmap - Image histogram remapping. imgSeek - A project for searching a collection of images using visual similarity. nude.py - Nudity detection. pagan - Retro identicon (Avatar) generation based on input string and hash. pillow - Pillow is the friendly PIL fork. pyBarcode - Create barcodes in Python without needing PIL. pygram - Instagram-like image filters. python-qrcode - A pure Python QR Code generator. Quads - Computer art based on quadtrees. scikit-image - A Python library for (scientific) image processing. thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images. wand - Python bindings for MagickWand , C API for ImageMagick. Implementations Implementations of Python. CPython - Default, most widely used implementation of the Python programming language written in C. Cython - Optimizing Static Compiler for Python. CLPython - Implementation of the Python programming language written in Common Lisp. Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha). IronPython - Implementation of the Python programming language written in C#. Jython - Implementation of Python programming language written in Java for the JVM. MicroPython - A lean and efficient Python programming language implementation. Numba - Python JIT compiler to LLVM aimed at scientific Python. PeachPy - x86-64 assembler embedded in Python. Pyjion - A JIT for Python based upon CoreCLR. PyPy - A very fast and compliant implementation of the Python language. Pyston - A Python implementation using JIT techniques. Stackless Python - An enhanced version of the Python programming language. Interactive Interpreter Interactive Python interpreters (REPL). bpython - A fancy interface to the Python interpreter. Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively. awesome-jupyter ptpython - Advanced Python REPL built on top of the python-prompt-toolkit . Internationalization Libraries for working with i18n. Babel - An internationalization library for Python. PyICU - A wrapper of International Components for Unicode C++ library ( ICU ). Job Scheduler Libraries for scheduling jobs. APScheduler - A light but powerful in-process task scheduler that lets you schedule functions. django-schedule - A calendaring app for Django. doit - A task runner and build tool. gunnery - Multipurpose task execution tool for distributed systems with web-based interface. Joblib - A set of tools to provide lightweight pipelining in Python. Plan - Writing crontab file in Python like a charm. schedule - Python job scheduling for humans. Spiff - A powerful workflow engine implemented in pure Python. TaskFlow - A Python library that helps to make task execution easy, consistent and reliable. Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows. Logging Libraries for generating and working with logs. Eliot - Logging for complex distributed systems. logbook - Logging replacement for Python. logging - (Python standard library) Logging facility for Python. raven - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications. Machine Learning Libraries for Machine Learning. Also see awesome-machine-learning . H2O - Open Source Fast Scalable Machine Learning Platform. Metrics - Machine learning evaluation metrics. NuPIC - Numenta Platform for Intelligent Computing. scikit-learn - The most popular Python library for Machine Learning. Spark ML - Apache Spark 's scalable Machine Learning library. vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit . xgboost - A scalable, portable, and distributed gradient boosting library. Microsoft Windows Python programming on Microsoft Windows. Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder. pythonlibs - Unofficial Windows binaries for Python extension packages. PythonNet - Python Integration with the .NET Common Language Runtime (CLR). PyWin32 - Python Extensions for Windows. WinPython - Portable development environment for Windows 7/8. Miscellaneous Useful libraries or tools that don't fit in the categories above. blinker - A fast Python in-process signal/event dispatching system. boltons - A set of pure-Python utilities. itsdangerous - Various helpers to pass trusted data to untrusted environments. pluginbase - A simple but flexible plugin system for Python. tryton - A general purpose business framework. Natural Language Processing Libraries for working with human languages. General gensim - Topic Modeling for Humans. langid.py - Stand-alone language identification system. nltk - A leading platform for building Python programs to work with human language data. pattern - A web mining module. polyglot - Natural language pipeline supporting hundreds of languages. pytext - A natural language modeling framework based on PyTorch. PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research. spacy - A library for industrial-strength natural language processing in Python and Cython. Stanza - The Stanford NLP Group's official Python library, supporting 60+ languages. Chinese jieba - The most popular Chinese text segmentation library. pkuseg-python - A toolkit for Chinese word segmentation in various domains. snownlp - A library for processing Chinese text. funNLP - A collection of tools and datasets for Chinese NLP. Network Virtualization Tools and libraries for Virtual Networking and SDN (Software Defined Networking). mininet - A popular network emulator and API written in Python. napalm - Cross-vendor API to manipulate network devices. pox - A Python-based SDN control applications, such as OpenFlow SDN controllers. News Feed Libraries for building user's activities. django-activity-stream - Generating generic activity streams from the actions on your site. Stream Framework - Building news feed and notification systems using Cassandra and Redis. ORM Libraries that implement Object-Relational Mapping or data mapping techniques. Relational Databases Django Models - The Django ORM. SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper. awesome-sqlalchemy dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL. orator - The Orator ORM provides a simple yet beautiful ActiveRecord implementation. orm - An async ORM. peewee - A small, expressive ORM. pony - ORM that provides a generator-oriented interface to SQL. pydal - A pure Python Database Abstraction Layer. NoSQL Databases hot-redis - Rich Python data types for Redis. mongoengine - A Python Object-Document-Mapper for working with MongoDB. PynamoDB - A Pythonic interface for Amazon DynamoDB . redisco - A Python Library for Simple Models and Containers Persisted in Redis. Package Management Libraries for package and dependency management. pip - The package installer for Python. PyPI pip-tools - A set of tools to keep your pinned Python dependencies fresh. poetry - Python dependency management and packaging made easy. conda - Cross-platform, Python-agnostic binary package manager. Package Repositories Local PyPI repository server and proxies. warehouse - Next generation Python Package Repository (PyPI). bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA). devpi - PyPI server and packaging/testing/release tool. localshop - Local PyPI server (custom packages and auto-mirroring of pypi). Permissions Libraries that allow or deny users access to data or functionality. django-guardian - Implementation of per object permissions for Django 1.2+ django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database. Processes Libraries for starting and communicating with OS processes. delegator.py - Subprocesses for Humans 2.0. sarge - Yet another wrapper for subprocess. sh - A full-fledged subprocess replacement for Python. Recommender Systems Libraries for building recommender systems. annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage. fastFM - A library for Factorization Machines. implicit - A fast Python implementation of collaborative filtering for implicit datasets. libffm - A library for Field-aware Factorization Machine (FFM). lightfm - A Python implementation of a number of popular recommendation algorithms. spotlight - Deep recommender models using PyTorch. Surprise - A scikit for building and analyzing recommender systems. tensorrec - A Recommendation Engine Framework in TensorFlow. Refactoring Refactoring tools and libraries for Python Bicycle Repair Man - Bicycle Repair Man, a refactoring tool for Python. Bowler - Safe code refactoring for modern Python. Rope - Rope is a python refactoring library. RESTful API Libraries for building RESTful APIs. Django django-rest-framework - A powerful and flexible toolkit to build web APIs. django-tastypie - Creating delicious APIs for Django apps. Flask eve - REST API framework powered by Flask, MongoDB and good intentions. flask-api - Browsable Web APIs for Flask. flask-restful - Quickly building REST APIs for Flask. Pyramid cornice - A RESTful framework for Pyramid. Framework agnostic apistar - A smart Web API framework, designed for Python 3. falcon - A high-performance framework for building cloud APIs and web app backends. fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints. hug - A Python 3 framework for cleanly exposing APIs. sandman2 - Automated REST APIs for existing database-driven systems. sanic - A Python 3.6+ web server and web framework that's written to go fast. vibora - Fast, efficient and asynchronous Web framework inspired by Flask. Robotics Libraries for robotics. PythonRobotics - This is a compilation of various robotics algorithms with visualizations. rospy - This is a library for ROS (Robot Operating System). RPC Servers RPC-compatible servers. zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack . RPyC (Remote Python Call) - A transparent and symmetric RPC library for Python Science Libraries for scientific computing. Also see Python-for-Scientists astropy - A community Python library for Astronomy. bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis. bccb - Collection of useful code related to biological analysis. Biopython - Biopython is a set of freely available tools for biological computation. cclib - A library for parsing and interpreting the results of computational chemistry packages. Colour - Implementing a comprehensive number of colour theory transformations and algorithms. Karate Club - Unsupervised machine learning toolbox for graph structured data. NetworkX - A high-productivity software for complex networks. NIPY - A collection of neuroimaging toolkits. NumPy - A fundamental package for scientific computing with Python. Open Babel - A chemical toolbox designed to speak the many languages of chemical data. ObsPy - A Python toolbox for seismology. PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion. PyMC - Markov Chain Monte Carlo sampling toolkit. QuTiP - Quantum Toolbox in Python. RDKit - Cheminformatics and Machine Learning Software. SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering. statsmodels - Statistical modeling and econometrics in Python. SymPy - A Python library for symbolic mathematics. Zipline - A Pythonic algorithmic trading library. SimPy - A process-based discrete-event simulation framework. Search Libraries and software for indexing and performing search queries on data. elasticsearch-py - The official low-level Python client for Elasticsearch . elasticsearch-dsl-py - The official high-level Python client for Elasticsearch. django-haystack - Modular search for Django. pysolr - A lightweight Python wrapper for Apache Solr . whoosh - A fast, pure Python search engine library. Serialization Libraries for serializing complex data types marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes. pysimdjson - A Python bindings for simdjson . python-rapidjson - A Python wrapper around RapidJSON . ultrajson - A fast JSON decoder and encoder written in C with Python bindings. Serverless Frameworks Frameworks for developing serverless Python code. python-lambda - A toolkit for developing and deploying Python code in AWS Lambda. Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway. Specific Formats Processing Libraries for parsing and manipulating specific text formats. General tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML. Office openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files. pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files. python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files. python-pptx - Python library for creating and updating PowerPoint (.pptx) files. unoconv - Convert between any document format supported by LibreOffice/OpenOffice. XlsxWriter - A Python module for creating Excel .xlsx files. xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa. xlwt / xlrd - Writing and reading data and formatting information from Excel files. PDF PDFMiner - A tool for extracting information from PDF documents. PyPDF2 - A library capable of splitting, merging and transforming PDF pages. ReportLab - Allowing Rapid creation of rich PDF documents. Markdown Mistune - Fastest and full featured pure Python parsers of Markdown. Python-Markdown - A Python implementation of John Gruber\u2019s Markdown. YAML PyYAML - YAML implementations for Python. CSV csvkit - Utilities for converting to and working with CSV. Archive unp - A command line tool that can unpack archives easily. Static Site Generator Static site generator is a software that takes some text + templates as input and produces HTML files on the output. mkdocs - Markdown friendly documentation generator. pelican - Static site generator that supports Markdown and reST syntax. lektor - An easy to use static CMS and blog engine. nikola - A static website and blog generator. Tagging Libraries for tagging items. django-taggit - Simple tagging for Django. Task Queues Libraries for working with task queues. celery - An asynchronous task queue/job queue based on distributed message passing. huey - Little multi-threaded task queue. mrq - A distributed worker task queue in Python using Redis gevent. rq - Simple job queues for Python. Template Engine Libraries and tools for templating and lexing. Jinja2 - A modern and designer friendly templating language. Genshi - Python templating toolkit for generation of web-aware output. Mako - Hyperfast and lightweight templating for the Python platform. Testing Libraries for testing codebases and generating test data. Testing Frameworks pytest - A mature full-featured Python testing tool. hypothesis - Hypothesis is an advanced Quickcheck style property based testing library. nose2 - The successor to nose , based on `unittest2. Robot Framework - A generic test automation framework. unittest - (Python standard library) Unit testing framework. Test Runners green - A clean, colorful test runner. mamba - The definitive testing tool for Python. Born under the banner of BDD. tox - Auto builds and tests distributions in multiple Python versions GUI / Web Testing locust - Scalable user load testing tool written in Python. PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings. Selenium - Python bindings for Selenium WebDriver. sixpack - A language-agnostic A/B Testing framework. splinter - Open source tool for testing web applications. Mock mock - (Python standard library) A mocking and patching library. doublex - Powerful test doubles framework for Python. freezegun - Travel through time by mocking the datetime module. httmock - A mocking library for requests for Python 2.6+ and 3.2+. httpretty - HTTP request mock tool for Python. mocket - A socket mock framework with gevent/asyncio/SSL support. responses - A utility library for mocking out the requests Python library. VCR.py - Record and replay HTTP interactions on your tests. Object Factories factory_boy - A test fixtures replacement for Python. mixer - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc. model_mommy - Creating random fixtures for testing in Django. Code Coverage coverage - Code coverage measurement. Fake Data mimesis - is a Python library that help you generate fake data. fake2db - Fake database generator. faker - A Python package that generates fake data. radar - Generate random datetime / time. Text Processing Libraries for parsing and manipulating plain texts. General chardet - Python 2/3 compatible character encoding detector. difflib - (Python standard library) Helpers for computing deltas. ftfy - Makes Unicode text less broken and more consistent automagically. fuzzywuzzy - Fuzzy String Matching. Levenshtein - Fast computation of Levenshtein distance and string similarity. pangu.py - Paranoid text spacing. pyfiglet - An implementation of figlet written in Python. pypinyin - Convert Chinese hanzi (\u6f22\u5b57) to pinyin (\u62fc\u97f3). textdistance - Compute distance between sequences with 30+ algorithms. unidecode - ASCII transliterations of Unicode text. Slugify awesome-slugify - A Python slugify library that can preserve unicode. python-slugify - A Python slugify library that translates unicode to ASCII. unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency. Unique identifiers hashids - Implementation of hashids in Python. shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs. Parser ply - Implementation of lex and yacc parsing tools for Python. pygments - A generic syntax highlighter. pyparsing - A general purpose framework for generating parsers. python-nameparser - Parsing human names into their individual components. python-phonenumbers - Parsing, formatting, storing and validating international phone numbers. python-user-agents - Browser user agent parser. sqlparse - A non-validating SQL parser. Third-party APIs Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries . apache-libcloud - One Python library for all clouds. boto3 - Python interface to Amazon Web Services. django-wordpress - WordPress models and views for Django. facebook-sdk - Facebook Platform Python SDK. google-api-python-client - Google APIs Client Library for Python. gspread - Google Spreadsheets Python API. twython - A Python wrapper for the Twitter API. URL Manipulation Libraries for parsing URLs. furl - A small Python library that makes parsing and manipulating URLs easy. purl - A simple, immutable URL class with a clean API for interrogation and manipulation. pyshorteners - A pure Python URL shortening lib. webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks. Video Libraries for manipulating video and GIFs. vidgear - Most Powerful multi-threaded Video Processing framework. moviepy - A module for script-based movie editing with many formats, including animated GIFs. scikit-video - Video processing routines for SciPy. Web Asset Management Tools for managing, compressing and minifying website assets. django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file. django-pipeline - An asset packaging library for Django. django-storages - A collection of custom storage back ends for Django. fanstatic - Packages, optimizes, and serves static file dependencies as Python packages. fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP. flask-assets - Helps you integrate webassets into your Flask app. webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources. Web Content Extracting Libraries for extracting web contents. html2text - Convert HTML to Markdown-formatted text. lassie - Web Content Retrieval for Humans. micawber - A small library for extracting rich content from URLs. newspaper - News extraction, article extraction and content curation in Python. python-readability - Fast Python port of arc90's readability tool. requests-html - Pythonic HTML Parsing for Humans. sumy - A module for automatic summarization of text documents and HTML pages. textract - Extract text from any document, Word, PowerPoint, PDFs, etc. toapi - Every web site provides APIs. Web Crawling Libraries to automate web scraping. cola - A distributed crawling framework. feedparser - Universal feed parser. grab - Site scraping framework. MechanicalSoup - A Python library for automating interaction with websites. pyspider - A powerful spider system. robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser. scrapy - A fast high-level screen scraping and web crawling framework. portia - Visual scraping for Scrapy. Web Frameworks Traditional full stack web frameworks. Also see RESTful API Synchronous Django - The most popular web framework in Python. awesome-django Flask - A microframework for Python. awesome-flask Pyramid - A small, fast, down-to-earth, open source Python web framework. awesome-pyramid Masonite - The modern and developer centric Python web framework. Asynchronous Tornado - A web framework and asynchronous networking library. WebSocket Libraries for working with WebSocket. autobahn-python - WebSocket WAMP for Python on Twisted and asyncio . channels - Developer-friendly asynchrony for Django. websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity. WSGI Servers WSGI-compatible web servers. bjoern - Asynchronous, very fast and written in C. gunicorn - Pre-forked, partly written in C. uWSGI - A project aims at developing a full stack for building hosting services, written in C. waitress - Multi-threaded, powers Pyramid. werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects. Resources Where to discover new Python libraries. Podcasts From Python Import Podcast Podcast.init Python Bytes Python Testing Radio Free Python Talk Python To Me Test and Code The Real Python Podcast Twitter @codetengu @getpy @importpython @planetpython @pycoders @pypi @pythontrending @PythonWeekly @TalkPython @realpython Websites /r/CoolGithubProjects /r/Python Awesome Python @LibHunt Django Packages Full Stack Python Python Cheatsheet Python ZEEF Python \u5f00\u53d1\u793e\u533a Real Python Trending Python repositories on GitHub today \u0421\u043e\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u043e Python \u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0441\u0442\u043e\u0432 Pythonic News Weekly CodeTengu Weekly \u78bc\u5929\u72d7\u9031\u520a Import Python Newsletter Pycoder's Weekly Python Weekly Python Tricks Contributing Your contributions are always welcome! Please take a look at the contribution guidelines first. I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding :+1: to them. Pull requests will be merged when their votes reach 20 . If you have any question about this opinionated list, do not hesitate to contact me @vinta on Twitter or open an issue on GitHub.","title":"Awe_python"},{"location":"awe_python/#awesome-python","text":"A curated list of awesome Python frameworks, libraries, software and resources. Inspired by awesome-php . Awesome Python Admin Panels Algorithms and Design Patterns ASGI Servers Asynchronous Programming Audio Authentication Build Tools Built-in Classes Enhancement Caching ChatOps Tools CMS Code Analysis Command-line Interface Development Command-line Tools Compatibility Computer Vision Concurrency and Parallelism Configuration Cryptography Data Analysis Data Validation Data Visualization Database Drivers Database Date and Time Debugging Tools Deep Learning DevOps Tools Distributed Computing Distribution Documentation Downloader E-commerce Editor Plugins and IDEs Email Environment Management Files Foreign Function Interface Forms Functional Programming Game Development Geolocation GUI Development Hardware HTML Manipulation HTTP Clients Image Processing Implementations Interactive Interpreter Internationalization Job Scheduler Logging Machine Learning Miscellaneous Natural Language Processing Network Virtualization News Feed ORM Package Management Package Repositories Permissions Processes Recommender Systems Refactoring RESTful API Robotics RPC Servers Science Search Serialization Serverless Frameworks Specific Formats Processing Static Site Generator Tagging Task Queues Template Engine Testing Text Processing Third-party APIs URL Manipulation Video Web Asset Management Web Content Extracting Web Crawling Web Frameworks WebSocket WSGI Servers Resources Podcasts Twitter Websites Weekly Contributing","title":"Awesome Python"},{"location":"awe_python/#admin-panels","text":"Libraries for administrative interfaces. ajenti - The admin panel your servers deserve. django-grappelli - A jazzy skin for the Django Admin-Interface. django-jet - Modern responsive template for the Django admin interface with improved functionality. django-suit - Alternative Django Admin-Interface (free only for Non-commercial use). django-xadmin - Drop-in replacement of Django admin comes with lots of goodies. jet-bridge - Admin panel framework for any application with nice UI (ex Jet Django) flask-admin - Simple and extensible administrative interface framework for Flask. flower - Real-time monitor and web admin for Celery. wooey - A Django app which creates automatic web UIs for Python scripts.","title":"Admin Panels"},{"location":"awe_python/#algorithms-and-design-patterns","text":"Python implementation of algorithms and design patterns. algorithms - Minimal examples of data structures and algorithms in Python. PyPattyrn - A simple yet effective library for implementing common design patterns. python-ds - Clean and simple collection of data structure and algorithms in Python for coding interviews. python-patterns - A collection of design patterns in Python. sortedcontainers - Fast, pure-Python implementation of SortedList, SortedDict, and SortedSet types. transitions - A lightweight, object-oriented finite state machine implementation in Python.","title":"Algorithms and Design Patterns"},{"location":"awe_python/#asgi-servers","text":"ASGI-compatible web servers. uvicorn - Uvicorn is a lightning-fast ASGI server implementation, using uvloop and httptools.","title":"ASGI Servers"},{"location":"awe_python/#asynchronous-programming","text":"asyncio - (Python standard library) Asynchronous I/O, event loop, coroutines and tasks. awesome-asyncio uvloop - Ultra fast asyncio event loop. Twisted - An event-driven networking engine.","title":"Asynchronous Programming"},{"location":"awe_python/#audio","text":"Libraries for manipulating audio and its metadata. Audio audioread - Cross-library (GStreamer + Core Audio + MAD + FFmpeg) audio decoding. dejavu - Audio fingerprinting and recognition. matchering - A library for automated reference audio mastering. mingus - An advanced music theory and notation package with MIDI file and playback support. pyAudioAnalysis - Audio feature extraction, classification, segmentation and applications. pydub - Manipulate audio with a simple and easy high level interface. TimeSide - Open web audio processing framework. Metadata beets - A music library manager and MusicBrainz tagger. eyeD3 - A tool for working with audio files, specifically MP3 files containing ID3 metadata. mutagen - A Python module to handle audio metadata. tinytag - A library for reading music meta data of MP3, OGG, FLAC and Wave files.","title":"Audio"},{"location":"awe_python/#authentication","text":"Libraries for implementing authentications schemes. OAuth authlib - JavaScript Object Signing and Encryption draft implementation. django-allauth - Authentication app for Django that \"just works.\" django-oauth-toolkit - OAuth 2 goodies for Django. oauthlib - A generic and thorough implementation of the OAuth request-signing logic. python-oauth2 - A fully tested, abstract interface to creating OAuth clients and servers. python-social-auth - An easy-to-setup social authentication mechanism. JWT pyjwt - JSON Web Token implementation in Python. python-jose - A JOSE implementation in Python. python-jwt - A module for generating and verifying JSON Web Tokens.","title":"Authentication"},{"location":"awe_python/#build-tools","text":"Compile software from source code. BitBake - A make-like build tool for embedded Linux. buildout - A build system for creating, assembling and deploying applications from multiple parts. PlatformIO - A console tool to build code with different development platforms. pybuilder - A continuous build tool written in pure Python. SCons - A software construction tool.","title":"Build Tools"},{"location":"awe_python/#built-in-classes-enhancement","text":"Libraries for enhancing Python built-in classes. dataclasses - (Python standard library) Data classes. attrs - Replacement for __init__ , __eq__ , __repr__ , etc. boilerplate in class definitions. bidict - Efficient, Pythonic bidirectional map data structures and related functionality.. Box - Python dictionaries with advanced dot notation access. DottedDict - A library that provides a method of accessing lists and dicts with a dotted path notation.","title":"Built-in Classes Enhancement"},{"location":"awe_python/#cms","text":"Content Management Systems. wagtail - A Django content management system. django-cms - An Open source enterprise CMS based on the Django. feincms - One of the most advanced Content Management Systems built on Django. indico - A feature-rich event management system, made @ CERN . Kotti - A high-level, Pythonic web application framework built on Pyramid. mezzanine - A powerful, consistent, and flexible content management platform. plone - A CMS built on top of the open source application server Zope. quokka - Flexible, extensible, small CMS powered by Flask and MongoDB.","title":"CMS"},{"location":"awe_python/#caching","text":"Libraries for caching data. beaker - A WSGI middleware for sessions and caching. django-cache-machine - Automatic caching and invalidation for Django models. django-cacheops - A slick ORM cache with automatic granular event-driven invalidation. dogpile.cache - dogpile.cache is next generation replacement for Beaker made by same authors. HermesCache - Python caching library with tag-based invalidation and dogpile effect prevention. pylibmc - A Python wrapper around the libmemcached interface. python-diskcache - SQLite and file backed cache backend with faster lookups than memcached and redis.","title":"Caching"},{"location":"awe_python/#chatops-tools","text":"Libraries for chatbot development. errbot - The easiest and most popular chatbot to implement ChatOps.","title":"ChatOps Tools"},{"location":"awe_python/#code-analysis","text":"Tools of static analysis, linters and code quality checkers. Also see awesome-static-analysis . Code Analysis coala - Language independent and easily extendable code analysis application. code2flow - Turn your Python and JavaScript code into DOT flowcharts. prospector - A tool to analyse Python code. pycallgraph - A library that visualises the flow (call graph) of your Python application. vulture - A tool for finding and analysing dead Python code. Code Linters flake8 - A wrapper around pycodestyle , pyflakes and McCabe. awesome-flake8-extensions pylint - A fully customizable source code analyzer. pylama - A code audit tool for Python and JavaScript. wemake-python-styleguide - The strictest and most opinionated python linter ever. Code Formatters black - The uncompromising Python code formatter. yapf - Yet another Python code formatter from Google. Static Type Checkers, also see awesome-python-typing mypy - Check variable types during compile time. typeshed - Collection of library stubs for Python, with static types. pyre-check - Performant type checking. Static Type Annotations Generators MonkeyType - A system for Python that generates static type annotations by collecting runtime types","title":"Code Analysis"},{"location":"awe_python/#command-line-interface-development","text":"Libraries for building command-line applications. Command-line Application Development cement - CLI Application Framework for Python. click - A package for creating beautiful command line interfaces in a composable way. cliff - A framework for creating command-line programs with multi-level commands. docopt - Pythonic command line arguments parser. python-fire - A library for creating command line interfaces from absolutely any Python object. python-prompt-toolkit - A library for building powerful interactive command lines. Terminal Rendering asciimatics - A package to create full-screen text UIs (from interactive forms to ASCII animations). bashplotlib - Making basic plots in the terminal. colorama - Cross-platform colored terminal text. rich - Python library for rich text and beautiful formatting in the terminal. Also provides a great RichHandler log handler. tqdm - Fast, extensible progress bar for loops and CLI.","title":"Command-line Interface Development"},{"location":"awe_python/#command-line-tools","text":"Useful CLI-based tools for productivity. Productivity Tools cookiecutter - A command-line utility that creates projects from cookiecutters (project templates). doitlive - A tool for live presentations in the terminal. howdoi - Instant coding answers via the command line. Invoke - A tool for managing shell-oriented subprocesses and organizing executable Python code into CLI-invokable tasks. PathPicker - Select files out of bash output. percol - Adds flavor of interactive selection to the traditional pipe concept on UNIX. thefuck - Correcting your previous console command. tmuxp - A tmux session manager. try - A dead simple CLI to try out python packages - it's never been easier. CLI Enhancements httpie - A command line HTTP client, a user-friendly cURL replacement. kube-shell - An integrated shell for working with the Kubernetes CLI. mycli - MySQL CLI with autocompletion and syntax highlighting. pgcli - PostgreSQL CLI with autocompletion and syntax highlighting. iredis - Redis CLI with autocompletion and syntax highlighting. litecli - SQLite CLI with autocompletion and syntax highlighting. saws - A Supercharged aws-cli .","title":"Command-line Tools"},{"location":"awe_python/#compatibility","text":"Libraries for migrating from Python 2 to 3. python-future - The missing compatibility layer between Python 2 and Python 3. python-modernize - Modernizes Python code for eventual Python 3 migration. six - Python 2 and 3 compatibility utilities.","title":"Compatibility"},{"location":"awe_python/#computer-vision","text":"Libraries for Computer Vision. Kornia - Open Source Differentiable Computer Vision Library for PyTorch. OpenCV - Open Source Computer Vision Library. pytesseract - Another wrapper for Google Tesseract OCR . tesserocr - A simple, Pillow-friendly, wrapper around the tesseract-ocr API for OCR. SimpleCV - An open source framework for building computer vision applications.","title":"Computer Vision"},{"location":"awe_python/#concurrency-and-parallelism","text":"Libraries for concurrent and parallel execution. Also see awesome-asyncio . concurrent.futures - (Python standard library) A high-level interface for asynchronously executing callables. multiprocessing - (Python standard library) Process-based parallelism. eventlet - Asynchronous framework with WSGI support. gevent - A coroutine-based Python networking library that uses greenlet . uvloop - Ultra fast implementation of asyncio event loop on top of libuv . scoop - Scalable Concurrent Operations in Python.","title":"Concurrency and Parallelism"},{"location":"awe_python/#configuration","text":"Libraries for storing and parsing configuration options. configobj - INI file parser with validation. configparser - (Python standard library) INI file parser. profig - Config from multiple formats with value conversion. python-decouple - Strict separation of settings from code.","title":"Configuration"},{"location":"awe_python/#cryptography","text":"cryptography - A package designed to expose cryptographic primitives and recipes to Python developers. paramiko - The leading native Python SSHv2 protocol library. passlib - Secure password storage/hashing library, very high level. pynacl - Python binding to the Networking and Cryptography (NaCl) library.","title":"Cryptography"},{"location":"awe_python/#data-analysis","text":"Libraries for data analyzing. Blaze - NumPy and Pandas interface to Big Data. Open Mining - Business Intelligence (BI) in Pandas interface. Orange - Data mining, data visualization, analysis and machine learning through visual programming or scripts. Pandas - A library providing high-performance, easy-to-use data structures and data analysis tools. Optimus - Agile Data Science Workflows made easy with PySpark.","title":"Data Analysis"},{"location":"awe_python/#data-validation","text":"Libraries for validating data. Used for forms in many cases. Cerberus - A lightweight and extensible data validation library. colander - Validating and deserializing data obtained via XML, JSON, an HTML form post. jsonschema - An implementation of JSON Schema for Python. schema - A library for validating Python data structures. Schematics - Data Structure Validation. valideer - Lightweight extensible data validation and adaptation library. voluptuous - A Python data validation library.","title":"Data Validation"},{"location":"awe_python/#data-visualization","text":"Libraries for visualizing data. Also see awesome-javascript . Altair - Declarative statistical visualization library for Python. Bokeh - Interactive Web Plotting for Python. bqplot - Interactive Plotting Library for the Jupyter Notebook Dash - Built on top of Flask, React and Plotly aimed at analytical web applications. awesome-dash diagrams - Diagram as Code. plotnine - A grammar of graphics for Python based on ggplot2. Matplotlib - A Python 2D plotting library. Pygal - A Python SVG Charts Creator. PyGraphviz - Python interface to Graphviz . PyQtGraph - Interactive and realtime 2D/3D/Image plotting and science/engineering widgets. Seaborn - Statistical data visualization using Matplotlib. VisPy - High-performance scientific visualization based on OpenGL.","title":"Data Visualization"},{"location":"awe_python/#database","text":"Databases implemented in Python. pickleDB - A simple and lightweight key-value store for Python. tinydb - A tiny, document-oriented database. ZODB - A native object database for Python. A key-value and object graph database.","title":"Database"},{"location":"awe_python/#database-drivers","text":"Libraries for connecting and operating databases. MySQL - awesome-mysql mysqlclient - MySQL connector with Python 3 support ( mysql-python fork). PyMySQL - A pure Python MySQL driver compatible to mysql-python. PostgreSQL - awesome-postgres psycopg2 - The most popular PostgreSQL adapter for Python. queries - A wrapper of the psycopg2 library for interacting with PostgreSQL. Other Relational Databases pymssql - A simple database interface to Microsoft SQL Server. SuperSQLite - A supercharged SQLite library built on top of apsw . NoSQL Databases cassandra-driver - The Python Driver for Apache Cassandra. happybase - A developer-friendly library for Apache HBase. kafka-python - The Python client for Apache Kafka. py2neo - A client library and toolkit for working with Neo4j. pymongo - The official Python client for MongoDB. redis-py - The Python client for Redis. Asynchronous Clients motor - The async Python driver for MongoDB.","title":"Database Drivers"},{"location":"awe_python/#date-and-time","text":"Libraries for working with dates and times. Arrow - A Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. Chronyk - A Python 3 library for parsing human-written times and dates. dateutil - Extensions to the standard Python datetime module. delorean - A library for clearing up the inconvenient truths that arise dealing with datetimes. moment - A Python library for dealing with dates/times. Inspired by Moment.js . Pendulum - Python datetimes made easy. PyTime - An easy-to-use Python module which aims to operate date/time/datetime by string. pytz - World timezone definitions, modern and historical. Brings the tz database into Python. when.py - Providing user-friendly functions to help perform common date and time actions. maya - Datetimes for Humans.","title":"Date and Time"},{"location":"awe_python/#debugging-tools","text":"Libraries for debugging code. pdb-like Debugger ipdb - IPython-enabled pdb . pdb++ - Another drop-in replacement for pdb. pudb - A full-screen, console-based Python debugger. wdb - An improbable web debugger through WebSockets. Tracing lptrace - strace for Python programs. manhole - Debugging UNIX socket connections and present the stacktraces for all threads and an interactive prompt. pyringe - Debugger capable of attaching to and injecting code into Python processes. python-hunter - A flexible code tracing toolkit. Profiler line_profiler - Line-by-line profiling. memory_profiler - Monitor Memory usage of Python code. profiling - An interactive Python profiler. py-spy - A sampling profiler for Python programs. Written in Rust. pyflame - A ptracing profiler For Python. vprof - Visual Python profiler. Others icecream - Inspect variables, expressions, and program execution with a single, simple function call. django-debug-toolbar - Display various debug information for Django. django-devserver - A drop-in replacement for Django's runserver. flask-debugtoolbar - A port of the django-debug-toolbar to flask. pyelftools - Parsing and analyzing ELF files and DWARF debugging information.","title":"Debugging Tools"},{"location":"awe_python/#deep-learning","text":"Frameworks for Neural Networks and Deep Learning. Also see awesome-deep-learning . caffe - A fast open framework for deep learning.. keras - A high-level neural networks library and capable of running on top of either TensorFlow or Theano. mxnet - A deep learning framework designed for both efficiency and flexibility. pytorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration. SerpentAI - Game agent framework. Use any video game as a deep learning sandbox. tensorflow - The most popular Deep Learning framework created by Google. Theano - A library for fast numerical computation.","title":"Deep Learning"},{"location":"awe_python/#devops-tools","text":"Software and libraries for DevOps. ansible - A radically simple IT automation platform. cloudinit - A multi-distribution package that handles early initialization of a cloud instance. cuisine - Chef-like functionality for Fabric. docker-compose - Fast, isolated development environments using Docker . fabric - A simple, Pythonic tool for remote execution and deployment. fabtools - Tools for writing awesome Fabric files. honcho - A Python clone of Foreman , for managing Procfile-based applications. OpenStack - Open source software for building private and public clouds. pexpect - Controlling interactive programs in a pseudo-terminal like GNU expect. psutil - A cross-platform process and system utilities module. saltstack - Infrastructure automation and management system. supervisor - Supervisor process control system for UNIX.","title":"DevOps Tools"},{"location":"awe_python/#distributed-computing","text":"Frameworks and libraries for Distributed Computing. Batch Processing PySpark - Apache Spark Python API. dask - A flexible parallel computing library for analytic computing. luigi - A module that helps you build complex pipelines of batch jobs. mrjob - Run MapReduce jobs on Hadoop or Amazon Web Services. Ray - A system for parallel and distributed Python that unifies the machine learning ecosystem. Stream Processing faust - A stream processing library, porting the ideas from Kafka Streams to Python. streamparse - Run Python code against real-time streams of data via Apache Storm .","title":"Distributed Computing"},{"location":"awe_python/#distribution","text":"Libraries to create packaged executables for release distribution. dh-virtualenv - Build and distribute a virtualenv as a Debian package. Nuitka - Compile scripts, modules, packages to an executable or extension module. py2app - Freezes Python scripts (Mac OS X). py2exe - Freezes Python scripts (Windows). PyInstaller - Converts Python programs into stand-alone executables (cross-platform). pynsist - A tool to build Windows installers, installers bundle Python itself. pyarmor - A tool used to obfuscate python scripts, bind obfuscated scripts to fixed machine or expire obfuscated scripts. shiv - A command line utility for building fully self-contained zipapps (PEP 441), but with all their dependencies included.","title":"Distribution"},{"location":"awe_python/#documentation","text":"Libraries for generating project documentation. sphinx - Python Documentation generator. awesome-sphinxdoc pdoc - Epydoc replacement to auto generate API documentation for Python libraries. pycco - The literate-programming-style documentation generator.","title":"Documentation"},{"location":"awe_python/#downloader","text":"Libraries for downloading. s3cmd - A command line tool for managing Amazon S3 and CloudFront. s4cmd - Super S3 command line tool, good for higher performance. you-get - A YouTube/Youku/Niconico video downloader written in Python 3. youtube-dl - A small command-line program to download videos from YouTube. akshare - A financial data interface library, built for human beings!","title":"Downloader"},{"location":"awe_python/#e-commerce","text":"Frameworks and libraries for e-commerce and payments. alipay - Unofficial Alipay API for Python. Cartridge - A shopping cart app built using the Mezzanine. django-oscar - An open-source e-commerce framework for Django. django-shop - A Django based shop system. merchant - A Django app to accept payments from various payment processors. money - Money class with optional CLDR-backed locale-aware formatting and an extensible currency exchange. python-currencies - Display money format and its filthy currencies. forex-python - Foreign exchange rates, Bitcoin price index and currency conversion. saleor - An e-commerce storefront for Django. shoop - An open source E-Commerce platform based on Django.","title":"E-commerce"},{"location":"awe_python/#editor-plugins-and-ides","text":"Emacs elpy - Emacs Python Development Environment. Sublime Text anaconda - Anaconda turns your Sublime Text 3 in a full featured Python development IDE. SublimeJEDI - A Sublime Text plugin to the awesome auto-complete library Jedi. Vim jedi-vim - Vim bindings for the Jedi auto-completion library for Python. python-mode - An all in one plugin for turning Vim into a Python IDE. YouCompleteMe - Includes Jedi -based completion engine for Python. Visual Studio PTVS - Python Tools for Visual Studio. Visual Studio Code Python - The official VSCode extension with rich support for Python. IDE PyCharm - Commercial Python IDE by JetBrains. Has free community edition available. spyder - Open Source Python IDE.","title":"Editor Plugins and IDEs"},{"location":"awe_python/#email","text":"Libraries for sending and parsing email. Mail Servers modoboa - A mail hosting and management platform including a modern Web UI. salmon - A Python Mail Server. Clients imbox - Python IMAP for Humans. yagmail - Yet another Gmail/SMTP client. Others flanker - An email address and Mime parsing library. mailer - High-performance extensible mail delivery framework.","title":"Email"},{"location":"awe_python/#environment-management","text":"Libraries for Python version and virtual environment management. pyenv - Simple Python version management. virtualenv - A tool to create isolated Python environments.","title":"Environment Management"},{"location":"awe_python/#files","text":"Libraries for file manipulation and MIME type detection. mimetypes - (Python standard library) Map filenames to MIME types. path.py - A module wrapper for os.path . pathlib - (Python standard library) An cross-platform, object-oriented path library. PyFilesystem2 - Python's filesystem abstraction layer. python-magic - A Python interface to the libmagic file type identification library. Unipath - An object-oriented approach to file/directory operations. watchdog - API and shell utilities to monitor file system events.","title":"Files"},{"location":"awe_python/#foreign-function-interface","text":"Libraries for providing foreign function interface. cffi - Foreign Function Interface for Python calling C code. ctypes - (Python standard library) Foreign Function Interface for Python calling C code. PyCUDA - A Python wrapper for Nvidia's CUDA API. SWIG - Simplified Wrapper and Interface Generator.","title":"Foreign Function Interface"},{"location":"awe_python/#forms","text":"Libraries for working with forms. Deform - Python HTML form generation library influenced by the formish form generation library. django-bootstrap3 - Bootstrap 3 integration with Django. django-bootstrap4 - Bootstrap 4 integration with Django. django-crispy-forms - A Django app which lets you create beautiful forms in a very elegant and DRY way. django-remote-forms - A platform independent Django form serializer. WTForms - A flexible forms validation and rendering library.","title":"Forms"},{"location":"awe_python/#functional-programming","text":"Functional Programming with Python. Coconut - A variant of Python built for simple, elegant, Pythonic functional programming. fn.py - Functional programming in Python: implementation of missing features to enjoy FP. funcy - A fancy and practical functional tools. more-itertools - More routines for operating on iterables, beyond itertools . returns - A set of type-safe monads, tranformers, and composition utilities. Toolz - A collection of functional utilities for iterators, functions, and dictionaries. CyToolz - Cython implementation of Toolz : High performance functional utilities.","title":"Functional Programming"},{"location":"awe_python/#gui-development","text":"Libraries for working with graphical user interface applications. curses - Built-in wrapper for ncurses used to create terminal GUI applications. Eel - A library for making simple Electron-like offline HTML/JS GUI apps. enaml - Creating beautiful user-interfaces with Declarative Syntax like QML. Flexx - Flexx is a pure Python toolkit for creating GUI's, that uses web technology for its rendering. Gooey - Turn command line programs into a full GUI application with one line. kivy - A library for creating NUI applications, running on Windows, Linux, Mac OS X, Android and iOS. pyglet - A cross-platform windowing and multimedia library for Python. PyGObject - Python Bindings for GLib/GObject/GIO/GTK+ (GTK+3). PyQt - Python bindings for the Qt cross-platform application and UI framework. PySimpleGUI - Wrapper for tkinter, Qt, WxPython and Remi. pywebview - A lightweight cross-platform native wrapper around a webview component. Tkinter - Tkinter is Python's de-facto standard GUI package. Toga - A Python native, OS native GUI toolkit. urwid - A library for creating terminal GUI applications with strong support for widgets, events, rich colors, etc. wxPython - A blending of the wxWidgets C++ class library with the Python.","title":"GUI Development"},{"location":"awe_python/#graphql","text":"Libraries for working with GraphQL. tartiflette - SDL-first GraphQL engine implementation for Python 3.6+ and asyncio. tartiflette-aiohttp - An aiohttp -based wrapper for Tartiflette to expose GraphQL APIs over HTTP. tartiflette-asgi - ASGI support for the Tartiflette GraphQL engine.","title":"GraphQL"},{"location":"awe_python/#game-development","text":"Awesome game development libraries. Cocos2d - cocos2d is a framework for building 2D games, demos, and other graphical/interactive applications. Harfang3D - Python framework for 3D, VR and game development. Panda3D - 3D game engine developed by Disney. Pygame - Pygame is a set of Python modules designed for writing games. PyOgre - Python bindings for the Ogre 3D render engine, can be used for games, simulations, anything 3D. PyOpenGL - Python ctypes bindings for OpenGL and it's related APIs. PySDL2 - A ctypes based wrapper for the SDL2 library. RenPy - A Visual Novel engine.","title":"Game Development"},{"location":"awe_python/#geolocation","text":"Libraries for geocoding addresses and working with latitudes and longitudes. django-countries - A Django app that provides a country field for models and forms. GeoDjango - A world-class geographic web framework. GeoIP - Python API for MaxMind GeoIP Legacy Database. geojson - Python bindings and utilities for GeoJSON. geopy - Python Geocoding Toolbox. pygeoip - Pure Python GeoIP API.","title":"Geolocation"},{"location":"awe_python/#html-manipulation","text":"Libraries for working with HTML and XML. BeautifulSoup - Providing Pythonic idioms for iterating, searching, and modifying HTML or XML. bleach - A whitelist-based HTML sanitization and text linkification library. cssutils - A CSS library for Python. html5lib - A standards-compliant library for parsing and serializing HTML documents and fragments. lxml - A very fast, easy-to-use and versatile library for handling HTML and XML. MarkupSafe - Implements a XML/HTML/XHTML Markup safe string for Python. pyquery - A jQuery-like library for parsing HTML. untangle - Converts XML documents to Python objects for easy access. WeasyPrint - A visual rendering engine for HTML and CSS that can export to PDF. xmldataset - Simple XML Parsing. xmltodict - Working with XML feel like you are working with JSON.","title":"HTML Manipulation"},{"location":"awe_python/#http-clients","text":"Libraries for working with HTTP. grequests - requests + gevent for asynchronous HTTP requests. httplib2 - Comprehensive HTTP client library. httpx - A next generation HTTP client for Python. requests - HTTP Requests for Humans. treq - Python requests like API built on top of Twisted's HTTP client. urllib3 - A HTTP library with thread-safe connection pooling, file post support, sanity friendly.","title":"HTTP Clients"},{"location":"awe_python/#hardware","text":"Libraries for programming with hardware. ino - Command line toolkit for working with Arduino . keyboard - Hook and simulate global keyboard events on Windows and Linux. mouse - Hook and simulate global mouse events on Windows and Linux. Pingo - Pingo provides a uniform API to program devices like the Raspberry Pi, pcDuino, Intel Galileo, etc. PyUserInput - A module for cross-platform control of the mouse and keyboard. scapy - A brilliant packet manipulation library. wifi - A Python library and command line tool for working with WiFi on Linux.","title":"Hardware"},{"location":"awe_python/#image-processing","text":"Libraries for manipulating images. hmap - Image histogram remapping. imgSeek - A project for searching a collection of images using visual similarity. nude.py - Nudity detection. pagan - Retro identicon (Avatar) generation based on input string and hash. pillow - Pillow is the friendly PIL fork. pyBarcode - Create barcodes in Python without needing PIL. pygram - Instagram-like image filters. python-qrcode - A pure Python QR Code generator. Quads - Computer art based on quadtrees. scikit-image - A Python library for (scientific) image processing. thumbor - A smart imaging service. It enables on-demand crop, re-sizing and flipping of images. wand - Python bindings for MagickWand , C API for ImageMagick.","title":"Image Processing"},{"location":"awe_python/#implementations","text":"Implementations of Python. CPython - Default, most widely used implementation of the Python programming language written in C. Cython - Optimizing Static Compiler for Python. CLPython - Implementation of the Python programming language written in Common Lisp. Grumpy - More compiler than interpreter as more powerful CPython2.7 replacement (alpha). IronPython - Implementation of the Python programming language written in C#. Jython - Implementation of Python programming language written in Java for the JVM. MicroPython - A lean and efficient Python programming language implementation. Numba - Python JIT compiler to LLVM aimed at scientific Python. PeachPy - x86-64 assembler embedded in Python. Pyjion - A JIT for Python based upon CoreCLR. PyPy - A very fast and compliant implementation of the Python language. Pyston - A Python implementation using JIT techniques. Stackless Python - An enhanced version of the Python programming language.","title":"Implementations"},{"location":"awe_python/#interactive-interpreter","text":"Interactive Python interpreters (REPL). bpython - A fancy interface to the Python interpreter. Jupyter Notebook (IPython) - A rich toolkit to help you make the most out of using Python interactively. awesome-jupyter ptpython - Advanced Python REPL built on top of the python-prompt-toolkit .","title":"Interactive Interpreter"},{"location":"awe_python/#internationalization","text":"Libraries for working with i18n. Babel - An internationalization library for Python. PyICU - A wrapper of International Components for Unicode C++ library ( ICU ).","title":"Internationalization"},{"location":"awe_python/#job-scheduler","text":"Libraries for scheduling jobs. APScheduler - A light but powerful in-process task scheduler that lets you schedule functions. django-schedule - A calendaring app for Django. doit - A task runner and build tool. gunnery - Multipurpose task execution tool for distributed systems with web-based interface. Joblib - A set of tools to provide lightweight pipelining in Python. Plan - Writing crontab file in Python like a charm. schedule - Python job scheduling for humans. Spiff - A powerful workflow engine implemented in pure Python. TaskFlow - A Python library that helps to make task execution easy, consistent and reliable. Airflow - Airflow is a platform to programmatically author, schedule and monitor workflows.","title":"Job Scheduler"},{"location":"awe_python/#logging","text":"Libraries for generating and working with logs. Eliot - Logging for complex distributed systems. logbook - Logging replacement for Python. logging - (Python standard library) Logging facility for Python. raven - Python client for Sentry, a log/error tracking, crash reporting and aggregation platform for web applications.","title":"Logging"},{"location":"awe_python/#machine-learning","text":"Libraries for Machine Learning. Also see awesome-machine-learning . H2O - Open Source Fast Scalable Machine Learning Platform. Metrics - Machine learning evaluation metrics. NuPIC - Numenta Platform for Intelligent Computing. scikit-learn - The most popular Python library for Machine Learning. Spark ML - Apache Spark 's scalable Machine Learning library. vowpal_porpoise - A lightweight Python wrapper for Vowpal Wabbit . xgboost - A scalable, portable, and distributed gradient boosting library.","title":"Machine Learning"},{"location":"awe_python/#microsoft-windows","text":"Python programming on Microsoft Windows. Python(x,y) - Scientific-applications-oriented Python Distribution based on Qt and Spyder. pythonlibs - Unofficial Windows binaries for Python extension packages. PythonNet - Python Integration with the .NET Common Language Runtime (CLR). PyWin32 - Python Extensions for Windows. WinPython - Portable development environment for Windows 7/8.","title":"Microsoft Windows"},{"location":"awe_python/#miscellaneous","text":"Useful libraries or tools that don't fit in the categories above. blinker - A fast Python in-process signal/event dispatching system. boltons - A set of pure-Python utilities. itsdangerous - Various helpers to pass trusted data to untrusted environments. pluginbase - A simple but flexible plugin system for Python. tryton - A general purpose business framework.","title":"Miscellaneous"},{"location":"awe_python/#natural-language-processing","text":"Libraries for working with human languages. General gensim - Topic Modeling for Humans. langid.py - Stand-alone language identification system. nltk - A leading platform for building Python programs to work with human language data. pattern - A web mining module. polyglot - Natural language pipeline supporting hundreds of languages. pytext - A natural language modeling framework based on PyTorch. PyTorch-NLP - A toolkit enabling rapid deep learning NLP prototyping for research. spacy - A library for industrial-strength natural language processing in Python and Cython. Stanza - The Stanford NLP Group's official Python library, supporting 60+ languages. Chinese jieba - The most popular Chinese text segmentation library. pkuseg-python - A toolkit for Chinese word segmentation in various domains. snownlp - A library for processing Chinese text. funNLP - A collection of tools and datasets for Chinese NLP.","title":"Natural Language Processing"},{"location":"awe_python/#network-virtualization","text":"Tools and libraries for Virtual Networking and SDN (Software Defined Networking). mininet - A popular network emulator and API written in Python. napalm - Cross-vendor API to manipulate network devices. pox - A Python-based SDN control applications, such as OpenFlow SDN controllers.","title":"Network Virtualization"},{"location":"awe_python/#news-feed","text":"Libraries for building user's activities. django-activity-stream - Generating generic activity streams from the actions on your site. Stream Framework - Building news feed and notification systems using Cassandra and Redis.","title":"News Feed"},{"location":"awe_python/#orm","text":"Libraries that implement Object-Relational Mapping or data mapping techniques. Relational Databases Django Models - The Django ORM. SQLAlchemy - The Python SQL Toolkit and Object Relational Mapper. awesome-sqlalchemy dataset - Store Python dicts in a database - works with SQLite, MySQL, and PostgreSQL. orator - The Orator ORM provides a simple yet beautiful ActiveRecord implementation. orm - An async ORM. peewee - A small, expressive ORM. pony - ORM that provides a generator-oriented interface to SQL. pydal - A pure Python Database Abstraction Layer. NoSQL Databases hot-redis - Rich Python data types for Redis. mongoengine - A Python Object-Document-Mapper for working with MongoDB. PynamoDB - A Pythonic interface for Amazon DynamoDB . redisco - A Python Library for Simple Models and Containers Persisted in Redis.","title":"ORM"},{"location":"awe_python/#package-management","text":"Libraries for package and dependency management. pip - The package installer for Python. PyPI pip-tools - A set of tools to keep your pinned Python dependencies fresh. poetry - Python dependency management and packaging made easy. conda - Cross-platform, Python-agnostic binary package manager.","title":"Package Management"},{"location":"awe_python/#package-repositories","text":"Local PyPI repository server and proxies. warehouse - Next generation Python Package Repository (PyPI). bandersnatch - PyPI mirroring tool provided by Python Packaging Authority (PyPA). devpi - PyPI server and packaging/testing/release tool. localshop - Local PyPI server (custom packages and auto-mirroring of pypi).","title":"Package Repositories"},{"location":"awe_python/#permissions","text":"Libraries that allow or deny users access to data or functionality. django-guardian - Implementation of per object permissions for Django 1.2+ django-rules - A tiny but powerful app providing object-level permissions to Django, without requiring a database.","title":"Permissions"},{"location":"awe_python/#processes","text":"Libraries for starting and communicating with OS processes. delegator.py - Subprocesses for Humans 2.0. sarge - Yet another wrapper for subprocess. sh - A full-fledged subprocess replacement for Python.","title":"Processes"},{"location":"awe_python/#recommender-systems","text":"Libraries for building recommender systems. annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage. fastFM - A library for Factorization Machines. implicit - A fast Python implementation of collaborative filtering for implicit datasets. libffm - A library for Field-aware Factorization Machine (FFM). lightfm - A Python implementation of a number of popular recommendation algorithms. spotlight - Deep recommender models using PyTorch. Surprise - A scikit for building and analyzing recommender systems. tensorrec - A Recommendation Engine Framework in TensorFlow.","title":"Recommender Systems"},{"location":"awe_python/#refactoring","text":"Refactoring tools and libraries for Python Bicycle Repair Man - Bicycle Repair Man, a refactoring tool for Python. Bowler - Safe code refactoring for modern Python. Rope - Rope is a python refactoring library.","title":"Refactoring"},{"location":"awe_python/#restful-api","text":"Libraries for building RESTful APIs. Django django-rest-framework - A powerful and flexible toolkit to build web APIs. django-tastypie - Creating delicious APIs for Django apps. Flask eve - REST API framework powered by Flask, MongoDB and good intentions. flask-api - Browsable Web APIs for Flask. flask-restful - Quickly building REST APIs for Flask. Pyramid cornice - A RESTful framework for Pyramid. Framework agnostic apistar - A smart Web API framework, designed for Python 3. falcon - A high-performance framework for building cloud APIs and web app backends. fastapi - A modern, fast, web framework for building APIs with Python 3.6+ based on standard Python type hints. hug - A Python 3 framework for cleanly exposing APIs. sandman2 - Automated REST APIs for existing database-driven systems. sanic - A Python 3.6+ web server and web framework that's written to go fast. vibora - Fast, efficient and asynchronous Web framework inspired by Flask.","title":"RESTful API"},{"location":"awe_python/#robotics","text":"Libraries for robotics. PythonRobotics - This is a compilation of various robotics algorithms with visualizations. rospy - This is a library for ROS (Robot Operating System).","title":"Robotics"},{"location":"awe_python/#rpc-servers","text":"RPC-compatible servers. zeroRPC - zerorpc is a flexible RPC implementation based on ZeroMQ and MessagePack . RPyC (Remote Python Call) - A transparent and symmetric RPC library for Python","title":"RPC Servers"},{"location":"awe_python/#science","text":"Libraries for scientific computing. Also see Python-for-Scientists astropy - A community Python library for Astronomy. bcbio-nextgen - Providing best-practice pipelines for fully automated high throughput sequencing analysis. bccb - Collection of useful code related to biological analysis. Biopython - Biopython is a set of freely available tools for biological computation. cclib - A library for parsing and interpreting the results of computational chemistry packages. Colour - Implementing a comprehensive number of colour theory transformations and algorithms. Karate Club - Unsupervised machine learning toolbox for graph structured data. NetworkX - A high-productivity software for complex networks. NIPY - A collection of neuroimaging toolkits. NumPy - A fundamental package for scientific computing with Python. Open Babel - A chemical toolbox designed to speak the many languages of chemical data. ObsPy - A Python toolbox for seismology. PyDy - Short for Python Dynamics, used to assist with workflow in the modeling of dynamic motion. PyMC - Markov Chain Monte Carlo sampling toolkit. QuTiP - Quantum Toolbox in Python. RDKit - Cheminformatics and Machine Learning Software. SciPy - A Python-based ecosystem of open-source software for mathematics, science, and engineering. statsmodels - Statistical modeling and econometrics in Python. SymPy - A Python library for symbolic mathematics. Zipline - A Pythonic algorithmic trading library. SimPy - A process-based discrete-event simulation framework.","title":"Science"},{"location":"awe_python/#search","text":"Libraries and software for indexing and performing search queries on data. elasticsearch-py - The official low-level Python client for Elasticsearch . elasticsearch-dsl-py - The official high-level Python client for Elasticsearch. django-haystack - Modular search for Django. pysolr - A lightweight Python wrapper for Apache Solr . whoosh - A fast, pure Python search engine library.","title":"Search"},{"location":"awe_python/#serialization","text":"Libraries for serializing complex data types marshmallow - A lightweight library for converting complex objects to and from simple Python datatypes. pysimdjson - A Python bindings for simdjson . python-rapidjson - A Python wrapper around RapidJSON . ultrajson - A fast JSON decoder and encoder written in C with Python bindings.","title":"Serialization"},{"location":"awe_python/#serverless-frameworks","text":"Frameworks for developing serverless Python code. python-lambda - A toolkit for developing and deploying Python code in AWS Lambda. Zappa - A tool for deploying WSGI applications on AWS Lambda and API Gateway.","title":"Serverless Frameworks"},{"location":"awe_python/#specific-formats-processing","text":"Libraries for parsing and manipulating specific text formats. General tablib - A module for Tabular Datasets in XLS, CSV, JSON, YAML. Office openpyxl - A library for reading and writing Excel 2010 xlsx/xlsm/xltx/xltm files. pyexcel - Providing one API for reading, manipulating and writing csv, ods, xls, xlsx and xlsm files. python-docx - Reads, queries and modifies Microsoft Word 2007/2008 docx files. python-pptx - Python library for creating and updating PowerPoint (.pptx) files. unoconv - Convert between any document format supported by LibreOffice/OpenOffice. XlsxWriter - A Python module for creating Excel .xlsx files. xlwings - A BSD-licensed library that makes it easy to call Python from Excel and vice versa. xlwt / xlrd - Writing and reading data and formatting information from Excel files. PDF PDFMiner - A tool for extracting information from PDF documents. PyPDF2 - A library capable of splitting, merging and transforming PDF pages. ReportLab - Allowing Rapid creation of rich PDF documents. Markdown Mistune - Fastest and full featured pure Python parsers of Markdown. Python-Markdown - A Python implementation of John Gruber\u2019s Markdown. YAML PyYAML - YAML implementations for Python. CSV csvkit - Utilities for converting to and working with CSV. Archive unp - A command line tool that can unpack archives easily.","title":"Specific Formats Processing"},{"location":"awe_python/#static-site-generator","text":"Static site generator is a software that takes some text + templates as input and produces HTML files on the output. mkdocs - Markdown friendly documentation generator. pelican - Static site generator that supports Markdown and reST syntax. lektor - An easy to use static CMS and blog engine. nikola - A static website and blog generator.","title":"Static Site Generator"},{"location":"awe_python/#tagging","text":"Libraries for tagging items. django-taggit - Simple tagging for Django.","title":"Tagging"},{"location":"awe_python/#task-queues","text":"Libraries for working with task queues. celery - An asynchronous task queue/job queue based on distributed message passing. huey - Little multi-threaded task queue. mrq - A distributed worker task queue in Python using Redis gevent. rq - Simple job queues for Python.","title":"Task Queues"},{"location":"awe_python/#template-engine","text":"Libraries and tools for templating and lexing. Jinja2 - A modern and designer friendly templating language. Genshi - Python templating toolkit for generation of web-aware output. Mako - Hyperfast and lightweight templating for the Python platform.","title":"Template Engine"},{"location":"awe_python/#testing","text":"Libraries for testing codebases and generating test data. Testing Frameworks pytest - A mature full-featured Python testing tool. hypothesis - Hypothesis is an advanced Quickcheck style property based testing library. nose2 - The successor to nose , based on `unittest2. Robot Framework - A generic test automation framework. unittest - (Python standard library) Unit testing framework. Test Runners green - A clean, colorful test runner. mamba - The definitive testing tool for Python. Born under the banner of BDD. tox - Auto builds and tests distributions in multiple Python versions GUI / Web Testing locust - Scalable user load testing tool written in Python. PyAutoGUI - PyAutoGUI is a cross-platform GUI automation Python module for human beings. Selenium - Python bindings for Selenium WebDriver. sixpack - A language-agnostic A/B Testing framework. splinter - Open source tool for testing web applications. Mock mock - (Python standard library) A mocking and patching library. doublex - Powerful test doubles framework for Python. freezegun - Travel through time by mocking the datetime module. httmock - A mocking library for requests for Python 2.6+ and 3.2+. httpretty - HTTP request mock tool for Python. mocket - A socket mock framework with gevent/asyncio/SSL support. responses - A utility library for mocking out the requests Python library. VCR.py - Record and replay HTTP interactions on your tests. Object Factories factory_boy - A test fixtures replacement for Python. mixer - Another fixtures replacement. Supported Django, Flask, SQLAlchemy, Peewee and etc. model_mommy - Creating random fixtures for testing in Django. Code Coverage coverage - Code coverage measurement. Fake Data mimesis - is a Python library that help you generate fake data. fake2db - Fake database generator. faker - A Python package that generates fake data. radar - Generate random datetime / time.","title":"Testing"},{"location":"awe_python/#text-processing","text":"Libraries for parsing and manipulating plain texts. General chardet - Python 2/3 compatible character encoding detector. difflib - (Python standard library) Helpers for computing deltas. ftfy - Makes Unicode text less broken and more consistent automagically. fuzzywuzzy - Fuzzy String Matching. Levenshtein - Fast computation of Levenshtein distance and string similarity. pangu.py - Paranoid text spacing. pyfiglet - An implementation of figlet written in Python. pypinyin - Convert Chinese hanzi (\u6f22\u5b57) to pinyin (\u62fc\u97f3). textdistance - Compute distance between sequences with 30+ algorithms. unidecode - ASCII transliterations of Unicode text. Slugify awesome-slugify - A Python slugify library that can preserve unicode. python-slugify - A Python slugify library that translates unicode to ASCII. unicode-slugify - A slugifier that generates unicode slugs with Django as a dependency. Unique identifiers hashids - Implementation of hashids in Python. shortuuid - A generator library for concise, unambiguous and URL-safe UUIDs. Parser ply - Implementation of lex and yacc parsing tools for Python. pygments - A generic syntax highlighter. pyparsing - A general purpose framework for generating parsers. python-nameparser - Parsing human names into their individual components. python-phonenumbers - Parsing, formatting, storing and validating international phone numbers. python-user-agents - Browser user agent parser. sqlparse - A non-validating SQL parser.","title":"Text Processing"},{"location":"awe_python/#third-party-apis","text":"Libraries for accessing third party services APIs. Also see List of Python API Wrappers and Libraries . apache-libcloud - One Python library for all clouds. boto3 - Python interface to Amazon Web Services. django-wordpress - WordPress models and views for Django. facebook-sdk - Facebook Platform Python SDK. google-api-python-client - Google APIs Client Library for Python. gspread - Google Spreadsheets Python API. twython - A Python wrapper for the Twitter API.","title":"Third-party APIs"},{"location":"awe_python/#url-manipulation","text":"Libraries for parsing URLs. furl - A small Python library that makes parsing and manipulating URLs easy. purl - A simple, immutable URL class with a clean API for interrogation and manipulation. pyshorteners - A pure Python URL shortening lib. webargs - A friendly library for parsing HTTP request arguments with built-in support for popular web frameworks.","title":"URL Manipulation"},{"location":"awe_python/#video","text":"Libraries for manipulating video and GIFs. vidgear - Most Powerful multi-threaded Video Processing framework. moviepy - A module for script-based movie editing with many formats, including animated GIFs. scikit-video - Video processing routines for SciPy.","title":"Video"},{"location":"awe_python/#web-asset-management","text":"Tools for managing, compressing and minifying website assets. django-compressor - Compresses linked and inline JavaScript or CSS into a single cached file. django-pipeline - An asset packaging library for Django. django-storages - A collection of custom storage back ends for Django. fanstatic - Packages, optimizes, and serves static file dependencies as Python packages. fileconveyor - A daemon to detect and sync files to CDNs, S3 and FTP. flask-assets - Helps you integrate webassets into your Flask app. webassets - Bundles, optimizes, and manages unique cache-busting URLs for static resources.","title":"Web Asset Management"},{"location":"awe_python/#web-content-extracting","text":"Libraries for extracting web contents. html2text - Convert HTML to Markdown-formatted text. lassie - Web Content Retrieval for Humans. micawber - A small library for extracting rich content from URLs. newspaper - News extraction, article extraction and content curation in Python. python-readability - Fast Python port of arc90's readability tool. requests-html - Pythonic HTML Parsing for Humans. sumy - A module for automatic summarization of text documents and HTML pages. textract - Extract text from any document, Word, PowerPoint, PDFs, etc. toapi - Every web site provides APIs.","title":"Web Content Extracting"},{"location":"awe_python/#web-crawling","text":"Libraries to automate web scraping. cola - A distributed crawling framework. feedparser - Universal feed parser. grab - Site scraping framework. MechanicalSoup - A Python library for automating interaction with websites. pyspider - A powerful spider system. robobrowser - A simple, Pythonic library for browsing the web without a standalone web browser. scrapy - A fast high-level screen scraping and web crawling framework. portia - Visual scraping for Scrapy.","title":"Web Crawling"},{"location":"awe_python/#web-frameworks","text":"Traditional full stack web frameworks. Also see RESTful API Synchronous Django - The most popular web framework in Python. awesome-django Flask - A microframework for Python. awesome-flask Pyramid - A small, fast, down-to-earth, open source Python web framework. awesome-pyramid Masonite - The modern and developer centric Python web framework. Asynchronous Tornado - A web framework and asynchronous networking library.","title":"Web Frameworks"},{"location":"awe_python/#websocket","text":"Libraries for working with WebSocket. autobahn-python - WebSocket WAMP for Python on Twisted and asyncio . channels - Developer-friendly asynchrony for Django. websockets - A library for building WebSocket servers and clients with a focus on correctness and simplicity.","title":"WebSocket"},{"location":"awe_python/#wsgi-servers","text":"WSGI-compatible web servers. bjoern - Asynchronous, very fast and written in C. gunicorn - Pre-forked, partly written in C. uWSGI - A project aims at developing a full stack for building hosting services, written in C. waitress - Multi-threaded, powers Pyramid. werkzeug - A WSGI utility library for Python that powers Flask and can easily be embedded into your own projects.","title":"WSGI Servers"},{"location":"awe_python/#resources","text":"Where to discover new Python libraries.","title":"Resources"},{"location":"awe_python/#podcasts","text":"From Python Import Podcast Podcast.init Python Bytes Python Testing Radio Free Python Talk Python To Me Test and Code The Real Python Podcast","title":"Podcasts"},{"location":"awe_python/#twitter","text":"@codetengu @getpy @importpython @planetpython @pycoders @pypi @pythontrending @PythonWeekly @TalkPython @realpython","title":"Twitter"},{"location":"awe_python/#websites","text":"/r/CoolGithubProjects /r/Python Awesome Python @LibHunt Django Packages Full Stack Python Python Cheatsheet Python ZEEF Python \u5f00\u53d1\u793e\u533a Real Python Trending Python repositories on GitHub today \u0421\u043e\u043e\u0431\u0449\u0435\u0441\u0442\u0432\u043e Python \u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0441\u0442\u043e\u0432 Pythonic News","title":"Websites"},{"location":"awe_python/#weekly","text":"CodeTengu Weekly \u78bc\u5929\u72d7\u9031\u520a Import Python Newsletter Pycoder's Weekly Python Weekly Python Tricks","title":"Weekly"},{"location":"awe_python/#contributing","text":"Your contributions are always welcome! Please take a look at the contribution guidelines first. I will keep some pull requests open if I'm not sure whether those libraries are awesome, you could vote for them by adding :+1: to them. Pull requests will be merged when their votes reach 20 . If you have any question about this opinionated list, do not hesitate to contact me @vinta on Twitter or open an issue on GitHub.","title":"Contributing"},{"location":"awe_spider/","text":"awesome-spider \u6536\u96c6\u5404\u79cd\u722c\u866b \uff08\u9ed8\u8ba4\u722c\u866b\u8bed\u8a00\u4e3a python\uff09, \u6b22\u8fce\u5927\u5bb6 \u63d0 pr \u6216 issue, \u6536\u96c6\u811a\u672c\u89c1\u6b64\u9879\u76ee github-search warning: \u722c\u866b\u6709\u65f6\u6548\u6027\uff0c\u5927\u90e8\u5206\u5df2\u6ca1\u6cd5\u8fd0\u884c A \u6697\u7f51\u722c\u866b(Go) \u7231\u4e1dAPP\u56fe\u7247\u722c\u866b B Bilibili \u7528\u6237 Bilibili \u89c6\u9891 Bilibili \u5c0f\u89c6\u9891 Bing\u7f8e\u56fe\u722c\u866b B\u7ad9760\u4e07\u89c6\u9891\u4fe1\u606f\u722c\u866b \u535a\u5ba2\u56ed(node.js) \u767e\u5ea6\u767e\u79d1(node.js) \u5317\u90ae\u4eba\u6c34\u6728\u6e05\u534e\u62db\u8058 \u767e\u5ea6\u4e91\u7f51\u76d8 \u7409\u7483\u795e\u793e\u722c\u866b Boss \u76f4\u8058 \u8d1d\u58f3\u7f51\u627e\u623f\u722c\u866b C cnblog caoliu 1024 D \u8c46\u74e3\u8bfb\u4e66 \u8c46\u74e3\u722c\u866b\u96c6 \u8c46\u74e3\u5bb3\u7f9e\u7ec4 \u8c46\u74e3\u56fe\u4e66\u5e7f\u5ea6\u722c\u53d6 DNS\u8bb0\u5f55\u548c\u5b50\u57df\u540d DHT\u7f51\u7edc\u78c1\u529b\u79cd\u5b50\u722c\u866b \u6296\u97f3 \u6296\u97f3\u63a8\u8350 E E\u7ec5\u58eb G Girl-atlas girl13 github trending Github \u4ed3\u5e93\u53ca\u7528\u6237\u5206\u6790\u722c\u866b \u56fd\u5bb6\u7edf\u8ba1\u7528\u533a\u5212\u4ee3\u7801\u548c\u57ce\u4e61\u5212\u5206\u4ee3\u7801\u722c\u866b H HDOJ\u722c\u866b I Instagram INC500 \u4e16\u754c5000\u5f3a\u722c\u866b J \u4eac\u4e1c \u4eac\u4e1c\u641c\u7d22+\u8bc4\u8bba \u4eac\u4e1c\u5546\u54c1+\u8bc4\u8bba \u673a\u7968 \u714e\u86cb\u59b9\u7eb8 \u714e\u86cb\u59b9\u7eb8selenium\u7248\u672c \u4eca\u65e5\u5934\u6761\uff0c\u7f51\u6613\uff0c\u817e\u8baf\u7b49\u65b0\u95fb \u8ba1\u7b97\u673a\u4e66\u7c4d\u63a7\u56fe\u4e66 JK (\u5236\u670d\u5199\u771f) \u722c\u866b K \u770b\u77e5\u4e4e \u8bfe\u7a0b\u683c\u5b50\u6821\u82b1\u699c konachan L \u94fe\u5bb6 \u94fe\u5bb6\u6210\u4ea4\u5728\u552e\u5728\u79df\u623f\u6e90 \u62c9\u52fe \u7089\u77f3\u4f20\u8bf4 leetcode \u9886\u82f1\u9500\u552e\u5bfc\u822a\u5668\u722c\u866b LinkedInSalesNavigator M \u9a6c\u8702\u7a9d \u7528\u6237\u8db3\u8ff9 MyCar \u6f2b\u753b\u55b5 \u4e00\u952e\u4e0b\u8f7d\u6f2b\u753b~ MM131\u6027\u611f\u7f8e\u5973\u5199\u771f\u56fe\u5168\u722c\u53d6 \u7f8e\u5973\u5199\u771f\u5957\u56fe\u722c\u866b \uff08\u4e00\uff09 \uff08\u4e8c\uff09 \uff08\u4e09\uff09 \u59b9\u5b50\u56fe \u732b\u773c\u7f51\u7535\u5f71\u8bc4\u5206 N \u65b0\u95fb\u76d1\u63a7 \u4f60\u597d\u6c61\u554a O ofo\u5171\u4eab\u5355\u8f66\u722c\u866b P Pixiv PornHub packtpub 91porn Q QQ\u7a7a\u95f4 QQ \u7fa4 \u6e05\u534e\u5927\u5b66\u7f51\u7edc\u5b66\u5802\u722c\u866b \u53bb\u54ea\u513f \u524d\u7a0b\u65e0\u5fe7Python\u62db\u8058\u5c97\u4f4d\u4fe1\u606f\u722c\u53d6\u5206\u6790 qqzhpt\u7f8e\u5973\u5199\u771f\u722c\u866b/\u6279\u91cf\u4e0b\u8f7d R \u4eba\u4eba\u5f71\u89c6 RSS \u722c\u866b rosi \u59b9\u5b50\u56fe reddit \u58c1\u7eb8 reddit S soundcloud Stackoverflow 100\u4e07\u95ee\u7b54\u722c\u866b Shadowsocks \u8d26\u53f7\u722c\u866b spider163 \u7f51\u6613\u4e91\u97f3\u4e50\u722c\u866b \u65f6\u5149\u7f51\u7535\u5f71\u6570\u636e\u548c\u6d77\u62a5\u722c\u866b T tumblr \u4e0b\u8f7dtumblr\u559c\u6b22\u5185\u5bb9 TuShare \u5929\u732b\u53cc12\u722c\u866b Taobao mm Tmall \u5973\u6027\u6587\u80f8\u5c3a\u7801\u722c\u866b \u6dd8\u5b9d\u76f4\u64ad\u5f39\u5e55\u722c\u866b(node) \u5929\u6daf\u8bba\u575b\u6587\u7ae0 \u5929\u773c\u67e5\u722c\u866b V Youtube\u5b57\u5e55\u4e0b\u8f7d \u89c6\u9891\u4fe1\u606f\u722c\u866b \u7535\u5f71\u7f51\u7ad9 W \u4e4c\u4e91\u516c\u5f00\u6f0f\u6d1e \u5fae\u4fe1\u516c\u4f17\u53f7 \u201c\u4ee3\u7406\u201d\u65b9\u5f0f\u6293\u53d6\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0 \u7f51\u6613\u65b0\u95fb \u7f51\u6613\u7cbe\u5f69\u8bc4\u8bba \u5fae\u535a\u4e3b\u9898\u641c\u7d22\u5206\u6790 \u7f51\u6613\u4e91\u97f3\u4e50 \u65b0.\u7f51\u6613\u70ed\u8bc4 \u552f\u54c1\u4f1a\u5546\u54c1 X \u96ea\u7403\u80a1\u7968\u4fe1\u606f(java) \u65b0\u6d6a\u5fae\u535a \u65b0\u6d6a\u5fae\u535a\u5206\u5e03\u5f0f\u722c\u866b \u5fc3\u7075\u6bd2\u9e21\u6c64 \u95f2\u9c7c\u6700\u65b0\u5546\u54c1\u722c\u53d6 Y \u82f1\u7f8e\u5267 TV (node.js) Z ZOL \u624b\u673a\u58c1\u7eb8\u722c\u866b \u77e5\u4e4e(python) \u77e5\u4e4e(php) \u77e5\u7f51 \u77e5\u4e4e\u59b9\u5b50 \u81ea\u5982\u5b9e\u65f6\u623f\u6e90\u63d0\u9192 \u4e2d\u56fd\u5927\u9646\u9ad8\u6821\u5217\u8868\u722c\u866b \u7ad9\u9177\uff08zcool.com.cn\uff09\u56fe\u7247\u722c\u866b # 80s \u5f71\u89c6\u8d44\u6e90\u722c\u866b - JianSo_Movie \u5176\u4ed6 \u5404\u79cd\u722c\u866b DHT \u722c\u866b SimDHT p2pspider","title":"Awe_spider"},{"location":"awe_spider/#awesome-spider","text":"\u6536\u96c6\u5404\u79cd\u722c\u866b \uff08\u9ed8\u8ba4\u722c\u866b\u8bed\u8a00\u4e3a python\uff09, \u6b22\u8fce\u5927\u5bb6 \u63d0 pr \u6216 issue, \u6536\u96c6\u811a\u672c\u89c1\u6b64\u9879\u76ee github-search warning: \u722c\u866b\u6709\u65f6\u6548\u6027\uff0c\u5927\u90e8\u5206\u5df2\u6ca1\u6cd5\u8fd0\u884c","title":"awesome-spider"},{"location":"awe_spider/#a","text":"\u6697\u7f51\u722c\u866b(Go) \u7231\u4e1dAPP\u56fe\u7247\u722c\u866b","title":"A"},{"location":"awe_spider/#b","text":"Bilibili \u7528\u6237 Bilibili \u89c6\u9891 Bilibili \u5c0f\u89c6\u9891 Bing\u7f8e\u56fe\u722c\u866b B\u7ad9760\u4e07\u89c6\u9891\u4fe1\u606f\u722c\u866b \u535a\u5ba2\u56ed(node.js) \u767e\u5ea6\u767e\u79d1(node.js) \u5317\u90ae\u4eba\u6c34\u6728\u6e05\u534e\u62db\u8058 \u767e\u5ea6\u4e91\u7f51\u76d8 \u7409\u7483\u795e\u793e\u722c\u866b Boss \u76f4\u8058 \u8d1d\u58f3\u7f51\u627e\u623f\u722c\u866b","title":"B"},{"location":"awe_spider/#c","text":"cnblog caoliu 1024","title":"C"},{"location":"awe_spider/#d","text":"\u8c46\u74e3\u8bfb\u4e66 \u8c46\u74e3\u722c\u866b\u96c6 \u8c46\u74e3\u5bb3\u7f9e\u7ec4 \u8c46\u74e3\u56fe\u4e66\u5e7f\u5ea6\u722c\u53d6 DNS\u8bb0\u5f55\u548c\u5b50\u57df\u540d DHT\u7f51\u7edc\u78c1\u529b\u79cd\u5b50\u722c\u866b \u6296\u97f3 \u6296\u97f3\u63a8\u8350","title":"D"},{"location":"awe_spider/#e","text":"E\u7ec5\u58eb","title":"E"},{"location":"awe_spider/#g","text":"Girl-atlas girl13 github trending Github \u4ed3\u5e93\u53ca\u7528\u6237\u5206\u6790\u722c\u866b \u56fd\u5bb6\u7edf\u8ba1\u7528\u533a\u5212\u4ee3\u7801\u548c\u57ce\u4e61\u5212\u5206\u4ee3\u7801\u722c\u866b","title":"G"},{"location":"awe_spider/#h","text":"HDOJ\u722c\u866b","title":"H"},{"location":"awe_spider/#i","text":"Instagram INC500 \u4e16\u754c5000\u5f3a\u722c\u866b","title":"I"},{"location":"awe_spider/#j","text":"\u4eac\u4e1c \u4eac\u4e1c\u641c\u7d22+\u8bc4\u8bba \u4eac\u4e1c\u5546\u54c1+\u8bc4\u8bba \u673a\u7968 \u714e\u86cb\u59b9\u7eb8 \u714e\u86cb\u59b9\u7eb8selenium\u7248\u672c \u4eca\u65e5\u5934\u6761\uff0c\u7f51\u6613\uff0c\u817e\u8baf\u7b49\u65b0\u95fb \u8ba1\u7b97\u673a\u4e66\u7c4d\u63a7\u56fe\u4e66 JK (\u5236\u670d\u5199\u771f) \u722c\u866b","title":"J"},{"location":"awe_spider/#k","text":"\u770b\u77e5\u4e4e \u8bfe\u7a0b\u683c\u5b50\u6821\u82b1\u699c konachan","title":"K"},{"location":"awe_spider/#l","text":"\u94fe\u5bb6 \u94fe\u5bb6\u6210\u4ea4\u5728\u552e\u5728\u79df\u623f\u6e90 \u62c9\u52fe \u7089\u77f3\u4f20\u8bf4 leetcode \u9886\u82f1\u9500\u552e\u5bfc\u822a\u5668\u722c\u866b LinkedInSalesNavigator","title":"L"},{"location":"awe_spider/#m","text":"\u9a6c\u8702\u7a9d \u7528\u6237\u8db3\u8ff9 MyCar \u6f2b\u753b\u55b5 \u4e00\u952e\u4e0b\u8f7d\u6f2b\u753b~ MM131\u6027\u611f\u7f8e\u5973\u5199\u771f\u56fe\u5168\u722c\u53d6 \u7f8e\u5973\u5199\u771f\u5957\u56fe\u722c\u866b \uff08\u4e00\uff09 \uff08\u4e8c\uff09 \uff08\u4e09\uff09 \u59b9\u5b50\u56fe \u732b\u773c\u7f51\u7535\u5f71\u8bc4\u5206","title":"M"},{"location":"awe_spider/#n","text":"\u65b0\u95fb\u76d1\u63a7 \u4f60\u597d\u6c61\u554a","title":"N"},{"location":"awe_spider/#o","text":"ofo\u5171\u4eab\u5355\u8f66\u722c\u866b","title":"O"},{"location":"awe_spider/#p","text":"Pixiv PornHub packtpub 91porn","title":"P"},{"location":"awe_spider/#q","text":"QQ\u7a7a\u95f4 QQ \u7fa4 \u6e05\u534e\u5927\u5b66\u7f51\u7edc\u5b66\u5802\u722c\u866b \u53bb\u54ea\u513f \u524d\u7a0b\u65e0\u5fe7Python\u62db\u8058\u5c97\u4f4d\u4fe1\u606f\u722c\u53d6\u5206\u6790 qqzhpt\u7f8e\u5973\u5199\u771f\u722c\u866b/\u6279\u91cf\u4e0b\u8f7d","title":"Q"},{"location":"awe_spider/#r","text":"\u4eba\u4eba\u5f71\u89c6 RSS \u722c\u866b rosi \u59b9\u5b50\u56fe reddit \u58c1\u7eb8 reddit","title":"R"},{"location":"awe_spider/#s","text":"soundcloud Stackoverflow 100\u4e07\u95ee\u7b54\u722c\u866b Shadowsocks \u8d26\u53f7\u722c\u866b spider163 \u7f51\u6613\u4e91\u97f3\u4e50\u722c\u866b \u65f6\u5149\u7f51\u7535\u5f71\u6570\u636e\u548c\u6d77\u62a5\u722c\u866b","title":"S"},{"location":"awe_spider/#t","text":"tumblr \u4e0b\u8f7dtumblr\u559c\u6b22\u5185\u5bb9 TuShare \u5929\u732b\u53cc12\u722c\u866b Taobao mm Tmall \u5973\u6027\u6587\u80f8\u5c3a\u7801\u722c\u866b \u6dd8\u5b9d\u76f4\u64ad\u5f39\u5e55\u722c\u866b(node) \u5929\u6daf\u8bba\u575b\u6587\u7ae0 \u5929\u773c\u67e5\u722c\u866b","title":"T"},{"location":"awe_spider/#v","text":"Youtube\u5b57\u5e55\u4e0b\u8f7d \u89c6\u9891\u4fe1\u606f\u722c\u866b \u7535\u5f71\u7f51\u7ad9","title":"V"},{"location":"awe_spider/#w","text":"\u4e4c\u4e91\u516c\u5f00\u6f0f\u6d1e \u5fae\u4fe1\u516c\u4f17\u53f7 \u201c\u4ee3\u7406\u201d\u65b9\u5f0f\u6293\u53d6\u5fae\u4fe1\u516c\u4f17\u53f7\u6587\u7ae0 \u7f51\u6613\u65b0\u95fb \u7f51\u6613\u7cbe\u5f69\u8bc4\u8bba \u5fae\u535a\u4e3b\u9898\u641c\u7d22\u5206\u6790 \u7f51\u6613\u4e91\u97f3\u4e50 \u65b0.\u7f51\u6613\u70ed\u8bc4 \u552f\u54c1\u4f1a\u5546\u54c1","title":"W"},{"location":"awe_spider/#x","text":"\u96ea\u7403\u80a1\u7968\u4fe1\u606f(java) \u65b0\u6d6a\u5fae\u535a \u65b0\u6d6a\u5fae\u535a\u5206\u5e03\u5f0f\u722c\u866b \u5fc3\u7075\u6bd2\u9e21\u6c64 \u95f2\u9c7c\u6700\u65b0\u5546\u54c1\u722c\u53d6","title":"X"},{"location":"awe_spider/#y","text":"\u82f1\u7f8e\u5267 TV (node.js)","title":"Y"},{"location":"awe_spider/#z","text":"ZOL \u624b\u673a\u58c1\u7eb8\u722c\u866b \u77e5\u4e4e(python) \u77e5\u4e4e(php) \u77e5\u7f51 \u77e5\u4e4e\u59b9\u5b50 \u81ea\u5982\u5b9e\u65f6\u623f\u6e90\u63d0\u9192 \u4e2d\u56fd\u5927\u9646\u9ad8\u6821\u5217\u8868\u722c\u866b \u7ad9\u9177\uff08zcool.com.cn\uff09\u56fe\u7247\u722c\u866b","title":"Z"},{"location":"awe_spider/#35","text":"80s \u5f71\u89c6\u8d44\u6e90\u722c\u866b - JianSo_Movie","title":"#"},{"location":"awe_spider/#_1","text":"\u5404\u79cd\u722c\u866b DHT \u722c\u866b SimDHT p2pspider","title":"\u5176\u4ed6"},{"location":"awesome-mysql/","text":"Awesome MySQL \u8fd9\u662f\u4e00\u4e2aMySQL\u8f6f\u4ef6\u5e93\u548c\u8d44\u6e90\u5217\u8868\u6e05\u5355\u3002\u90e8\u5206\u8d44\u6e90\u641c\u96c6\u6765\u6e90\u4e8e shlomi-noach/awesome-mysql \uff0c\u5b83\u7684 \u4e2d\u6587\u7ffb\u8bd1 \u5728\u8fd9\u91cc\uff0c\u6ca1\u6709\u5b8c\u5168fork\u8fc7\u6765\uff0c\u6ca1\u6709\u5b8c\u5168\u62f7\u8d1d\uff0c\u662f\u56e0\u4e3a\u90e8\u5206\u6392\u7248\u6bd4\u8f83\u4e11\u964b\uff0c\u90e8\u5206\u5de5\u5177\u6162\u6162\u641c\u96c6\u7406\u89e3\u3002 \u8bf4\u660e \u8868\u793a \u5f00\u6e90 \u6807\u793a\uff0c\u70b9\u51fb\u8fdb\u5165 \u5f00\u6e90 \u4ed3\u5e93\uff1b \u8868\u793a \u514d\u8d39 \u6807\u793a\uff0c\u6216\u8005\u4e2a\u4eba \u514d\u8d39 \uff1b \u8868\u793a \u70ed\u95e8 \u7684\u8d44\u6e90\u6807\u793a\uff1b \u8868\u793a \u63a8\u8350 \u8d44\u6e90\u6807\u793a\uff1b \u8868\u793a \u5fc5\u5907 \u8d44\u6e90\u6807\u793a\uff1b \u8868\u793a\u5f3a\u70c8\u63a8\u8350\u7684\uff0c\u661f\u661f\u7684\u6570\u91cf\u6765\u8868\u8fbe\u5f3a\u70c8\u7684\u7a0b\u5ea6\uff1b \u76ee\u5f55 \u624b\u518c\u6587\u6863 \u5206\u6790\u5de5\u5177 GUI \u670d\u52a1\u5668 \u5907\u4efd \u5b98\u65b9\u8d44\u6599 \u4f18\u79c0\u6587\u7ae0 \u624b\u518c\u6587\u6863 \u8fd9\u91cc\u662f\u4e00\u4e9b\u975e\u5b98\u65b9\u7684\u624b\u518c\u6587\u6863 http://www.w3school.com.cn/sql/index.asp http://www.1keydata.com/cn/sql/sql-count.php http://www.runoob.com/mysql/mysql-tutorial.html \u5206\u6790\u5de5\u5177 \u6027\u80fd\uff0c\u7ed3\u6784\u548c\u6570\u636e\u5206\u6790\u5de5\u5177 Anemometer - \u4e00\u4e2a SQL \u6162\u67e5\u8be2\u76d1\u63a7\u5668\u3002 innodb-ruby - \u4e00\u4e2a\u5bf9 InooDB \u683c\u5f0f\u6587\u4ef6\u7684\u89e3\u6790\u5668\uff0c\u7528\u4e8e Ruby \u8bed\u8a00\u3002 innotop - \u4e00\u4e2a\u5177\u5907\u591a\u79cd\u7279\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684 MySQL \u7248 'top' \u5de5\u5177\u3002 pstop - \u4e00\u4e2a\u9488\u5bf9 MySQL \u7684\u7c7b top \u7a0b\u5e8f\uff0c\u7528\u4e8e\u6536\u96c6\uff0c\u6c47\u603b\u4ee5\u53ca\u5c55\u793a\u6765\u81ea performance_schema \u7684\u4fe1\u606f\u3002 mysql-statsd - \u4e00\u4e2a\u6536\u96c6 MySQL \u4fe1\u606f\u7684 Python \u5b88\u62a4\u8fdb\u7a0b\uff0c\u5e76\u901a\u8fc7 StatsD \u53d1\u9001\u5230 Graphite\u3002 sql.ohrz.net - SQL\u8bed\u53e5\u89e3\u8bfb\u670d\u52a1\uff0c\u652f\u6301\u9ad8\u4eae\u663e\u793a\u89e3\u6790\u7ed3\u679c\u4e2d\u8868\u540d\u3001\u5b57\u6bb5\u540d\u3001\u5b50\u67e5\u8be2\u8bed\u53e5\u7b49\u5728\u539f\u6587\u4e2d\u5bf9\u5e94\u7684\u4f4d\u7f6e\u3002 GUI \u8fd9\u91cc\u662f\u641c\u96c6\u7684\u4e00\u4e9b MySQL \u7684\u5ba2\u6237\u7aef\uff0c\u4e5f\u6709\u547d\u4ee4\u884c\u5ba2\u6237\u7aef\u3002 TablePlus - \u652f\u6301PostgreSQL\uff0cMySQL\uff0cRedShift\uff0cMariaDB ...\u5404\u79cd\u6570\u636e\u5e93\u7684\u9ad8\u989c\u503c\u5ba2\u6237\u7aef\u3002 Sequel Pro - \u4e00\u4e2aMySQL\u6570\u636e\u5e93\u7ba1\u7406\u8f6f\u4ef6\u3002 MySQL Workbench - MySQL\u6570\u636e\u5e93\u5b98\u65b9\u7ba1\u7406\u8f6f\u4ef6\u3002\u63d0\u4f9b\u7ed9\u6570\u636e\u5e93\u7ba1\u7406\u5458\u548c\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u5efa\u6a21\u7684\u96c6\u6210\u5de5\u5177\u73af\u5883\uff1bSQL \u5f00\u53d1\uff1b\u6570\u636e\u5e93\u7ba1\u7406\u3002 Navicat - \u540c\u6837\u8de8\u5e73\u53f0\uff0c\u540c\u65f6\u652f\u6301\u591a\u4e2a\u6570\u636e\u5e93\u7cfb\u7edf\uff08MySQL\u3001SQL Server\u3001Oracle\uff09\u3002 ElectroCRUD - MySQL\u6570\u636e\u5e93CRUD\u5e94\u7528\u7a0b\u5e8f\u3002 Chrome MySQL Admin - \u4e00\u4e2aChrome\u63d2\u4ef6\uff0c\u662fMySQL\u5f00\u53d1\u7684\u8de8\u5e73\u53f0\u3001\u53ef\u89c6\u5316\u6570\u636e\u5e93\u5de5\u5177\u3002 Adminer - \u4e00\u4e2a PHP \u7f16\u5199\u7684\u6570\u636e\u5e93\u7ba1\u7406\u5de5\u5177\u3002 HeidiSQL - Windows \u4e0b\u7684 MySQL \u56fe\u5f62\u5316\u7ba1\u7406\u5de5\u5177\u3002 phpMyAdmin - \u4e00\u4e2a PHP \u5199\u6210\u7684\u5f00\u6e90\u8f6f\u4ef6\uff0c\u610f\u56fe\u5bf9 web \u4e0a\u7684 MySQL \u8fdb\u884c\u7ba1\u7406\u3002 mycli - \u4e3aMySQL\u547d\u4ee4\u884c\u5ba2\u6237\u7aef\uff0c\u63d0\u4f9b\u8bed\u6cd5\u9ad8\u4eae\u548c\u63d0\u793a\u529f\u80fd\u7684\u5de5\u5177\uff01 dbdiagram - \u521b\u5efa\u6570\u636e\u5e93\u7684\u5b9e\u4f53-\u5173\u7cfb\u56fe\u7684\u5de5\u5177\u3002 \u670d\u52a1\u5668 MySQL\u670d\u52a1\u5668\u7684\u884d\u751f\u54c1 MariaDB - MySQL server \u7684\u4e00\u4e2a\u7531\u793e\u533a\u5f00\u53d1\u7684\u5206\u652f\u3002 MySQL Server MySQL Cluster - Oracle \u5b98\u65b9\u7684 MySQL server \u548c MySQL \u96c6\u7fa4\u5206\u5e03\u3002 Percona Server - \u4e00\u4e2a\u52a0\u5f3a\u7248\u7684 MySQL \u66ff\u4ee3\u54c1 WebScaleSQL - WebScaleSQL\uff0c5.6 \u7248\u672c\uff0c\u57fa\u4e8e MySQL 5.6 \u793e\u533a\u7248\u672c\u3002 \u5907\u4efd \u5907\u4efd/\u5b58\u50a8/\u6062\u590d \u5de5\u5177 MyDumper - \u903b\u8f91\u7684\uff0c\u5e76\u884c\u7684 MySQL \u5907\u4efd/\u8f6c\u50a8\u5de5\u5177\u3002 MySQLDumper - \u57fa\u4e8e web \u7684\u5f00\u6e90\u5907\u4efd\u5de5\u5177-\u5bf9\u4e8e\u5171\u4eab\u865a\u62df\u4e3b\u673a\u975e\u5e38\u6709\u7528\u3002 mysqldump-secure - \u5c06\u52a0\u5bc6\uff0c\u538b\u7f29\uff0c\u65e5\u5fd7\uff0c\u9ed1\u540d\u5355\u548c Nagios \u76d1\u63a7\u4e00\u4f53\u5316\u7684 mysqldump \u5b89\u5168\u811a\u672c\u3002 Percona Xtrabackup - \u9488\u5bf9 MySQL \u7684\u4e00\u4e2a\u5f00\u6e90\u70ed\u5907\u4efd\u5b9e\u7528\u7a0b\u5e8f\u2014\u2014\u5728\u670d\u52a1\u5668\u7684\u5907\u4efd\u671f\u95f4\u4e0d\u4f1a\u9501\u5b9a\u4f60\u7684\u6570\u636e\u5e93\u3002 \u5b98\u65b9\u8d44\u6599 \u5b98\u65b9\u7684\u4e00\u4e9b\u7f51\u7ad9\u548c\u6587\u7ae0 MySQL\u5b98\u7f51\uff1ahttp://www.mysql.com/ MySQL\u5f00\u53d1\u8005\u4e3b\u9875\uff1ahttp://dev.mysql.com/ MySQL\u793e\u533a\uff1ahttp://www.mysqlpub.com/ What is MySQL? \u5bc6\u7801\u9a8c\u8bc1\u63d2\u4ef6 \u4f18\u79c0\u6587\u7ae0 \u4e00\u4e9b\u4f18\u79c0\u7684\u6587\u7ae0 MySQL\u7d22\u5f15\u80cc\u540e\u7684\u6570\u636e\u7ed3\u6784\u53ca\u7b97\u6cd5\u539f\u7406 MySQL\u6570\u636e\u5e93\u5f15\u64ce Nodejs\u8fde\u63a5MySQL\u6570\u636e\u5e93 MySQL\u4f18\u5316 10\u5206\u949f\u8ba9\u4f60\u660e\u767dMySQL\u662f\u5982\u4f55\u5229\u7528\u7d22\u5f15\u7684 \u4e00\u4e2aMySQL 5.7 \u5206\u533a\u8868\u6027\u80fd\u4e0b\u964d\u7684\u6848\u4f8b\u5206\u6790 \u4e00\u4e2a\u4e0d\u53ef\u601d\u8bae\u7684MySQL\u6162\u67e5\u5206\u6790\u4e0e\u89e3\u51b3 \u2b06 \u8fd4\u56de\u9876\u90e8","title":"Awesome mysql"},{"location":"awesome-mysql/#awesome-mysql","text":"\u8fd9\u662f\u4e00\u4e2aMySQL\u8f6f\u4ef6\u5e93\u548c\u8d44\u6e90\u5217\u8868\u6e05\u5355\u3002\u90e8\u5206\u8d44\u6e90\u641c\u96c6\u6765\u6e90\u4e8e shlomi-noach/awesome-mysql \uff0c\u5b83\u7684 \u4e2d\u6587\u7ffb\u8bd1 \u5728\u8fd9\u91cc\uff0c\u6ca1\u6709\u5b8c\u5168fork\u8fc7\u6765\uff0c\u6ca1\u6709\u5b8c\u5168\u62f7\u8d1d\uff0c\u662f\u56e0\u4e3a\u90e8\u5206\u6392\u7248\u6bd4\u8f83\u4e11\u964b\uff0c\u90e8\u5206\u5de5\u5177\u6162\u6162\u641c\u96c6\u7406\u89e3\u3002","title":"Awesome MySQL"},{"location":"awesome-mysql/#_1","text":"\u8868\u793a \u5f00\u6e90 \u6807\u793a\uff0c\u70b9\u51fb\u8fdb\u5165 \u5f00\u6e90 \u4ed3\u5e93\uff1b \u8868\u793a \u514d\u8d39 \u6807\u793a\uff0c\u6216\u8005\u4e2a\u4eba \u514d\u8d39 \uff1b \u8868\u793a \u70ed\u95e8 \u7684\u8d44\u6e90\u6807\u793a\uff1b \u8868\u793a \u63a8\u8350 \u8d44\u6e90\u6807\u793a\uff1b \u8868\u793a \u5fc5\u5907 \u8d44\u6e90\u6807\u793a\uff1b \u8868\u793a\u5f3a\u70c8\u63a8\u8350\u7684\uff0c\u661f\u661f\u7684\u6570\u91cf\u6765\u8868\u8fbe\u5f3a\u70c8\u7684\u7a0b\u5ea6\uff1b","title":"\u8bf4\u660e"},{"location":"awesome-mysql/#_2","text":"\u624b\u518c\u6587\u6863 \u5206\u6790\u5de5\u5177 GUI \u670d\u52a1\u5668 \u5907\u4efd \u5b98\u65b9\u8d44\u6599 \u4f18\u79c0\u6587\u7ae0","title":"\u76ee\u5f55"},{"location":"awesome-mysql/#_3","text":"\u8fd9\u91cc\u662f\u4e00\u4e9b\u975e\u5b98\u65b9\u7684\u624b\u518c\u6587\u6863 http://www.w3school.com.cn/sql/index.asp http://www.1keydata.com/cn/sql/sql-count.php http://www.runoob.com/mysql/mysql-tutorial.html","title":"\u624b\u518c\u6587\u6863"},{"location":"awesome-mysql/#_4","text":"\u6027\u80fd\uff0c\u7ed3\u6784\u548c\u6570\u636e\u5206\u6790\u5de5\u5177 Anemometer - \u4e00\u4e2a SQL \u6162\u67e5\u8be2\u76d1\u63a7\u5668\u3002 innodb-ruby - \u4e00\u4e2a\u5bf9 InooDB \u683c\u5f0f\u6587\u4ef6\u7684\u89e3\u6790\u5668\uff0c\u7528\u4e8e Ruby \u8bed\u8a00\u3002 innotop - \u4e00\u4e2a\u5177\u5907\u591a\u79cd\u7279\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684 MySQL \u7248 'top' \u5de5\u5177\u3002 pstop - \u4e00\u4e2a\u9488\u5bf9 MySQL \u7684\u7c7b top \u7a0b\u5e8f\uff0c\u7528\u4e8e\u6536\u96c6\uff0c\u6c47\u603b\u4ee5\u53ca\u5c55\u793a\u6765\u81ea performance_schema \u7684\u4fe1\u606f\u3002 mysql-statsd - \u4e00\u4e2a\u6536\u96c6 MySQL \u4fe1\u606f\u7684 Python \u5b88\u62a4\u8fdb\u7a0b\uff0c\u5e76\u901a\u8fc7 StatsD \u53d1\u9001\u5230 Graphite\u3002 sql.ohrz.net - SQL\u8bed\u53e5\u89e3\u8bfb\u670d\u52a1\uff0c\u652f\u6301\u9ad8\u4eae\u663e\u793a\u89e3\u6790\u7ed3\u679c\u4e2d\u8868\u540d\u3001\u5b57\u6bb5\u540d\u3001\u5b50\u67e5\u8be2\u8bed\u53e5\u7b49\u5728\u539f\u6587\u4e2d\u5bf9\u5e94\u7684\u4f4d\u7f6e\u3002","title":"\u5206\u6790\u5de5\u5177"},{"location":"awesome-mysql/#gui","text":"\u8fd9\u91cc\u662f\u641c\u96c6\u7684\u4e00\u4e9b MySQL \u7684\u5ba2\u6237\u7aef\uff0c\u4e5f\u6709\u547d\u4ee4\u884c\u5ba2\u6237\u7aef\u3002 TablePlus - \u652f\u6301PostgreSQL\uff0cMySQL\uff0cRedShift\uff0cMariaDB ...\u5404\u79cd\u6570\u636e\u5e93\u7684\u9ad8\u989c\u503c\u5ba2\u6237\u7aef\u3002 Sequel Pro - \u4e00\u4e2aMySQL\u6570\u636e\u5e93\u7ba1\u7406\u8f6f\u4ef6\u3002 MySQL Workbench - MySQL\u6570\u636e\u5e93\u5b98\u65b9\u7ba1\u7406\u8f6f\u4ef6\u3002\u63d0\u4f9b\u7ed9\u6570\u636e\u5e93\u7ba1\u7406\u5458\u548c\u5f00\u53d1\u4eba\u5458\u8fdb\u884c\u6570\u636e\u5e93\u8bbe\u8ba1\u548c\u5efa\u6a21\u7684\u96c6\u6210\u5de5\u5177\u73af\u5883\uff1bSQL \u5f00\u53d1\uff1b\u6570\u636e\u5e93\u7ba1\u7406\u3002 Navicat - \u540c\u6837\u8de8\u5e73\u53f0\uff0c\u540c\u65f6\u652f\u6301\u591a\u4e2a\u6570\u636e\u5e93\u7cfb\u7edf\uff08MySQL\u3001SQL Server\u3001Oracle\uff09\u3002 ElectroCRUD - MySQL\u6570\u636e\u5e93CRUD\u5e94\u7528\u7a0b\u5e8f\u3002 Chrome MySQL Admin - \u4e00\u4e2aChrome\u63d2\u4ef6\uff0c\u662fMySQL\u5f00\u53d1\u7684\u8de8\u5e73\u53f0\u3001\u53ef\u89c6\u5316\u6570\u636e\u5e93\u5de5\u5177\u3002 Adminer - \u4e00\u4e2a PHP \u7f16\u5199\u7684\u6570\u636e\u5e93\u7ba1\u7406\u5de5\u5177\u3002 HeidiSQL - Windows \u4e0b\u7684 MySQL \u56fe\u5f62\u5316\u7ba1\u7406\u5de5\u5177\u3002 phpMyAdmin - \u4e00\u4e2a PHP \u5199\u6210\u7684\u5f00\u6e90\u8f6f\u4ef6\uff0c\u610f\u56fe\u5bf9 web \u4e0a\u7684 MySQL \u8fdb\u884c\u7ba1\u7406\u3002 mycli - \u4e3aMySQL\u547d\u4ee4\u884c\u5ba2\u6237\u7aef\uff0c\u63d0\u4f9b\u8bed\u6cd5\u9ad8\u4eae\u548c\u63d0\u793a\u529f\u80fd\u7684\u5de5\u5177\uff01 dbdiagram - \u521b\u5efa\u6570\u636e\u5e93\u7684\u5b9e\u4f53-\u5173\u7cfb\u56fe\u7684\u5de5\u5177\u3002","title":"GUI"},{"location":"awesome-mysql/#_5","text":"MySQL\u670d\u52a1\u5668\u7684\u884d\u751f\u54c1 MariaDB - MySQL server \u7684\u4e00\u4e2a\u7531\u793e\u533a\u5f00\u53d1\u7684\u5206\u652f\u3002 MySQL Server MySQL Cluster - Oracle \u5b98\u65b9\u7684 MySQL server \u548c MySQL \u96c6\u7fa4\u5206\u5e03\u3002 Percona Server - \u4e00\u4e2a\u52a0\u5f3a\u7248\u7684 MySQL \u66ff\u4ee3\u54c1 WebScaleSQL - WebScaleSQL\uff0c5.6 \u7248\u672c\uff0c\u57fa\u4e8e MySQL 5.6 \u793e\u533a\u7248\u672c\u3002","title":"\u670d\u52a1\u5668"},{"location":"awesome-mysql/#_6","text":"\u5907\u4efd/\u5b58\u50a8/\u6062\u590d \u5de5\u5177 MyDumper - \u903b\u8f91\u7684\uff0c\u5e76\u884c\u7684 MySQL \u5907\u4efd/\u8f6c\u50a8\u5de5\u5177\u3002 MySQLDumper - \u57fa\u4e8e web \u7684\u5f00\u6e90\u5907\u4efd\u5de5\u5177-\u5bf9\u4e8e\u5171\u4eab\u865a\u62df\u4e3b\u673a\u975e\u5e38\u6709\u7528\u3002 mysqldump-secure - \u5c06\u52a0\u5bc6\uff0c\u538b\u7f29\uff0c\u65e5\u5fd7\uff0c\u9ed1\u540d\u5355\u548c Nagios \u76d1\u63a7\u4e00\u4f53\u5316\u7684 mysqldump \u5b89\u5168\u811a\u672c\u3002 Percona Xtrabackup - \u9488\u5bf9 MySQL \u7684\u4e00\u4e2a\u5f00\u6e90\u70ed\u5907\u4efd\u5b9e\u7528\u7a0b\u5e8f\u2014\u2014\u5728\u670d\u52a1\u5668\u7684\u5907\u4efd\u671f\u95f4\u4e0d\u4f1a\u9501\u5b9a\u4f60\u7684\u6570\u636e\u5e93\u3002","title":"\u5907\u4efd"},{"location":"awesome-mysql/#_7","text":"\u5b98\u65b9\u7684\u4e00\u4e9b\u7f51\u7ad9\u548c\u6587\u7ae0 MySQL\u5b98\u7f51\uff1ahttp://www.mysql.com/ MySQL\u5f00\u53d1\u8005\u4e3b\u9875\uff1ahttp://dev.mysql.com/ MySQL\u793e\u533a\uff1ahttp://www.mysqlpub.com/ What is MySQL? \u5bc6\u7801\u9a8c\u8bc1\u63d2\u4ef6","title":"\u5b98\u65b9\u8d44\u6599"},{"location":"awesome-mysql/#_8","text":"\u4e00\u4e9b\u4f18\u79c0\u7684\u6587\u7ae0 MySQL\u7d22\u5f15\u80cc\u540e\u7684\u6570\u636e\u7ed3\u6784\u53ca\u7b97\u6cd5\u539f\u7406 MySQL\u6570\u636e\u5e93\u5f15\u64ce Nodejs\u8fde\u63a5MySQL\u6570\u636e\u5e93 MySQL\u4f18\u5316 10\u5206\u949f\u8ba9\u4f60\u660e\u767dMySQL\u662f\u5982\u4f55\u5229\u7528\u7d22\u5f15\u7684 \u4e00\u4e2aMySQL 5.7 \u5206\u533a\u8868\u6027\u80fd\u4e0b\u964d\u7684\u6848\u4f8b\u5206\u6790 \u4e00\u4e2a\u4e0d\u53ef\u601d\u8bae\u7684MySQL\u6162\u67e5\u5206\u6790\u4e0e\u89e3\u51b3 \u2b06 \u8fd4\u56de\u9876\u90e8","title":"\u4f18\u79c0\u6587\u7ae0"},{"location":"cmd_line/","text":"\u547d\u4ee4\u884c\u7684\u827a\u672f \u524d\u8a00 \u57fa\u7840 \u65e5\u5e38\u4f7f\u7528 \u6587\u4ef6\u53ca\u6570\u636e\u5904\u7406 \u7cfb\u7edf\u8c03\u8bd5 \u5355\u884c\u811a\u672c \u51b7\u95e8\u4f46\u6709\u7528 \u4ec5\u9650 OS X \u7cfb\u7edf \u4ec5\u9650 Windows \u7cfb\u7edf \u66f4\u591a\u8d44\u6e90 \u514d\u8d23\u58f0\u660e curl -s 'https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md' | egrep -o ' \\w+ ' | tr -d '`' | cowsay -W50 \u719f\u7ec3\u4f7f\u7528\u547d\u4ee4\u884c\u662f\u4e00\u79cd\u5e38\u5e38\u88ab\u5ffd\u89c6\uff0c\u6216\u88ab\u8ba4\u4e3a\u96be\u4ee5\u638c\u63e1\u7684\u6280\u80fd\uff0c\u4f46\u5b9e\u9645\u4e0a\uff0c\u5b83\u4f1a\u63d0\u9ad8\u4f60\u4f5c\u4e3a\u5de5\u7a0b\u5e08\u7684\u7075\u6d3b\u6027\u4ee5\u53ca\u751f\u4ea7\u529b\u3002\u672c\u6587\u662f\u4e00\u4efd\u6211\u5728 Linux \u4e0a\u5de5\u4f5c\u65f6\uff0c\u53d1\u73b0\u7684\u4e00\u4e9b\u547d\u4ee4\u884c\u4f7f\u7528\u6280\u5de7\u7684\u6458\u8981\u3002\u6709\u4e9b\u6280\u5de7\u975e\u5e38\u57fa\u7840\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u76f8\u5f53\u590d\u6742\uff0c\u751a\u81f3\u6666\u6da9\u96be\u61c2\u3002\u8fd9\u7bc7\u6587\u7ae0\u5e76\u4e0d\u957f\uff0c\u4f46\u5f53\u4f60\u80fd\u591f\u719f\u7ec3\u638c\u63e1\u8fd9\u91cc\u5217\u51fa\u7684\u6240\u6709\u6280\u5de7\u65f6\uff0c\u4f60\u5c31\u5b66\u4f1a\u4e86\u5f88\u591a\u5173\u4e8e\u547d\u4ee4\u884c\u7684\u4e1c\u897f\u4e86\u3002 \u8fd9\u7bc7\u6587\u7ae0\u662f\u8bb8\u591a\u4f5c\u8005\u548c\u8bd1\u8005\u5171\u540c\u7684\u6210\u679c\u3002 \u8fd9\u91cc\u7684\u90e8\u5206\u5185\u5bb9 \u9996\u6b21 \u51fa\u73b0 \u4e8e Quora \uff0c \u4f46\u5df2\u7ecf\u8fc1\u79fb\u5230\u4e86 Github\uff0c\u5e76\u7531\u4f17\u591a\u9ad8\u624b\u505a\u51fa\u4e86\u8bb8\u591a\u6539\u8fdb\u3002 \u5982\u679c\u4f60\u5728\u672c\u6587\u4e2d\u53d1\u73b0\u4e86\u9519\u8bef\u6216\u8005\u5b58\u5728\u53ef\u4ee5\u6539\u5584\u7684\u5730\u65b9\uff0c\u8bf7 \u8d21\u732e\u4f60\u7684\u4e00\u4efd\u529b\u91cf \u3002 \u524d\u8a00 \u6db5\u76d6\u8303\u56f4\uff1a \u8fd9\u7bc7\u6587\u7ae0\u4e0d\u4ec5\u80fd\u5e2e\u52a9\u521a\u63a5\u89e6\u547d\u4ee4\u884c\u7684\u65b0\u624b\uff0c\u800c\u4e14\u5bf9\u5177\u6709\u7ecf\u9a8c\u7684\u4eba\u4e5f\u5927\u6709\u88e8\u76ca\u3002\u672c\u6587\u81f4\u529b\u4e8e\u505a\u5230 \u8986\u76d6\u9762\u5e7f \uff08\u6d89\u53ca\u6240\u6709\u91cd\u8981\u7684\u5185\u5bb9\uff09\uff0c \u5177\u4f53 \uff08\u7ed9\u51fa\u5177\u4f53\u7684\u6700\u5e38\u7528\u7684\u4f8b\u5b50\uff09\uff0c\u4ee5\u53ca \u7b80\u6d01 \uff08\u907f\u514d\u5197\u4f59\u7684\u5185\u5bb9\uff0c\u6216\u662f\u53ef\u4ee5\u5728\u5176\u4ed6\u5730\u65b9\u8f7b\u677e\u67e5\u5230\u7684\u7ec6\u679d\u672b\u8282\uff09\u3002\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u672c\u6587\u7684\u5185\u5bb9\u5c5e\u4e8e\u57fa\u672c\u529f\u6216\u8005\u80fd\u5e2e\u52a9\u60a8\u8282\u7ea6\u5927\u91cf\u7684\u65f6\u95f4\u3002 \u672c\u6587\u4e3b\u8981\u4e3a Linux \u6240\u5199\uff0c\u4f46\u5728 \u4ec5\u9650 OS X \u7cfb\u7edf \u7ae0\u8282\u548c \u4ec5\u9650 Windows \u7cfb\u7edf \u7ae0\u8282\u4e2d\u4e5f\u5305\u542b\u6709\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u7684\u5185\u5bb9\u3002\u9664\u53bb\u8fd9\u4e24\u4e2a\u7ae0\u8282\u5916\uff0c\u5176\u5b83\u7684\u5185\u5bb9\u5927\u90e8\u5206\u5747\u53ef\u5728\u5176\u4ed6\u7c7b Unix \u7cfb\u7edf\u6216 OS X\uff0c\u751a\u81f3 Cygwin \u4e2d\u5f97\u5230\u5e94\u7528\u3002 \u672c\u6587\u4e3b\u8981\u5173\u6ce8\u4e8e\u4ea4\u4e92\u5f0f Bash\uff0c\u4f46\u4e5f\u6709\u5f88\u591a\u6280\u5de7\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6 shell \u548c Bash \u811a\u672c\u5f53\u4e2d\u3002 \u9664\u53bb\u201c\u6807\u51c6\u7684\u201dUnix \u547d\u4ee4\uff0c\u672c\u6587\u8fd8\u5305\u62ec\u4e86\u4e00\u4e9b\u4f9d\u8d56\u4e8e\u7279\u5b9a\u8f6f\u4ef6\u5305\u7684\u547d\u4ee4\uff08\u524d\u63d0\u662f\u5b83\u4eec\u5177\u6709\u8db3\u591f\u7684\u4ef7\u503c\uff09\u3002 \u6ce8\u610f\u4e8b\u9879\uff1a \u4e3a\u4e86\u80fd\u5728\u4e00\u9875\u5185\u5c55\u793a\u5c3d\u91cf\u591a\u7684\u4e1c\u897f\uff0c\u4e00\u4e9b\u5177\u4f53\u7684\u4fe1\u606f\u53ef\u4ee5\u5728\u5f15\u7528\u7684\u9875\u9762\u4e2d\u627e\u5230\u3002\u6211\u4eec\u76f8\u4fe1\u673a\u667a\u7684\u4f60\u77e5\u9053\u5982\u4f55\u4f7f\u7528 Google \u6216\u8005\u5176\u4ed6\u641c\u7d22\u5f15\u64ce\u6765\u67e5\u9605\u5230\u66f4\u591a\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\u6587\u4e2d\u90e8\u5206\u547d\u4ee4\u9700\u8981\u60a8\u4f7f\u7528 apt-get \uff0c yum \uff0c dnf \uff0c pacman \uff0c pip \u6216 brew \uff08\u4ee5\u53ca\u5176\u5b83\u5408\u9002\u7684\u5305\u7ba1\u7406\u5668\uff09\u6765\u5b89\u88c5\u4f9d\u8d56\u7684\u7a0b\u5e8f\u3002 \u9047\u5230\u95ee\u9898\u7684\u8bdd\uff0c\u8bf7\u5c1d\u8bd5\u4f7f\u7528 Explainshell \u53bb\u83b7\u53d6\u76f8\u5173\u547d\u4ee4\u3001\u53c2\u6570\u3001\u7ba1\u9053\u7b49\u5185\u5bb9\u7684\u89e3\u91ca\u3002 \u57fa\u7840 \u5b66\u4e60 Bash \u7684\u57fa\u7840\u77e5\u8bc6\u3002\u5177\u4f53\u5730\uff0c\u5728\u547d\u4ee4\u884c\u4e2d\u8f93\u5165 man bash \u5e76\u81f3\u5c11\u5168\u6587\u6d4f\u89c8\u4e00\u904d; \u5b83\u7406\u89e3\u8d77\u6765\u5f88\u7b80\u5355\u5e76\u4e14\u4e0d\u5197\u957f\u3002\u5176\u4ed6\u7684 shell \u53ef\u80fd\u5f88\u597d\u7528\uff0c\u4f46 Bash \u7684\u529f\u80fd\u5df2\u7ecf\u8db3\u591f\u5f3a\u5927\u5e76\u4e14\u5230\u51e0\u4e4e\u603b\u662f\u53ef\u7528\u7684\uff08 \u5982\u679c\u4f60 \u53ea \u5b66\u4e60 zsh\uff0cfish \u6216\u5176\u4ed6\u7684 shell \u7684\u8bdd\uff0c\u5728\u4f60\u81ea\u5df1\u7684\u8bbe\u5907\u4e0a\u4f1a\u663e\u5f97\u5f88\u65b9\u4fbf\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u8fd9\u4e9b\u529f\u80fd\u4f1a\u7ed9\u60a8\u5e26\u6765\u4e0d\u4fbf\uff0c\u4f8b\u5982\u5f53\u4f60\u9700\u8981\u5728\u670d\u52a1\u5668\u4e0a\u5de5\u4f5c\u65f6\uff09\u3002 \u719f\u6089\u81f3\u5c11\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u7f16\u8f91\u5668\u3002\u901a\u5e38\u800c\u8a00 Vim \uff08 vi \uff09 \u4f1a\u662f\u4f60\u6700\u597d\u7684\u9009\u62e9\uff0c\u6bd5\u7adf\u5728\u7ec8\u7aef\u4e2d\u7f16\u8f91\u6587\u672c\u65f6 Vim \u662f\u6700\u597d\u7528\u7684\u5de5\u5177\uff08\u751a\u81f3\u5927\u90e8\u5206\u60c5\u51b5\u4e0b Vim \u8981\u6bd4 Emacs\u3001\u5927\u578b IDE \u6216\u662f\u70ab\u9177\u7684\u7f16\u8f91\u5668\u66f4\u597d\u7528\uff09\u3002 \u5b66\u4f1a\u5982\u4f55\u4f7f\u7528 man \u547d\u4ee4\u53bb\u9605\u8bfb\u6587\u6863\u3002\u5b66\u4f1a\u4f7f\u7528 apropos \u53bb\u67e5\u627e\u6587\u6863\u3002\u77e5\u9053\u6709\u4e9b\u547d\u4ee4\u5e76\u4e0d\u5bf9\u5e94\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u800c\u662f\u5728 Bash \u5185\u7f6e\u597d\u7684\uff0c\u6b64\u65f6\u53ef\u4ee5\u4f7f\u7528 help \u548c help -d \u547d\u4ee4\u83b7\u53d6\u5e2e\u52a9\u4fe1\u606f\u3002\u4f60\u53ef\u4ee5\u7528 type \u547d\u4ee4 \u6765\u5224\u65ad\u8fd9\u4e2a\u547d\u4ee4\u5230\u5e95\u662f\u53ef\u6267\u884c\u6587\u4ef6\u3001shell \u5185\u7f6e\u547d\u4ee4\u8fd8\u662f\u522b\u540d\u3002 \u5b66\u4f1a\u4f7f\u7528 \u548c \u6765\u91cd\u5b9a\u5411\u8f93\u51fa\u548c\u8f93\u5165\uff0c\u5b66\u4f1a\u4f7f\u7528 | \u6765\u91cd\u5b9a\u5411\u7ba1\u9053\u3002\u660e\u767d \u4f1a\u8986\u76d6\u4e86\u8f93\u51fa\u6587\u4ef6\u800c \u662f\u5728\u6587\u4ef6\u672b\u6dfb\u52a0\u3002\u4e86\u89e3\u6807\u51c6\u8f93\u51fa stdout \u548c\u6807\u51c6\u9519\u8bef stderr\u3002 \u5b66\u4f1a\u4f7f\u7528\u901a\u914d\u7b26 * \uff08\u6216\u8bb8\u518d\u7b97\u4e0a ? \u548c [ ... ] \uff09 \u548c\u5f15\u7528\u4ee5\u53ca\u5f15\u7528\u4e2d ' \u548c \" \u7684\u533a\u522b\uff08\u540e\u6587\u4e2d\u6709\u4e00\u4e9b\u5177\u4f53\u7684\u4f8b\u5b50\uff09\u3002 \u719f\u6089 Bash \u4e2d\u7684\u4efb\u52a1\u7ba1\u7406\u5de5\u5177\uff1a \uff0c ctrl-z \uff0c ctrl-c \uff0c jobs \uff0c fg \uff0c bg \uff0c kill \u7b49\u3002 \u5b66\u4f1a\u4f7f\u7528 ssh \u8fdb\u884c\u8fdc\u7a0b\u547d\u4ee4\u884c\u767b\u5f55\uff0c\u6700\u597d\u77e5\u9053\u5982\u4f55\u4f7f\u7528 ssh-agent \uff0c ssh-add \u7b49\u547d\u4ee4\u6765\u5b9e\u73b0\u57fa\u7840\u7684\u65e0\u5bc6\u7801\u8ba4\u8bc1\u767b\u5f55\u3002 \u5b66\u4f1a\u57fa\u672c\u7684\u6587\u4ef6\u7ba1\u7406\u5de5\u5177\uff1a ls \u548c ls -l \uff08\u4e86\u89e3 ls -l \u4e2d\u6bcf\u4e00\u5217\u4ee3\u8868\u7684\u610f\u4e49\uff09\uff0c less \uff0c head \uff0c tail \u548c tail -f \uff08\u751a\u81f3 less +F \uff09\uff0c ln \u548c ln -s \uff08\u4e86\u89e3\u786c\u94fe\u63a5\u4e0e\u8f6f\u94fe\u63a5\u7684\u533a\u522b\uff09\uff0c chown \uff0c chmod \uff0c du \uff08\u786c\u76d8\u4f7f\u7528\u60c5\u51b5\u6982\u8ff0\uff1a du -hs * \uff09\u3002 \u5173\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u7ba1\u7406\uff0c\u5b66\u4e60 df \uff0c mount \uff0c fdisk \uff0c mkfs \uff0c lsblk \u3002\u77e5\u9053 inode \u662f\u4ec0\u4e48\uff08\u4e0e ls -i \u548c df -i \u7b49\u547d\u4ee4\u76f8\u5173\uff09\u3002 \u5b66\u4e60\u57fa\u672c\u7684\u7f51\u7edc\u7ba1\u7406\u5de5\u5177\uff1a ip \u6216 ifconfig \uff0c dig \u3002 \u5b66\u4e60\u5e76\u4f7f\u7528\u4e00\u79cd\u7248\u672c\u63a7\u5236\u7ba1\u7406\u7cfb\u7edf\uff0c\u4f8b\u5982 git \u3002 \u719f\u6089\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u5b66\u4f1a\u4f7f\u7528 grep \uff0f egrep \uff0c\u5b83\u4eec\u7684\u53c2\u6570\u4e2d -i \uff0c -o \uff0c -v \uff0c -A \uff0c -B \u548c -C \u8fd9\u4e9b\u662f\u5f88\u5e38\u7528\u5e76\u503c\u5f97\u8ba4\u771f\u5b66\u4e60\u7684\u3002 \u5b66\u4f1a\u4f7f\u7528 apt-get \uff0c yum \uff0c dnf \u6216 pacman \uff08\u5177\u4f53\u4f7f\u7528\u54ea\u4e2a\u53d6\u51b3\u4e8e\u4f60\u4f7f\u7528\u7684 Linux \u53d1\u884c\u7248\uff09\u6765\u67e5\u627e\u548c\u5b89\u88c5\u8f6f\u4ef6\u5305\u3002\u5e76\u786e\u4fdd\u4f60\u7684\u73af\u5883\u4e2d\u6709 pip \u6765\u5b89\u88c5\u57fa\u4e8e Python \u7684\u547d\u4ee4\u884c\u5de5\u5177 \uff08\u63a5\u4e0b\u6765\u63d0\u5230\u7684\u90e8\u5206\u7a0b\u5e8f\u4f7f\u7528 pip \u6765\u5b89\u88c5\u4f1a\u5f88\u65b9\u4fbf\uff09\u3002 \u65e5\u5e38\u4f7f\u7528 \u5728 Bash \u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u6309 Tab \u952e\u5b9e\u73b0\u81ea\u52a8\u8865\u5168\u53c2\u6570\uff0c\u4f7f\u7528 ctrl-r \u641c\u7d22\u547d\u4ee4\u884c\u5386\u53f2\u8bb0\u5f55\uff08\u6309\u4e0b\u6309\u952e\u4e4b\u540e\uff0c\u8f93\u5165\u5173\u952e\u5b57\u4fbf\u53ef\u4ee5\u641c\u7d22\uff0c\u91cd\u590d\u6309\u4e0b ctrl-r \u4f1a\u5411\u540e\u67e5\u627e\u5339\u914d\u9879\uff0c\u6309\u4e0b Enter \u952e\u4f1a\u6267\u884c\u5f53\u524d\u5339\u914d\u7684\u547d\u4ee4\uff0c\u800c\u6309\u4e0b\u53f3\u65b9\u5411\u952e\u4f1a\u5c06\u5339\u914d\u9879\u653e\u5165\u5f53\u524d\u884c\u4e2d\uff0c\u4e0d\u4f1a\u76f4\u63a5\u6267\u884c\uff0c\u4ee5\u4fbf\u505a\u51fa\u4fee\u6539\uff09\u3002 \u5728 Bash \u4e2d\uff0c\u53ef\u4ee5\u6309\u4e0b ctrl-w \u5220\u9664\u4f60\u952e\u5165\u7684\u6700\u540e\u4e00\u4e2a\u5355\u8bcd\uff0c ctrl-u \u53ef\u4ee5\u5220\u9664\u884c\u5185\u5149\u6807\u6240\u5728\u4f4d\u7f6e\u4e4b\u524d\u7684\u5185\u5bb9\uff0c alt-b \u548c alt-f \u53ef\u4ee5\u4ee5\u5355\u8bcd\u4e3a\u5355\u4f4d\u79fb\u52a8\u5149\u6807\uff0c ctrl-a \u53ef\u4ee5\u5c06\u5149\u6807\u79fb\u81f3\u884c\u9996\uff0c ctrl-e \u53ef\u4ee5\u5c06\u5149\u6807\u79fb\u81f3\u884c\u5c3e\uff0c ctrl-k \u53ef\u4ee5\u5220\u9664\u5149\u6807\u81f3\u884c\u5c3e\u7684\u6240\u6709\u5185\u5bb9\uff0c ctrl-l \u53ef\u4ee5\u6e05\u5c4f\u3002\u952e\u5165 man readline \u53ef\u4ee5\u67e5\u770b Bash \u4e2d\u7684\u9ed8\u8ba4\u5feb\u6377\u952e\u3002\u5185\u5bb9\u6709\u5f88\u591a\uff0c\u4f8b\u5982 alt-. \u5faa\u73af\u5730\u79fb\u5411\u524d\u4e00\u4e2a\u53c2\u6570\uff0c\u800c alt- * \u53ef\u4ee5\u5c55\u5f00\u901a\u914d\u7b26\u3002 \u4f60\u559c\u6b22\u7684\u8bdd\uff0c\u53ef\u4ee5\u6267\u884c set -o vi \u6765\u4f7f\u7528 vi \u98ce\u683c\u7684\u5feb\u6377\u952e\uff0c\u800c\u6267\u884c set -o emacs \u53ef\u4ee5\u628a\u5b83\u6539\u56de\u6765\u3002 \u4e3a\u4e86\u4fbf\u4e8e\u7f16\u8f91\u957f\u547d\u4ee4\uff0c\u5728\u8bbe\u7f6e\u4f60\u7684\u9ed8\u8ba4\u7f16\u8f91\u5668\u540e\uff08\u4f8b\u5982 export EDITOR=vim \uff09\uff0c ctrl-x ctrl-e \u4f1a\u6253\u5f00\u4e00\u4e2a\u7f16\u8f91\u5668\u6765\u7f16\u8f91\u5f53\u524d\u8f93\u5165\u7684\u547d\u4ee4\u3002\u5728 vi \u98ce\u683c\u4e0b\u5feb\u6377\u952e\u5219\u662f escape-v \u3002 \u952e\u5165 history \u67e5\u770b\u547d\u4ee4\u884c\u5386\u53f2\u8bb0\u5f55\uff0c\u518d\u7528 !n \uff08 n \u662f\u547d\u4ee4\u7f16\u53f7\uff09\u5c31\u53ef\u4ee5\u518d\u6b21\u6267\u884c\u3002\u5176\u4e2d\u6709\u8bb8\u591a\u7f29\u5199\uff0c\u6700\u6709\u7528\u7684\u5927\u6982\u5c31\u662f !$ \uff0c \u5b83\u7528\u4e8e\u6307\u4ee3\u4e0a\u6b21\u952e\u5165\u7684\u53c2\u6570\uff0c\u800c !! \u53ef\u4ee5\u6307\u4ee3\u4e0a\u6b21\u952e\u5165\u7684\u547d\u4ee4\u4e86\uff08\u53c2\u8003 man \u9875\u9762\u4e2d\u7684\u201cHISTORY EXPANSION\u201d\uff09\u3002\u4e0d\u8fc7\u8fd9\u4e9b\u529f\u80fd\uff0c\u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5feb\u6377\u952e ctrl-r \u548c alt-. \u6765\u5b9e\u73b0\u3002 cd \u547d\u4ee4\u53ef\u4ee5\u5207\u6362\u5de5\u4f5c\u8def\u5f84\uff0c\u8f93\u5165 cd ~ \u53ef\u4ee5\u8fdb\u5165 home \u76ee\u5f55\u3002\u8981\u8bbf\u95ee\u4f60\u7684 home \u76ee\u5f55\u4e2d\u7684\u6587\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u524d\u7f00 ~ \uff08\u4f8b\u5982 ~/.bashrc \uff09\u3002\u5728 sh \u811a\u672c\u91cc\u5219\u7528\u73af\u5883\u53d8\u91cf $HOME \u6307\u4ee3 home \u76ee\u5f55\u7684\u8def\u5f84\u3002 \u56de\u5230\u524d\u4e00\u4e2a\u5de5\u4f5c\u8def\u5f84\uff1a cd - \u3002 \u5982\u679c\u4f60\u8f93\u5165\u547d\u4ee4\u7684\u65f6\u5019\u4e2d\u9014\u6539\u4e86\u4e3b\u610f\uff0c\u6309\u4e0b alt-# \u5728\u884c\u9996\u6dfb\u52a0 # \u628a\u5b83\u5f53\u505a\u6ce8\u91ca\u518d\u6309\u4e0b\u56de\u8f66\u6267\u884c\uff08\u6216\u8005\u4f9d\u6b21\u6309\u4e0b ctrl-a \uff0c # \uff0c enter \uff09\u3002\u8fd9\u6837\u505a\u7684\u8bdd\uff0c\u4e4b\u540e\u501f\u52a9\u547d\u4ee4\u884c\u5386\u53f2\u8bb0\u5f55\uff0c\u4f60\u53ef\u4ee5\u5f88\u65b9\u4fbf\u6062\u590d\u4f60\u521a\u624d\u8f93\u5165\u5230\u4e00\u534a\u7684\u547d\u4ee4\u3002 \u4f7f\u7528 xargs \uff08 \u6216 parallel \uff09\u3002\u4ed6\u4eec\u975e\u5e38\u7ed9\u529b\u3002\u6ce8\u610f\u5230\u4f60\u53ef\u4ee5\u63a7\u5236\u6bcf\u884c\u53c2\u6570\u4e2a\u6570\uff08 -L \uff09\u548c\u6700\u5927\u5e76\u884c\u6570\uff08 -P \uff09\u3002\u5982\u679c\u4f60\u4e0d\u786e\u5b9a\u5b83\u4eec\u662f\u5426\u4f1a\u6309\u4f60\u60f3\u7684\u90a3\u6837\u5de5\u4f5c\uff0c\u5148\u4f7f\u7528 xargs echo \u67e5\u770b\u4e00\u4e0b\u3002\u6b64\u5916\uff0c\u4f7f\u7528 -I{} \u4f1a\u5f88\u65b9\u4fbf\u3002\u4f8b\u5982\uff1a find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p \u4ee5\u4e00\u79cd\u4f18\u96c5\u7684\u65b9\u5f0f\u5c55\u793a\u8fdb\u7a0b\u6811\u3002 \u4f7f\u7528 pgrep \u548c pkill \u6839\u636e\u540d\u5b57\u67e5\u627e\u8fdb\u7a0b\u6216\u53d1\u9001\u4fe1\u53f7\uff08 -f \u53c2\u6570\u901a\u5e38\u6709\u7528\uff09\u3002 \u4e86\u89e3\u4f60\u53ef\u4ee5\u53d1\u5f80\u8fdb\u7a0b\u7684\u4fe1\u53f7\u7684\u79cd\u7c7b\u3002\u6bd4\u5982\uff0c\u4f7f\u7528 kill -STOP [pid] \u505c\u6b62\u4e00\u4e2a\u8fdb\u7a0b\u3002\u4f7f\u7528 man 7 signal \u67e5\u770b\u8be6\u7ec6\u5217\u8868\u3002 \u4f7f\u7528 nohup \u6216 disown \u4f7f\u4e00\u4e2a\u540e\u53f0\u8fdb\u7a0b\u6301\u7eed\u8fd0\u884c\u3002 \u4f7f\u7528 netstat -lntp \u6216 ss -plat \u68c0\u67e5\u54ea\u4e9b\u8fdb\u7a0b\u5728\u76d1\u542c\u7aef\u53e3\uff08\u9ed8\u8ba4\u662f\u68c0\u67e5 TCP \u7aef\u53e3; \u6dfb\u52a0\u53c2\u6570 -u \u5219\u68c0\u67e5 UDP \u7aef\u53e3\uff09\u6216\u8005 lsof -iTCP -sTCP:LISTEN -P -n (\u8fd9\u4e5f\u53ef\u4ee5\u5728 OS X \u4e0a\u8fd0\u884c)\u3002 lsof \u6765\u67e5\u770b\u5f00\u542f\u7684\u5957\u63a5\u5b57\u548c\u6587\u4ef6\u3002 \u4f7f\u7528 uptime \u6216 w \u6765\u67e5\u770b\u7cfb\u7edf\u5df2\u7ecf\u8fd0\u884c\u591a\u957f\u65f6\u95f4\u3002 \u4f7f\u7528 alias \u6765\u521b\u5efa\u5e38\u7528\u547d\u4ee4\u7684\u5feb\u6377\u5f62\u5f0f\u3002\u4f8b\u5982\uff1a alias ll='ls -latr' \u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u547d\u4ee4\u522b\u540d ll \u3002 \u53ef\u4ee5\u628a\u522b\u540d\u3001shell \u9009\u9879\u548c\u5e38\u7528\u51fd\u6570\u4fdd\u5b58\u5728 ~/.bashrc \uff0c\u5177\u4f53\u770b\u4e0b\u8fd9\u7bc7 \u6587\u7ae0 \u3002\u8fd9\u6837\u505a\u7684\u8bdd\u4f60\u5c31\u53ef\u4ee5\u5728\u6240\u6709 shell \u4f1a\u8bdd\u4e2d\u4f7f\u7528\u4f60\u7684\u8bbe\u5b9a\u3002 \u628a\u73af\u5883\u53d8\u91cf\u7684\u8bbe\u5b9a\u4ee5\u53ca\u767b\u9646\u65f6\u8981\u6267\u884c\u7684\u547d\u4ee4\u4fdd\u5b58\u5728 ~/.bash_profile \u3002\u800c\u5bf9\u4e8e\u4ece\u56fe\u5f62\u754c\u9762\u542f\u52a8\u7684 shell \u548c cron \u542f\u52a8\u7684 shell\uff0c\u5219\u9700\u8981\u5355\u72ec\u914d\u7f6e\u6587\u4ef6\u3002 \u8981\u60f3\u5728\u51e0\u53f0\u7535\u8111\u4e2d\u540c\u6b65\u4f60\u7684\u914d\u7f6e\u6587\u4ef6\uff08\u4f8b\u5982 .bashrc \u548c .bash_profile \uff09\uff0c\u53ef\u4ee5\u501f\u52a9 Git\u3002 \u5f53\u53d8\u91cf\u548c\u6587\u4ef6\u540d\u4e2d\u5305\u542b\u7a7a\u683c\u7684\u65f6\u5019\u8981\u683c\u5916\u5c0f\u5fc3\u3002Bash \u53d8\u91cf\u8981\u7528\u5f15\u53f7\u62ec\u8d77\u6765\uff0c\u6bd4\u5982 \"$FOO\" \u3002\u5c3d\u91cf\u4f7f\u7528 -0 \u6216 -print0 \u9009\u9879\u4ee5\u4fbf\u7528 NULL \u6765\u5206\u9694\u6587\u4ef6\u540d\uff0c\u4f8b\u5982 locate -0 pattern | xargs -0 ls -al \u6216 find / -print0 -type d | xargs -0 ls -al \u3002\u5982\u679c for \u5faa\u73af\u4e2d\u5faa\u73af\u8bbf\u95ee\u7684\u6587\u4ef6\u540d\u542b\u6709\u7a7a\u5b57\u7b26\uff08\u7a7a\u683c\u3001tab \u7b49\u5b57\u7b26\uff09\uff0c\u53ea\u9700\u7528 IFS=$'\\n' \u628a\u5185\u90e8\u5b57\u6bb5\u5206\u9694\u7b26\u8bbe\u4e3a\u6362\u884c\u7b26\u3002 \u5728 Bash \u811a\u672c\u4e2d\uff0c\u4f7f\u7528 set -x \u53bb\u8c03\u8bd5\u8f93\u51fa\uff08\u6216\u8005\u4f7f\u7528\u5b83\u7684\u53d8\u4f53 set -v \uff0c\u5b83\u4f1a\u8bb0\u5f55\u539f\u59cb\u8f93\u5165\uff0c\u5305\u62ec\u591a\u4f59\u7684\u53c2\u6570\u548c\u6ce8\u91ca\uff09\u3002\u5c3d\u53ef\u80fd\u5730\u4f7f\u7528\u4e25\u683c\u6a21\u5f0f\uff1a\u4f7f\u7528 set -e \u4ee4\u811a\u672c\u5728\u53d1\u751f\u9519\u8bef\u65f6\u9000\u51fa\u800c\u4e0d\u662f\u7ee7\u7eed\u8fd0\u884c\uff1b\u4f7f\u7528 set -u \u6765\u68c0\u67e5\u662f\u5426\u4f7f\u7528\u4e86\u672a\u8d4b\u503c\u7684\u53d8\u91cf\uff1b\u8bd5\u8bd5 set -o pipefail \uff0c\u5b83\u53ef\u4ee5\u76d1\u6d4b\u7ba1\u9053\u4e2d\u7684\u9519\u8bef\u3002\u5f53\u7275\u626f\u5230\u5f88\u591a\u811a\u672c\u65f6\uff0c\u4f7f\u7528 trap \u6765\u68c0\u6d4b ERR \u548c EXIT\u3002\u4e00\u4e2a\u597d\u7684\u4e60\u60ef\u662f\u5728\u811a\u672c\u6587\u4ef6\u5f00\u5934\u8fd9\u6837\u5199\uff0c\u8fd9\u4f1a\u4f7f\u5b83\u80fd\u591f\u68c0\u6d4b\u4e00\u4e9b\u9519\u8bef\uff0c\u5e76\u5728\u9519\u8bef\u53d1\u751f\u65f6\u4e2d\u65ad\u7a0b\u5e8f\u5e76\u8f93\u51fa\u4fe1\u606f\uff1a set -euo pipefail trap echo 'error: Script failed: see failed command above' ERR \u5728 Bash \u811a\u672c\u4e2d\uff0c\u5b50 shell\uff08\u4f7f\u7528\u62ec\u53f7 (...) \uff09\u662f\u4e00\u79cd\u7ec4\u7ec7\u53c2\u6570\u7684\u4fbf\u6377\u65b9\u5f0f\u3002\u4e00\u4e2a\u5e38\u89c1\u7684\u4f8b\u5b50\u662f\u4e34\u65f6\u5730\u79fb\u52a8\u5de5\u4f5c\u8def\u5f84\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a # do something in current dir (cd /some/other/dir other-command) # continue in original dir \u5728 Bash \u4e2d\uff0c\u53d8\u91cf\u6709\u8bb8\u591a\u7684\u6269\u5c55\u65b9\u5f0f\u3002 ${name:?error message} \u7528\u4e8e\u68c0\u67e5\u53d8\u91cf\u662f\u5426\u5b58\u5728\u3002\u6b64\u5916\uff0c\u5f53 Bash \u811a\u672c\u53ea\u9700\u8981\u4e00\u4e2a\u53c2\u6570\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u8fd9\u6837\u7684\u4ee3\u7801 input_file=${1:?usage: $0 input_file} \u3002\u5728\u53d8\u91cf\u4e3a\u7a7a\u65f6\u4f7f\u7528\u9ed8\u8ba4\u503c\uff1a ${name:-default} \u3002\u5982\u679c\u4f60\u8981\u5728\u4e4b\u524d\u7684\u4f8b\u5b50\u4e2d\u518d\u52a0\u4e00\u4e2a\uff08\u53ef\u9009\u7684\uff09\u53c2\u6570\uff0c\u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c\u8fd9\u6837\u7684\u4ee3\u7801 output_file=${2:-logfile} \uff0c\u5982\u679c\u7701\u7565\u4e86 $2\uff0c\u5b83\u7684\u503c\u5c31\u4e3a\u7a7a\uff0c\u4e8e\u662f output_file \u5c31\u4f1a\u88ab\u8bbe\u4e3a logfile \u3002\u6570\u5b66\u8868\u8fbe\u5f0f\uff1a i=$(( (i + 1) % 5 )) \u3002\u5e8f\u5217\uff1a {1..10} \u3002\u622a\u65ad\u5b57\u7b26\u4e32\uff1a ${var%suffix} \u548c ${var#prefix} \u3002\u4f8b\u5982\uff0c\u5047\u8bbe var=foo.pdf \uff0c\u90a3\u4e48 echo ${var%.pdf}.txt \u5c06\u8f93\u51fa foo.txt \u3002 \u4f7f\u7528\u62ec\u53f7\u6269\u5c55\uff08 { ... } \uff09\u6765\u51cf\u5c11\u8f93\u5165\u76f8\u4f3c\u6587\u672c\uff0c\u5e76\u81ea\u52a8\u5316\u6587\u672c\u7ec4\u5408\u3002\u8fd9\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f1a\u5f88\u6709\u7528\uff0c\u4f8b\u5982 mv foo.{txt,pdf} some-dir \uff08\u540c\u65f6\u79fb\u52a8\u4e24\u4e2a\u6587\u4ef6\uff09\uff0c cp somefile{,.bak} \uff08\u4f1a\u88ab\u6269\u5c55\u6210 cp somefile somefile.bak \uff09\u6216\u8005 mkdir -p test-{a,b,c}/subtest-{1,2,3} \uff08\u4f1a\u88ab\u6269\u5c55\u6210\u6240\u6709\u53ef\u80fd\u7684\u7ec4\u5408\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a\u76ee\u5f55\u6811\uff09\u3002 \u901a\u8fc7\u4f7f\u7528 (some command) \u53ef\u4ee5\u5c06\u8f93\u51fa\u89c6\u4e3a\u6587\u4ef6\u3002\u4f8b\u5982\uff0c\u5bf9\u6bd4\u672c\u5730\u6587\u4ef6 /etc/hosts \u548c\u4e00\u4e2a\u8fdc\u7a0b\u6587\u4ef6\uff1a diff /etc/hosts (ssh somehost cat /etc/hosts) \u7f16\u5199\u811a\u672c\u65f6\uff0c\u4f60\u53ef\u80fd\u4f1a\u60f3\u8981\u628a\u4ee3\u7801\u90fd\u653e\u5728\u5927\u62ec\u53f7\u91cc\u3002\u7f3a\u5c11\u53f3\u62ec\u53f7\u7684\u8bdd\uff0c\u4ee3\u7801\u5c31\u4f1a\u56e0\u4e3a\u8bed\u6cd5\u9519\u8bef\u800c\u65e0\u6cd5\u6267\u884c\u3002\u5982\u679c\u4f60\u7684\u811a\u672c\u662f\u8981\u653e\u5728\u7f51\u4e0a\u5206\u4eab\u4f9b\u4ed6\u4eba\u4f7f\u7528\u7684\uff0c\u8fd9\u6837\u7684\u5199\u6cd5\u5c31\u4f53\u73b0\u51fa\u5b83\u7684\u597d\u5904\u4e86\uff0c\u56e0\u4e3a\u8fd9\u6837\u53ef\u4ee5\u9632\u6b62\u4e0b\u8f7d\u4e0d\u5b8c\u5168\u4ee3\u7801\u88ab\u6267\u884c\u3002 { # \u5728\u8fd9\u91cc\u5199\u4ee3\u7801 } \u4e86\u89e3 Bash \u4e2d\u7684\u201chere documents\u201d\uff0c\u4f8b\u5982 cat EOF ... \u3002 \u5728 Bash \u4e2d\uff0c\u540c\u65f6\u91cd\u5b9a\u5411\u6807\u51c6\u8f93\u51fa\u548c\u6807\u51c6\u9519\u8bef\uff1a some-command logfile 2 1 \u6216\u8005 some-command logfile \u3002\u901a\u5e38\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u547d\u4ee4\u4e0d\u4f1a\u5728\u6807\u51c6\u8f93\u5165\u91cc\u6b8b\u7559\u4e00\u4e2a\u672a\u5173\u95ed\u7684\u6587\u4ef6\u53e5\u67c4\u6346\u7ed1\u5728\u4f60\u5f53\u524d\u6240\u5728\u7684\u7ec8\u7aef\u4e0a\uff0c\u5728\u547d\u4ee4\u540e\u6dfb\u52a0 /dev/null \u662f\u4e00\u4e2a\u597d\u4e60\u60ef\u3002 \u4f7f\u7528 man ascii \u67e5\u770b\u5177\u6709\u5341\u516d\u8fdb\u5236\u548c\u5341\u8fdb\u5236\u503c\u7684ASCII\u8868\u3002 man unicode \uff0c man utf-8 \uff0c\u4ee5\u53ca man latin1 \u6709\u52a9\u4e8e\u4f60\u53bb\u4e86\u89e3\u901a\u7528\u7684\u7f16\u7801\u4fe1\u606f\u3002 \u4f7f\u7528 screen \u6216 tmux \u6765\u4f7f\u7528\u591a\u4efd\u5c4f\u5e55\uff0c\u5f53\u4f60\u5728\u4f7f\u7528 ssh \u65f6\uff08\u4fdd\u5b58 session \u4fe1\u606f\uff09\u5c06\u5c24\u4e3a\u6709\u7528\u3002\u800c byobu \u53ef\u4ee5\u4e3a\u5b83\u4eec\u63d0\u4f9b\u66f4\u591a\u7684\u4fe1\u606f\u548c\u6613\u7528\u7684\u7ba1\u7406\u5de5\u5177\u3002\u53e6\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684 session \u6301\u4e45\u5316\u89e3\u51b3\u65b9\u6848\u662f dtach \u3002 ssh \u4e2d\uff0c\u4e86\u89e3\u5982\u4f55\u4f7f\u7528 -L \u6216 -D \uff08\u5076\u5c14\u9700\u8981\u7528 -R \uff09\u5f00\u542f\u96a7\u9053\u662f\u975e\u5e38\u6709\u7528\u7684\uff0c\u6bd4\u5982\u5f53\u4f60\u9700\u8981\u4ece\u4e00\u53f0\u8fdc\u7a0b\u670d\u52a1\u5668\u4e0a\u8bbf\u95ee web \u9875\u9762\u3002 \u5bf9 ssh \u8bbe\u7f6e\u505a\u4e00\u4e9b\u5c0f\u4f18\u5316\u53ef\u80fd\u662f\u5f88\u6709\u7528\u7684\uff0c\u4f8b\u5982\u8fd9\u4e2a ~/.ssh/config \u6587\u4ef6\u5305\u542b\u4e86\u9632\u6b62\u7279\u5b9a\u7f51\u7edc\u73af\u5883\u4e0b\u8fde\u63a5\u65ad\u5f00\u3001\u538b\u7f29\u6570\u636e\u3001\u591a\u901a\u9053\u7b49\u9009\u9879\uff1a TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes \u4e00\u4e9b\u5176\u4ed6\u7684\u5173\u4e8e ssh \u7684\u9009\u9879\u662f\u4e0e\u5b89\u5168\u76f8\u5173\u7684\uff0c\u5e94\u5f53\u5c0f\u5fc3\u7ffc\u7ffc\u7684\u4f7f\u7528\u3002\u4f8b\u5982\u4f60\u5e94\u5f53\u53ea\u80fd\u5728\u53ef\u4fe1\u4efb\u7684\u7f51\u7edc\u4e2d\u542f\u7528 StrictHostKeyChecking=no \uff0c ForwardAgent=yes \u3002 \u8003\u8651\u4f7f\u7528 mosh \u4f5c\u4e3a ssh \u7684\u66ff\u4ee3\u54c1\uff0c\u5b83\u4f7f\u7528 UDP \u534f\u8bae\u3002\u5b83\u53ef\u4ee5\u907f\u514d\u8fde\u63a5\u88ab\u4e2d\u65ad\u5e76\u4e14\u5bf9\u5e26\u5bbd\u9700\u6c42\u66f4\u5c0f\uff0c\u4f46\u5b83\u9700\u8981\u5728\u670d\u52a1\u7aef\u505a\u76f8\u5e94\u7684\u914d\u7f6e\u3002 \u83b7\u53d6\u516b\u8fdb\u5236\u5f62\u5f0f\u7684\u6587\u4ef6\u8bbf\u95ee\u6743\u9650\uff08\u4fee\u6539\u7cfb\u7edf\u8bbe\u7f6e\u65f6\u901a\u5e38\u9700\u8981\uff0c\u4f46 ls \u7684\u529f\u80fd\u4e0d\u90a3\u4e48\u597d\u7528\u5e76\u4e14\u901a\u5e38\u4f1a\u641e\u7838\uff09\uff0c\u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c\u5982\u4e0b\u7684\u4ee3\u7801\uff1a stat -c '%A %a %n' /etc/timezone \u4f7f\u7528 percol \u6216\u8005 fzf \u53ef\u4ee5\u4ea4\u4e92\u5f0f\u5730\u4ece\u53e6\u4e00\u4e2a\u547d\u4ee4\u8f93\u51fa\u4e2d\u9009\u53d6\u503c\u3002 \u4f7f\u7528 fpp \uff08 PathPicker \uff09\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u53e6\u4e00\u4e2a\u547d\u4ee4(\u4f8b\u5982 git \uff09\u8f93\u51fa\u7684\u6587\u4ef6\u4ea4\u4e92\u3002 \u5c06 web \u670d\u52a1\u5668\u4e0a\u5f53\u524d\u76ee\u5f55\u4e0b\u6240\u6709\u7684\u6587\u4ef6\uff08\u4ee5\u53ca\u5b50\u76ee\u5f55\uff09\u66b4\u9732\u7ed9\u4f60\u6240\u5904\u7f51\u7edc\u7684\u6240\u6709\u7528\u6237\uff0c\u4f7f\u7528\uff1a python -m SimpleHTTPServer 7777 \uff08\u4f7f\u7528\u7aef\u53e3 7777 \u548c Python 2\uff09\u6216 python -m http.server 7777 \uff08\u4f7f\u7528\u7aef\u53e3 7777 \u548c Python 3\uff09\u3002 \u4ee5\u5176\u4ed6\u7528\u6237\u7684\u8eab\u4efd\u6267\u884c\u547d\u4ee4\uff0c\u4f7f\u7528 sudo \u3002\u9ed8\u8ba4\u4ee5 root \u7528\u6237\u7684\u8eab\u4efd\u6267\u884c\uff1b\u4f7f\u7528 -u \u6765\u6307\u5b9a\u5176\u4ed6\u7528\u6237\u3002\u4f7f\u7528 -i \u6765\u4ee5\u8be5\u7528\u6237\u767b\u5f55\uff08\u9700\u8981\u8f93\u5165_\u4f60\u81ea\u5df1\u7684_\u5bc6\u7801\uff09\u3002 \u5c06 shell \u5207\u6362\u4e3a\u5176\u4ed6\u7528\u6237\uff0c\u4f7f\u7528 su username \u6216\u8005 sudo - username \u3002\u52a0\u5165 - \u4f1a\u4f7f\u5f97\u5207\u6362\u540e\u7684\u73af\u5883\u4e0e\u4f7f\u7528\u8be5\u7528\u6237\u767b\u5f55\u540e\u7684\u73af\u5883\u76f8\u540c\u3002\u7701\u7565\u7528\u6237\u540d\u5219\u9ed8\u8ba4\u4e3a root\u3002\u5207\u6362\u5230\u54ea\u4e2a\u7528\u6237\uff0c\u5c31\u9700\u8981\u8f93\u5165_\u54ea\u4e2a\u7528\u6237\u7684_\u5bc6\u7801\u3002 \u4e86\u89e3\u547d\u4ee4\u884c\u7684 128K \u9650\u5236 \u3002\u4f7f\u7528\u901a\u914d\u7b26\u5339\u914d\u5927\u91cf\u6587\u4ef6\u540d\u65f6\uff0c\u5e38\u4f1a\u9047\u5230\u201cArgument list too long\u201d\u7684\u9519\u8bef\u4fe1\u606f\u3002\uff08\u8fd9\u79cd\u60c5\u51b5\u4e0b\u6362\u7528 find \u6216 xargs \u901a\u5e38\u53ef\u4ee5\u89e3\u51b3\u3002\uff09 \u5f53\u4f60\u9700\u8981\u4e00\u4e2a\u57fa\u672c\u7684\u8ba1\u7b97\u5668\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528 python \u89e3\u91ca\u5668\uff08\u5f53\u7136\u4f60\u8981\u7528 python \u7684\u65f6\u5019\u4e5f\u662f\u8fd9\u6837\uff09\u3002\u4f8b\u5982\uff1a 2+3 5 \u6587\u4ef6\u53ca\u6570\u636e\u5904\u7406 \u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u901a\u8fc7\u6587\u4ef6\u540d\u67e5\u627e\u4e00\u4e2a\u6587\u4ef6\uff0c\u4f7f\u7528\u7c7b\u4f3c\u4e8e\u8fd9\u6837\u7684\u547d\u4ee4\uff1a find . -iname '*something*' \u3002\u5728\u6240\u6709\u8def\u5f84\u4e0b\u901a\u8fc7\u6587\u4ef6\u540d\u67e5\u627e\u6587\u4ef6\uff0c\u4f7f\u7528 locate something \uff08\u4f46\u6ce8\u610f\u5230 updatedb \u53ef\u80fd\u6ca1\u6709\u5bf9\u6700\u8fd1\u65b0\u5efa\u7684\u6587\u4ef6\u5efa\u7acb\u7d22\u5f15\uff0c\u6240\u4ee5\u4f60\u53ef\u80fd\u65e0\u6cd5\u5b9a\u4f4d\u5230\u8fd9\u4e9b\u672a\u88ab\u7d22\u5f15\u7684\u6587\u4ef6\uff09\u3002 \u4f7f\u7528 ag \u5728\u6e90\u4ee3\u7801\u6216\u6570\u636e\u6587\u4ef6\u91cc\u68c0\u7d22\uff08 grep -r \u540c\u6837\u53ef\u4ee5\u505a\u5230\uff0c\u4f46\u76f8\u6bd4\u4e4b\u4e0b ag \u66f4\u52a0\u5148\u8fdb\uff09\u3002 \u5c06 HTML \u8f6c\u4e3a\u6587\u672c\uff1a lynx -dump -stdin \u3002 Markdown\uff0cHTML\uff0c\u4ee5\u53ca\u6240\u6709\u6587\u6863\u683c\u5f0f\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u8bd5\u8bd5 pandoc \u3002 \u5f53\u4f60\u8981\u5904\u7406\u68d8\u624b\u7684 XML \u65f6\u5019\uff0c xmlstarlet \u7b97\u662f\u4e0a\u53e4\u65f6\u4ee3\u6d41\u4f20\u4e0b\u6765\u7684\u795e\u5668\u3002 \u4f7f\u7528 jq \u5904\u7406 JSON\u3002 \u4f7f\u7528 shyaml \u5904\u7406 YAML\u3002 \u8981\u5904\u7406 Excel \u6216 CSV \u6587\u4ef6\u7684\u8bdd\uff0c csvkit \u63d0\u4f9b\u4e86 in2csv \uff0c csvcut \uff0c csvjoin \uff0c csvgrep \u7b49\u65b9\u4fbf\u6613\u7528\u7684\u5de5\u5177\u3002 \u5f53\u4f60\u8981\u5904\u7406 Amazon S3 \u76f8\u5173\u7684\u5de5\u4f5c\u7684\u65f6\u5019\uff0c s3cmd \u662f\u4e00\u4e2a\u5f88\u65b9\u4fbf\u7684\u5de5\u5177\u800c s4cmd \u7684\u6548\u7387\u66f4\u9ad8\u3002Amazon \u5b98\u65b9\u63d0\u4f9b\u7684 aws \u4ee5\u53ca saws \u662f\u5176\u4ed6 AWS \u76f8\u5173\u5de5\u4f5c\u7684\u57fa\u7840\uff0c\u503c\u5f97\u5b66\u4e60\u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 sort \u548c uniq \uff0c\u5305\u62ec uniq \u7684 -u \u53c2\u6570\u548c -d \u53c2\u6570\uff0c\u5177\u4f53\u5185\u5bb9\u5728\u540e\u6587\u5355\u884c\u811a\u672c\u8282\u4e2d\u3002\u53e6\u5916\u53ef\u4ee5\u4e86\u89e3\u4e00\u4e0b comm \u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 cut \uff0c paste \u548c join \u6765\u66f4\u6539\u6587\u4ef6\u3002\u5f88\u591a\u4eba\u90fd\u4f1a\u4f7f\u7528 cut \uff0c\u4f46\u9057\u5fd8\u4e86 join \u3002 \u4e86\u89e3\u5982\u4f55\u8fd0\u7528 wc \u53bb\u8ba1\u7b97\u65b0\u884c\u6570\uff08 -l \uff09\uff0c\u5b57\u7b26\u6570\uff08 -m \uff09\uff0c\u5355\u8bcd\u6570\uff08 -w \uff09\u4ee5\u53ca\u5b57\u8282\u6570\uff08 -c \uff09\u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 tee \u5c06\u6807\u51c6\u8f93\u5165\u590d\u5236\u5230\u6587\u4ef6\u751a\u81f3\u6807\u51c6\u8f93\u51fa\uff0c\u4f8b\u5982 ls -al | tee file.txt \u3002 \u8981\u8fdb\u884c\u4e00\u4e9b\u590d\u6742\u7684\u8ba1\u7b97\uff0c\u6bd4\u5982\u5206\u7ec4\u3001\u9006\u5e8f\u548c\u4e00\u4e9b\u5176\u4ed6\u7684\u7edf\u8ba1\u5206\u6790\uff0c\u53ef\u4ee5\u8003\u8651\u4f7f\u7528 datamash \u3002 \u6ce8\u610f\u5230\u8bed\u8a00\u8bbe\u7f6e\uff08\u4e2d\u6587\u6216\u82f1\u6587\u7b49\uff09\u5bf9\u8bb8\u591a\u547d\u4ee4\u884c\u5de5\u5177\u6709\u4e00\u4e9b\u5fae\u5999\u7684\u5f71\u54cd\uff0c\u6bd4\u5982\u6392\u5e8f\u7684\u987a\u5e8f\u548c\u6027\u80fd\u3002\u5927\u591a\u6570 Linux \u7684\u5b89\u88c5\u8fc7\u7a0b\u4f1a\u5c06 LANG \u6216\u5176\u4ed6\u6709\u5173\u7684\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u7b26\u5408\u672c\u5730\u7684\u8bbe\u7f6e\u3002\u8981\u610f\u8bc6\u5230\u5f53\u4f60\u6539\u53d8\u8bed\u8a00\u8bbe\u7f6e\u65f6\uff0c\u6392\u5e8f\u7684\u7ed3\u679c\u53ef\u80fd\u4f1a\u6539\u53d8\u3002\u660e\u767d\u56fd\u9645\u5316\u53ef\u80fd\u4f1a\u4f7f sort \u6216\u5176\u4ed6\u547d\u4ee4\u8fd0\u884c\u6548\u7387\u4e0b\u964d \u8bb8\u591a\u500d \u3002\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\u96c6\u5408\u8fd0\u7b97\uff09\u4f60\u53ef\u4ee5\u653e\u5fc3\u7684\u4f7f\u7528 export LC_ALL=C \u6765\u5ffd\u7565\u6389\u56fd\u9645\u5316\u5e76\u6309\u7167\u5b57\u8282\u6765\u5224\u65ad\u987a\u5e8f\u3002 \u4f60\u53ef\u4ee5\u5355\u72ec\u6307\u5b9a\u67d0\u4e00\u6761\u547d\u4ee4\u7684\u73af\u5883\uff0c\u53ea\u9700\u5728\u8c03\u7528\u65f6\u628a\u73af\u5883\u53d8\u91cf\u8bbe\u5b9a\u653e\u5728\u547d\u4ee4\u7684\u524d\u9762\uff0c\u4f8b\u5982 TZ=Pacific/Fiji date \u53ef\u4ee5\u83b7\u53d6\u6590\u6d4e\u7684\u65f6\u95f4\u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 awk \u548c sed \u6765\u8fdb\u884c\u7b80\u5355\u7684\u6570\u636e\u5904\u7406\u3002 \u53c2\u9605 One-liners \u83b7\u53d6\u793a\u4f8b\u3002 \u66ff\u6362\u4e00\u4e2a\u6216\u591a\u4e2a\u6587\u4ef6\u4e2d\u51fa\u73b0\u7684\u5b57\u7b26\u4e32\uff1a perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt \u4f7f\u7528 repren \u6765\u6279\u91cf\u91cd\u547d\u540d\u6587\u4ef6\uff0c\u6216\u662f\u5728\u591a\u4e2a\u6587\u4ef6\u4e2d\u641c\u7d22\u66ff\u6362\u5185\u5bb9\u3002\uff08\u6709\u4e9b\u65f6\u5019 rename \u547d\u4ee4\u4e5f\u53ef\u4ee5\u6279\u91cf\u91cd\u547d\u540d\uff0c\u4f46\u8981\u6ce8\u610f\uff0c\u5b83\u5728\u4e0d\u540c Linux \u53d1\u884c\u7248\u4e2d\u7684\u529f\u80fd\u5e76\u4e0d\u5b8c\u5168\u4e00\u6837\u3002\uff09 # \u5c06\u6587\u4ef6\u3001\u76ee\u5f55\u548c\u5185\u5bb9\u5168\u90e8\u91cd\u547d\u540d foo - bar: repren --full --preserve-case --from foo --to bar . # \u8fd8\u539f\u6240\u6709\u5907\u4efd\u6587\u4ef6 whatever.bak - whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # \u7528 rename \u5b9e\u73b0\u4e0a\u8ff0\u529f\u80fd\uff08\u82e5\u53ef\u7528\uff09: rename 's/\\.bak$//' *.bak \u6839\u636e man \u9875\u9762\u7684\u63cf\u8ff0\uff0c rsync \u662f\u4e00\u4e2a\u5feb\u901f\u4e14\u975e\u5e38\u7075\u6d3b\u7684\u6587\u4ef6\u590d\u5236\u5de5\u5177\u3002\u5b83\u95fb\u540d\u4e8e\u8bbe\u5907\u4e4b\u95f4\u7684\u6587\u4ef6\u540c\u6b65\uff0c\u4f46\u5176\u5b9e\u5b83\u5728\u672c\u5730\u60c5\u51b5\u4e0b\u4e5f\u540c\u6837\u6709\u7528\u3002\u5728\u5b89\u5168\u8bbe\u7f6e\u5141\u8bb8\u4e0b\uff0c\u7528 rsync \u4ee3\u66ff scp \u53ef\u4ee5\u5b9e\u73b0\u6587\u4ef6\u7eed\u4f20\uff0c\u800c\u4e0d\u7528\u91cd\u65b0\u4ece\u5934\u5f00\u59cb\u3002\u5b83\u540c\u65f6\u4e5f\u662f\u5220\u9664\u5927\u91cf\u6587\u4ef6\u7684 \u6700\u5feb\u65b9\u6cd5 \u4e4b\u4e00\uff1a mkdir empty rsync -r --delete empty/ some-dir rmdir some-dir \u82e5\u8981\u5728\u590d\u5236\u6587\u4ef6\u65f6\u83b7\u53d6\u5f53\u524d\u8fdb\u5ea6\uff0c\u53ef\u4f7f\u7528 pv \uff0c pycp \uff0c progress \uff0c rsync --progress \u3002\u82e5\u6240\u6267\u884c\u7684\u590d\u5236\u4e3ablock\u5757\u62f7\u8d1d\uff0c\u53ef\u4ee5\u4f7f\u7528 dd status=progress \u3002 \u4f7f\u7528 shuf \u53ef\u4ee5\u4ee5\u884c\u4e3a\u5355\u4f4d\u6765\u6253\u4e71\u6587\u4ef6\u7684\u5185\u5bb9\u6216\u4ece\u4e00\u4e2a\u6587\u4ef6\u4e2d\u968f\u673a\u9009\u53d6\u591a\u884c\u3002 \u4e86\u89e3 sort \u7684\u53c2\u6570\u3002\u663e\u793a\u6570\u5b57\u65f6\uff0c\u4f7f\u7528 -n \u6216\u8005 -h \u6765\u663e\u793a\u66f4\u6613\u8bfb\u7684\u6570\uff08\u4f8b\u5982 du -h \u7684\u8f93\u51fa\uff09\u3002\u660e\u767d\u6392\u5e8f\u65f6\u5173\u952e\u5b57\u7684\u5de5\u4f5c\u539f\u7406\uff08 -t \u548c -k \uff09\u3002\u4f8b\u5982\uff0c\u6ce8\u610f\u5230\u4f60\u9700\u8981 -k1\uff0c1 \u6765\u4ec5\u6309\u7b2c\u4e00\u4e2a\u57df\u6765\u6392\u5e8f\uff0c\u800c -k1 \u610f\u5473\u7740\u6309\u6574\u884c\u6392\u5e8f\u3002\u7a33\u5b9a\u6392\u5e8f\uff08 sort -s \uff09\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5f88\u6709\u7528\u3002\u4f8b\u5982\uff0c\u4ee5\u7b2c\u4e8c\u4e2a\u57df\u4e3a\u4e3b\u5173\u952e\u5b57\uff0c\u7b2c\u4e00\u4e2a\u57df\u4e3a\u6b21\u5173\u952e\u5b57\u8fdb\u884c\u6392\u5e8f\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 sort -k1\uff0c1 | sort -s -k2\uff0c2 \u3002 \u5982\u679c\u4f60\u60f3\u5728 Bash \u547d\u4ee4\u884c\u4e2d\u5199 tab \u5236\u8868\u7b26\uff0c\u6309\u4e0b ctrl-v [Tab] \u6216\u952e\u5165 $'\\t' \uff08\u540e\u8005\u53ef\u80fd\u66f4\u597d\uff0c\u56e0\u4e3a\u4f60\u53ef\u4ee5\u590d\u5236\u7c98\u8d34\u5b83\uff09\u3002 \u6807\u51c6\u7684\u6e90\u4ee3\u7801\u5bf9\u6bd4\u53ca\u5408\u5e76\u5de5\u5177\u662f diff \u548c patch \u3002\u4f7f\u7528 diffstat \u67e5\u770b\u53d8\u66f4\u603b\u89c8\u6570\u636e\u3002\u6ce8\u610f\u5230 diff -r \u5bf9\u6574\u4e2a\u6587\u4ef6\u5939\u6709\u6548\u3002\u4f7f\u7528 diff -r tree1 tree2 | diffstat \u67e5\u770b\u53d8\u66f4\u7684\u7edf\u8ba1\u6570\u636e\u3002 vimdiff \u7528\u4e8e\u6bd4\u5bf9\u5e76\u7f16\u8f91\u6587\u4ef6\u3002 \u5bf9\u4e8e\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4f7f\u7528 hd \uff0c hexdump \u6216\u8005 xxd \u4f7f\u5176\u4ee5\u5341\u516d\u8fdb\u5236\u663e\u793a\uff0c\u4f7f\u7528 bvi \uff0c hexedit \u6216\u8005 biew \u6765\u8fdb\u884c\u4e8c\u8fdb\u5236\u7f16\u8f91\u3002 \u540c\u6837\u5bf9\u4e8e\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c strings \uff08\u5305\u62ec grep \u7b49\u5de5\u5177\uff09\u53ef\u4ee5\u5e2e\u52a9\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\u67e5\u627e\u7279\u5b9a\u6bd4\u7279\u3002 \u5236\u4f5c\u4e8c\u8fdb\u5236\u5dee\u5206\u6587\u4ef6\uff08Delta \u538b\u7f29\uff09\uff0c\u4f7f\u7528 xdelta3 \u3002 \u4f7f\u7528 iconv \u66f4\u6539\u6587\u672c\u7f16\u7801\u3002\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u4f7f\u7528 uconv \uff0c\u5b83\u652f\u6301\u4e00\u4e9b\u9ad8\u7ea7\u7684 Unicode \u529f\u80fd\u3002\u4f8b\u5982\uff0c\u8fd9\u6761\u547d\u4ee4\u79fb\u9664\u4e86\u6240\u6709\u91cd\u97f3\u7b26\u53f7\uff1a uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] ; ::Any-NFC; ' input.txt output.txt \u62c6\u5206\u6587\u4ef6\u53ef\u4ee5\u4f7f\u7528 split \uff08\u6309\u5927\u5c0f\u62c6\u5206\uff09\u548c csplit \uff08\u6309\u6a21\u5f0f\u62c6\u5206\uff09\u3002 \u64cd\u4f5c\u65e5\u671f\u548c\u65f6\u95f4\u8868\u8fbe\u5f0f\uff0c\u53ef\u4ee5\u7528 dateutils \u4e2d\u7684 dateadd \u3001 datediff \u3001 strptime \u7b49\u5de5\u5177\u3002 \u4f7f\u7528 zless \u3001 zmore \u3001 zcat \u548c zgrep \u5bf9\u538b\u7f29\u8fc7\u7684\u6587\u4ef6\u8fdb\u884c\u64cd\u4f5c\u3002 \u6587\u4ef6\u5c5e\u6027\u53ef\u4ee5\u901a\u8fc7 chattr \u8fdb\u884c\u8bbe\u7f6e\uff0c\u5b83\u6bd4\u6587\u4ef6\u6743\u9650\u66f4\u52a0\u5e95\u5c42\u3002\u4f8b\u5982\uff0c\u4e3a\u4e86\u4fdd\u62a4\u6587\u4ef6\u4e0d\u88ab\u610f\u5916\u5220\u9664\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0d\u53ef\u4fee\u6539\u6807\u8bb0\uff1a sudo chattr +i /critical/directory/or/file \u4f7f\u7528 getfacl \u548c setfacl \u4ee5\u4fdd\u5b58\u548c\u6062\u590d\u6587\u4ef6\u6743\u9650\u3002\u4f8b\u5982\uff1a getfacl -R /some/path permissions.txt setfacl --restore=permissions.txt \u4e3a\u4e86\u9ad8\u6548\u5730\u521b\u5efa\u7a7a\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528 truncate \uff08\u521b\u5efa \u7a00\u758f\u6587\u4ef6 \uff09\uff0c fallocate \uff08\u7528\u4e8e ext4\uff0cxfs\uff0cbtrf \u548c ocfs2 \u6587\u4ef6\u7cfb\u7edf\uff09\uff0c xfs_mkfile \uff08\u9002\u7528\u4e8e\u51e0\u4e4e\u6240\u6709\u7684\u6587\u4ef6\u7cfb\u7edf\uff0c\u5305\u542b\u5728 xfsprogs \u5305\u4e2d\uff09\uff0c mkfile \uff08\u7528\u4e8e\u7c7b Unix \u64cd\u4f5c\u7cfb\u7edf\uff0c\u6bd4\u5982 Solaris \u548c Mac OS\uff09\u3002 \u7cfb\u7edf\u8c03\u8bd5 curl \u548c curl -I \u53ef\u4ee5\u88ab\u8f7b\u677e\u5730\u5e94\u7528\u4e8e web \u8c03\u8bd5\u4e2d\uff0c\u5b83\u4eec\u7684\u597d\u5144\u5f1f wget \u4e5f\u662f\u5982\u6b64\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u8bd5\u8bd5\u66f4\u6f6e\u7684 httpie \u3002 \u83b7\u53d6 CPU \u548c\u786c\u76d8\u7684\u4f7f\u7528\u72b6\u6001\uff0c\u901a\u5e38\u4f7f\u7528\u4f7f\u7528 top \uff08 htop \u66f4\u4f73\uff09\uff0c iostat \u548c iotop \u3002\u800c iostat -mxz 15 \u53ef\u4ee5\u8ba9\u4f60\u83b7\u6089 CPU \u548c\u6bcf\u4e2a\u786c\u76d8\u5206\u533a\u7684\u57fa\u672c\u4fe1\u606f\u548c\u6027\u80fd\u8868\u73b0\u3002 \u4f7f\u7528 netstat \u548c ss \u67e5\u770b\u7f51\u7edc\u8fde\u63a5\u7684\u7ec6\u8282\u3002 dstat \u5728\u4f60\u60f3\u8981\u5bf9\u7cfb\u7edf\u7684\u73b0\u72b6\u6709\u4e00\u4e2a\u7c97\u7565\u7684\u8ba4\u8bc6\u65f6\u662f\u975e\u5e38\u6709\u7528\u7684\u3002\u7136\u800c\u82e5\u8981\u5bf9\u7cfb\u7edf\u6709\u4e00\u4e2a\u6df1\u5ea6\u7684\u603b\u4f53\u8ba4\u8bc6\uff0c\u4f7f\u7528 glances \uff0c\u5b83\u4f1a\u5728\u4e00\u4e2a\u7ec8\u7aef\u7a97\u53e3\u4e2d\u5411\u4f60\u63d0\u4f9b\u4e00\u4e9b\u7cfb\u7edf\u7ea7\u7684\u6570\u636e\u3002 \u82e5\u8981\u4e86\u89e3\u5185\u5b58\u72b6\u6001\uff0c\u8fd0\u884c\u5e76\u7406\u89e3 free \u548c vmstat \u7684\u8f93\u51fa\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\u201ccached\u201d\u7684\u503c\uff0c\u5b83\u6307\u7684\u662f Linux \u5185\u6838\u7528\u6765\u4f5c\u4e3a\u6587\u4ef6\u7f13\u5b58\u7684\u5185\u5b58\u5927\u5c0f\uff0c\u800c\u4e0e\u7a7a\u95f2\u5185\u5b58\u65e0\u5173\u3002 Java \u7cfb\u7edf\u8c03\u8bd5\u5219\u662f\u4e00\u4ef6\u622a\u7136\u4e0d\u540c\u7684\u4e8b\uff0c\u4e00\u4e2a\u53ef\u4ee5\u7528\u4e8e Oracle \u7684 JVM \u6216\u5176\u4ed6 JVM \u4e0a\u7684\u8c03\u8bd5\u7684\u6280\u5de7\u662f\u4f60\u53ef\u4ee5\u8fd0\u884c kill -3 pid \u540c\u65f6\u4e00\u4e2a\u5b8c\u6574\u7684\u6808\u8f68\u8ff9\u548c\u5806\u6982\u8ff0\uff08\u5305\u62ec GC \u7684\u7ec6\u8282\uff09\u4f1a\u88ab\u4fdd\u5b58\u5230\u6807\u51c6\u9519\u8bef\u6216\u662f\u65e5\u5fd7\u6587\u4ef6\u3002JDK \u4e2d\u7684 jps \uff0c jstat \uff0c jstack \uff0c jmap \u5f88\u6709\u7528\u3002 SJK tools \u66f4\u9ad8\u7ea7\u3002 \u4f7f\u7528 mtr \u53bb\u8ddf\u8e2a\u8def\u7531\uff0c\u7528\u4e8e\u786e\u5b9a\u7f51\u7edc\u95ee\u9898\u3002 \u7528 ncdu \u6765\u67e5\u770b\u78c1\u76d8\u4f7f\u7528\u60c5\u51b5\uff0c\u5b83\u6bd4\u5bfb\u5e38\u7684\u547d\u4ee4\uff0c\u5982 du -sh * \uff0c\u66f4\u8282\u7701\u65f6\u95f4\u3002 \u67e5\u627e\u6b63\u5728\u4f7f\u7528\u5e26\u5bbd\u7684\u5957\u63a5\u5b57\u8fde\u63a5\u6216\u8fdb\u7a0b\uff0c\u4f7f\u7528 iftop \u6216 nethogs \u3002 ab \u5de5\u5177\uff08Apache \u4e2d\u81ea\u5e26\uff09\u53ef\u4ee5\u7b80\u5355\u7c97\u66b4\u5730\u68c0\u67e5 web \u670d\u52a1\u5668\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u4f7f\u7528 siege \u3002 wireshark \uff0c tshark \u548c ngrep \u53ef\u7528\u4e8e\u590d\u6742\u7684\u7f51\u7edc\u8c03\u8bd5\u3002 \u4e86\u89e3 strace \u548c ltrace \u3002\u8fd9\u4fe9\u5de5\u5177\u5728\u4f60\u7684\u7a0b\u5e8f\u8fd0\u884c\u5931\u8d25\u3001\u6302\u8d77\u751a\u81f3\u5d29\u6e83\uff0c\u800c\u4f60\u5374\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u6216\u4f60\u60f3\u5bf9\u6027\u80fd\u6709\u4e2a\u603b\u4f53\u7684\u8ba4\u8bc6\u7684\u65f6\u5019\u662f\u975e\u5e38\u6709\u7528\u7684\u3002\u6ce8\u610f profile \u53c2\u6570\uff08 -c \uff09\u548c\u9644\u52a0\u5230\u4e00\u4e2a\u8fd0\u884c\u7684\u8fdb\u7a0b\u53c2\u6570 \uff08 -p \uff09\u3002 \u4e86\u89e3\u4f7f\u7528 ldd \u6765\u68c0\u67e5\u5171\u4eab\u5e93\u3002\u4f46\u662f \u6c38\u8fdc\u4e0d\u8981\u5728\u4e0d\u4fe1\u4efb\u7684\u6587\u4ef6\u4e0a\u8fd0\u884c \u3002 \u4e86\u89e3\u5982\u4f55\u8fd0\u7528 gdb \u8fde\u63a5\u5230\u4e00\u4e2a\u8fd0\u884c\u7740\u7684\u8fdb\u7a0b\u5e76\u83b7\u53d6\u5b83\u7684\u5806\u6808\u8f68\u8ff9\u3002 \u5b66\u4f1a\u4f7f\u7528 /proc \u3002\u5b83\u5728\u8c03\u8bd5\u6b63\u5728\u51fa\u73b0\u7684\u95ee\u9898\u7684\u65f6\u5019\u6709\u65f6\u4f1a\u6548\u679c\u60ca\u4eba\u3002\u6bd4\u5982\uff1a /proc/cpuinfo \uff0c /proc/meminfo \uff0c /proc/cmdline \uff0c /proc/xxx/cwd \uff0c /proc/xxx/exe \uff0c /proc/xxx/fd/ \uff0c /proc/xxx/smaps \uff08\u8fd9\u91cc\u7684 xxx \u8868\u793a\u8fdb\u7a0b\u7684 id \u6216 pid\uff09\u3002 \u5f53\u8c03\u8bd5\u4e00\u4e9b\u4e4b\u524d\u51fa\u73b0\u7684\u95ee\u9898\u7684\u65f6\u5019\uff0c sar \u975e\u5e38\u6709\u7528\u3002\u5b83\u5c55\u793a\u4e86 cpu\u3001\u5185\u5b58\u4ee5\u53ca\u7f51\u7edc\u7b49\u7684\u5386\u53f2\u6570\u636e\u3002 \u5173\u4e8e\u66f4\u6df1\u5c42\u6b21\u7684\u7cfb\u7edf\u5206\u6790\u4ee5\u53ca\u6027\u80fd\u5206\u6790\uff0c\u770b\u770b stap \uff08 SystemTap \uff09\uff0c perf \uff0c\u4ee5\u53ca sysdig \u3002 \u67e5\u770b\u4f60\u5f53\u524d\u4f7f\u7528\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528 uname \uff0c uname -a \uff08Unix\uff0fkernel \u4fe1\u606f\uff09\u6216\u8005 lsb_release -a \uff08Linux \u53d1\u884c\u7248\u4fe1\u606f\uff09\u3002 \u65e0\u8bba\u4ec0\u4e48\u4e1c\u897f\u5de5\u4f5c\u5f97\u5f88\u6b22\u4e50\uff08\u53ef\u80fd\u662f\u786c\u4ef6\u6216\u9a71\u52a8\u95ee\u9898\uff09\u65f6\u53ef\u4ee5\u8bd5\u8bd5 dmesg \u3002 \u5982\u679c\u4f60\u5220\u9664\u4e86\u4e00\u4e2a\u6587\u4ef6\uff0c\u4f46\u901a\u8fc7 du \u53d1\u73b0\u6ca1\u6709\u91ca\u653e\u9884\u671f\u7684\u78c1\u76d8\u7a7a\u95f4\uff0c\u8bf7\u68c0\u67e5\u6587\u4ef6\u662f\u5426\u88ab\u8fdb\u7a0b\u5360\u7528\uff1a lsof | grep deleted | grep \"filename-of-my-big-file\" \u5355\u884c\u811a\u672c \u4e00\u4e9b\u547d\u4ee4\u7ec4\u5408\u7684\u4f8b\u5b50\uff1a \u5f53\u4f60\u9700\u8981\u5bf9\u6587\u672c\u6587\u4ef6\u505a\u96c6\u5408\u4ea4\u3001\u5e76\u3001\u5dee\u8fd0\u7b97\u65f6\uff0c sort \u548c uniq \u4f1a\u662f\u4f60\u7684\u597d\u5e2e\u624b\u3002\u5177\u4f53\u4f8b\u5b50\u8bf7\u53c2\u7167\u4ee3\u7801\u540e\u9762\u7684\uff0c\u6b64\u5904\u5047\u8bbe a \u4e0e b \u662f\u4e24\u5185\u5bb9\u4e0d\u540c\u7684\u6587\u4ef6\u3002\u8fd9\u79cd\u65b9\u5f0f\u6548\u7387\u5f88\u9ad8\uff0c\u5e76\u4e14\u5728\u5c0f\u6587\u4ef6\u548c\u4e0a G \u7684\u6587\u4ef6\u4e0a\u90fd\u80fd\u8fd0\u7528\uff08\u6ce8\u610f\u5c3d\u7ba1\u5728 /tmp \u5728\u4e00\u4e2a\u5c0f\u7684\u6839\u5206\u533a\u4e0a\u65f6\u4f60\u53ef\u80fd\u9700\u8981 -T \u53c2\u6570\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a sort \u5e76\u4e0d\u88ab\u5185\u5b58\u5927\u5c0f\u7ea6\u675f\uff09\uff0c\u53c2\u9605\u524d\u6587\u4e2d\u5173\u4e8e LC_ALL \u548c sort \u7684 -u \u53c2\u6570\u7684\u90e8\u5206\u3002 sort a b | uniq c # c \u662f a \u5e76 b sort a b | uniq -d c # c \u662f a \u4ea4 b sort a b b | uniq -u c # c \u662f a - b \u4f7f\u7528 grep . * \uff08\u6bcf\u884c\u90fd\u4f1a\u9644\u4e0a\u6587\u4ef6\u540d\uff09\u6216\u8005 head -100 * \uff08\u6bcf\u4e2a\u6587\u4ef6\u6709\u4e00\u4e2a\u6807\u9898\uff09\u6765\u9605\u8bfb\u68c0\u67e5\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\u7684\u5185\u5bb9\u3002\u8fd9\u5728\u68c0\u67e5\u4e00\u4e2a\u5145\u6ee1\u914d\u7f6e\u6587\u4ef6\u7684\u76ee\u5f55\uff08\u5982 /sys \u3001 /proc \u3001 /etc \uff09\u65f6\u7279\u522b\u597d\u7528\u3002 \u8ba1\u7b97\u6587\u672c\u6587\u4ef6\u7b2c\u4e09\u5217\u4e2d\u6240\u6709\u6570\u7684\u548c\uff08\u53ef\u80fd\u6bd4\u540c\u7b49\u4f5c\u7528\u7684 Python \u4ee3\u7801\u5feb\u4e09\u500d\u4e14\u4ee3\u7801\u91cf\u5c11\u4e09\u500d\uff09\uff1a awk '{ x += $3 } END { print x }' myfile \u5982\u679c\u4f60\u60f3\u5728\u6587\u4ef6\u6811\u4e0a\u67e5\u770b\u5927\u5c0f/\u65e5\u671f\uff0c\u8fd9\u53ef\u80fd\u770b\u8d77\u6765\u50cf\u9012\u5f52\u7248\u7684 ls -l \u4f46\u6bd4 ls -lR \u66f4\u6613\u4e8e\u7406\u89e3\uff1a find . -type f -ls \u5047\u8bbe\u4f60\u6709\u4e00\u4e2a\u7c7b\u4f3c\u4e8e web \u670d\u52a1\u5668\u65e5\u5fd7\u6587\u4ef6\u7684\u6587\u672c\u6587\u4ef6\uff0c\u5e76\u4e14\u4e00\u4e2a\u786e\u5b9a\u7684\u503c\u53ea\u4f1a\u51fa\u73b0\u5728\u67d0\u4e9b\u884c\u4e0a\uff0c\u5047\u8bbe\u4e00\u4e2a acct_id \u53c2\u6570\u5728 URI \u4e2d\u3002\u5982\u679c\u4f60\u60f3\u8ba1\u7b97\u51fa\u6bcf\u4e2a acct_id \u503c\u6709\u591a\u5c11\u6b21\u8bf7\u6c42\uff0c\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\uff1a egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn \u8981\u6301\u7eed\u76d1\u6d4b\u6587\u4ef6\u6539\u52a8\uff0c\u53ef\u4ee5\u4f7f\u7528 watch \uff0c\u4f8b\u5982\u68c0\u67e5\u67d0\u4e2a\u6587\u4ef6\u5939\u4e2d\u6587\u4ef6\u7684\u6539\u53d8\uff0c\u53ef\u4ee5\u7528 watch -d -n 2 'ls -rtlh | tail' \uff1b\u6216\u8005\u5728\u6392\u67e5 WiFi \u8bbe\u7f6e\u6545\u969c\u65f6\u8981\u76d1\u6d4b\u7f51\u7edc\u8bbe\u7f6e\u7684\u66f4\u6539\uff0c\u53ef\u4ee5\u7528 watch -d -n 2 ifconfig \u3002 \u8fd0\u884c\u8fd9\u4e2a\u51fd\u6570\u4ece\u8fd9\u7bc7\u6587\u6863\u4e2d\u968f\u673a\u83b7\u53d6\u4e00\u6761\u6280\u5de7\uff08\u89e3\u6790 Markdown \u6587\u4ef6\u5e76\u62bd\u53d6\u9879\u76ee\uff09\uff1a function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README-zh.md| pandoc -f markdown -t html | iconv -f 'utf-8' -t 'unicode' | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v (html/body/ul/li[count(p) 0])[$RANDOM mod last()+1] | xmlstarlet unesc | fmt -80 } \u51b7\u95e8\u4f46\u6709\u7528 expr \uff1a\u8ba1\u7b97\u8868\u8fbe\u5f0f\u6216\u6b63\u5219\u5339\u914d m4 \uff1a\u7b80\u5355\u7684\u5b8f\u5904\u7406\u5668 yes \uff1a\u591a\u6b21\u6253\u5370\u5b57\u7b26\u4e32 cal \uff1a\u6f02\u4eae\u7684\u65e5\u5386 env \uff1a\u6267\u884c\u4e00\u4e2a\u547d\u4ee4\uff08\u811a\u672c\u6587\u4ef6\u4e2d\u5f88\u6709\u7528\uff09 printenv \uff1a\u6253\u5370\u73af\u5883\u53d8\u91cf\uff08\u8c03\u8bd5\u65f6\u6216\u5728\u5199\u811a\u672c\u6587\u4ef6\u65f6\u5f88\u6709\u7528\uff09 look \uff1a\u67e5\u627e\u4ee5\u7279\u5b9a\u5b57\u7b26\u4e32\u5f00\u5934\u7684\u5355\u8bcd\u6216\u884c cut \uff0c paste \u548c join \uff1a\u6570\u636e\u4fee\u6539 fmt \uff1a\u683c\u5f0f\u5316\u6587\u672c\u6bb5\u843d pr \uff1a\u5c06\u6587\u672c\u683c\u5f0f\u5316\u6210\u9875\uff0f\u5217\u5f62\u5f0f fold \uff1a\u5305\u88f9\u6587\u672c\u4e2d\u7684\u51e0\u884c column \uff1a\u5c06\u6587\u672c\u683c\u5f0f\u5316\u6210\u591a\u4e2a\u5bf9\u9f50\u3001\u5b9a\u5bbd\u7684\u5217\u6216\u8868\u683c expand \u548c unexpand \uff1a\u5236\u8868\u7b26\u4e0e\u7a7a\u683c\u4e4b\u95f4\u8f6c\u6362 nl \uff1a\u6dfb\u52a0\u884c\u53f7 seq \uff1a\u6253\u5370\u6570\u5b57 bc \uff1a\u8ba1\u7b97\u5668 factor \uff1a\u5206\u89e3\u56e0\u6570 gpg \uff1a\u52a0\u5bc6\u5e76\u7b7e\u540d\u6587\u4ef6 toe \uff1aterminfo \u5165\u53e3\u5217\u8868 nc \uff1a\u7f51\u7edc\u8c03\u8bd5\u53ca\u6570\u636e\u4f20\u8f93 socat \uff1a\u5957\u63a5\u5b57\u4ee3\u7406\uff0c\u4e0e netcat \u7c7b\u4f3c slurm \uff1a\u7f51\u7edc\u6d41\u91cf\u53ef\u89c6\u5316 dd \uff1a\u6587\u4ef6\u6216\u8bbe\u5907\u95f4\u4f20\u8f93\u6570\u636e file \uff1a\u786e\u5b9a\u6587\u4ef6\u7c7b\u578b tree \uff1a\u4ee5\u6811\u7684\u5f62\u5f0f\u663e\u793a\u8def\u5f84\u548c\u6587\u4ef6\uff0c\u7c7b\u4f3c\u4e8e\u9012\u5f52\u7684 ls stat \uff1a\u6587\u4ef6\u4fe1\u606f time \uff1a\u6267\u884c\u547d\u4ee4\uff0c\u5e76\u8ba1\u7b97\u6267\u884c\u65f6\u95f4 timeout \uff1a\u5728\u6307\u5b9a\u65f6\u957f\u8303\u56f4\u5185\u6267\u884c\u547d\u4ee4\uff0c\u5e76\u5728\u89c4\u5b9a\u65f6\u95f4\u7ed3\u675f\u540e\u505c\u6b62\u8fdb\u7a0b lockfile \uff1a\u4f7f\u6587\u4ef6\u53ea\u80fd\u901a\u8fc7 rm -f \u79fb\u9664 logrotate \uff1a \u5207\u6362\u3001\u538b\u7f29\u4ee5\u53ca\u53d1\u9001\u65e5\u5fd7\u6587\u4ef6 watch \uff1a\u91cd\u590d\u8fd0\u884c\u540c\u4e00\u4e2a\u547d\u4ee4\uff0c\u5c55\u793a\u7ed3\u679c\u5e76\uff0f\u6216\u9ad8\u4eae\u6709\u66f4\u6539\u7684\u90e8\u5206 when-changed \uff1a\u5f53\u68c0\u6d4b\u5230\u6587\u4ef6\u66f4\u6539\u65f6\u6267\u884c\u6307\u5b9a\u547d\u4ee4\u3002\u53c2\u9605 inotifywait \u548c entr \u3002 tac \uff1a\u53cd\u5411\u8f93\u51fa\u6587\u4ef6 shuf \uff1a\u6587\u4ef6\u4e2d\u968f\u673a\u9009\u53d6\u51e0\u884c comm \uff1a\u4e00\u884c\u4e00\u884c\u7684\u6bd4\u8f83\u6392\u5e8f\u8fc7\u7684\u6587\u4ef6 strings \uff1a\u4ece\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\u62bd\u53d6\u6587\u672c tr \uff1a\u8f6c\u6362\u5b57\u6bcd iconv \u6216 uconv \uff1a\u6587\u672c\u7f16\u7801\u8f6c\u6362 split \u548c csplit \uff1a\u5206\u5272\u6587\u4ef6 sponge \uff1a\u5728\u5199\u5165\u524d\u8bfb\u53d6\u6240\u6709\u8f93\u5165\uff0c\u5728\u8bfb\u53d6\u6587\u4ef6\u540e\u518d\u5411\u540c\u4e00\u6587\u4ef6\u5199\u5165\u65f6\u6bd4\u8f83\u6709\u7528\uff0c\u4f8b\u5982 grep -v something some-file | sponge some-file units \uff1a\u5c06\u4e00\u79cd\u8ba1\u91cf\u5355\u4f4d\u8f6c\u6362\u4e3a\u53e6\u4e00\u79cd\u7b49\u6548\u7684\u8ba1\u91cf\u5355\u4f4d\uff08\u53c2\u9605 /usr/share/units/definitions.units \uff09 apg \uff1a\u968f\u673a\u751f\u6210\u5bc6\u7801 xz \uff1a\u9ad8\u6bd4\u4f8b\u7684\u6587\u4ef6\u538b\u7f29 ldd \uff1a\u52a8\u6001\u5e93\u4fe1\u606f nm \uff1a\u63d0\u53d6 obj \u6587\u4ef6\u4e2d\u7684\u7b26\u53f7 ab \u6216 wrk \uff1aweb \u670d\u52a1\u5668\u6027\u80fd\u5206\u6790 strace \uff1a\u8c03\u8bd5\u7cfb\u7edf\u8c03\u7528 mtr \uff1a\u66f4\u597d\u7684\u7f51\u7edc\u8c03\u8bd5\u8ddf\u8e2a\u5de5\u5177 cssh \uff1a\u53ef\u89c6\u5316\u7684\u5e76\u53d1 shell rsync \uff1a\u901a\u8fc7 ssh \u6216\u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u540c\u6b65\u6587\u4ef6\u548c\u6587\u4ef6\u5939 wireshark \u548c tshark \uff1a\u6293\u5305\u548c\u7f51\u7edc\u8c03\u8bd5\u5de5\u5177 ngrep \uff1a\u7f51\u7edc\u5c42\u7684 grep host \u548c dig \uff1aDNS \u67e5\u627e lsof \uff1a\u5217\u51fa\u5f53\u524d\u7cfb\u7edf\u6253\u5f00\u6587\u4ef6\u7684\u5de5\u5177\u4ee5\u53ca\u67e5\u770b\u7aef\u53e3\u4fe1\u606f dstat \uff1a\u7cfb\u7edf\u72b6\u6001\u67e5\u770b glances \uff1a\u9ad8\u5c42\u6b21\u7684\u591a\u5b50\u7cfb\u7edf\u603b\u89c8 iostat \uff1a\u786c\u76d8\u4f7f\u7528\u72b6\u6001 mpstat \uff1a CPU \u4f7f\u7528\u72b6\u6001 vmstat \uff1a \u5185\u5b58\u4f7f\u7528\u72b6\u6001 htop \uff1atop \u7684\u52a0\u5f3a\u7248 last \uff1a\u767b\u5165\u8bb0\u5f55 w \uff1a\u67e5\u770b\u5904\u4e8e\u767b\u5f55\u72b6\u6001\u7684\u7528\u6237 id \uff1a\u7528\u6237/\u7ec4 ID \u4fe1\u606f sar \uff1a\u7cfb\u7edf\u5386\u53f2\u6570\u636e iftop \u6216 nethogs \uff1a\u5957\u63a5\u5b57\u53ca\u8fdb\u7a0b\u7684\u7f51\u7edc\u5229\u7528\u60c5\u51b5 ss \uff1a\u5957\u63a5\u5b57\u6570\u636e dmesg \uff1a\u5f15\u5bfc\u53ca\u7cfb\u7edf\u9519\u8bef\u4fe1\u606f sysctl \uff1a \u5728\u5185\u6838\u8fd0\u884c\u65f6\u52a8\u6001\u5730\u67e5\u770b\u548c\u4fee\u6539\u5185\u6838\u7684\u8fd0\u884c\u53c2\u6570 hdparm \uff1aSATA/ATA \u78c1\u76d8\u66f4\u6539\u53ca\u6027\u80fd\u5206\u6790 lsblk \uff1a\u5217\u51fa\u5757\u8bbe\u5907\u4fe1\u606f\uff1a\u4ee5\u6811\u5f62\u5c55\u793a\u4f60\u7684\u78c1\u76d8\u4ee5\u53ca\u78c1\u76d8\u5206\u533a\u4fe1\u606f lshw \uff0c lscpu \uff0c lspci \uff0c lsusb \u548c dmidecode \uff1a\u67e5\u770b\u786c\u4ef6\u4fe1\u606f\uff0c\u5305\u62ec CPU\u3001BIOS\u3001RAID\u3001\u663e\u5361\u3001USB\u8bbe\u5907\u7b49 lsmod \u548c modinfo \uff1a\u5217\u51fa\u5185\u6838\u6a21\u5757\uff0c\u5e76\u663e\u793a\u5176\u7ec6\u8282 fortune \uff0c ddate \u548c sl \uff1a\u989d\uff0c\u8fd9\u4e3b\u8981\u53d6\u51b3\u4e8e\u4f60\u662f\u5426\u8ba4\u4e3a\u84b8\u6c7d\u706b\u8f66\u548c\u83ab\u540d\u5176\u5999\u7684\u540d\u4eba\u540d\u8a00\u662f\u5426\u201c\u6709\u7528\u201d \u4ec5\u9650 OS X \u7cfb\u7edf \u4ee5\u4e0b\u662f \u4ec5\u9650\u4e8e OS X \u7cfb\u7edf\u7684\u6280\u5de7\u3002 \u7528 brew \uff08Homebrew\uff09\u6216\u8005 port \uff08MacPorts\uff09\u8fdb\u884c\u5305\u7ba1\u7406\u3002\u8fd9\u4e9b\u53ef\u4ee5\u7528\u6765\u5728 OS X \u7cfb\u7edf\u4e0a\u5b89\u88c5\u4ee5\u4e0a\u7684\u5927\u591a\u6570\u547d\u4ee4\u3002 \u7528 pbcopy \u590d\u5236\u4efb\u4f55\u547d\u4ee4\u7684\u8f93\u51fa\u5230\u684c\u9762\u5e94\u7528\uff0c\u7528 pbpaste \u7c98\u8d34\u8f93\u5165\u3002 \u82e5\u8981\u5728 OS X \u7ec8\u7aef\u4e2d\u5c06 Option \u952e\u89c6\u4e3a alt \u952e\uff08\u4f8b\u5982\u5728\u4e0a\u9762\u4ecb\u7ecd\u7684 alt-b \u3001 alt-f \u7b49\u547d\u4ee4\u4e2d\u7528\u5230\uff09\uff0c\u6253\u5f00 \u504f\u597d\u8bbe\u7f6e - \u63cf\u8ff0\u6587\u4ef6 - \u952e\u76d8 \u5e76\u52fe\u9009\u201c\u4f7f\u7528 Option \u952e\u4f5c\u4e3a Meta \u952e\u201d\u3002 \u7528 open \u6216\u8005 open -a /Applications/Whatever.app \u4f7f\u7528\u684c\u9762\u5e94\u7528\u6253\u5f00\u6587\u4ef6\u3002 Spotlight\uff1a\u7528 mdfind \u641c\u7d22\u6587\u4ef6\uff0c\u7528 mdls \u5217\u51fa\u5143\u6570\u636e\uff08\u4f8b\u5982\u7167\u7247\u7684 EXIF \u4fe1\u606f\uff09\u3002 \u6ce8\u610f OS X \u7cfb\u7edf\u662f\u57fa\u4e8e BSD UNIX \u7684\uff0c\u8bb8\u591a\u547d\u4ee4\uff08\u4f8b\u5982 ps \uff0c ls \uff0c tail \uff0c awk \uff0c sed \uff09\u90fd\u548c Linux \u4e2d\u6709\u5fae\u5999\u7684\u4e0d\u540c\uff08 Linux \u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5230\u4e86 System V-style Unix \u548c GNU \u5de5\u5177\u5f71\u54cd\uff09\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u6807\u9898\u4e3a \"BSD General Commands Manual\" \u7684 man \u9875\u9762\u53d1\u73b0\u8fd9\u4e9b\u4e0d\u540c\u3002\u5728\u6709\u4e9b\u60c5\u51b5\u4e0b GNU \u7248\u672c\u7684\u547d\u4ee4\u4e5f\u53ef\u80fd\u88ab\u5b89\u88c5\uff08\u4f8b\u5982 gawk \u548c gsed \u5bf9\u5e94 GNU \u4e2d\u7684 awk \u548c sed \uff09\u3002\u5982\u679c\u8981\u5199\u8de8\u5e73\u53f0\u7684 Bash \u811a\u672c\uff0c\u907f\u514d\u4f7f\u7528\u8fd9\u4e9b\u547d\u4ee4\uff08\u4f8b\u5982\uff0c\u8003\u8651 Python \u6216\u8005 perl \uff09\u6216\u8005\u7ecf\u8fc7\u4ed4\u7ec6\u7684\u6d4b\u8bd5\u3002 \u7528 sw_vers \u83b7\u53d6 OS X \u7684\u7248\u672c\u4fe1\u606f\u3002 \u4ec5\u9650 Windows \u7cfb\u7edf \u4ee5\u4e0b\u662f \u4ec5\u9650\u4e8e Windows \u7cfb\u7edf\u7684\u6280\u5de7\u3002 \u5728 Winodws \u4e0b\u83b7\u53d6 Unix \u5de5\u5177 \u53ef\u4ee5\u5b89\u88c5 Cygwin \u5141\u8bb8\u4f60\u5728 Microsoft Windows \u4e2d\u4f53\u9a8c Unix shell \u7684\u5a01\u529b\u3002\u8fd9\u6837\u7684\u8bdd\uff0c\u672c\u6587\u4e2d\u4ecb\u7ecd\u7684\u5927\u591a\u6570\u5185\u5bb9\u90fd\u5c06\u9002\u7528\u3002 \u5728 Windows 10 \u4e0a\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 Bash on Ubuntu on Windows \uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u719f\u6089\u7684 Bash \u73af\u5883\uff0c\u5305\u542b\u4e86\u4e0d\u5c11 Unix \u547d\u4ee4\u884c\u5de5\u5177\u3002\u597d\u5904\u662f\u5b83\u5141\u8bb8 Linux \u4e0a\u7f16\u5199\u7684\u7a0b\u5e8f\u5728 Windows \u4e0a\u8fd0\u884c\uff0c\u800c\u53e6\u4e00\u65b9\u9762\uff0cWindows \u4e0a\u7f16\u5199\u7684\u7a0b\u5e8f\u5374\u65e0\u6cd5\u5728 Bash \u547d\u4ee4\u884c\u4e2d\u8fd0\u884c\u3002 \u5982\u679c\u4f60\u5728 Windows \u4e0a\u4e3b\u8981\u60f3\u7528 GNU \u5f00\u53d1\u8005\u5de5\u5177\uff08\u4f8b\u5982 GCC\uff09\uff0c\u53ef\u4ee5\u8003\u8651 MinGW \u4ee5\u53ca\u5b83\u7684 MSYS \u5305\uff0c\u8fd9\u4e2a\u5305\u63d0\u4f9b\u4e86\u4f8b\u5982 bash\uff0cgawk\uff0cmake \u548c grep \u7684\u5de5\u5177\u3002MSYS \u5e76\u4e0d\u5305\u542b\u6240\u6709\u53ef\u4ee5\u4e0e Cygwin \u5ab2\u7f8e\u7684\u7279\u6027\u3002\u5f53\u5236\u4f5c Unix \u5de5\u5177\u7684\u539f\u751f Windows \u7aef\u53e3\u65f6 MinGW \u5c06\u7279\u522b\u5730\u6709\u7528\u3002 \u53e6\u4e00\u4e2a\u5728 Windows \u4e0b\u5b9e\u73b0\u63a5\u8fd1 Unix \u73af\u5883\u5916\u89c2\u6548\u679c\u7684\u9009\u9879\u662f Cash \u3002\u6ce8\u610f\u5728\u6b64\u73af\u5883\u4e0b\u53ea\u6709\u5f88\u5c11\u7684 Unix \u547d\u4ee4\u548c\u547d\u4ee4\u884c\u53ef\u7528\u3002 \u5b9e\u7528 Windows \u547d\u4ee4\u884c\u5de5\u5177 \u53ef\u4ee5\u4f7f\u7528 wmic \u5728\u547d\u4ee4\u884c\u73af\u5883\u4e0b\u7ed9\u5927\u90e8\u5206 Windows \u7cfb\u7edf\u7ba1\u7406\u4efb\u52a1\u7f16\u5199\u811a\u672c\u4ee5\u53ca\u6267\u884c\u8fd9\u4e9b\u4efb\u52a1\u3002 Windows \u5b9e\u7528\u7684\u539f\u751f\u547d\u4ee4\u884c\u7f51\u7edc\u5de5\u5177\u5305\u62ec ping \uff0c ipconfig \uff0c tracert \uff0c\u548c netstat \u3002 \u53ef\u4ee5\u4f7f\u7528 Rundll32 \u547d\u4ee4\u6765\u5b9e\u73b0 \u8bb8\u591a\u6709\u7528\u7684 Windows \u4efb\u52a1 \u3002 Cygwin \u6280\u5de7 \u901a\u8fc7 Cygwin \u7684\u5305\u7ba1\u7406\u5668\u6765\u5b89\u88c5\u989d\u5916\u7684 Unix \u7a0b\u5e8f\u3002 \u4f7f\u7528 mintty \u4f5c\u4e3a\u4f60\u7684\u547d\u4ee4\u884c\u7a97\u53e3\u3002 \u8981\u8bbf\u95ee Windows \u526a\u8d34\u677f\uff0c\u53ef\u4ee5\u901a\u8fc7 /dev/clipboard \u3002 \u8fd0\u884c cygstart \u4ee5\u901a\u8fc7\u9ed8\u8ba4\u7a0b\u5e8f\u6253\u5f00\u4e00\u4e2a\u6587\u4ef6\u3002 \u8981\u8bbf\u95ee Windows \u6ce8\u518c\u8868\uff0c\u53ef\u4ee5\u4f7f\u7528 regtool \u3002 \u6ce8\u610f Windows \u9a71\u52a8\u5668\u8def\u5f84 C:\\ \u5728 Cygwin \u4e2d\u7528 /cygdrive/c \u4ee3\u8868\uff0c\u800c Cygwin \u7684 / \u4ee3\u8868 Windows \u4e2d\u7684 C:\\cygwin \u3002\u8981\u8f6c\u6362 Cygwin \u548c Windows \u98ce\u683c\u7684\u8def\u5f84\u53ef\u4ee5\u7528 cygpath \u3002\u8fd9\u5728\u9700\u8981\u8c03\u7528 Windows \u7a0b\u5e8f\u7684\u811a\u672c\u91cc\u5f88\u6709\u7528\u3002 \u5b66\u4f1a\u4f7f\u7528 wmic \uff0c\u4f60\u5c31\u53ef\u4ee5\u4ece\u547d\u4ee4\u884c\u6267\u884c\u5927\u591a\u6570 Windows \u7cfb\u7edf\u7ba1\u7406\u4efb\u52a1\uff0c\u5e76\u7f16\u6210\u811a\u672c\u3002 \u8981\u5728 Windows \u4e0b\u83b7\u5f97 Unix \u7684\u754c\u9762\u548c\u4f53\u9a8c\uff0c\u53e6\u4e00\u4e2a\u529e\u6cd5\u662f\u4f7f\u7528 Cash \u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e2a\u73af\u5883\u652f\u6301\u7684 Unix \u547d\u4ee4\u548c\u547d\u4ee4\u884c\u53c2\u6570\u975e\u5e38\u5c11\u3002 \u8981\u5728 Windows \u4e0a\u83b7\u53d6 GNU \u5f00\u53d1\u8005\u5de5\u5177\uff08\u6bd4\u5982 GCC\uff09\u7684\u53e6\u4e00\u4e2a\u529e\u6cd5\u662f\u4f7f\u7528 MinGW \u4ee5\u53ca\u5b83\u7684 MSYS \u8f6f\u4ef6\u5305\uff0c\u8be5\u8f6f\u4ef6\u5305\u63d0\u4f9b\u4e86 bash\u3001gawk\u3001make\u3001grep \u7b49\u5de5\u5177\u3002\u7136\u800c MSYS \u63d0\u4f9b\u7684\u529f\u80fd\u6ca1\u6709 Cygwin \u5b8c\u5584\u3002MinGW \u5728\u521b\u5efa Unix \u5de5\u5177\u7684 Windows \u539f\u751f\u79fb\u690d\u65b9\u9762\u975e\u5e38\u6709\u7528\u3002 \u66f4\u591a\u8d44\u6e90 awesome-shell \uff1a\u4e00\u4efd\u7cbe\u5fc3\u7ec4\u7ec7\u7684\u547d\u4ee4\u884c\u5de5\u5177\u53ca\u8d44\u6e90\u7684\u5217\u8868\u3002 awesome-osx-command-line \uff1a\u4e00\u4efd\u9488\u5bf9 OS X \u547d\u4ee4\u884c\u7684\u66f4\u6df1\u5165\u7684\u6307\u5357\u3002 Strict mode \uff1a\u4e3a\u4e86\u7f16\u5199\u66f4\u597d\u7684\u811a\u672c\u6587\u4ef6\u3002 shellcheck \uff1a\u4e00\u4e2a\u9759\u6001 shell \u811a\u672c\u5206\u6790\u5de5\u5177\uff0c\u672c\u8d28\u4e0a\u662f bash\uff0fsh\uff0fzsh \u7684 lint\u3002 Filenames and Pathnames in Shell \uff1a\u6709\u5173\u5982\u4f55\u5728 shell \u811a\u672c\u91cc\u6b63\u786e\u5904\u7406\u6587\u4ef6\u540d\u7684\u7ec6\u679d\u672b\u8282\u3002 Data Science at the Command Line \uff1a\u7528\u4e8e\u6570\u636e\u79d1\u5b66\u7684\u4e00\u4e9b\u547d\u4ee4\u548c\u5de5\u5177\uff0c\u6458\u81ea\u540c\u540d\u4e66\u7c4d\u3002 \u514d\u8d23\u58f0\u660e \u9664\u53bb\u7279\u522b\u5c0f\u7684\u5de5\u4f5c\uff0c\u4f60\u7f16\u5199\u7684\u4ee3\u7801\u5e94\u5f53\u65b9\u4fbf\u4ed6\u4eba\u9605\u8bfb\u3002\u80fd\u529b\u5f80\u5f80\u4f34\u968f\u7740\u8d23\u4efb\uff0c\u4f60 \u6709\u80fd\u529b \u5728 Bash \u4e2d\u73a9\u4e00\u4e9b\u5947\u6280\u6deb\u5de7\u5e76\u4e0d\u610f\u5473\u7740\u4f60\u5e94\u8be5\u53bb\u505a\uff01;) \u6388\u6743\u6761\u6b3e \u672c\u6587\u4f7f\u7528\u6388\u6743\u534f\u8bae Creative Commons Attribution-ShareAlike 4.0 International License \u3002","title":"Cmd_line"},{"location":"cmd_line/#_1","text":"\u524d\u8a00 \u57fa\u7840 \u65e5\u5e38\u4f7f\u7528 \u6587\u4ef6\u53ca\u6570\u636e\u5904\u7406 \u7cfb\u7edf\u8c03\u8bd5 \u5355\u884c\u811a\u672c \u51b7\u95e8\u4f46\u6709\u7528 \u4ec5\u9650 OS X \u7cfb\u7edf \u4ec5\u9650 Windows \u7cfb\u7edf \u66f4\u591a\u8d44\u6e90 \u514d\u8d23\u58f0\u660e curl -s 'https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README.md' | egrep -o ' \\w+ ' | tr -d '`' | cowsay -W50 \u719f\u7ec3\u4f7f\u7528\u547d\u4ee4\u884c\u662f\u4e00\u79cd\u5e38\u5e38\u88ab\u5ffd\u89c6\uff0c\u6216\u88ab\u8ba4\u4e3a\u96be\u4ee5\u638c\u63e1\u7684\u6280\u80fd\uff0c\u4f46\u5b9e\u9645\u4e0a\uff0c\u5b83\u4f1a\u63d0\u9ad8\u4f60\u4f5c\u4e3a\u5de5\u7a0b\u5e08\u7684\u7075\u6d3b\u6027\u4ee5\u53ca\u751f\u4ea7\u529b\u3002\u672c\u6587\u662f\u4e00\u4efd\u6211\u5728 Linux \u4e0a\u5de5\u4f5c\u65f6\uff0c\u53d1\u73b0\u7684\u4e00\u4e9b\u547d\u4ee4\u884c\u4f7f\u7528\u6280\u5de7\u7684\u6458\u8981\u3002\u6709\u4e9b\u6280\u5de7\u975e\u5e38\u57fa\u7840\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u76f8\u5f53\u590d\u6742\uff0c\u751a\u81f3\u6666\u6da9\u96be\u61c2\u3002\u8fd9\u7bc7\u6587\u7ae0\u5e76\u4e0d\u957f\uff0c\u4f46\u5f53\u4f60\u80fd\u591f\u719f\u7ec3\u638c\u63e1\u8fd9\u91cc\u5217\u51fa\u7684\u6240\u6709\u6280\u5de7\u65f6\uff0c\u4f60\u5c31\u5b66\u4f1a\u4e86\u5f88\u591a\u5173\u4e8e\u547d\u4ee4\u884c\u7684\u4e1c\u897f\u4e86\u3002 \u8fd9\u7bc7\u6587\u7ae0\u662f\u8bb8\u591a\u4f5c\u8005\u548c\u8bd1\u8005\u5171\u540c\u7684\u6210\u679c\u3002 \u8fd9\u91cc\u7684\u90e8\u5206\u5185\u5bb9 \u9996\u6b21 \u51fa\u73b0 \u4e8e Quora \uff0c \u4f46\u5df2\u7ecf\u8fc1\u79fb\u5230\u4e86 Github\uff0c\u5e76\u7531\u4f17\u591a\u9ad8\u624b\u505a\u51fa\u4e86\u8bb8\u591a\u6539\u8fdb\u3002 \u5982\u679c\u4f60\u5728\u672c\u6587\u4e2d\u53d1\u73b0\u4e86\u9519\u8bef\u6216\u8005\u5b58\u5728\u53ef\u4ee5\u6539\u5584\u7684\u5730\u65b9\uff0c\u8bf7 \u8d21\u732e\u4f60\u7684\u4e00\u4efd\u529b\u91cf \u3002","title":"\u547d\u4ee4\u884c\u7684\u827a\u672f"},{"location":"cmd_line/#_2","text":"\u6db5\u76d6\u8303\u56f4\uff1a \u8fd9\u7bc7\u6587\u7ae0\u4e0d\u4ec5\u80fd\u5e2e\u52a9\u521a\u63a5\u89e6\u547d\u4ee4\u884c\u7684\u65b0\u624b\uff0c\u800c\u4e14\u5bf9\u5177\u6709\u7ecf\u9a8c\u7684\u4eba\u4e5f\u5927\u6709\u88e8\u76ca\u3002\u672c\u6587\u81f4\u529b\u4e8e\u505a\u5230 \u8986\u76d6\u9762\u5e7f \uff08\u6d89\u53ca\u6240\u6709\u91cd\u8981\u7684\u5185\u5bb9\uff09\uff0c \u5177\u4f53 \uff08\u7ed9\u51fa\u5177\u4f53\u7684\u6700\u5e38\u7528\u7684\u4f8b\u5b50\uff09\uff0c\u4ee5\u53ca \u7b80\u6d01 \uff08\u907f\u514d\u5197\u4f59\u7684\u5185\u5bb9\uff0c\u6216\u662f\u53ef\u4ee5\u5728\u5176\u4ed6\u5730\u65b9\u8f7b\u677e\u67e5\u5230\u7684\u7ec6\u679d\u672b\u8282\uff09\u3002\u5728\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4e0b\uff0c\u672c\u6587\u7684\u5185\u5bb9\u5c5e\u4e8e\u57fa\u672c\u529f\u6216\u8005\u80fd\u5e2e\u52a9\u60a8\u8282\u7ea6\u5927\u91cf\u7684\u65f6\u95f4\u3002 \u672c\u6587\u4e3b\u8981\u4e3a Linux \u6240\u5199\uff0c\u4f46\u5728 \u4ec5\u9650 OS X \u7cfb\u7edf \u7ae0\u8282\u548c \u4ec5\u9650 Windows \u7cfb\u7edf \u7ae0\u8282\u4e2d\u4e5f\u5305\u542b\u6709\u5bf9\u5e94\u64cd\u4f5c\u7cfb\u7edf\u7684\u5185\u5bb9\u3002\u9664\u53bb\u8fd9\u4e24\u4e2a\u7ae0\u8282\u5916\uff0c\u5176\u5b83\u7684\u5185\u5bb9\u5927\u90e8\u5206\u5747\u53ef\u5728\u5176\u4ed6\u7c7b Unix \u7cfb\u7edf\u6216 OS X\uff0c\u751a\u81f3 Cygwin \u4e2d\u5f97\u5230\u5e94\u7528\u3002 \u672c\u6587\u4e3b\u8981\u5173\u6ce8\u4e8e\u4ea4\u4e92\u5f0f Bash\uff0c\u4f46\u4e5f\u6709\u5f88\u591a\u6280\u5de7\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed6 shell \u548c Bash \u811a\u672c\u5f53\u4e2d\u3002 \u9664\u53bb\u201c\u6807\u51c6\u7684\u201dUnix \u547d\u4ee4\uff0c\u672c\u6587\u8fd8\u5305\u62ec\u4e86\u4e00\u4e9b\u4f9d\u8d56\u4e8e\u7279\u5b9a\u8f6f\u4ef6\u5305\u7684\u547d\u4ee4\uff08\u524d\u63d0\u662f\u5b83\u4eec\u5177\u6709\u8db3\u591f\u7684\u4ef7\u503c\uff09\u3002 \u6ce8\u610f\u4e8b\u9879\uff1a \u4e3a\u4e86\u80fd\u5728\u4e00\u9875\u5185\u5c55\u793a\u5c3d\u91cf\u591a\u7684\u4e1c\u897f\uff0c\u4e00\u4e9b\u5177\u4f53\u7684\u4fe1\u606f\u53ef\u4ee5\u5728\u5f15\u7528\u7684\u9875\u9762\u4e2d\u627e\u5230\u3002\u6211\u4eec\u76f8\u4fe1\u673a\u667a\u7684\u4f60\u77e5\u9053\u5982\u4f55\u4f7f\u7528 Google \u6216\u8005\u5176\u4ed6\u641c\u7d22\u5f15\u64ce\u6765\u67e5\u9605\u5230\u66f4\u591a\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\u6587\u4e2d\u90e8\u5206\u547d\u4ee4\u9700\u8981\u60a8\u4f7f\u7528 apt-get \uff0c yum \uff0c dnf \uff0c pacman \uff0c pip \u6216 brew \uff08\u4ee5\u53ca\u5176\u5b83\u5408\u9002\u7684\u5305\u7ba1\u7406\u5668\uff09\u6765\u5b89\u88c5\u4f9d\u8d56\u7684\u7a0b\u5e8f\u3002 \u9047\u5230\u95ee\u9898\u7684\u8bdd\uff0c\u8bf7\u5c1d\u8bd5\u4f7f\u7528 Explainshell \u53bb\u83b7\u53d6\u76f8\u5173\u547d\u4ee4\u3001\u53c2\u6570\u3001\u7ba1\u9053\u7b49\u5185\u5bb9\u7684\u89e3\u91ca\u3002","title":"\u524d\u8a00"},{"location":"cmd_line/#_3","text":"\u5b66\u4e60 Bash \u7684\u57fa\u7840\u77e5\u8bc6\u3002\u5177\u4f53\u5730\uff0c\u5728\u547d\u4ee4\u884c\u4e2d\u8f93\u5165 man bash \u5e76\u81f3\u5c11\u5168\u6587\u6d4f\u89c8\u4e00\u904d; \u5b83\u7406\u89e3\u8d77\u6765\u5f88\u7b80\u5355\u5e76\u4e14\u4e0d\u5197\u957f\u3002\u5176\u4ed6\u7684 shell \u53ef\u80fd\u5f88\u597d\u7528\uff0c\u4f46 Bash \u7684\u529f\u80fd\u5df2\u7ecf\u8db3\u591f\u5f3a\u5927\u5e76\u4e14\u5230\u51e0\u4e4e\u603b\u662f\u53ef\u7528\u7684\uff08 \u5982\u679c\u4f60 \u53ea \u5b66\u4e60 zsh\uff0cfish \u6216\u5176\u4ed6\u7684 shell \u7684\u8bdd\uff0c\u5728\u4f60\u81ea\u5df1\u7684\u8bbe\u5907\u4e0a\u4f1a\u663e\u5f97\u5f88\u65b9\u4fbf\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u8fd9\u4e9b\u529f\u80fd\u4f1a\u7ed9\u60a8\u5e26\u6765\u4e0d\u4fbf\uff0c\u4f8b\u5982\u5f53\u4f60\u9700\u8981\u5728\u670d\u52a1\u5668\u4e0a\u5de5\u4f5c\u65f6\uff09\u3002 \u719f\u6089\u81f3\u5c11\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u7684\u7f16\u8f91\u5668\u3002\u901a\u5e38\u800c\u8a00 Vim \uff08 vi \uff09 \u4f1a\u662f\u4f60\u6700\u597d\u7684\u9009\u62e9\uff0c\u6bd5\u7adf\u5728\u7ec8\u7aef\u4e2d\u7f16\u8f91\u6587\u672c\u65f6 Vim \u662f\u6700\u597d\u7528\u7684\u5de5\u5177\uff08\u751a\u81f3\u5927\u90e8\u5206\u60c5\u51b5\u4e0b Vim \u8981\u6bd4 Emacs\u3001\u5927\u578b IDE \u6216\u662f\u70ab\u9177\u7684\u7f16\u8f91\u5668\u66f4\u597d\u7528\uff09\u3002 \u5b66\u4f1a\u5982\u4f55\u4f7f\u7528 man \u547d\u4ee4\u53bb\u9605\u8bfb\u6587\u6863\u3002\u5b66\u4f1a\u4f7f\u7528 apropos \u53bb\u67e5\u627e\u6587\u6863\u3002\u77e5\u9053\u6709\u4e9b\u547d\u4ee4\u5e76\u4e0d\u5bf9\u5e94\u53ef\u6267\u884c\u6587\u4ef6\uff0c\u800c\u662f\u5728 Bash \u5185\u7f6e\u597d\u7684\uff0c\u6b64\u65f6\u53ef\u4ee5\u4f7f\u7528 help \u548c help -d \u547d\u4ee4\u83b7\u53d6\u5e2e\u52a9\u4fe1\u606f\u3002\u4f60\u53ef\u4ee5\u7528 type \u547d\u4ee4 \u6765\u5224\u65ad\u8fd9\u4e2a\u547d\u4ee4\u5230\u5e95\u662f\u53ef\u6267\u884c\u6587\u4ef6\u3001shell \u5185\u7f6e\u547d\u4ee4\u8fd8\u662f\u522b\u540d\u3002 \u5b66\u4f1a\u4f7f\u7528 \u548c \u6765\u91cd\u5b9a\u5411\u8f93\u51fa\u548c\u8f93\u5165\uff0c\u5b66\u4f1a\u4f7f\u7528 | \u6765\u91cd\u5b9a\u5411\u7ba1\u9053\u3002\u660e\u767d \u4f1a\u8986\u76d6\u4e86\u8f93\u51fa\u6587\u4ef6\u800c \u662f\u5728\u6587\u4ef6\u672b\u6dfb\u52a0\u3002\u4e86\u89e3\u6807\u51c6\u8f93\u51fa stdout \u548c\u6807\u51c6\u9519\u8bef stderr\u3002 \u5b66\u4f1a\u4f7f\u7528\u901a\u914d\u7b26 * \uff08\u6216\u8bb8\u518d\u7b97\u4e0a ? \u548c [ ... ] \uff09 \u548c\u5f15\u7528\u4ee5\u53ca\u5f15\u7528\u4e2d ' \u548c \" \u7684\u533a\u522b\uff08\u540e\u6587\u4e2d\u6709\u4e00\u4e9b\u5177\u4f53\u7684\u4f8b\u5b50\uff09\u3002 \u719f\u6089 Bash \u4e2d\u7684\u4efb\u52a1\u7ba1\u7406\u5de5\u5177\uff1a \uff0c ctrl-z \uff0c ctrl-c \uff0c jobs \uff0c fg \uff0c bg \uff0c kill \u7b49\u3002 \u5b66\u4f1a\u4f7f\u7528 ssh \u8fdb\u884c\u8fdc\u7a0b\u547d\u4ee4\u884c\u767b\u5f55\uff0c\u6700\u597d\u77e5\u9053\u5982\u4f55\u4f7f\u7528 ssh-agent \uff0c ssh-add \u7b49\u547d\u4ee4\u6765\u5b9e\u73b0\u57fa\u7840\u7684\u65e0\u5bc6\u7801\u8ba4\u8bc1\u767b\u5f55\u3002 \u5b66\u4f1a\u57fa\u672c\u7684\u6587\u4ef6\u7ba1\u7406\u5de5\u5177\uff1a ls \u548c ls -l \uff08\u4e86\u89e3 ls -l \u4e2d\u6bcf\u4e00\u5217\u4ee3\u8868\u7684\u610f\u4e49\uff09\uff0c less \uff0c head \uff0c tail \u548c tail -f \uff08\u751a\u81f3 less +F \uff09\uff0c ln \u548c ln -s \uff08\u4e86\u89e3\u786c\u94fe\u63a5\u4e0e\u8f6f\u94fe\u63a5\u7684\u533a\u522b\uff09\uff0c chown \uff0c chmod \uff0c du \uff08\u786c\u76d8\u4f7f\u7528\u60c5\u51b5\u6982\u8ff0\uff1a du -hs * \uff09\u3002 \u5173\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7684\u7ba1\u7406\uff0c\u5b66\u4e60 df \uff0c mount \uff0c fdisk \uff0c mkfs \uff0c lsblk \u3002\u77e5\u9053 inode \u662f\u4ec0\u4e48\uff08\u4e0e ls -i \u548c df -i \u7b49\u547d\u4ee4\u76f8\u5173\uff09\u3002 \u5b66\u4e60\u57fa\u672c\u7684\u7f51\u7edc\u7ba1\u7406\u5de5\u5177\uff1a ip \u6216 ifconfig \uff0c dig \u3002 \u5b66\u4e60\u5e76\u4f7f\u7528\u4e00\u79cd\u7248\u672c\u63a7\u5236\u7ba1\u7406\u7cfb\u7edf\uff0c\u4f8b\u5982 git \u3002 \u719f\u6089\u6b63\u5219\u8868\u8fbe\u5f0f\uff0c\u5b66\u4f1a\u4f7f\u7528 grep \uff0f egrep \uff0c\u5b83\u4eec\u7684\u53c2\u6570\u4e2d -i \uff0c -o \uff0c -v \uff0c -A \uff0c -B \u548c -C \u8fd9\u4e9b\u662f\u5f88\u5e38\u7528\u5e76\u503c\u5f97\u8ba4\u771f\u5b66\u4e60\u7684\u3002 \u5b66\u4f1a\u4f7f\u7528 apt-get \uff0c yum \uff0c dnf \u6216 pacman \uff08\u5177\u4f53\u4f7f\u7528\u54ea\u4e2a\u53d6\u51b3\u4e8e\u4f60\u4f7f\u7528\u7684 Linux \u53d1\u884c\u7248\uff09\u6765\u67e5\u627e\u548c\u5b89\u88c5\u8f6f\u4ef6\u5305\u3002\u5e76\u786e\u4fdd\u4f60\u7684\u73af\u5883\u4e2d\u6709 pip \u6765\u5b89\u88c5\u57fa\u4e8e Python \u7684\u547d\u4ee4\u884c\u5de5\u5177 \uff08\u63a5\u4e0b\u6765\u63d0\u5230\u7684\u90e8\u5206\u7a0b\u5e8f\u4f7f\u7528 pip \u6765\u5b89\u88c5\u4f1a\u5f88\u65b9\u4fbf\uff09\u3002","title":"\u57fa\u7840"},{"location":"cmd_line/#_4","text":"\u5728 Bash \u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7\u6309 Tab \u952e\u5b9e\u73b0\u81ea\u52a8\u8865\u5168\u53c2\u6570\uff0c\u4f7f\u7528 ctrl-r \u641c\u7d22\u547d\u4ee4\u884c\u5386\u53f2\u8bb0\u5f55\uff08\u6309\u4e0b\u6309\u952e\u4e4b\u540e\uff0c\u8f93\u5165\u5173\u952e\u5b57\u4fbf\u53ef\u4ee5\u641c\u7d22\uff0c\u91cd\u590d\u6309\u4e0b ctrl-r \u4f1a\u5411\u540e\u67e5\u627e\u5339\u914d\u9879\uff0c\u6309\u4e0b Enter \u952e\u4f1a\u6267\u884c\u5f53\u524d\u5339\u914d\u7684\u547d\u4ee4\uff0c\u800c\u6309\u4e0b\u53f3\u65b9\u5411\u952e\u4f1a\u5c06\u5339\u914d\u9879\u653e\u5165\u5f53\u524d\u884c\u4e2d\uff0c\u4e0d\u4f1a\u76f4\u63a5\u6267\u884c\uff0c\u4ee5\u4fbf\u505a\u51fa\u4fee\u6539\uff09\u3002 \u5728 Bash \u4e2d\uff0c\u53ef\u4ee5\u6309\u4e0b ctrl-w \u5220\u9664\u4f60\u952e\u5165\u7684\u6700\u540e\u4e00\u4e2a\u5355\u8bcd\uff0c ctrl-u \u53ef\u4ee5\u5220\u9664\u884c\u5185\u5149\u6807\u6240\u5728\u4f4d\u7f6e\u4e4b\u524d\u7684\u5185\u5bb9\uff0c alt-b \u548c alt-f \u53ef\u4ee5\u4ee5\u5355\u8bcd\u4e3a\u5355\u4f4d\u79fb\u52a8\u5149\u6807\uff0c ctrl-a \u53ef\u4ee5\u5c06\u5149\u6807\u79fb\u81f3\u884c\u9996\uff0c ctrl-e \u53ef\u4ee5\u5c06\u5149\u6807\u79fb\u81f3\u884c\u5c3e\uff0c ctrl-k \u53ef\u4ee5\u5220\u9664\u5149\u6807\u81f3\u884c\u5c3e\u7684\u6240\u6709\u5185\u5bb9\uff0c ctrl-l \u53ef\u4ee5\u6e05\u5c4f\u3002\u952e\u5165 man readline \u53ef\u4ee5\u67e5\u770b Bash \u4e2d\u7684\u9ed8\u8ba4\u5feb\u6377\u952e\u3002\u5185\u5bb9\u6709\u5f88\u591a\uff0c\u4f8b\u5982 alt-. \u5faa\u73af\u5730\u79fb\u5411\u524d\u4e00\u4e2a\u53c2\u6570\uff0c\u800c alt- * \u53ef\u4ee5\u5c55\u5f00\u901a\u914d\u7b26\u3002 \u4f60\u559c\u6b22\u7684\u8bdd\uff0c\u53ef\u4ee5\u6267\u884c set -o vi \u6765\u4f7f\u7528 vi \u98ce\u683c\u7684\u5feb\u6377\u952e\uff0c\u800c\u6267\u884c set -o emacs \u53ef\u4ee5\u628a\u5b83\u6539\u56de\u6765\u3002 \u4e3a\u4e86\u4fbf\u4e8e\u7f16\u8f91\u957f\u547d\u4ee4\uff0c\u5728\u8bbe\u7f6e\u4f60\u7684\u9ed8\u8ba4\u7f16\u8f91\u5668\u540e\uff08\u4f8b\u5982 export EDITOR=vim \uff09\uff0c ctrl-x ctrl-e \u4f1a\u6253\u5f00\u4e00\u4e2a\u7f16\u8f91\u5668\u6765\u7f16\u8f91\u5f53\u524d\u8f93\u5165\u7684\u547d\u4ee4\u3002\u5728 vi \u98ce\u683c\u4e0b\u5feb\u6377\u952e\u5219\u662f escape-v \u3002 \u952e\u5165 history \u67e5\u770b\u547d\u4ee4\u884c\u5386\u53f2\u8bb0\u5f55\uff0c\u518d\u7528 !n \uff08 n \u662f\u547d\u4ee4\u7f16\u53f7\uff09\u5c31\u53ef\u4ee5\u518d\u6b21\u6267\u884c\u3002\u5176\u4e2d\u6709\u8bb8\u591a\u7f29\u5199\uff0c\u6700\u6709\u7528\u7684\u5927\u6982\u5c31\u662f !$ \uff0c \u5b83\u7528\u4e8e\u6307\u4ee3\u4e0a\u6b21\u952e\u5165\u7684\u53c2\u6570\uff0c\u800c !! \u53ef\u4ee5\u6307\u4ee3\u4e0a\u6b21\u952e\u5165\u7684\u547d\u4ee4\u4e86\uff08\u53c2\u8003 man \u9875\u9762\u4e2d\u7684\u201cHISTORY EXPANSION\u201d\uff09\u3002\u4e0d\u8fc7\u8fd9\u4e9b\u529f\u80fd\uff0c\u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5feb\u6377\u952e ctrl-r \u548c alt-. \u6765\u5b9e\u73b0\u3002 cd \u547d\u4ee4\u53ef\u4ee5\u5207\u6362\u5de5\u4f5c\u8def\u5f84\uff0c\u8f93\u5165 cd ~ \u53ef\u4ee5\u8fdb\u5165 home \u76ee\u5f55\u3002\u8981\u8bbf\u95ee\u4f60\u7684 home \u76ee\u5f55\u4e2d\u7684\u6587\u4ef6\uff0c\u53ef\u4ee5\u4f7f\u7528\u524d\u7f00 ~ \uff08\u4f8b\u5982 ~/.bashrc \uff09\u3002\u5728 sh \u811a\u672c\u91cc\u5219\u7528\u73af\u5883\u53d8\u91cf $HOME \u6307\u4ee3 home \u76ee\u5f55\u7684\u8def\u5f84\u3002 \u56de\u5230\u524d\u4e00\u4e2a\u5de5\u4f5c\u8def\u5f84\uff1a cd - \u3002 \u5982\u679c\u4f60\u8f93\u5165\u547d\u4ee4\u7684\u65f6\u5019\u4e2d\u9014\u6539\u4e86\u4e3b\u610f\uff0c\u6309\u4e0b alt-# \u5728\u884c\u9996\u6dfb\u52a0 # \u628a\u5b83\u5f53\u505a\u6ce8\u91ca\u518d\u6309\u4e0b\u56de\u8f66\u6267\u884c\uff08\u6216\u8005\u4f9d\u6b21\u6309\u4e0b ctrl-a \uff0c # \uff0c enter \uff09\u3002\u8fd9\u6837\u505a\u7684\u8bdd\uff0c\u4e4b\u540e\u501f\u52a9\u547d\u4ee4\u884c\u5386\u53f2\u8bb0\u5f55\uff0c\u4f60\u53ef\u4ee5\u5f88\u65b9\u4fbf\u6062\u590d\u4f60\u521a\u624d\u8f93\u5165\u5230\u4e00\u534a\u7684\u547d\u4ee4\u3002 \u4f7f\u7528 xargs \uff08 \u6216 parallel \uff09\u3002\u4ed6\u4eec\u975e\u5e38\u7ed9\u529b\u3002\u6ce8\u610f\u5230\u4f60\u53ef\u4ee5\u63a7\u5236\u6bcf\u884c\u53c2\u6570\u4e2a\u6570\uff08 -L \uff09\u548c\u6700\u5927\u5e76\u884c\u6570\uff08 -P \uff09\u3002\u5982\u679c\u4f60\u4e0d\u786e\u5b9a\u5b83\u4eec\u662f\u5426\u4f1a\u6309\u4f60\u60f3\u7684\u90a3\u6837\u5de5\u4f5c\uff0c\u5148\u4f7f\u7528 xargs echo \u67e5\u770b\u4e00\u4e0b\u3002\u6b64\u5916\uff0c\u4f7f\u7528 -I{} \u4f1a\u5f88\u65b9\u4fbf\u3002\u4f8b\u5982\uff1a find . -name '*.py' | xargs grep some_function cat hosts | xargs -I{} ssh root@{} hostname pstree -p \u4ee5\u4e00\u79cd\u4f18\u96c5\u7684\u65b9\u5f0f\u5c55\u793a\u8fdb\u7a0b\u6811\u3002 \u4f7f\u7528 pgrep \u548c pkill \u6839\u636e\u540d\u5b57\u67e5\u627e\u8fdb\u7a0b\u6216\u53d1\u9001\u4fe1\u53f7\uff08 -f \u53c2\u6570\u901a\u5e38\u6709\u7528\uff09\u3002 \u4e86\u89e3\u4f60\u53ef\u4ee5\u53d1\u5f80\u8fdb\u7a0b\u7684\u4fe1\u53f7\u7684\u79cd\u7c7b\u3002\u6bd4\u5982\uff0c\u4f7f\u7528 kill -STOP [pid] \u505c\u6b62\u4e00\u4e2a\u8fdb\u7a0b\u3002\u4f7f\u7528 man 7 signal \u67e5\u770b\u8be6\u7ec6\u5217\u8868\u3002 \u4f7f\u7528 nohup \u6216 disown \u4f7f\u4e00\u4e2a\u540e\u53f0\u8fdb\u7a0b\u6301\u7eed\u8fd0\u884c\u3002 \u4f7f\u7528 netstat -lntp \u6216 ss -plat \u68c0\u67e5\u54ea\u4e9b\u8fdb\u7a0b\u5728\u76d1\u542c\u7aef\u53e3\uff08\u9ed8\u8ba4\u662f\u68c0\u67e5 TCP \u7aef\u53e3; \u6dfb\u52a0\u53c2\u6570 -u \u5219\u68c0\u67e5 UDP \u7aef\u53e3\uff09\u6216\u8005 lsof -iTCP -sTCP:LISTEN -P -n (\u8fd9\u4e5f\u53ef\u4ee5\u5728 OS X \u4e0a\u8fd0\u884c)\u3002 lsof \u6765\u67e5\u770b\u5f00\u542f\u7684\u5957\u63a5\u5b57\u548c\u6587\u4ef6\u3002 \u4f7f\u7528 uptime \u6216 w \u6765\u67e5\u770b\u7cfb\u7edf\u5df2\u7ecf\u8fd0\u884c\u591a\u957f\u65f6\u95f4\u3002 \u4f7f\u7528 alias \u6765\u521b\u5efa\u5e38\u7528\u547d\u4ee4\u7684\u5feb\u6377\u5f62\u5f0f\u3002\u4f8b\u5982\uff1a alias ll='ls -latr' \u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u547d\u4ee4\u522b\u540d ll \u3002 \u53ef\u4ee5\u628a\u522b\u540d\u3001shell \u9009\u9879\u548c\u5e38\u7528\u51fd\u6570\u4fdd\u5b58\u5728 ~/.bashrc \uff0c\u5177\u4f53\u770b\u4e0b\u8fd9\u7bc7 \u6587\u7ae0 \u3002\u8fd9\u6837\u505a\u7684\u8bdd\u4f60\u5c31\u53ef\u4ee5\u5728\u6240\u6709 shell \u4f1a\u8bdd\u4e2d\u4f7f\u7528\u4f60\u7684\u8bbe\u5b9a\u3002 \u628a\u73af\u5883\u53d8\u91cf\u7684\u8bbe\u5b9a\u4ee5\u53ca\u767b\u9646\u65f6\u8981\u6267\u884c\u7684\u547d\u4ee4\u4fdd\u5b58\u5728 ~/.bash_profile \u3002\u800c\u5bf9\u4e8e\u4ece\u56fe\u5f62\u754c\u9762\u542f\u52a8\u7684 shell \u548c cron \u542f\u52a8\u7684 shell\uff0c\u5219\u9700\u8981\u5355\u72ec\u914d\u7f6e\u6587\u4ef6\u3002 \u8981\u60f3\u5728\u51e0\u53f0\u7535\u8111\u4e2d\u540c\u6b65\u4f60\u7684\u914d\u7f6e\u6587\u4ef6\uff08\u4f8b\u5982 .bashrc \u548c .bash_profile \uff09\uff0c\u53ef\u4ee5\u501f\u52a9 Git\u3002 \u5f53\u53d8\u91cf\u548c\u6587\u4ef6\u540d\u4e2d\u5305\u542b\u7a7a\u683c\u7684\u65f6\u5019\u8981\u683c\u5916\u5c0f\u5fc3\u3002Bash \u53d8\u91cf\u8981\u7528\u5f15\u53f7\u62ec\u8d77\u6765\uff0c\u6bd4\u5982 \"$FOO\" \u3002\u5c3d\u91cf\u4f7f\u7528 -0 \u6216 -print0 \u9009\u9879\u4ee5\u4fbf\u7528 NULL \u6765\u5206\u9694\u6587\u4ef6\u540d\uff0c\u4f8b\u5982 locate -0 pattern | xargs -0 ls -al \u6216 find / -print0 -type d | xargs -0 ls -al \u3002\u5982\u679c for \u5faa\u73af\u4e2d\u5faa\u73af\u8bbf\u95ee\u7684\u6587\u4ef6\u540d\u542b\u6709\u7a7a\u5b57\u7b26\uff08\u7a7a\u683c\u3001tab \u7b49\u5b57\u7b26\uff09\uff0c\u53ea\u9700\u7528 IFS=$'\\n' \u628a\u5185\u90e8\u5b57\u6bb5\u5206\u9694\u7b26\u8bbe\u4e3a\u6362\u884c\u7b26\u3002 \u5728 Bash \u811a\u672c\u4e2d\uff0c\u4f7f\u7528 set -x \u53bb\u8c03\u8bd5\u8f93\u51fa\uff08\u6216\u8005\u4f7f\u7528\u5b83\u7684\u53d8\u4f53 set -v \uff0c\u5b83\u4f1a\u8bb0\u5f55\u539f\u59cb\u8f93\u5165\uff0c\u5305\u62ec\u591a\u4f59\u7684\u53c2\u6570\u548c\u6ce8\u91ca\uff09\u3002\u5c3d\u53ef\u80fd\u5730\u4f7f\u7528\u4e25\u683c\u6a21\u5f0f\uff1a\u4f7f\u7528 set -e \u4ee4\u811a\u672c\u5728\u53d1\u751f\u9519\u8bef\u65f6\u9000\u51fa\u800c\u4e0d\u662f\u7ee7\u7eed\u8fd0\u884c\uff1b\u4f7f\u7528 set -u \u6765\u68c0\u67e5\u662f\u5426\u4f7f\u7528\u4e86\u672a\u8d4b\u503c\u7684\u53d8\u91cf\uff1b\u8bd5\u8bd5 set -o pipefail \uff0c\u5b83\u53ef\u4ee5\u76d1\u6d4b\u7ba1\u9053\u4e2d\u7684\u9519\u8bef\u3002\u5f53\u7275\u626f\u5230\u5f88\u591a\u811a\u672c\u65f6\uff0c\u4f7f\u7528 trap \u6765\u68c0\u6d4b ERR \u548c EXIT\u3002\u4e00\u4e2a\u597d\u7684\u4e60\u60ef\u662f\u5728\u811a\u672c\u6587\u4ef6\u5f00\u5934\u8fd9\u6837\u5199\uff0c\u8fd9\u4f1a\u4f7f\u5b83\u80fd\u591f\u68c0\u6d4b\u4e00\u4e9b\u9519\u8bef\uff0c\u5e76\u5728\u9519\u8bef\u53d1\u751f\u65f6\u4e2d\u65ad\u7a0b\u5e8f\u5e76\u8f93\u51fa\u4fe1\u606f\uff1a set -euo pipefail trap echo 'error: Script failed: see failed command above' ERR \u5728 Bash \u811a\u672c\u4e2d\uff0c\u5b50 shell\uff08\u4f7f\u7528\u62ec\u53f7 (...) \uff09\u662f\u4e00\u79cd\u7ec4\u7ec7\u53c2\u6570\u7684\u4fbf\u6377\u65b9\u5f0f\u3002\u4e00\u4e2a\u5e38\u89c1\u7684\u4f8b\u5b50\u662f\u4e34\u65f6\u5730\u79fb\u52a8\u5de5\u4f5c\u8def\u5f84\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a # do something in current dir (cd /some/other/dir other-command) # continue in original dir \u5728 Bash \u4e2d\uff0c\u53d8\u91cf\u6709\u8bb8\u591a\u7684\u6269\u5c55\u65b9\u5f0f\u3002 ${name:?error message} \u7528\u4e8e\u68c0\u67e5\u53d8\u91cf\u662f\u5426\u5b58\u5728\u3002\u6b64\u5916\uff0c\u5f53 Bash \u811a\u672c\u53ea\u9700\u8981\u4e00\u4e2a\u53c2\u6570\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u8fd9\u6837\u7684\u4ee3\u7801 input_file=${1:?usage: $0 input_file} \u3002\u5728\u53d8\u91cf\u4e3a\u7a7a\u65f6\u4f7f\u7528\u9ed8\u8ba4\u503c\uff1a ${name:-default} \u3002\u5982\u679c\u4f60\u8981\u5728\u4e4b\u524d\u7684\u4f8b\u5b50\u4e2d\u518d\u52a0\u4e00\u4e2a\uff08\u53ef\u9009\u7684\uff09\u53c2\u6570\uff0c\u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c\u8fd9\u6837\u7684\u4ee3\u7801 output_file=${2:-logfile} \uff0c\u5982\u679c\u7701\u7565\u4e86 $2\uff0c\u5b83\u7684\u503c\u5c31\u4e3a\u7a7a\uff0c\u4e8e\u662f output_file \u5c31\u4f1a\u88ab\u8bbe\u4e3a logfile \u3002\u6570\u5b66\u8868\u8fbe\u5f0f\uff1a i=$(( (i + 1) % 5 )) \u3002\u5e8f\u5217\uff1a {1..10} \u3002\u622a\u65ad\u5b57\u7b26\u4e32\uff1a ${var%suffix} \u548c ${var#prefix} \u3002\u4f8b\u5982\uff0c\u5047\u8bbe var=foo.pdf \uff0c\u90a3\u4e48 echo ${var%.pdf}.txt \u5c06\u8f93\u51fa foo.txt \u3002 \u4f7f\u7528\u62ec\u53f7\u6269\u5c55\uff08 { ... } \uff09\u6765\u51cf\u5c11\u8f93\u5165\u76f8\u4f3c\u6587\u672c\uff0c\u5e76\u81ea\u52a8\u5316\u6587\u672c\u7ec4\u5408\u3002\u8fd9\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f1a\u5f88\u6709\u7528\uff0c\u4f8b\u5982 mv foo.{txt,pdf} some-dir \uff08\u540c\u65f6\u79fb\u52a8\u4e24\u4e2a\u6587\u4ef6\uff09\uff0c cp somefile{,.bak} \uff08\u4f1a\u88ab\u6269\u5c55\u6210 cp somefile somefile.bak \uff09\u6216\u8005 mkdir -p test-{a,b,c}/subtest-{1,2,3} \uff08\u4f1a\u88ab\u6269\u5c55\u6210\u6240\u6709\u53ef\u80fd\u7684\u7ec4\u5408\uff0c\u5e76\u521b\u5efa\u4e00\u4e2a\u76ee\u5f55\u6811\uff09\u3002 \u901a\u8fc7\u4f7f\u7528 (some command) \u53ef\u4ee5\u5c06\u8f93\u51fa\u89c6\u4e3a\u6587\u4ef6\u3002\u4f8b\u5982\uff0c\u5bf9\u6bd4\u672c\u5730\u6587\u4ef6 /etc/hosts \u548c\u4e00\u4e2a\u8fdc\u7a0b\u6587\u4ef6\uff1a diff /etc/hosts (ssh somehost cat /etc/hosts) \u7f16\u5199\u811a\u672c\u65f6\uff0c\u4f60\u53ef\u80fd\u4f1a\u60f3\u8981\u628a\u4ee3\u7801\u90fd\u653e\u5728\u5927\u62ec\u53f7\u91cc\u3002\u7f3a\u5c11\u53f3\u62ec\u53f7\u7684\u8bdd\uff0c\u4ee3\u7801\u5c31\u4f1a\u56e0\u4e3a\u8bed\u6cd5\u9519\u8bef\u800c\u65e0\u6cd5\u6267\u884c\u3002\u5982\u679c\u4f60\u7684\u811a\u672c\u662f\u8981\u653e\u5728\u7f51\u4e0a\u5206\u4eab\u4f9b\u4ed6\u4eba\u4f7f\u7528\u7684\uff0c\u8fd9\u6837\u7684\u5199\u6cd5\u5c31\u4f53\u73b0\u51fa\u5b83\u7684\u597d\u5904\u4e86\uff0c\u56e0\u4e3a\u8fd9\u6837\u53ef\u4ee5\u9632\u6b62\u4e0b\u8f7d\u4e0d\u5b8c\u5168\u4ee3\u7801\u88ab\u6267\u884c\u3002 { # \u5728\u8fd9\u91cc\u5199\u4ee3\u7801 } \u4e86\u89e3 Bash \u4e2d\u7684\u201chere documents\u201d\uff0c\u4f8b\u5982 cat EOF ... \u3002 \u5728 Bash \u4e2d\uff0c\u540c\u65f6\u91cd\u5b9a\u5411\u6807\u51c6\u8f93\u51fa\u548c\u6807\u51c6\u9519\u8bef\uff1a some-command logfile 2 1 \u6216\u8005 some-command logfile \u3002\u901a\u5e38\uff0c\u4e3a\u4e86\u4fdd\u8bc1\u547d\u4ee4\u4e0d\u4f1a\u5728\u6807\u51c6\u8f93\u5165\u91cc\u6b8b\u7559\u4e00\u4e2a\u672a\u5173\u95ed\u7684\u6587\u4ef6\u53e5\u67c4\u6346\u7ed1\u5728\u4f60\u5f53\u524d\u6240\u5728\u7684\u7ec8\u7aef\u4e0a\uff0c\u5728\u547d\u4ee4\u540e\u6dfb\u52a0 /dev/null \u662f\u4e00\u4e2a\u597d\u4e60\u60ef\u3002 \u4f7f\u7528 man ascii \u67e5\u770b\u5177\u6709\u5341\u516d\u8fdb\u5236\u548c\u5341\u8fdb\u5236\u503c\u7684ASCII\u8868\u3002 man unicode \uff0c man utf-8 \uff0c\u4ee5\u53ca man latin1 \u6709\u52a9\u4e8e\u4f60\u53bb\u4e86\u89e3\u901a\u7528\u7684\u7f16\u7801\u4fe1\u606f\u3002 \u4f7f\u7528 screen \u6216 tmux \u6765\u4f7f\u7528\u591a\u4efd\u5c4f\u5e55\uff0c\u5f53\u4f60\u5728\u4f7f\u7528 ssh \u65f6\uff08\u4fdd\u5b58 session \u4fe1\u606f\uff09\u5c06\u5c24\u4e3a\u6709\u7528\u3002\u800c byobu \u53ef\u4ee5\u4e3a\u5b83\u4eec\u63d0\u4f9b\u66f4\u591a\u7684\u4fe1\u606f\u548c\u6613\u7528\u7684\u7ba1\u7406\u5de5\u5177\u3002\u53e6\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684 session \u6301\u4e45\u5316\u89e3\u51b3\u65b9\u6848\u662f dtach \u3002 ssh \u4e2d\uff0c\u4e86\u89e3\u5982\u4f55\u4f7f\u7528 -L \u6216 -D \uff08\u5076\u5c14\u9700\u8981\u7528 -R \uff09\u5f00\u542f\u96a7\u9053\u662f\u975e\u5e38\u6709\u7528\u7684\uff0c\u6bd4\u5982\u5f53\u4f60\u9700\u8981\u4ece\u4e00\u53f0\u8fdc\u7a0b\u670d\u52a1\u5668\u4e0a\u8bbf\u95ee web \u9875\u9762\u3002 \u5bf9 ssh \u8bbe\u7f6e\u505a\u4e00\u4e9b\u5c0f\u4f18\u5316\u53ef\u80fd\u662f\u5f88\u6709\u7528\u7684\uff0c\u4f8b\u5982\u8fd9\u4e2a ~/.ssh/config \u6587\u4ef6\u5305\u542b\u4e86\u9632\u6b62\u7279\u5b9a\u7f51\u7edc\u73af\u5883\u4e0b\u8fde\u63a5\u65ad\u5f00\u3001\u538b\u7f29\u6570\u636e\u3001\u591a\u901a\u9053\u7b49\u9009\u9879\uff1a TCPKeepAlive=yes ServerAliveInterval=15 ServerAliveCountMax=6 Compression=yes ControlMaster auto ControlPath /tmp/%r@%h:%p ControlPersist yes \u4e00\u4e9b\u5176\u4ed6\u7684\u5173\u4e8e ssh \u7684\u9009\u9879\u662f\u4e0e\u5b89\u5168\u76f8\u5173\u7684\uff0c\u5e94\u5f53\u5c0f\u5fc3\u7ffc\u7ffc\u7684\u4f7f\u7528\u3002\u4f8b\u5982\u4f60\u5e94\u5f53\u53ea\u80fd\u5728\u53ef\u4fe1\u4efb\u7684\u7f51\u7edc\u4e2d\u542f\u7528 StrictHostKeyChecking=no \uff0c ForwardAgent=yes \u3002 \u8003\u8651\u4f7f\u7528 mosh \u4f5c\u4e3a ssh \u7684\u66ff\u4ee3\u54c1\uff0c\u5b83\u4f7f\u7528 UDP \u534f\u8bae\u3002\u5b83\u53ef\u4ee5\u907f\u514d\u8fde\u63a5\u88ab\u4e2d\u65ad\u5e76\u4e14\u5bf9\u5e26\u5bbd\u9700\u6c42\u66f4\u5c0f\uff0c\u4f46\u5b83\u9700\u8981\u5728\u670d\u52a1\u7aef\u505a\u76f8\u5e94\u7684\u914d\u7f6e\u3002 \u83b7\u53d6\u516b\u8fdb\u5236\u5f62\u5f0f\u7684\u6587\u4ef6\u8bbf\u95ee\u6743\u9650\uff08\u4fee\u6539\u7cfb\u7edf\u8bbe\u7f6e\u65f6\u901a\u5e38\u9700\u8981\uff0c\u4f46 ls \u7684\u529f\u80fd\u4e0d\u90a3\u4e48\u597d\u7528\u5e76\u4e14\u901a\u5e38\u4f1a\u641e\u7838\uff09\uff0c\u53ef\u4ee5\u4f7f\u7528\u7c7b\u4f3c\u5982\u4e0b\u7684\u4ee3\u7801\uff1a stat -c '%A %a %n' /etc/timezone \u4f7f\u7528 percol \u6216\u8005 fzf \u53ef\u4ee5\u4ea4\u4e92\u5f0f\u5730\u4ece\u53e6\u4e00\u4e2a\u547d\u4ee4\u8f93\u51fa\u4e2d\u9009\u53d6\u503c\u3002 \u4f7f\u7528 fpp \uff08 PathPicker \uff09\u53ef\u4ee5\u4e0e\u57fa\u4e8e\u53e6\u4e00\u4e2a\u547d\u4ee4(\u4f8b\u5982 git \uff09\u8f93\u51fa\u7684\u6587\u4ef6\u4ea4\u4e92\u3002 \u5c06 web \u670d\u52a1\u5668\u4e0a\u5f53\u524d\u76ee\u5f55\u4e0b\u6240\u6709\u7684\u6587\u4ef6\uff08\u4ee5\u53ca\u5b50\u76ee\u5f55\uff09\u66b4\u9732\u7ed9\u4f60\u6240\u5904\u7f51\u7edc\u7684\u6240\u6709\u7528\u6237\uff0c\u4f7f\u7528\uff1a python -m SimpleHTTPServer 7777 \uff08\u4f7f\u7528\u7aef\u53e3 7777 \u548c Python 2\uff09\u6216 python -m http.server 7777 \uff08\u4f7f\u7528\u7aef\u53e3 7777 \u548c Python 3\uff09\u3002 \u4ee5\u5176\u4ed6\u7528\u6237\u7684\u8eab\u4efd\u6267\u884c\u547d\u4ee4\uff0c\u4f7f\u7528 sudo \u3002\u9ed8\u8ba4\u4ee5 root \u7528\u6237\u7684\u8eab\u4efd\u6267\u884c\uff1b\u4f7f\u7528 -u \u6765\u6307\u5b9a\u5176\u4ed6\u7528\u6237\u3002\u4f7f\u7528 -i \u6765\u4ee5\u8be5\u7528\u6237\u767b\u5f55\uff08\u9700\u8981\u8f93\u5165_\u4f60\u81ea\u5df1\u7684_\u5bc6\u7801\uff09\u3002 \u5c06 shell \u5207\u6362\u4e3a\u5176\u4ed6\u7528\u6237\uff0c\u4f7f\u7528 su username \u6216\u8005 sudo - username \u3002\u52a0\u5165 - \u4f1a\u4f7f\u5f97\u5207\u6362\u540e\u7684\u73af\u5883\u4e0e\u4f7f\u7528\u8be5\u7528\u6237\u767b\u5f55\u540e\u7684\u73af\u5883\u76f8\u540c\u3002\u7701\u7565\u7528\u6237\u540d\u5219\u9ed8\u8ba4\u4e3a root\u3002\u5207\u6362\u5230\u54ea\u4e2a\u7528\u6237\uff0c\u5c31\u9700\u8981\u8f93\u5165_\u54ea\u4e2a\u7528\u6237\u7684_\u5bc6\u7801\u3002 \u4e86\u89e3\u547d\u4ee4\u884c\u7684 128K \u9650\u5236 \u3002\u4f7f\u7528\u901a\u914d\u7b26\u5339\u914d\u5927\u91cf\u6587\u4ef6\u540d\u65f6\uff0c\u5e38\u4f1a\u9047\u5230\u201cArgument list too long\u201d\u7684\u9519\u8bef\u4fe1\u606f\u3002\uff08\u8fd9\u79cd\u60c5\u51b5\u4e0b\u6362\u7528 find \u6216 xargs \u901a\u5e38\u53ef\u4ee5\u89e3\u51b3\u3002\uff09 \u5f53\u4f60\u9700\u8981\u4e00\u4e2a\u57fa\u672c\u7684\u8ba1\u7b97\u5668\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528 python \u89e3\u91ca\u5668\uff08\u5f53\u7136\u4f60\u8981\u7528 python \u7684\u65f6\u5019\u4e5f\u662f\u8fd9\u6837\uff09\u3002\u4f8b\u5982\uff1a 2+3 5","title":"\u65e5\u5e38\u4f7f\u7528"},{"location":"cmd_line/#_5","text":"\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u901a\u8fc7\u6587\u4ef6\u540d\u67e5\u627e\u4e00\u4e2a\u6587\u4ef6\uff0c\u4f7f\u7528\u7c7b\u4f3c\u4e8e\u8fd9\u6837\u7684\u547d\u4ee4\uff1a find . -iname '*something*' \u3002\u5728\u6240\u6709\u8def\u5f84\u4e0b\u901a\u8fc7\u6587\u4ef6\u540d\u67e5\u627e\u6587\u4ef6\uff0c\u4f7f\u7528 locate something \uff08\u4f46\u6ce8\u610f\u5230 updatedb \u53ef\u80fd\u6ca1\u6709\u5bf9\u6700\u8fd1\u65b0\u5efa\u7684\u6587\u4ef6\u5efa\u7acb\u7d22\u5f15\uff0c\u6240\u4ee5\u4f60\u53ef\u80fd\u65e0\u6cd5\u5b9a\u4f4d\u5230\u8fd9\u4e9b\u672a\u88ab\u7d22\u5f15\u7684\u6587\u4ef6\uff09\u3002 \u4f7f\u7528 ag \u5728\u6e90\u4ee3\u7801\u6216\u6570\u636e\u6587\u4ef6\u91cc\u68c0\u7d22\uff08 grep -r \u540c\u6837\u53ef\u4ee5\u505a\u5230\uff0c\u4f46\u76f8\u6bd4\u4e4b\u4e0b ag \u66f4\u52a0\u5148\u8fdb\uff09\u3002 \u5c06 HTML \u8f6c\u4e3a\u6587\u672c\uff1a lynx -dump -stdin \u3002 Markdown\uff0cHTML\uff0c\u4ee5\u53ca\u6240\u6709\u6587\u6863\u683c\u5f0f\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u8bd5\u8bd5 pandoc \u3002 \u5f53\u4f60\u8981\u5904\u7406\u68d8\u624b\u7684 XML \u65f6\u5019\uff0c xmlstarlet \u7b97\u662f\u4e0a\u53e4\u65f6\u4ee3\u6d41\u4f20\u4e0b\u6765\u7684\u795e\u5668\u3002 \u4f7f\u7528 jq \u5904\u7406 JSON\u3002 \u4f7f\u7528 shyaml \u5904\u7406 YAML\u3002 \u8981\u5904\u7406 Excel \u6216 CSV \u6587\u4ef6\u7684\u8bdd\uff0c csvkit \u63d0\u4f9b\u4e86 in2csv \uff0c csvcut \uff0c csvjoin \uff0c csvgrep \u7b49\u65b9\u4fbf\u6613\u7528\u7684\u5de5\u5177\u3002 \u5f53\u4f60\u8981\u5904\u7406 Amazon S3 \u76f8\u5173\u7684\u5de5\u4f5c\u7684\u65f6\u5019\uff0c s3cmd \u662f\u4e00\u4e2a\u5f88\u65b9\u4fbf\u7684\u5de5\u5177\u800c s4cmd \u7684\u6548\u7387\u66f4\u9ad8\u3002Amazon \u5b98\u65b9\u63d0\u4f9b\u7684 aws \u4ee5\u53ca saws \u662f\u5176\u4ed6 AWS \u76f8\u5173\u5de5\u4f5c\u7684\u57fa\u7840\uff0c\u503c\u5f97\u5b66\u4e60\u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 sort \u548c uniq \uff0c\u5305\u62ec uniq \u7684 -u \u53c2\u6570\u548c -d \u53c2\u6570\uff0c\u5177\u4f53\u5185\u5bb9\u5728\u540e\u6587\u5355\u884c\u811a\u672c\u8282\u4e2d\u3002\u53e6\u5916\u53ef\u4ee5\u4e86\u89e3\u4e00\u4e0b comm \u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 cut \uff0c paste \u548c join \u6765\u66f4\u6539\u6587\u4ef6\u3002\u5f88\u591a\u4eba\u90fd\u4f1a\u4f7f\u7528 cut \uff0c\u4f46\u9057\u5fd8\u4e86 join \u3002 \u4e86\u89e3\u5982\u4f55\u8fd0\u7528 wc \u53bb\u8ba1\u7b97\u65b0\u884c\u6570\uff08 -l \uff09\uff0c\u5b57\u7b26\u6570\uff08 -m \uff09\uff0c\u5355\u8bcd\u6570\uff08 -w \uff09\u4ee5\u53ca\u5b57\u8282\u6570\uff08 -c \uff09\u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 tee \u5c06\u6807\u51c6\u8f93\u5165\u590d\u5236\u5230\u6587\u4ef6\u751a\u81f3\u6807\u51c6\u8f93\u51fa\uff0c\u4f8b\u5982 ls -al | tee file.txt \u3002 \u8981\u8fdb\u884c\u4e00\u4e9b\u590d\u6742\u7684\u8ba1\u7b97\uff0c\u6bd4\u5982\u5206\u7ec4\u3001\u9006\u5e8f\u548c\u4e00\u4e9b\u5176\u4ed6\u7684\u7edf\u8ba1\u5206\u6790\uff0c\u53ef\u4ee5\u8003\u8651\u4f7f\u7528 datamash \u3002 \u6ce8\u610f\u5230\u8bed\u8a00\u8bbe\u7f6e\uff08\u4e2d\u6587\u6216\u82f1\u6587\u7b49\uff09\u5bf9\u8bb8\u591a\u547d\u4ee4\u884c\u5de5\u5177\u6709\u4e00\u4e9b\u5fae\u5999\u7684\u5f71\u54cd\uff0c\u6bd4\u5982\u6392\u5e8f\u7684\u987a\u5e8f\u548c\u6027\u80fd\u3002\u5927\u591a\u6570 Linux \u7684\u5b89\u88c5\u8fc7\u7a0b\u4f1a\u5c06 LANG \u6216\u5176\u4ed6\u6709\u5173\u7684\u53d8\u91cf\u8bbe\u7f6e\u4e3a\u7b26\u5408\u672c\u5730\u7684\u8bbe\u7f6e\u3002\u8981\u610f\u8bc6\u5230\u5f53\u4f60\u6539\u53d8\u8bed\u8a00\u8bbe\u7f6e\u65f6\uff0c\u6392\u5e8f\u7684\u7ed3\u679c\u53ef\u80fd\u4f1a\u6539\u53d8\u3002\u660e\u767d\u56fd\u9645\u5316\u53ef\u80fd\u4f1a\u4f7f sort \u6216\u5176\u4ed6\u547d\u4ee4\u8fd0\u884c\u6548\u7387\u4e0b\u964d \u8bb8\u591a\u500d \u3002\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\u96c6\u5408\u8fd0\u7b97\uff09\u4f60\u53ef\u4ee5\u653e\u5fc3\u7684\u4f7f\u7528 export LC_ALL=C \u6765\u5ffd\u7565\u6389\u56fd\u9645\u5316\u5e76\u6309\u7167\u5b57\u8282\u6765\u5224\u65ad\u987a\u5e8f\u3002 \u4f60\u53ef\u4ee5\u5355\u72ec\u6307\u5b9a\u67d0\u4e00\u6761\u547d\u4ee4\u7684\u73af\u5883\uff0c\u53ea\u9700\u5728\u8c03\u7528\u65f6\u628a\u73af\u5883\u53d8\u91cf\u8bbe\u5b9a\u653e\u5728\u547d\u4ee4\u7684\u524d\u9762\uff0c\u4f8b\u5982 TZ=Pacific/Fiji date \u53ef\u4ee5\u83b7\u53d6\u6590\u6d4e\u7684\u65f6\u95f4\u3002 \u4e86\u89e3\u5982\u4f55\u4f7f\u7528 awk \u548c sed \u6765\u8fdb\u884c\u7b80\u5355\u7684\u6570\u636e\u5904\u7406\u3002 \u53c2\u9605 One-liners \u83b7\u53d6\u793a\u4f8b\u3002 \u66ff\u6362\u4e00\u4e2a\u6216\u591a\u4e2a\u6587\u4ef6\u4e2d\u51fa\u73b0\u7684\u5b57\u7b26\u4e32\uff1a perl -pi.bak -e 's/old-string/new-string/g' my-files-*.txt \u4f7f\u7528 repren \u6765\u6279\u91cf\u91cd\u547d\u540d\u6587\u4ef6\uff0c\u6216\u662f\u5728\u591a\u4e2a\u6587\u4ef6\u4e2d\u641c\u7d22\u66ff\u6362\u5185\u5bb9\u3002\uff08\u6709\u4e9b\u65f6\u5019 rename \u547d\u4ee4\u4e5f\u53ef\u4ee5\u6279\u91cf\u91cd\u547d\u540d\uff0c\u4f46\u8981\u6ce8\u610f\uff0c\u5b83\u5728\u4e0d\u540c Linux \u53d1\u884c\u7248\u4e2d\u7684\u529f\u80fd\u5e76\u4e0d\u5b8c\u5168\u4e00\u6837\u3002\uff09 # \u5c06\u6587\u4ef6\u3001\u76ee\u5f55\u548c\u5185\u5bb9\u5168\u90e8\u91cd\u547d\u540d foo - bar: repren --full --preserve-case --from foo --to bar . # \u8fd8\u539f\u6240\u6709\u5907\u4efd\u6587\u4ef6 whatever.bak - whatever: repren --renames --from '(.*)\\.bak' --to '\\1' *.bak # \u7528 rename \u5b9e\u73b0\u4e0a\u8ff0\u529f\u80fd\uff08\u82e5\u53ef\u7528\uff09: rename 's/\\.bak$//' *.bak \u6839\u636e man \u9875\u9762\u7684\u63cf\u8ff0\uff0c rsync \u662f\u4e00\u4e2a\u5feb\u901f\u4e14\u975e\u5e38\u7075\u6d3b\u7684\u6587\u4ef6\u590d\u5236\u5de5\u5177\u3002\u5b83\u95fb\u540d\u4e8e\u8bbe\u5907\u4e4b\u95f4\u7684\u6587\u4ef6\u540c\u6b65\uff0c\u4f46\u5176\u5b9e\u5b83\u5728\u672c\u5730\u60c5\u51b5\u4e0b\u4e5f\u540c\u6837\u6709\u7528\u3002\u5728\u5b89\u5168\u8bbe\u7f6e\u5141\u8bb8\u4e0b\uff0c\u7528 rsync \u4ee3\u66ff scp \u53ef\u4ee5\u5b9e\u73b0\u6587\u4ef6\u7eed\u4f20\uff0c\u800c\u4e0d\u7528\u91cd\u65b0\u4ece\u5934\u5f00\u59cb\u3002\u5b83\u540c\u65f6\u4e5f\u662f\u5220\u9664\u5927\u91cf\u6587\u4ef6\u7684 \u6700\u5feb\u65b9\u6cd5 \u4e4b\u4e00\uff1a mkdir empty rsync -r --delete empty/ some-dir rmdir some-dir \u82e5\u8981\u5728\u590d\u5236\u6587\u4ef6\u65f6\u83b7\u53d6\u5f53\u524d\u8fdb\u5ea6\uff0c\u53ef\u4f7f\u7528 pv \uff0c pycp \uff0c progress \uff0c rsync --progress \u3002\u82e5\u6240\u6267\u884c\u7684\u590d\u5236\u4e3ablock\u5757\u62f7\u8d1d\uff0c\u53ef\u4ee5\u4f7f\u7528 dd status=progress \u3002 \u4f7f\u7528 shuf \u53ef\u4ee5\u4ee5\u884c\u4e3a\u5355\u4f4d\u6765\u6253\u4e71\u6587\u4ef6\u7684\u5185\u5bb9\u6216\u4ece\u4e00\u4e2a\u6587\u4ef6\u4e2d\u968f\u673a\u9009\u53d6\u591a\u884c\u3002 \u4e86\u89e3 sort \u7684\u53c2\u6570\u3002\u663e\u793a\u6570\u5b57\u65f6\uff0c\u4f7f\u7528 -n \u6216\u8005 -h \u6765\u663e\u793a\u66f4\u6613\u8bfb\u7684\u6570\uff08\u4f8b\u5982 du -h \u7684\u8f93\u51fa\uff09\u3002\u660e\u767d\u6392\u5e8f\u65f6\u5173\u952e\u5b57\u7684\u5de5\u4f5c\u539f\u7406\uff08 -t \u548c -k \uff09\u3002\u4f8b\u5982\uff0c\u6ce8\u610f\u5230\u4f60\u9700\u8981 -k1\uff0c1 \u6765\u4ec5\u6309\u7b2c\u4e00\u4e2a\u57df\u6765\u6392\u5e8f\uff0c\u800c -k1 \u610f\u5473\u7740\u6309\u6574\u884c\u6392\u5e8f\u3002\u7a33\u5b9a\u6392\u5e8f\uff08 sort -s \uff09\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5f88\u6709\u7528\u3002\u4f8b\u5982\uff0c\u4ee5\u7b2c\u4e8c\u4e2a\u57df\u4e3a\u4e3b\u5173\u952e\u5b57\uff0c\u7b2c\u4e00\u4e2a\u57df\u4e3a\u6b21\u5173\u952e\u5b57\u8fdb\u884c\u6392\u5e8f\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 sort -k1\uff0c1 | sort -s -k2\uff0c2 \u3002 \u5982\u679c\u4f60\u60f3\u5728 Bash \u547d\u4ee4\u884c\u4e2d\u5199 tab \u5236\u8868\u7b26\uff0c\u6309\u4e0b ctrl-v [Tab] \u6216\u952e\u5165 $'\\t' \uff08\u540e\u8005\u53ef\u80fd\u66f4\u597d\uff0c\u56e0\u4e3a\u4f60\u53ef\u4ee5\u590d\u5236\u7c98\u8d34\u5b83\uff09\u3002 \u6807\u51c6\u7684\u6e90\u4ee3\u7801\u5bf9\u6bd4\u53ca\u5408\u5e76\u5de5\u5177\u662f diff \u548c patch \u3002\u4f7f\u7528 diffstat \u67e5\u770b\u53d8\u66f4\u603b\u89c8\u6570\u636e\u3002\u6ce8\u610f\u5230 diff -r \u5bf9\u6574\u4e2a\u6587\u4ef6\u5939\u6709\u6548\u3002\u4f7f\u7528 diff -r tree1 tree2 | diffstat \u67e5\u770b\u53d8\u66f4\u7684\u7edf\u8ba1\u6570\u636e\u3002 vimdiff \u7528\u4e8e\u6bd4\u5bf9\u5e76\u7f16\u8f91\u6587\u4ef6\u3002 \u5bf9\u4e8e\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4f7f\u7528 hd \uff0c hexdump \u6216\u8005 xxd \u4f7f\u5176\u4ee5\u5341\u516d\u8fdb\u5236\u663e\u793a\uff0c\u4f7f\u7528 bvi \uff0c hexedit \u6216\u8005 biew \u6765\u8fdb\u884c\u4e8c\u8fdb\u5236\u7f16\u8f91\u3002 \u540c\u6837\u5bf9\u4e8e\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c strings \uff08\u5305\u62ec grep \u7b49\u5de5\u5177\uff09\u53ef\u4ee5\u5e2e\u52a9\u5728\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\u67e5\u627e\u7279\u5b9a\u6bd4\u7279\u3002 \u5236\u4f5c\u4e8c\u8fdb\u5236\u5dee\u5206\u6587\u4ef6\uff08Delta \u538b\u7f29\uff09\uff0c\u4f7f\u7528 xdelta3 \u3002 \u4f7f\u7528 iconv \u66f4\u6539\u6587\u672c\u7f16\u7801\u3002\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u529f\u80fd\uff0c\u53ef\u4ee5\u4f7f\u7528 uconv \uff0c\u5b83\u652f\u6301\u4e00\u4e9b\u9ad8\u7ea7\u7684 Unicode \u529f\u80fd\u3002\u4f8b\u5982\uff0c\u8fd9\u6761\u547d\u4ee4\u79fb\u9664\u4e86\u6240\u6709\u91cd\u97f3\u7b26\u53f7\uff1a uconv -f utf-8 -t utf-8 -x '::Any-Lower; ::Any-NFD; [:Nonspacing Mark:] ; ::Any-NFC; ' input.txt output.txt \u62c6\u5206\u6587\u4ef6\u53ef\u4ee5\u4f7f\u7528 split \uff08\u6309\u5927\u5c0f\u62c6\u5206\uff09\u548c csplit \uff08\u6309\u6a21\u5f0f\u62c6\u5206\uff09\u3002 \u64cd\u4f5c\u65e5\u671f\u548c\u65f6\u95f4\u8868\u8fbe\u5f0f\uff0c\u53ef\u4ee5\u7528 dateutils \u4e2d\u7684 dateadd \u3001 datediff \u3001 strptime \u7b49\u5de5\u5177\u3002 \u4f7f\u7528 zless \u3001 zmore \u3001 zcat \u548c zgrep \u5bf9\u538b\u7f29\u8fc7\u7684\u6587\u4ef6\u8fdb\u884c\u64cd\u4f5c\u3002 \u6587\u4ef6\u5c5e\u6027\u53ef\u4ee5\u901a\u8fc7 chattr \u8fdb\u884c\u8bbe\u7f6e\uff0c\u5b83\u6bd4\u6587\u4ef6\u6743\u9650\u66f4\u52a0\u5e95\u5c42\u3002\u4f8b\u5982\uff0c\u4e3a\u4e86\u4fdd\u62a4\u6587\u4ef6\u4e0d\u88ab\u610f\u5916\u5220\u9664\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0d\u53ef\u4fee\u6539\u6807\u8bb0\uff1a sudo chattr +i /critical/directory/or/file \u4f7f\u7528 getfacl \u548c setfacl \u4ee5\u4fdd\u5b58\u548c\u6062\u590d\u6587\u4ef6\u6743\u9650\u3002\u4f8b\u5982\uff1a getfacl -R /some/path permissions.txt setfacl --restore=permissions.txt \u4e3a\u4e86\u9ad8\u6548\u5730\u521b\u5efa\u7a7a\u6587\u4ef6\uff0c\u8bf7\u4f7f\u7528 truncate \uff08\u521b\u5efa \u7a00\u758f\u6587\u4ef6 \uff09\uff0c fallocate \uff08\u7528\u4e8e ext4\uff0cxfs\uff0cbtrf \u548c ocfs2 \u6587\u4ef6\u7cfb\u7edf\uff09\uff0c xfs_mkfile \uff08\u9002\u7528\u4e8e\u51e0\u4e4e\u6240\u6709\u7684\u6587\u4ef6\u7cfb\u7edf\uff0c\u5305\u542b\u5728 xfsprogs \u5305\u4e2d\uff09\uff0c mkfile \uff08\u7528\u4e8e\u7c7b Unix \u64cd\u4f5c\u7cfb\u7edf\uff0c\u6bd4\u5982 Solaris \u548c Mac OS\uff09\u3002","title":"\u6587\u4ef6\u53ca\u6570\u636e\u5904\u7406"},{"location":"cmd_line/#_6","text":"curl \u548c curl -I \u53ef\u4ee5\u88ab\u8f7b\u677e\u5730\u5e94\u7528\u4e8e web \u8c03\u8bd5\u4e2d\uff0c\u5b83\u4eec\u7684\u597d\u5144\u5f1f wget \u4e5f\u662f\u5982\u6b64\uff0c\u6216\u8005\u4e5f\u53ef\u4ee5\u8bd5\u8bd5\u66f4\u6f6e\u7684 httpie \u3002 \u83b7\u53d6 CPU \u548c\u786c\u76d8\u7684\u4f7f\u7528\u72b6\u6001\uff0c\u901a\u5e38\u4f7f\u7528\u4f7f\u7528 top \uff08 htop \u66f4\u4f73\uff09\uff0c iostat \u548c iotop \u3002\u800c iostat -mxz 15 \u53ef\u4ee5\u8ba9\u4f60\u83b7\u6089 CPU \u548c\u6bcf\u4e2a\u786c\u76d8\u5206\u533a\u7684\u57fa\u672c\u4fe1\u606f\u548c\u6027\u80fd\u8868\u73b0\u3002 \u4f7f\u7528 netstat \u548c ss \u67e5\u770b\u7f51\u7edc\u8fde\u63a5\u7684\u7ec6\u8282\u3002 dstat \u5728\u4f60\u60f3\u8981\u5bf9\u7cfb\u7edf\u7684\u73b0\u72b6\u6709\u4e00\u4e2a\u7c97\u7565\u7684\u8ba4\u8bc6\u65f6\u662f\u975e\u5e38\u6709\u7528\u7684\u3002\u7136\u800c\u82e5\u8981\u5bf9\u7cfb\u7edf\u6709\u4e00\u4e2a\u6df1\u5ea6\u7684\u603b\u4f53\u8ba4\u8bc6\uff0c\u4f7f\u7528 glances \uff0c\u5b83\u4f1a\u5728\u4e00\u4e2a\u7ec8\u7aef\u7a97\u53e3\u4e2d\u5411\u4f60\u63d0\u4f9b\u4e00\u4e9b\u7cfb\u7edf\u7ea7\u7684\u6570\u636e\u3002 \u82e5\u8981\u4e86\u89e3\u5185\u5b58\u72b6\u6001\uff0c\u8fd0\u884c\u5e76\u7406\u89e3 free \u548c vmstat \u7684\u8f93\u51fa\u3002\u503c\u5f97\u7559\u610f\u7684\u662f\u201ccached\u201d\u7684\u503c\uff0c\u5b83\u6307\u7684\u662f Linux \u5185\u6838\u7528\u6765\u4f5c\u4e3a\u6587\u4ef6\u7f13\u5b58\u7684\u5185\u5b58\u5927\u5c0f\uff0c\u800c\u4e0e\u7a7a\u95f2\u5185\u5b58\u65e0\u5173\u3002 Java \u7cfb\u7edf\u8c03\u8bd5\u5219\u662f\u4e00\u4ef6\u622a\u7136\u4e0d\u540c\u7684\u4e8b\uff0c\u4e00\u4e2a\u53ef\u4ee5\u7528\u4e8e Oracle \u7684 JVM \u6216\u5176\u4ed6 JVM \u4e0a\u7684\u8c03\u8bd5\u7684\u6280\u5de7\u662f\u4f60\u53ef\u4ee5\u8fd0\u884c kill -3 pid \u540c\u65f6\u4e00\u4e2a\u5b8c\u6574\u7684\u6808\u8f68\u8ff9\u548c\u5806\u6982\u8ff0\uff08\u5305\u62ec GC \u7684\u7ec6\u8282\uff09\u4f1a\u88ab\u4fdd\u5b58\u5230\u6807\u51c6\u9519\u8bef\u6216\u662f\u65e5\u5fd7\u6587\u4ef6\u3002JDK \u4e2d\u7684 jps \uff0c jstat \uff0c jstack \uff0c jmap \u5f88\u6709\u7528\u3002 SJK tools \u66f4\u9ad8\u7ea7\u3002 \u4f7f\u7528 mtr \u53bb\u8ddf\u8e2a\u8def\u7531\uff0c\u7528\u4e8e\u786e\u5b9a\u7f51\u7edc\u95ee\u9898\u3002 \u7528 ncdu \u6765\u67e5\u770b\u78c1\u76d8\u4f7f\u7528\u60c5\u51b5\uff0c\u5b83\u6bd4\u5bfb\u5e38\u7684\u547d\u4ee4\uff0c\u5982 du -sh * \uff0c\u66f4\u8282\u7701\u65f6\u95f4\u3002 \u67e5\u627e\u6b63\u5728\u4f7f\u7528\u5e26\u5bbd\u7684\u5957\u63a5\u5b57\u8fde\u63a5\u6216\u8fdb\u7a0b\uff0c\u4f7f\u7528 iftop \u6216 nethogs \u3002 ab \u5de5\u5177\uff08Apache \u4e2d\u81ea\u5e26\uff09\u53ef\u4ee5\u7b80\u5355\u7c97\u66b4\u5730\u68c0\u67e5 web \u670d\u52a1\u5668\u7684\u6027\u80fd\u3002\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u8d1f\u8f7d\u6d4b\u8bd5\uff0c\u4f7f\u7528 siege \u3002 wireshark \uff0c tshark \u548c ngrep \u53ef\u7528\u4e8e\u590d\u6742\u7684\u7f51\u7edc\u8c03\u8bd5\u3002 \u4e86\u89e3 strace \u548c ltrace \u3002\u8fd9\u4fe9\u5de5\u5177\u5728\u4f60\u7684\u7a0b\u5e8f\u8fd0\u884c\u5931\u8d25\u3001\u6302\u8d77\u751a\u81f3\u5d29\u6e83\uff0c\u800c\u4f60\u5374\u4e0d\u77e5\u9053\u4e3a\u4ec0\u4e48\u6216\u4f60\u60f3\u5bf9\u6027\u80fd\u6709\u4e2a\u603b\u4f53\u7684\u8ba4\u8bc6\u7684\u65f6\u5019\u662f\u975e\u5e38\u6709\u7528\u7684\u3002\u6ce8\u610f profile \u53c2\u6570\uff08 -c \uff09\u548c\u9644\u52a0\u5230\u4e00\u4e2a\u8fd0\u884c\u7684\u8fdb\u7a0b\u53c2\u6570 \uff08 -p \uff09\u3002 \u4e86\u89e3\u4f7f\u7528 ldd \u6765\u68c0\u67e5\u5171\u4eab\u5e93\u3002\u4f46\u662f \u6c38\u8fdc\u4e0d\u8981\u5728\u4e0d\u4fe1\u4efb\u7684\u6587\u4ef6\u4e0a\u8fd0\u884c \u3002 \u4e86\u89e3\u5982\u4f55\u8fd0\u7528 gdb \u8fde\u63a5\u5230\u4e00\u4e2a\u8fd0\u884c\u7740\u7684\u8fdb\u7a0b\u5e76\u83b7\u53d6\u5b83\u7684\u5806\u6808\u8f68\u8ff9\u3002 \u5b66\u4f1a\u4f7f\u7528 /proc \u3002\u5b83\u5728\u8c03\u8bd5\u6b63\u5728\u51fa\u73b0\u7684\u95ee\u9898\u7684\u65f6\u5019\u6709\u65f6\u4f1a\u6548\u679c\u60ca\u4eba\u3002\u6bd4\u5982\uff1a /proc/cpuinfo \uff0c /proc/meminfo \uff0c /proc/cmdline \uff0c /proc/xxx/cwd \uff0c /proc/xxx/exe \uff0c /proc/xxx/fd/ \uff0c /proc/xxx/smaps \uff08\u8fd9\u91cc\u7684 xxx \u8868\u793a\u8fdb\u7a0b\u7684 id \u6216 pid\uff09\u3002 \u5f53\u8c03\u8bd5\u4e00\u4e9b\u4e4b\u524d\u51fa\u73b0\u7684\u95ee\u9898\u7684\u65f6\u5019\uff0c sar \u975e\u5e38\u6709\u7528\u3002\u5b83\u5c55\u793a\u4e86 cpu\u3001\u5185\u5b58\u4ee5\u53ca\u7f51\u7edc\u7b49\u7684\u5386\u53f2\u6570\u636e\u3002 \u5173\u4e8e\u66f4\u6df1\u5c42\u6b21\u7684\u7cfb\u7edf\u5206\u6790\u4ee5\u53ca\u6027\u80fd\u5206\u6790\uff0c\u770b\u770b stap \uff08 SystemTap \uff09\uff0c perf \uff0c\u4ee5\u53ca sysdig \u3002 \u67e5\u770b\u4f60\u5f53\u524d\u4f7f\u7528\u7684\u7cfb\u7edf\uff0c\u4f7f\u7528 uname \uff0c uname -a \uff08Unix\uff0fkernel \u4fe1\u606f\uff09\u6216\u8005 lsb_release -a \uff08Linux \u53d1\u884c\u7248\u4fe1\u606f\uff09\u3002 \u65e0\u8bba\u4ec0\u4e48\u4e1c\u897f\u5de5\u4f5c\u5f97\u5f88\u6b22\u4e50\uff08\u53ef\u80fd\u662f\u786c\u4ef6\u6216\u9a71\u52a8\u95ee\u9898\uff09\u65f6\u53ef\u4ee5\u8bd5\u8bd5 dmesg \u3002 \u5982\u679c\u4f60\u5220\u9664\u4e86\u4e00\u4e2a\u6587\u4ef6\uff0c\u4f46\u901a\u8fc7 du \u53d1\u73b0\u6ca1\u6709\u91ca\u653e\u9884\u671f\u7684\u78c1\u76d8\u7a7a\u95f4\uff0c\u8bf7\u68c0\u67e5\u6587\u4ef6\u662f\u5426\u88ab\u8fdb\u7a0b\u5360\u7528\uff1a lsof | grep deleted | grep \"filename-of-my-big-file\"","title":"\u7cfb\u7edf\u8c03\u8bd5"},{"location":"cmd_line/#_7","text":"\u4e00\u4e9b\u547d\u4ee4\u7ec4\u5408\u7684\u4f8b\u5b50\uff1a \u5f53\u4f60\u9700\u8981\u5bf9\u6587\u672c\u6587\u4ef6\u505a\u96c6\u5408\u4ea4\u3001\u5e76\u3001\u5dee\u8fd0\u7b97\u65f6\uff0c sort \u548c uniq \u4f1a\u662f\u4f60\u7684\u597d\u5e2e\u624b\u3002\u5177\u4f53\u4f8b\u5b50\u8bf7\u53c2\u7167\u4ee3\u7801\u540e\u9762\u7684\uff0c\u6b64\u5904\u5047\u8bbe a \u4e0e b \u662f\u4e24\u5185\u5bb9\u4e0d\u540c\u7684\u6587\u4ef6\u3002\u8fd9\u79cd\u65b9\u5f0f\u6548\u7387\u5f88\u9ad8\uff0c\u5e76\u4e14\u5728\u5c0f\u6587\u4ef6\u548c\u4e0a G \u7684\u6587\u4ef6\u4e0a\u90fd\u80fd\u8fd0\u7528\uff08\u6ce8\u610f\u5c3d\u7ba1\u5728 /tmp \u5728\u4e00\u4e2a\u5c0f\u7684\u6839\u5206\u533a\u4e0a\u65f6\u4f60\u53ef\u80fd\u9700\u8981 -T \u53c2\u6570\uff0c\u4f46\u662f\u5b9e\u9645\u4e0a sort \u5e76\u4e0d\u88ab\u5185\u5b58\u5927\u5c0f\u7ea6\u675f\uff09\uff0c\u53c2\u9605\u524d\u6587\u4e2d\u5173\u4e8e LC_ALL \u548c sort \u7684 -u \u53c2\u6570\u7684\u90e8\u5206\u3002 sort a b | uniq c # c \u662f a \u5e76 b sort a b | uniq -d c # c \u662f a \u4ea4 b sort a b b | uniq -u c # c \u662f a - b \u4f7f\u7528 grep . * \uff08\u6bcf\u884c\u90fd\u4f1a\u9644\u4e0a\u6587\u4ef6\u540d\uff09\u6216\u8005 head -100 * \uff08\u6bcf\u4e2a\u6587\u4ef6\u6709\u4e00\u4e2a\u6807\u9898\uff09\u6765\u9605\u8bfb\u68c0\u67e5\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\u7684\u5185\u5bb9\u3002\u8fd9\u5728\u68c0\u67e5\u4e00\u4e2a\u5145\u6ee1\u914d\u7f6e\u6587\u4ef6\u7684\u76ee\u5f55\uff08\u5982 /sys \u3001 /proc \u3001 /etc \uff09\u65f6\u7279\u522b\u597d\u7528\u3002 \u8ba1\u7b97\u6587\u672c\u6587\u4ef6\u7b2c\u4e09\u5217\u4e2d\u6240\u6709\u6570\u7684\u548c\uff08\u53ef\u80fd\u6bd4\u540c\u7b49\u4f5c\u7528\u7684 Python \u4ee3\u7801\u5feb\u4e09\u500d\u4e14\u4ee3\u7801\u91cf\u5c11\u4e09\u500d\uff09\uff1a awk '{ x += $3 } END { print x }' myfile \u5982\u679c\u4f60\u60f3\u5728\u6587\u4ef6\u6811\u4e0a\u67e5\u770b\u5927\u5c0f/\u65e5\u671f\uff0c\u8fd9\u53ef\u80fd\u770b\u8d77\u6765\u50cf\u9012\u5f52\u7248\u7684 ls -l \u4f46\u6bd4 ls -lR \u66f4\u6613\u4e8e\u7406\u89e3\uff1a find . -type f -ls \u5047\u8bbe\u4f60\u6709\u4e00\u4e2a\u7c7b\u4f3c\u4e8e web \u670d\u52a1\u5668\u65e5\u5fd7\u6587\u4ef6\u7684\u6587\u672c\u6587\u4ef6\uff0c\u5e76\u4e14\u4e00\u4e2a\u786e\u5b9a\u7684\u503c\u53ea\u4f1a\u51fa\u73b0\u5728\u67d0\u4e9b\u884c\u4e0a\uff0c\u5047\u8bbe\u4e00\u4e2a acct_id \u53c2\u6570\u5728 URI \u4e2d\u3002\u5982\u679c\u4f60\u60f3\u8ba1\u7b97\u51fa\u6bcf\u4e2a acct_id \u503c\u6709\u591a\u5c11\u6b21\u8bf7\u6c42\uff0c\u4f7f\u7528\u5982\u4e0b\u4ee3\u7801\uff1a egrep -o 'acct_id=[0-9]+' access.log | cut -d= -f2 | sort | uniq -c | sort -rn \u8981\u6301\u7eed\u76d1\u6d4b\u6587\u4ef6\u6539\u52a8\uff0c\u53ef\u4ee5\u4f7f\u7528 watch \uff0c\u4f8b\u5982\u68c0\u67e5\u67d0\u4e2a\u6587\u4ef6\u5939\u4e2d\u6587\u4ef6\u7684\u6539\u53d8\uff0c\u53ef\u4ee5\u7528 watch -d -n 2 'ls -rtlh | tail' \uff1b\u6216\u8005\u5728\u6392\u67e5 WiFi \u8bbe\u7f6e\u6545\u969c\u65f6\u8981\u76d1\u6d4b\u7f51\u7edc\u8bbe\u7f6e\u7684\u66f4\u6539\uff0c\u53ef\u4ee5\u7528 watch -d -n 2 ifconfig \u3002 \u8fd0\u884c\u8fd9\u4e2a\u51fd\u6570\u4ece\u8fd9\u7bc7\u6587\u6863\u4e2d\u968f\u673a\u83b7\u53d6\u4e00\u6761\u6280\u5de7\uff08\u89e3\u6790 Markdown \u6587\u4ef6\u5e76\u62bd\u53d6\u9879\u76ee\uff09\uff1a function taocl() { curl -s https://raw.githubusercontent.com/jlevy/the-art-of-command-line/master/README-zh.md| pandoc -f markdown -t html | iconv -f 'utf-8' -t 'unicode' | xmlstarlet fo --html --dropdtd | xmlstarlet sel -t -v (html/body/ul/li[count(p) 0])[$RANDOM mod last()+1] | xmlstarlet unesc | fmt -80 }","title":"\u5355\u884c\u811a\u672c"},{"location":"cmd_line/#_8","text":"expr \uff1a\u8ba1\u7b97\u8868\u8fbe\u5f0f\u6216\u6b63\u5219\u5339\u914d m4 \uff1a\u7b80\u5355\u7684\u5b8f\u5904\u7406\u5668 yes \uff1a\u591a\u6b21\u6253\u5370\u5b57\u7b26\u4e32 cal \uff1a\u6f02\u4eae\u7684\u65e5\u5386 env \uff1a\u6267\u884c\u4e00\u4e2a\u547d\u4ee4\uff08\u811a\u672c\u6587\u4ef6\u4e2d\u5f88\u6709\u7528\uff09 printenv \uff1a\u6253\u5370\u73af\u5883\u53d8\u91cf\uff08\u8c03\u8bd5\u65f6\u6216\u5728\u5199\u811a\u672c\u6587\u4ef6\u65f6\u5f88\u6709\u7528\uff09 look \uff1a\u67e5\u627e\u4ee5\u7279\u5b9a\u5b57\u7b26\u4e32\u5f00\u5934\u7684\u5355\u8bcd\u6216\u884c cut \uff0c paste \u548c join \uff1a\u6570\u636e\u4fee\u6539 fmt \uff1a\u683c\u5f0f\u5316\u6587\u672c\u6bb5\u843d pr \uff1a\u5c06\u6587\u672c\u683c\u5f0f\u5316\u6210\u9875\uff0f\u5217\u5f62\u5f0f fold \uff1a\u5305\u88f9\u6587\u672c\u4e2d\u7684\u51e0\u884c column \uff1a\u5c06\u6587\u672c\u683c\u5f0f\u5316\u6210\u591a\u4e2a\u5bf9\u9f50\u3001\u5b9a\u5bbd\u7684\u5217\u6216\u8868\u683c expand \u548c unexpand \uff1a\u5236\u8868\u7b26\u4e0e\u7a7a\u683c\u4e4b\u95f4\u8f6c\u6362 nl \uff1a\u6dfb\u52a0\u884c\u53f7 seq \uff1a\u6253\u5370\u6570\u5b57 bc \uff1a\u8ba1\u7b97\u5668 factor \uff1a\u5206\u89e3\u56e0\u6570 gpg \uff1a\u52a0\u5bc6\u5e76\u7b7e\u540d\u6587\u4ef6 toe \uff1aterminfo \u5165\u53e3\u5217\u8868 nc \uff1a\u7f51\u7edc\u8c03\u8bd5\u53ca\u6570\u636e\u4f20\u8f93 socat \uff1a\u5957\u63a5\u5b57\u4ee3\u7406\uff0c\u4e0e netcat \u7c7b\u4f3c slurm \uff1a\u7f51\u7edc\u6d41\u91cf\u53ef\u89c6\u5316 dd \uff1a\u6587\u4ef6\u6216\u8bbe\u5907\u95f4\u4f20\u8f93\u6570\u636e file \uff1a\u786e\u5b9a\u6587\u4ef6\u7c7b\u578b tree \uff1a\u4ee5\u6811\u7684\u5f62\u5f0f\u663e\u793a\u8def\u5f84\u548c\u6587\u4ef6\uff0c\u7c7b\u4f3c\u4e8e\u9012\u5f52\u7684 ls stat \uff1a\u6587\u4ef6\u4fe1\u606f time \uff1a\u6267\u884c\u547d\u4ee4\uff0c\u5e76\u8ba1\u7b97\u6267\u884c\u65f6\u95f4 timeout \uff1a\u5728\u6307\u5b9a\u65f6\u957f\u8303\u56f4\u5185\u6267\u884c\u547d\u4ee4\uff0c\u5e76\u5728\u89c4\u5b9a\u65f6\u95f4\u7ed3\u675f\u540e\u505c\u6b62\u8fdb\u7a0b lockfile \uff1a\u4f7f\u6587\u4ef6\u53ea\u80fd\u901a\u8fc7 rm -f \u79fb\u9664 logrotate \uff1a \u5207\u6362\u3001\u538b\u7f29\u4ee5\u53ca\u53d1\u9001\u65e5\u5fd7\u6587\u4ef6 watch \uff1a\u91cd\u590d\u8fd0\u884c\u540c\u4e00\u4e2a\u547d\u4ee4\uff0c\u5c55\u793a\u7ed3\u679c\u5e76\uff0f\u6216\u9ad8\u4eae\u6709\u66f4\u6539\u7684\u90e8\u5206 when-changed \uff1a\u5f53\u68c0\u6d4b\u5230\u6587\u4ef6\u66f4\u6539\u65f6\u6267\u884c\u6307\u5b9a\u547d\u4ee4\u3002\u53c2\u9605 inotifywait \u548c entr \u3002 tac \uff1a\u53cd\u5411\u8f93\u51fa\u6587\u4ef6 shuf \uff1a\u6587\u4ef6\u4e2d\u968f\u673a\u9009\u53d6\u51e0\u884c comm \uff1a\u4e00\u884c\u4e00\u884c\u7684\u6bd4\u8f83\u6392\u5e8f\u8fc7\u7684\u6587\u4ef6 strings \uff1a\u4ece\u4e8c\u8fdb\u5236\u6587\u4ef6\u4e2d\u62bd\u53d6\u6587\u672c tr \uff1a\u8f6c\u6362\u5b57\u6bcd iconv \u6216 uconv \uff1a\u6587\u672c\u7f16\u7801\u8f6c\u6362 split \u548c csplit \uff1a\u5206\u5272\u6587\u4ef6 sponge \uff1a\u5728\u5199\u5165\u524d\u8bfb\u53d6\u6240\u6709\u8f93\u5165\uff0c\u5728\u8bfb\u53d6\u6587\u4ef6\u540e\u518d\u5411\u540c\u4e00\u6587\u4ef6\u5199\u5165\u65f6\u6bd4\u8f83\u6709\u7528\uff0c\u4f8b\u5982 grep -v something some-file | sponge some-file units \uff1a\u5c06\u4e00\u79cd\u8ba1\u91cf\u5355\u4f4d\u8f6c\u6362\u4e3a\u53e6\u4e00\u79cd\u7b49\u6548\u7684\u8ba1\u91cf\u5355\u4f4d\uff08\u53c2\u9605 /usr/share/units/definitions.units \uff09 apg \uff1a\u968f\u673a\u751f\u6210\u5bc6\u7801 xz \uff1a\u9ad8\u6bd4\u4f8b\u7684\u6587\u4ef6\u538b\u7f29 ldd \uff1a\u52a8\u6001\u5e93\u4fe1\u606f nm \uff1a\u63d0\u53d6 obj \u6587\u4ef6\u4e2d\u7684\u7b26\u53f7 ab \u6216 wrk \uff1aweb \u670d\u52a1\u5668\u6027\u80fd\u5206\u6790 strace \uff1a\u8c03\u8bd5\u7cfb\u7edf\u8c03\u7528 mtr \uff1a\u66f4\u597d\u7684\u7f51\u7edc\u8c03\u8bd5\u8ddf\u8e2a\u5de5\u5177 cssh \uff1a\u53ef\u89c6\u5316\u7684\u5e76\u53d1 shell rsync \uff1a\u901a\u8fc7 ssh \u6216\u672c\u5730\u6587\u4ef6\u7cfb\u7edf\u540c\u6b65\u6587\u4ef6\u548c\u6587\u4ef6\u5939 wireshark \u548c tshark \uff1a\u6293\u5305\u548c\u7f51\u7edc\u8c03\u8bd5\u5de5\u5177 ngrep \uff1a\u7f51\u7edc\u5c42\u7684 grep host \u548c dig \uff1aDNS \u67e5\u627e lsof \uff1a\u5217\u51fa\u5f53\u524d\u7cfb\u7edf\u6253\u5f00\u6587\u4ef6\u7684\u5de5\u5177\u4ee5\u53ca\u67e5\u770b\u7aef\u53e3\u4fe1\u606f dstat \uff1a\u7cfb\u7edf\u72b6\u6001\u67e5\u770b glances \uff1a\u9ad8\u5c42\u6b21\u7684\u591a\u5b50\u7cfb\u7edf\u603b\u89c8 iostat \uff1a\u786c\u76d8\u4f7f\u7528\u72b6\u6001 mpstat \uff1a CPU \u4f7f\u7528\u72b6\u6001 vmstat \uff1a \u5185\u5b58\u4f7f\u7528\u72b6\u6001 htop \uff1atop \u7684\u52a0\u5f3a\u7248 last \uff1a\u767b\u5165\u8bb0\u5f55 w \uff1a\u67e5\u770b\u5904\u4e8e\u767b\u5f55\u72b6\u6001\u7684\u7528\u6237 id \uff1a\u7528\u6237/\u7ec4 ID \u4fe1\u606f sar \uff1a\u7cfb\u7edf\u5386\u53f2\u6570\u636e iftop \u6216 nethogs \uff1a\u5957\u63a5\u5b57\u53ca\u8fdb\u7a0b\u7684\u7f51\u7edc\u5229\u7528\u60c5\u51b5 ss \uff1a\u5957\u63a5\u5b57\u6570\u636e dmesg \uff1a\u5f15\u5bfc\u53ca\u7cfb\u7edf\u9519\u8bef\u4fe1\u606f sysctl \uff1a \u5728\u5185\u6838\u8fd0\u884c\u65f6\u52a8\u6001\u5730\u67e5\u770b\u548c\u4fee\u6539\u5185\u6838\u7684\u8fd0\u884c\u53c2\u6570 hdparm \uff1aSATA/ATA \u78c1\u76d8\u66f4\u6539\u53ca\u6027\u80fd\u5206\u6790 lsblk \uff1a\u5217\u51fa\u5757\u8bbe\u5907\u4fe1\u606f\uff1a\u4ee5\u6811\u5f62\u5c55\u793a\u4f60\u7684\u78c1\u76d8\u4ee5\u53ca\u78c1\u76d8\u5206\u533a\u4fe1\u606f lshw \uff0c lscpu \uff0c lspci \uff0c lsusb \u548c dmidecode \uff1a\u67e5\u770b\u786c\u4ef6\u4fe1\u606f\uff0c\u5305\u62ec CPU\u3001BIOS\u3001RAID\u3001\u663e\u5361\u3001USB\u8bbe\u5907\u7b49 lsmod \u548c modinfo \uff1a\u5217\u51fa\u5185\u6838\u6a21\u5757\uff0c\u5e76\u663e\u793a\u5176\u7ec6\u8282 fortune \uff0c ddate \u548c sl \uff1a\u989d\uff0c\u8fd9\u4e3b\u8981\u53d6\u51b3\u4e8e\u4f60\u662f\u5426\u8ba4\u4e3a\u84b8\u6c7d\u706b\u8f66\u548c\u83ab\u540d\u5176\u5999\u7684\u540d\u4eba\u540d\u8a00\u662f\u5426\u201c\u6709\u7528\u201d","title":"\u51b7\u95e8\u4f46\u6709\u7528"},{"location":"cmd_line/#os-x","text":"\u4ee5\u4e0b\u662f \u4ec5\u9650\u4e8e OS X \u7cfb\u7edf\u7684\u6280\u5de7\u3002 \u7528 brew \uff08Homebrew\uff09\u6216\u8005 port \uff08MacPorts\uff09\u8fdb\u884c\u5305\u7ba1\u7406\u3002\u8fd9\u4e9b\u53ef\u4ee5\u7528\u6765\u5728 OS X \u7cfb\u7edf\u4e0a\u5b89\u88c5\u4ee5\u4e0a\u7684\u5927\u591a\u6570\u547d\u4ee4\u3002 \u7528 pbcopy \u590d\u5236\u4efb\u4f55\u547d\u4ee4\u7684\u8f93\u51fa\u5230\u684c\u9762\u5e94\u7528\uff0c\u7528 pbpaste \u7c98\u8d34\u8f93\u5165\u3002 \u82e5\u8981\u5728 OS X \u7ec8\u7aef\u4e2d\u5c06 Option \u952e\u89c6\u4e3a alt \u952e\uff08\u4f8b\u5982\u5728\u4e0a\u9762\u4ecb\u7ecd\u7684 alt-b \u3001 alt-f \u7b49\u547d\u4ee4\u4e2d\u7528\u5230\uff09\uff0c\u6253\u5f00 \u504f\u597d\u8bbe\u7f6e - \u63cf\u8ff0\u6587\u4ef6 - \u952e\u76d8 \u5e76\u52fe\u9009\u201c\u4f7f\u7528 Option \u952e\u4f5c\u4e3a Meta \u952e\u201d\u3002 \u7528 open \u6216\u8005 open -a /Applications/Whatever.app \u4f7f\u7528\u684c\u9762\u5e94\u7528\u6253\u5f00\u6587\u4ef6\u3002 Spotlight\uff1a\u7528 mdfind \u641c\u7d22\u6587\u4ef6\uff0c\u7528 mdls \u5217\u51fa\u5143\u6570\u636e\uff08\u4f8b\u5982\u7167\u7247\u7684 EXIF \u4fe1\u606f\uff09\u3002 \u6ce8\u610f OS X \u7cfb\u7edf\u662f\u57fa\u4e8e BSD UNIX \u7684\uff0c\u8bb8\u591a\u547d\u4ee4\uff08\u4f8b\u5982 ps \uff0c ls \uff0c tail \uff0c awk \uff0c sed \uff09\u90fd\u548c Linux \u4e2d\u6709\u5fae\u5999\u7684\u4e0d\u540c\uff08 Linux \u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u5230\u4e86 System V-style Unix \u548c GNU \u5de5\u5177\u5f71\u54cd\uff09\u3002\u4f60\u53ef\u4ee5\u901a\u8fc7\u6807\u9898\u4e3a \"BSD General Commands Manual\" \u7684 man \u9875\u9762\u53d1\u73b0\u8fd9\u4e9b\u4e0d\u540c\u3002\u5728\u6709\u4e9b\u60c5\u51b5\u4e0b GNU \u7248\u672c\u7684\u547d\u4ee4\u4e5f\u53ef\u80fd\u88ab\u5b89\u88c5\uff08\u4f8b\u5982 gawk \u548c gsed \u5bf9\u5e94 GNU \u4e2d\u7684 awk \u548c sed \uff09\u3002\u5982\u679c\u8981\u5199\u8de8\u5e73\u53f0\u7684 Bash \u811a\u672c\uff0c\u907f\u514d\u4f7f\u7528\u8fd9\u4e9b\u547d\u4ee4\uff08\u4f8b\u5982\uff0c\u8003\u8651 Python \u6216\u8005 perl \uff09\u6216\u8005\u7ecf\u8fc7\u4ed4\u7ec6\u7684\u6d4b\u8bd5\u3002 \u7528 sw_vers \u83b7\u53d6 OS X \u7684\u7248\u672c\u4fe1\u606f\u3002","title":"\u4ec5\u9650 OS X \u7cfb\u7edf"},{"location":"cmd_line/#windows","text":"\u4ee5\u4e0b\u662f \u4ec5\u9650\u4e8e Windows \u7cfb\u7edf\u7684\u6280\u5de7\u3002","title":"\u4ec5\u9650 Windows \u7cfb\u7edf"},{"location":"cmd_line/#winodws-unix","text":"\u53ef\u4ee5\u5b89\u88c5 Cygwin \u5141\u8bb8\u4f60\u5728 Microsoft Windows \u4e2d\u4f53\u9a8c Unix shell \u7684\u5a01\u529b\u3002\u8fd9\u6837\u7684\u8bdd\uff0c\u672c\u6587\u4e2d\u4ecb\u7ecd\u7684\u5927\u591a\u6570\u5185\u5bb9\u90fd\u5c06\u9002\u7528\u3002 \u5728 Windows 10 \u4e0a\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528 Bash on Ubuntu on Windows \uff0c\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u719f\u6089\u7684 Bash \u73af\u5883\uff0c\u5305\u542b\u4e86\u4e0d\u5c11 Unix \u547d\u4ee4\u884c\u5de5\u5177\u3002\u597d\u5904\u662f\u5b83\u5141\u8bb8 Linux \u4e0a\u7f16\u5199\u7684\u7a0b\u5e8f\u5728 Windows \u4e0a\u8fd0\u884c\uff0c\u800c\u53e6\u4e00\u65b9\u9762\uff0cWindows \u4e0a\u7f16\u5199\u7684\u7a0b\u5e8f\u5374\u65e0\u6cd5\u5728 Bash \u547d\u4ee4\u884c\u4e2d\u8fd0\u884c\u3002 \u5982\u679c\u4f60\u5728 Windows \u4e0a\u4e3b\u8981\u60f3\u7528 GNU \u5f00\u53d1\u8005\u5de5\u5177\uff08\u4f8b\u5982 GCC\uff09\uff0c\u53ef\u4ee5\u8003\u8651 MinGW \u4ee5\u53ca\u5b83\u7684 MSYS \u5305\uff0c\u8fd9\u4e2a\u5305\u63d0\u4f9b\u4e86\u4f8b\u5982 bash\uff0cgawk\uff0cmake \u548c grep \u7684\u5de5\u5177\u3002MSYS \u5e76\u4e0d\u5305\u542b\u6240\u6709\u53ef\u4ee5\u4e0e Cygwin \u5ab2\u7f8e\u7684\u7279\u6027\u3002\u5f53\u5236\u4f5c Unix \u5de5\u5177\u7684\u539f\u751f Windows \u7aef\u53e3\u65f6 MinGW \u5c06\u7279\u522b\u5730\u6709\u7528\u3002 \u53e6\u4e00\u4e2a\u5728 Windows \u4e0b\u5b9e\u73b0\u63a5\u8fd1 Unix \u73af\u5883\u5916\u89c2\u6548\u679c\u7684\u9009\u9879\u662f Cash \u3002\u6ce8\u610f\u5728\u6b64\u73af\u5883\u4e0b\u53ea\u6709\u5f88\u5c11\u7684 Unix \u547d\u4ee4\u548c\u547d\u4ee4\u884c\u53ef\u7528\u3002","title":"\u5728 Winodws \u4e0b\u83b7\u53d6 Unix \u5de5\u5177"},{"location":"cmd_line/#windows_1","text":"\u53ef\u4ee5\u4f7f\u7528 wmic \u5728\u547d\u4ee4\u884c\u73af\u5883\u4e0b\u7ed9\u5927\u90e8\u5206 Windows \u7cfb\u7edf\u7ba1\u7406\u4efb\u52a1\u7f16\u5199\u811a\u672c\u4ee5\u53ca\u6267\u884c\u8fd9\u4e9b\u4efb\u52a1\u3002 Windows \u5b9e\u7528\u7684\u539f\u751f\u547d\u4ee4\u884c\u7f51\u7edc\u5de5\u5177\u5305\u62ec ping \uff0c ipconfig \uff0c tracert \uff0c\u548c netstat \u3002 \u53ef\u4ee5\u4f7f\u7528 Rundll32 \u547d\u4ee4\u6765\u5b9e\u73b0 \u8bb8\u591a\u6709\u7528\u7684 Windows \u4efb\u52a1 \u3002","title":"\u5b9e\u7528 Windows \u547d\u4ee4\u884c\u5de5\u5177"},{"location":"cmd_line/#cygwin","text":"\u901a\u8fc7 Cygwin \u7684\u5305\u7ba1\u7406\u5668\u6765\u5b89\u88c5\u989d\u5916\u7684 Unix \u7a0b\u5e8f\u3002 \u4f7f\u7528 mintty \u4f5c\u4e3a\u4f60\u7684\u547d\u4ee4\u884c\u7a97\u53e3\u3002 \u8981\u8bbf\u95ee Windows \u526a\u8d34\u677f\uff0c\u53ef\u4ee5\u901a\u8fc7 /dev/clipboard \u3002 \u8fd0\u884c cygstart \u4ee5\u901a\u8fc7\u9ed8\u8ba4\u7a0b\u5e8f\u6253\u5f00\u4e00\u4e2a\u6587\u4ef6\u3002 \u8981\u8bbf\u95ee Windows \u6ce8\u518c\u8868\uff0c\u53ef\u4ee5\u4f7f\u7528 regtool \u3002 \u6ce8\u610f Windows \u9a71\u52a8\u5668\u8def\u5f84 C:\\ \u5728 Cygwin \u4e2d\u7528 /cygdrive/c \u4ee3\u8868\uff0c\u800c Cygwin \u7684 / \u4ee3\u8868 Windows \u4e2d\u7684 C:\\cygwin \u3002\u8981\u8f6c\u6362 Cygwin \u548c Windows \u98ce\u683c\u7684\u8def\u5f84\u53ef\u4ee5\u7528 cygpath \u3002\u8fd9\u5728\u9700\u8981\u8c03\u7528 Windows \u7a0b\u5e8f\u7684\u811a\u672c\u91cc\u5f88\u6709\u7528\u3002 \u5b66\u4f1a\u4f7f\u7528 wmic \uff0c\u4f60\u5c31\u53ef\u4ee5\u4ece\u547d\u4ee4\u884c\u6267\u884c\u5927\u591a\u6570 Windows \u7cfb\u7edf\u7ba1\u7406\u4efb\u52a1\uff0c\u5e76\u7f16\u6210\u811a\u672c\u3002 \u8981\u5728 Windows \u4e0b\u83b7\u5f97 Unix \u7684\u754c\u9762\u548c\u4f53\u9a8c\uff0c\u53e6\u4e00\u4e2a\u529e\u6cd5\u662f\u4f7f\u7528 Cash \u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e2a\u73af\u5883\u652f\u6301\u7684 Unix \u547d\u4ee4\u548c\u547d\u4ee4\u884c\u53c2\u6570\u975e\u5e38\u5c11\u3002 \u8981\u5728 Windows \u4e0a\u83b7\u53d6 GNU \u5f00\u53d1\u8005\u5de5\u5177\uff08\u6bd4\u5982 GCC\uff09\u7684\u53e6\u4e00\u4e2a\u529e\u6cd5\u662f\u4f7f\u7528 MinGW \u4ee5\u53ca\u5b83\u7684 MSYS \u8f6f\u4ef6\u5305\uff0c\u8be5\u8f6f\u4ef6\u5305\u63d0\u4f9b\u4e86 bash\u3001gawk\u3001make\u3001grep \u7b49\u5de5\u5177\u3002\u7136\u800c MSYS \u63d0\u4f9b\u7684\u529f\u80fd\u6ca1\u6709 Cygwin \u5b8c\u5584\u3002MinGW \u5728\u521b\u5efa Unix \u5de5\u5177\u7684 Windows \u539f\u751f\u79fb\u690d\u65b9\u9762\u975e\u5e38\u6709\u7528\u3002","title":"Cygwin \u6280\u5de7"},{"location":"cmd_line/#_9","text":"awesome-shell \uff1a\u4e00\u4efd\u7cbe\u5fc3\u7ec4\u7ec7\u7684\u547d\u4ee4\u884c\u5de5\u5177\u53ca\u8d44\u6e90\u7684\u5217\u8868\u3002 awesome-osx-command-line \uff1a\u4e00\u4efd\u9488\u5bf9 OS X \u547d\u4ee4\u884c\u7684\u66f4\u6df1\u5165\u7684\u6307\u5357\u3002 Strict mode \uff1a\u4e3a\u4e86\u7f16\u5199\u66f4\u597d\u7684\u811a\u672c\u6587\u4ef6\u3002 shellcheck \uff1a\u4e00\u4e2a\u9759\u6001 shell \u811a\u672c\u5206\u6790\u5de5\u5177\uff0c\u672c\u8d28\u4e0a\u662f bash\uff0fsh\uff0fzsh \u7684 lint\u3002 Filenames and Pathnames in Shell \uff1a\u6709\u5173\u5982\u4f55\u5728 shell \u811a\u672c\u91cc\u6b63\u786e\u5904\u7406\u6587\u4ef6\u540d\u7684\u7ec6\u679d\u672b\u8282\u3002 Data Science at the Command Line \uff1a\u7528\u4e8e\u6570\u636e\u79d1\u5b66\u7684\u4e00\u4e9b\u547d\u4ee4\u548c\u5de5\u5177\uff0c\u6458\u81ea\u540c\u540d\u4e66\u7c4d\u3002","title":"\u66f4\u591a\u8d44\u6e90"},{"location":"cmd_line/#_10","text":"\u9664\u53bb\u7279\u522b\u5c0f\u7684\u5de5\u4f5c\uff0c\u4f60\u7f16\u5199\u7684\u4ee3\u7801\u5e94\u5f53\u65b9\u4fbf\u4ed6\u4eba\u9605\u8bfb\u3002\u80fd\u529b\u5f80\u5f80\u4f34\u968f\u7740\u8d23\u4efb\uff0c\u4f60 \u6709\u80fd\u529b \u5728 Bash \u4e2d\u73a9\u4e00\u4e9b\u5947\u6280\u6deb\u5de7\u5e76\u4e0d\u610f\u5473\u7740\u4f60\u5e94\u8be5\u53bb\u505a\uff01;)","title":"\u514d\u8d23\u58f0\u660e"},{"location":"cmd_line/#_11","text":"\u672c\u6587\u4f7f\u7528\u6388\u6743\u534f\u8bae Creative Commons Attribution-ShareAlike 4.0 International License \u3002","title":"\u6388\u6743\u6761\u6b3e"},{"location":"data_science/","text":"Awesome Data Science with Python A curated list of awesome resources for practicing data science using Python, including not only libraries, but also links to tutorials, code snippets, blog posts and talks. Core pandas - Data structures built on top of numpy . scikit-learn - Core ML library. matplotlib - Plotting library. seaborn - Data visualization library based on matplotlib. pandas_summary - Basic statistics using DataFrameSummary(df).summary() . pandas_profiling - Descriptive statistics using ProfileReport . sklearn_pandas - Helpful DataFrameMapper class. missingno - Missing data visualization. rainbow-csv - Plugin to display .csv files with nice colors. Environment and Jupyter General Jupyter Tricks Fixing environment: link Python debugger (pdb) - blog post , video , cheatsheet cookiecutter-data-science - Project template for data science projects. nteract - Open Jupyter Notebooks with doubleclick. papermill - Parameterize and execute Jupyter notebooks, tutorial . nbdime - Diff two notebook files, Alternative GitHub App: ReviewNB . RISE - Turn Jupyter notebooks into presentations. qgrid - Pandas DataFrame sorting. pivottablejs - Drag n drop Pivot Tables and Charts for jupyter notebooks. itables - Interactive tables in Jupyter. jupyter-datatables - Interactive tables in Jupyter. debugger - Visual debugger for Jupyter. Pandas Tricks, Alternatives and Additions Pandas Tricks Using df.pipe() (video) pandasvault - Large collection of pandas tricks. modin - Parallelization library for faster pandas DataFrame . vaex - Out-of-Core DataFrames. pandarallel - Parallelize pandas operations. xarray - Extends pandas to n-dimensional arrays. swifter - Apply any function to a pandas dataframe faster. pandas_flavor - Write custom accessors like .str and .dt . pandas-log - Find business logic issues and performance issues in pandas. pandapy - Additional features for pandas. Helpful tqdm - Progress bars for for-loops. icecream - Simple debugging output. loguru - Python logging. pyprojroot - Helpful here() command from R. intake - Loading datasets made easier, talk . Extraction textract - Extract text from any document. camelot - Extract text from PDF. Big Data spark - DataFrame for big data, cheatsheet , tutorial . sparkit-learn , spark-deep-learning - ML frameworks for spark. koalas - Pandas API on Apache Spark. dask , dask-ml - Pandas DataFrame for big data and machine learning library, resources , talk1 , talk2 , notebooks , videos . dask-gateway - Managing dask clusters. turicreate - Helpful SFrame class for out-of-memory dataframes. h2o - Helpful H2OFrame class for out-of-memory dataframes. datatable - Data Table for big data support. cuDF - GPU DataFrame Library. ray - Flexible, high-performance distributed execution framework. mars - Tensor-based unified framework for large-scale data computation. bottleneck - Fast NumPy array functions written in C. bolz - A columnar data container that can be compressed. cupy - NumPy-like API accelerated with CUDA. petastorm - Data access library for parquet files by Uber. zappy - Distributed numpy arrays. Command line tools, CSV ni - Command line tool for big data. xsv - Command line tool for indexing, slicing, analyzing, splitting and joining CSV files. csvkit - Another command line tool for CSV files. csvsort - Sort large csv files. tsv-utils - Tools for working with CSV files by ebay. cheat - Make cheatsheets for command line commands. Classical Statistics Texts Greenland - Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations Lindel\u00f8v - Common statistical tests are linear models Chatruc - The Central Limit Theorem and its misuse Al-Saleh - Properties of the Standard Deviation that are Rarely Mentioned in Classrooms Wainer - The Most Dangerous Equation Gigerenzer - The Bias Bias in Behavioral Economics Cook - Estimating the chances of something that hasn\u2019t happened yet Statistical Tests and Packages researchpy - Helpful summary_cont() function for summary statistics (Table 1). scikit-posthocs - Statistical post-hoc tests for pairwise multiple comparisons. Bland-Altman Plot - Plot for agreement between two methods of measurement. scipy.stats - Statistical tests. ANOVA , Tutorials: One-way , Two-way , Type 1,2,3 explained . Visualizations Null Hypothesis Significance Testing (NHST) and Sample Size Calculation Correlation Cohen's d Confidence Interval Equivalence, non-inferiority and superiority testing Bayesian two-sample t test Distribution of p-values when comparing two groups Understanding the t-distribution and its normal approximation Talks Inverse Propensity Weighting Dealing with Selection Bias By Propensity Based Feature Selection Frameworks scikit-learn - General machine learning framework. h2o - Machine learning framework. caffe - Deep learning framework, pretrained models . mxnet - Deep learning framework, book . Exploration and Cleaning Checklist . janitor - Clean messy column names. impyute - Imputations. fancyimpute - Matrix completion and imputation algorithms. imbalanced-learn - Resampling for imbalanced datasets. tspreprocess - Time series preprocessing: Denoising, Compression, Resampling. Kaggler - Utility functions ( OneHotEncoder(min_obs=100) ) pyupset - Visualizing intersecting sets. pyemd - Earth Mover's Distance, similarity between histograms. Train / Test Split iterative-stratification - Stratification of multilabel data. Feature Engineering Talk sklearn - Pipeline, examples . pdpipe - Pipelines for DataFrames. scikit-lego - Custom transformers for pipelines. few - Feature engineering wrapper for sklearn. skoot - Pipeline helper functions. categorical-encoding - Categorical encoding of variables, vtreat (R package) . dirty_cat - Encoding dirty categorical variables. patsy - R-like syntax for statistical models. mlxtend - LDA. featuretools - Automated feature engineering, example . tsfresh - Time series feature engineering. pypeln - Concurrent data pipelines. feature_engine - Encoders, transformers, etc. Feature Selection Talk Blog post series - 1 , 2 , 3 , 4 Tutorials - 1 , 2 sklearn - Feature selection. eli5 - Feature selection using permutation importance. scikit-feature - Feature selection algorithms. stability-selection - Stability selection. scikit-rebate - Relief-based feature selection algorithms. scikit-genetic - Genetic feature selection. boruta_py - Feature selection, explaination , example . linselect - Feature selection package. mlxtend - Exhaustive feature selection. BoostARoota - Xgboost feature selection algorithm. Dimensionality Reduction Talk prince - Dimensionality reduction, factor analysis (PCA, MCA, CA, FAMD). sklearn - Multidimensional scaling (MDS). sklearn - t-distributed Stochastic Neighbor Embedding (t-SNE), intro . Faster implementations: lvdmaaten , MulticoreTSNE . FIt-SNE - Fast Fourier Transform-accelerated Interpolation-based t-SNE. umap - Uniform Manifold Approximation and Projection, talk , explorer , explanation . sleepwalk - Explore embeddings, interactive visualization (R package). scikit-tda - Topological Data Analysis, paper , talk , talk . mdr - Dimensionality reduction, multifactor dimensionality reduction (MDR). sklearn - Truncated SVD (aka LSA). ivis - Dimensionality reduction using Siamese Networks. trimap - Dimensionality reduction using triplets. Visualization All charts , Austrian monuments . cufflinks - Dynamic visualization library, wrapper for plotly , medium , example . physt - Better histograms, talk , notebook . matplotlib_venn - Venn diagrams, alternative . joypy - Draw stacked density plots. mosaic plots - Categorical variable visualization, example . scikit-plot - ROC curves and other visualizations for ML models. yellowbrick - Visualizations for ML models (similar to scikit-plot). bokeh - Interactive visualization library, Examples , Examples . animatplot - Animate plots build on matplotlib. plotnine - ggplot for Python. altair - Declarative statistical visualization library. bqplot - Plotting library for IPython/Jupyter Notebooks. hvplot - High-level plotting library built on top of holoviews . dtreeviz - Decision tree visualization and model interpretation. chartify - Generate charts. VivaGraphJS - Graph visualization (JS package). pm - Navigatable 3D graph visualization (JS package), example . python-ternary - Triangle plots. falcon - Interactive visualizations for big data. Dashboards dash - Dashboarding solution by plot.ly. Tutorial: 1 , 2 , 3 , 4 , 5 , resources panel - Dashboarding solution. bokeh - Dashboarding solution. visdom - Dashboarding library by facebook. altair example - Video . voila - Turn Jupyter notebooks into standalone web applications. streamlit - Dashboards. Geopraphical Tools folium - Plot geographical maps using the Leaflet.js library, jupyter plugin . gmaps - Google Maps for Jupyter notebooks. stadiamaps - Plot geographical maps. datashader - Draw millions of points on a map. sklearn - BallTree, Example . pynndescent - Nearest neighbor descent for approximate nearest neighbors. geocoder - Geocoding of addresses, IP addresses. Conversion of different geo formats: talk , repo geopandas - Tools for geographic data Low Level Geospatial Tools (GEOS, GDAL/OGR, PROJ.4) Vector Data (Shapely, Fiona, Pyproj) Raster Data (Rasterio) Plotting (Descartes, Catropy) Predict economic indicators from Open Street Map ipynb . PySal - Python Spatial Analysis Library. geography - Extract countries, regions and cities from a URL or text. cartogram - Distorted maps based on population. Recommender Systems Examples: 1 , 2 , 2-ipynb , 3 . surprise - Recommender, talk . turicreate - Recommender. implicit - Fast Collaborative Filtering for Implicit Feedback Datasets. spotlight - Deep recommender models using PyTorch. lightfm - Recommendation algorithms for both implicit and explicit feedback. funk-svd - Fast SVD. pywFM - Factorization. Decision Tree Models Intro to Decision Trees and Random Forests , Intro to Gradient Boosting lightgbm - Gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, doc . xgboost - Gradient boosting (GBDT, GBRT or GBM) library, doc , Methods for CIs: link1 , link2 . catboost - Gradient boosting. thundergbm - GBDTs and Random Forest. h2o - Gradient boosting. forestci - Confidence intervals for random forests. scikit-garden - Quantile Regression. grf - Generalized random forest. dtreeviz - Decision tree visualization and model interpretation. Nuance - Decision tree visualization. rfpimp - Feature Importance for RandomForests using Permuation Importance. Why the default feature importance for random forests is wrong: link treeinterpreter - Interpreting scikit-learn's decision tree and random forest predictions. bartpy - Bayesian Additive Regression Trees. infiniteboost - Combination of RFs and GBDTs. merf - Mixed Effects Random Forest for Clustering, video rrcf - Robust Random Cut Forest algorithm for anomaly detection on streams. Natural Language Processing (NLP) / Text Processing talk - nb , nb2 , talk . Text classification Intro , Preprocessing blog post . gensim - NLP, doc2vec, word2vec, text processing, topic modelling (LSA, LDA), Example , Coherence Model for evaluation. Embeddings - GloVe ([ 1 ], [ 2 ]), StarSpace , wikipedia2vec , visualization . magnitude - Vector embedding utility package. pyldavis - Visualization for topic modelling. spaCy - NLP. NTLK - NLP, helpful KMeansClusterer with cosine_distance . pytext - NLP from Facebook. fastText - Efficient text classification and representation learning. annoy - Approximate nearest neighbor search. faiss - Approximate nearest neighbor search. pysparnn - Approximate nearest neighbor search. infomap - Cluster (word-)vectors to find topics, example . datasketch - Probabilistic data structures for large data (MinHash, HyperLogLog). flair - NLP Framework by Zalando. stanfordnlp - NLP Library. Chatistics - Turn Messenger, Hangouts, WhatsApp and Telegram chat logs into DataFrames. Papers Search Engine Correlation Biology Sequencing scanpy - Analyze single-cell gene expression data, tutorial . Image-related mahotas - Image processing (Bioinformatics), example . imagepy - Software package for bioimage analysis. CellProfiler - Biological image analysis. imglyb - Viewer for large images, talk , slides . microscopium - Unsupervised clustering of images + viewer, talk . cytokit - Analyzing properties of cells in fluorescent microscopy datasets. Image Processing Talk cv2 - OpenCV, classical algorithms: Gaussian Filter , Morphological Transformations . scikit-image - Image processing. Neural Networks Tutorials Viewer Convolutional Neural Networks for Visual Recognition fast.ai course - Lessons 1-7 , Lessons 8-14 Tensorflow without a PhD - Neural Network course by Google. Feature Visualization: Blog , PPT Tensorflow Playground Visualization of optimization algorithms , Another visualization cutouts-explorer - Image Viewer. Image Related imgaug - More sophisticated image preprocessing. Augmentor - Image augmentation library. keras preprocessing - Preprocess images. albumentations - Wrapper around imgaug and other libraries. augmix - Image augmentation from Google. kornia - Image augmentation, feature extraction and loss functions. Lossfunction Related SegLoss - List of loss functions for medical image segmentation. Text Related ktext - Utilities for pre-processing text for deep learning in Keras. textgenrnn - Ready-to-use LSTM for text generation. ctrl - Text generation. Libs keras - Neural Networks on top of tensorflow , examples . keras-contrib - Keras community contributions. keras-tuner - Hyperparameter tuning for Keras. hyperas - Keras + Hyperopt: Convenient hyperparameter optimization wrapper. elephas - Distributed Deep learning with Keras Spark. tflearn - Neural Networks on top of tensorflow. tensorlayer - Neural Networks on top of tensorflow, tricks . tensorforce - Tensorflow for applied reinforcement learning. fastai - Neural Networks in pytorch. pytorch-optimizer - Collection of optimizers for pytorch. ignite - Highlevel library for pytorch. skorch - Scikit-learn compatible neural network library that wraps pytorch, talk , slides . autokeras - AutoML for deep learning. PlotNeuralNet - Plot neural networks. lucid - Neural network interpretability, Activation Maps . tcav - Interpretability method. AdaBound - Optimizer that trains as fast as Adam and as good as SGD, alt . foolbox - Adversarial examples that fool neural networks. hiddenlayer - Training metrics. imgclsmob - Pretrained models. netron - Visualizer for deep learning and machine learning models. torchcv - Deep Learning in Computer Vision. Training-related livelossplot - Live training loss plot in Jupyter Notebook. Object detection / Instance Segmentation yolact - Fully convolutional model for real-time instance segmentation. EfficientDet Pytorch , EfficientDet Keras - Scalable and Efficient Object Detection. detectron2 - Object Detection (Mask R-CNN) by Facebook. simpledet - Object Detection and Instance Recognition. CenterNet - Object detection. FCOS - Fully Convolutional One-Stage Object Detection. Image Classification efficientnet - Promising neural network architecture. Applications and Snippets CycleGAN and Pix2pix - Various image-to-image tasks. SPADE - Semantic Image Synthesis. Entity Embeddings of Categorical Variables , code , kaggle Image Super-Resolution - Super-scaling using a Residual Dense Network. Cell Segmentation - Talk , Blog Posts: 1 , 2 deeplearning-models - Deep learning models. GPU cuML - Run traditional tabular ML tasks on GPUs. thundergbm - GBDTs and Random Forest. thundersvm - Support Vector Machines. Regression Understanding SVM Regression: slides , forum , paper pyearth - Multivariate Adaptive Regression Splines (MARS), tutorial . pygam - Generalized Additive Models (GAMs), Explanation . GLRM - Generalized Low Rank Models. tweedie - Specialized distribution for zero inflated targets, Talk . Classification Talk , Notebook Blog post: Probability Scoring All classification metrics DESlib - Dynamic classifier and ensemble selection Clustering Overview of clustering algorithms applied image data (= Deep Clustering) pyclustering - All sorts of clustering algorithms. GaussianMixture - Generalized k-means clustering using a mixture of Gaussian distributions, video . somoclu - Self-organizing map. hdbscan - Clustering algorithm, talk . nmslib - Similarity search library and toolkit for evaluation of k-NN methods. buckshotpp - Outlier-resistant and scalable clustering algorithm. merf - Mixed Effects Random Forest for Clustering, video tree-SNE - Hierarchical clustering algorithm based on t-SNE. Interpretable Classifiers and Regressors skope-rules - Interpretable classifier, IF-THEN rules. sklearn-expertsys - Interpretable classifiers, Bayesian Rule List classifier. Multi-label classification scikit-multilearn - Multi-label classification, talk . Signal Processing and Filtering Stanford Lecture Series on Fourier Transformation , Youtube , Lecture Notes . The Scientist Engineer's Guide to Digital Signal Processing (1999) . Kalman Filter book - Focuses on intuition using Jupyter Notebooks. Includes Baysian and various Kalman filters. Interactive Tool for FIR and IIR filters, Examples . filterpy - Kalman filtering and optimal estimation library. Time Series statsmodels - Time series analysis, seasonal decompose example , SARIMA , granger causality . pyramid , pmdarima - Wrapper for (Auto-) ARIMA. pyflux - Time series prediction algorithms (ARIMA, GARCH, GAS, Bayesian). prophet - Time series prediction library. atspy - Automated Time Series Models. pm-prophet - Time series prediction and decomposition library. htsprophet - Hierarchical Time Series Forecasting using Prophet. nupic - Hierarchical Temporal Memory (HTM) for Time Series Prediction and Anomaly Detection. tensorflow - LSTM and others, examples: link , link , link , Explain LSTM , seq2seq: 1 , 2 , 3 , 4 tspreprocess - Preprocessing: Denoising, Compression, Resampling. tsfresh - Time series feature engineering. thunder - Data structures and algorithms for loading, processing, and analyzing time series data. gatspy - General tools for Astronomical Time Series, talk . gendis - shapelets, example . tslearn - Time series clustering and classification, TimeSeriesKMeans , TimeSeriesKMeans . pastas - Simulation of time series. fastdtw - Dynamic Time Warp Distance. fable - Time Series Forecasting (R package). CausalImpact - Causal Impact Analysis ( R package ). pydlm - Bayesian time series modeling ( R package , Blog post ) PyAF - Automatic Time Series Forecasting. luminol - Anomaly Detection and Correlation library from Linkedin. matrixprofile-ts - Detecting patterns and anomalies, website , ppt , alternative . stumpy - Another matrix profile library. obspy - Seismology package. Useful classic_sta_lta function. RobustSTL - Robust Seasonal-Trend Decomposition. seglearn - Time Series library. pyts - Time series transformation and classification, Imaging time series . Turn time series into images and use Neural Nets: example , example . sktime , sktime-dl - Toolbox for (deep) learning with time series. adtk - Time Series Anomaly Detection. Time Series Evaluation TimeSeriesSplit - Sklearn time series split. tscv - Evaluation with gap. Financial Data pandas-datareader - Read stock data. ffn - Financial functions. bt - Backtesting algorithms. alpaca-trade-api-python - Commission-free trading through API. The Quantopian Stack (some features may require signup on their platform): pyfolio - Portfolio and risk analytics. zipline - Algorithmic trading. alphalens - Performance analysis of predictive stock factors. empyrical - Financial risk metrics. Survival Analysis Time-dependent Cox Model in R . lifelines - Survival analysis, Cox PH Regression, talk , talk2 . scikit-survival - Survival analysis. xgboost - \"objective\": \"survival:cox\" NHANES example survivalstan - Survival analysis, intro . convoys - Analyze time lagged conversions. RandomSurvivalForests (R packages: randomForestSRC, ggRandomForests). Outlier Detection Anomaly Detection sklearn - Isolation Forest and others. pyod - Outlier Detection / Anomaly Detection. eif - Extended Isolation Forest. AnomalyDetection - Anomaly detection (R package). luminol - Anomaly Detection and Correlation library from Linkedin. Distances for comparing histograms and detecting outliers - Talk : Kolmogorov-Smirnov , Wasserstein , Energy Distance (Cramer) , Kullback-Leibler divergence . banpei - Anomaly detection library based on singular spectrum transformation. telemanom - Detect anomalies in multivariate time series data using LSTMs. Ranking lightning - Large-scale linear classification, regression and ranking. Scoring SLIM - Scoring systems for classification, Supersparse linear integer models. Probabilistic Modeling and Bayes Intro , Guide PyMC3 - Baysian modelling, intro pomegranate - Probabilistic modelling, talk . pmlearn - Probabilistic machine learning. arviz - Exploratory analysis of Bayesian models. zhusuan - Bayesian deep learning, generative models. dowhy - Estimate causal effects. edward - Probabilistic modeling, inference, and criticism, Mixture Density Networks (MNDs) , MDN Explanation . Pyro - Deep Universal Probabilistic Programming. tensorflow probability - Deep learning and probabilistic modelling, talk , example . bambi - High-level Bayesian model-building interface on top of PyMC3. neural-tangents - Infinite Neural Networks. Stacking Models and Ensembles Model Stacking Blog Post mlxtend - EnsembleVoteClassifier , StackingRegressor , StackingCVRegressor for model stacking. vecstack - Stacking ML models. StackNet - Stacking ML models. mlens - Ensemble learning. Model Evaluation pycm - Multi-class confusion matrix. pandas_ml - Confusion matrix. Plotting learning curve: link . yellowbrick - Learning curve. Model Explanation, Interpretability, Feature Importance Book , Examples shap - Explain predictions of machine learning models, talk . treeinterpreter - Interpreting scikit-learn's decision tree and random forest predictions. lime - Explaining the predictions of any machine learning classifier, talk , Warning (Myth 7) . lime_xgboost - Create LIMEs for XGBoost. eli5 - Inspecting machine learning classifiers and explaining their predictions. lofo-importance - Leave One Feature Out Importance, talk , examples: 1 , 2 , 3 . pybreakdown - Generate feature contribution plots. FairML - Model explanation, feature importance. pycebox - Individual Conditional Expectation Plot Toolbox. pdpbox - Partial dependence plot toolbox, example . partial_dependence - Visualize and cluster partial dependence. skater - Unified framework to enable model interpretation. anchor - High-Precision Model-Agnostic Explanations for classifiers. l2x - Instancewise feature selection as methodology for model interpretation. contrastive_explanation - Contrastive explanations. DrWhy - Collection of tools for explainable AI. lucid - Neural network interpretability. xai - An eXplainability toolbox for machine learning. innvestigate - A toolbox to investigate neural network predictions. dalex - Explanations for ML models (R package). interpret - Fit interpretable models, explain models (Microsoft). causalml - Causal inference by Uber. Automated Machine Learning AdaNet - Automated machine learning based on tensorflow. tpot - Automated machine learning tool, optimizes machine learning pipelines. auto_ml - Automated machine learning for analytics production. autokeras - AutoML for deep learning. nni - Toolkit for neural architecture search and hyper-parameter tuning by Microsoft. automl-gs - Automated machine learning. mljar - Automated machine learning. automl_zero - Automatically discover computer programs that can solve machine learning tasks from Google. Graph Representation Learning Karate Club - Unsupervised learning on graphs. Pytorch Geometric - Graph representation learning with PyTorch. DLG - Graph representation learning with TensorFlow. Evolutionary Algorithms Optimization deap - Evolutionary computation framework (Genetic Algorithm, Evolution strategies). evol - DSL for composable evolutionary algorithms, talk . platypus - Multiobjective optimization. autograd - Efficiently computes derivatives of numpy code. nevergrad - Derivation-free optimization. gplearn - Sklearn-like interface for genetic programming. blackbox - Optimization of expensive black-box functions. Optometrist algorithm - paper . DeepSwarm - Neural architecture search. Hyperparameter Tuning sklearn - GridSearchCV , RandomizedSearchCV . sklearn-deap - Hyperparameter search using genetic algorithms. hyperopt - Hyperparameter optimization. hyperopt-sklearn - Hyperopt + sklearn. optuna - Hyperparamter optimization, Talk . skopt - BayesSearchCV for Hyperparameter search. tune - Hyperparameter search with a focus on deep learning and deep reinforcement learning. hypergraph - Global optimization methods and hyperparameter optimization. bbopt - Black box hyperparameter optimization. dragonfly - Scalable Bayesian optimisation. Incremental Learning, Online Learning sklearn - PassiveAggressiveClassifier , PassiveAggressiveRegressor . creme-ml - Incremental learning framework, talk . Kaggler - Online Learning algorithms. Active Learning Talk modAL - Active learning framework. Reinforcement Learning YouTube , YouTube Intro to Monte Carlo Tree Search (MCTS) - 1 , 2 , 3 AlphaZero methodology - 1 , 2 , 3 , Cheat Sheet RLLib - Library for reinforcement learning. Horizon - Facebook RL framework. Deployment and Lifecycle Management Dependency Management pipreqs - Generate a requirements.txt from import statements. dephell - Dependency management. poetry - Dependency management. pyup - Dependency management. pypi-timemachine - Install packages with pip as if you were in the past. Data Versioning and Pipelines dvc - Version control for large files. hangar - Version control for tensor data. kedro - Build data pipelines. Data Science Related m2cgen - Transpile trained ML models into other languages. sklearn-porter - Transpile trained scikit-learn estimators to C, Java, JavaScript and others. mlflow - Manage the machine learning lifecycle, including experimentation, reproducibility and deployment. modelchimp - Experiment Tracking. skll - Command-line utilities to make it easier to run machine learning experiments. BentoML - Package and deploy machine learning models for serving in production. dagster - Tool with focus on dependency graphs. knockknock - Be notified when your training ends. metaflow - Lifecycle Management Tool by Netflix. cortex - Deploy machine learning models. Math and Background All kinds of math and statistics resources Gilbert Strang - Linear Algebra Gilbert Strang - Matrix Methods in Data Analysis, Signal Processing, and Machine Learning Other daft - Render probabilistic graphical models using matplotlib. unyt - Working with units. scrapy - Web scraping library. VowpalWabbit - ML Toolkit from Microsoft. metric-learn - Metric learning. General Python Programming more_itertools - Extension of itertools. funcy - Fancy and practical functional tools. dateparser - A better date parser. jellyfish - Approximate string matching. coloredlogs - Colored logging output. Resources Distill.pub - Blog. Machine Learning Videos Data Science Notebooks Recommender Systems (Microsoft) The GAN Zoo - List of Generative Adversarial Networks Datascience Cheatsheets Other Awesome Lists Awesome Adversarial Machine Learning Awesome AI Booksmarks Awesome AI on Kubernetes Awesome Big Data Awesome Business Machine Learning Awesome Causality Awesome Community Detection Awesome CSV Awesome Data Science with Ruby Awesome Dash Awesome Decision Trees Awesome Deep Learning Awesome ETL Awesome Financial Machine Learning Awesome Fraud Detection Awesome GAN Applications Awesome Graph Classification Awesome Gradient Boosting Awesome Machine Learning Awesome Machine Learning Interpretability Awesome Machine Learning Operations Awesome Monte Carlo Tree Search Awesome Online Machine Learning Awesome Python Awesome Python Data Science Awesome Python Data Science Awesome Python Data Science Awesome Pytorch Awesome Recommender Systems Awesome Semantic Segmentation Awesome Sentence Embedding Awesome Time Series Awesome Time Series Anomaly Detection Things I google a lot Frequency codes for time series Date parsing codes Feature Calculators tsfresh Contributing Do you know a package that should be on this list? Did you spot a package that is no longer maintained and should be removed from this list? Then feel free to read the contribution guidelines and submit your pull request or create a new issue. License","title":"Awesome Data Science with Python"},{"location":"data_science/#awesome-data-science-with-python","text":"A curated list of awesome resources for practicing data science using Python, including not only libraries, but also links to tutorials, code snippets, blog posts and talks.","title":"Awesome Data Science with Python"},{"location":"data_science/#core","text":"pandas - Data structures built on top of numpy . scikit-learn - Core ML library. matplotlib - Plotting library. seaborn - Data visualization library based on matplotlib. pandas_summary - Basic statistics using DataFrameSummary(df).summary() . pandas_profiling - Descriptive statistics using ProfileReport . sklearn_pandas - Helpful DataFrameMapper class. missingno - Missing data visualization. rainbow-csv - Plugin to display .csv files with nice colors.","title":"Core"},{"location":"data_science/#environment-and-jupyter","text":"General Jupyter Tricks Fixing environment: link Python debugger (pdb) - blog post , video , cheatsheet cookiecutter-data-science - Project template for data science projects. nteract - Open Jupyter Notebooks with doubleclick. papermill - Parameterize and execute Jupyter notebooks, tutorial . nbdime - Diff two notebook files, Alternative GitHub App: ReviewNB . RISE - Turn Jupyter notebooks into presentations. qgrid - Pandas DataFrame sorting. pivottablejs - Drag n drop Pivot Tables and Charts for jupyter notebooks. itables - Interactive tables in Jupyter. jupyter-datatables - Interactive tables in Jupyter. debugger - Visual debugger for Jupyter.","title":"Environment and Jupyter"},{"location":"data_science/#pandas-tricks-alternatives-and-additions","text":"Pandas Tricks Using df.pipe() (video) pandasvault - Large collection of pandas tricks. modin - Parallelization library for faster pandas DataFrame . vaex - Out-of-Core DataFrames. pandarallel - Parallelize pandas operations. xarray - Extends pandas to n-dimensional arrays. swifter - Apply any function to a pandas dataframe faster. pandas_flavor - Write custom accessors like .str and .dt . pandas-log - Find business logic issues and performance issues in pandas. pandapy - Additional features for pandas.","title":"Pandas Tricks, Alternatives and Additions"},{"location":"data_science/#helpful","text":"tqdm - Progress bars for for-loops. icecream - Simple debugging output. loguru - Python logging. pyprojroot - Helpful here() command from R. intake - Loading datasets made easier, talk .","title":"Helpful"},{"location":"data_science/#extraction","text":"textract - Extract text from any document. camelot - Extract text from PDF.","title":"Extraction"},{"location":"data_science/#big-data","text":"spark - DataFrame for big data, cheatsheet , tutorial . sparkit-learn , spark-deep-learning - ML frameworks for spark. koalas - Pandas API on Apache Spark. dask , dask-ml - Pandas DataFrame for big data and machine learning library, resources , talk1 , talk2 , notebooks , videos . dask-gateway - Managing dask clusters. turicreate - Helpful SFrame class for out-of-memory dataframes. h2o - Helpful H2OFrame class for out-of-memory dataframes. datatable - Data Table for big data support. cuDF - GPU DataFrame Library. ray - Flexible, high-performance distributed execution framework. mars - Tensor-based unified framework for large-scale data computation. bottleneck - Fast NumPy array functions written in C. bolz - A columnar data container that can be compressed. cupy - NumPy-like API accelerated with CUDA. petastorm - Data access library for parquet files by Uber. zappy - Distributed numpy arrays.","title":"Big Data"},{"location":"data_science/#command-line-tools-csv","text":"ni - Command line tool for big data. xsv - Command line tool for indexing, slicing, analyzing, splitting and joining CSV files. csvkit - Another command line tool for CSV files. csvsort - Sort large csv files. tsv-utils - Tools for working with CSV files by ebay. cheat - Make cheatsheets for command line commands.","title":"Command line tools, CSV"},{"location":"data_science/#classical-statistics","text":"","title":"Classical Statistics"},{"location":"data_science/#texts","text":"Greenland - Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations Lindel\u00f8v - Common statistical tests are linear models Chatruc - The Central Limit Theorem and its misuse Al-Saleh - Properties of the Standard Deviation that are Rarely Mentioned in Classrooms Wainer - The Most Dangerous Equation Gigerenzer - The Bias Bias in Behavioral Economics Cook - Estimating the chances of something that hasn\u2019t happened yet","title":"Texts"},{"location":"data_science/#statistical-tests-and-packages","text":"researchpy - Helpful summary_cont() function for summary statistics (Table 1). scikit-posthocs - Statistical post-hoc tests for pairwise multiple comparisons. Bland-Altman Plot - Plot for agreement between two methods of measurement. scipy.stats - Statistical tests. ANOVA , Tutorials: One-way , Two-way , Type 1,2,3 explained .","title":"Statistical Tests and Packages"},{"location":"data_science/#visualizations","text":"Null Hypothesis Significance Testing (NHST) and Sample Size Calculation Correlation Cohen's d Confidence Interval Equivalence, non-inferiority and superiority testing Bayesian two-sample t test Distribution of p-values when comparing two groups Understanding the t-distribution and its normal approximation","title":"Visualizations"},{"location":"data_science/#talks","text":"Inverse Propensity Weighting Dealing with Selection Bias By Propensity Based Feature Selection","title":"Talks"},{"location":"data_science/#frameworks","text":"scikit-learn - General machine learning framework. h2o - Machine learning framework. caffe - Deep learning framework, pretrained models . mxnet - Deep learning framework, book .","title":"Frameworks"},{"location":"data_science/#exploration-and-cleaning","text":"Checklist . janitor - Clean messy column names. impyute - Imputations. fancyimpute - Matrix completion and imputation algorithms. imbalanced-learn - Resampling for imbalanced datasets. tspreprocess - Time series preprocessing: Denoising, Compression, Resampling. Kaggler - Utility functions ( OneHotEncoder(min_obs=100) ) pyupset - Visualizing intersecting sets. pyemd - Earth Mover's Distance, similarity between histograms.","title":"Exploration and Cleaning"},{"location":"data_science/#train-test-split","text":"iterative-stratification - Stratification of multilabel data.","title":"Train / Test Split"},{"location":"data_science/#feature-engineering","text":"Talk sklearn - Pipeline, examples . pdpipe - Pipelines for DataFrames. scikit-lego - Custom transformers for pipelines. few - Feature engineering wrapper for sklearn. skoot - Pipeline helper functions. categorical-encoding - Categorical encoding of variables, vtreat (R package) . dirty_cat - Encoding dirty categorical variables. patsy - R-like syntax for statistical models. mlxtend - LDA. featuretools - Automated feature engineering, example . tsfresh - Time series feature engineering. pypeln - Concurrent data pipelines. feature_engine - Encoders, transformers, etc.","title":"Feature Engineering"},{"location":"data_science/#feature-selection","text":"Talk Blog post series - 1 , 2 , 3 , 4 Tutorials - 1 , 2 sklearn - Feature selection. eli5 - Feature selection using permutation importance. scikit-feature - Feature selection algorithms. stability-selection - Stability selection. scikit-rebate - Relief-based feature selection algorithms. scikit-genetic - Genetic feature selection. boruta_py - Feature selection, explaination , example . linselect - Feature selection package. mlxtend - Exhaustive feature selection. BoostARoota - Xgboost feature selection algorithm.","title":"Feature Selection"},{"location":"data_science/#dimensionality-reduction","text":"Talk prince - Dimensionality reduction, factor analysis (PCA, MCA, CA, FAMD). sklearn - Multidimensional scaling (MDS). sklearn - t-distributed Stochastic Neighbor Embedding (t-SNE), intro . Faster implementations: lvdmaaten , MulticoreTSNE . FIt-SNE - Fast Fourier Transform-accelerated Interpolation-based t-SNE. umap - Uniform Manifold Approximation and Projection, talk , explorer , explanation . sleepwalk - Explore embeddings, interactive visualization (R package). scikit-tda - Topological Data Analysis, paper , talk , talk . mdr - Dimensionality reduction, multifactor dimensionality reduction (MDR). sklearn - Truncated SVD (aka LSA). ivis - Dimensionality reduction using Siamese Networks. trimap - Dimensionality reduction using triplets.","title":"Dimensionality Reduction"},{"location":"data_science/#visualization","text":"All charts , Austrian monuments . cufflinks - Dynamic visualization library, wrapper for plotly , medium , example . physt - Better histograms, talk , notebook . matplotlib_venn - Venn diagrams, alternative . joypy - Draw stacked density plots. mosaic plots - Categorical variable visualization, example . scikit-plot - ROC curves and other visualizations for ML models. yellowbrick - Visualizations for ML models (similar to scikit-plot). bokeh - Interactive visualization library, Examples , Examples . animatplot - Animate plots build on matplotlib. plotnine - ggplot for Python. altair - Declarative statistical visualization library. bqplot - Plotting library for IPython/Jupyter Notebooks. hvplot - High-level plotting library built on top of holoviews . dtreeviz - Decision tree visualization and model interpretation. chartify - Generate charts. VivaGraphJS - Graph visualization (JS package). pm - Navigatable 3D graph visualization (JS package), example . python-ternary - Triangle plots. falcon - Interactive visualizations for big data.","title":"Visualization"},{"location":"data_science/#dashboards","text":"dash - Dashboarding solution by plot.ly. Tutorial: 1 , 2 , 3 , 4 , 5 , resources panel - Dashboarding solution. bokeh - Dashboarding solution. visdom - Dashboarding library by facebook. altair example - Video . voila - Turn Jupyter notebooks into standalone web applications. streamlit - Dashboards.","title":"Dashboards"},{"location":"data_science/#geopraphical-tools","text":"folium - Plot geographical maps using the Leaflet.js library, jupyter plugin . gmaps - Google Maps for Jupyter notebooks. stadiamaps - Plot geographical maps. datashader - Draw millions of points on a map. sklearn - BallTree, Example . pynndescent - Nearest neighbor descent for approximate nearest neighbors. geocoder - Geocoding of addresses, IP addresses. Conversion of different geo formats: talk , repo geopandas - Tools for geographic data Low Level Geospatial Tools (GEOS, GDAL/OGR, PROJ.4) Vector Data (Shapely, Fiona, Pyproj) Raster Data (Rasterio) Plotting (Descartes, Catropy) Predict economic indicators from Open Street Map ipynb . PySal - Python Spatial Analysis Library. geography - Extract countries, regions and cities from a URL or text. cartogram - Distorted maps based on population.","title":"Geopraphical Tools"},{"location":"data_science/#recommender-systems","text":"Examples: 1 , 2 , 2-ipynb , 3 . surprise - Recommender, talk . turicreate - Recommender. implicit - Fast Collaborative Filtering for Implicit Feedback Datasets. spotlight - Deep recommender models using PyTorch. lightfm - Recommendation algorithms for both implicit and explicit feedback. funk-svd - Fast SVD. pywFM - Factorization.","title":"Recommender Systems"},{"location":"data_science/#decision-tree-models","text":"Intro to Decision Trees and Random Forests , Intro to Gradient Boosting lightgbm - Gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, doc . xgboost - Gradient boosting (GBDT, GBRT or GBM) library, doc , Methods for CIs: link1 , link2 . catboost - Gradient boosting. thundergbm - GBDTs and Random Forest. h2o - Gradient boosting. forestci - Confidence intervals for random forests. scikit-garden - Quantile Regression. grf - Generalized random forest. dtreeviz - Decision tree visualization and model interpretation. Nuance - Decision tree visualization. rfpimp - Feature Importance for RandomForests using Permuation Importance. Why the default feature importance for random forests is wrong: link treeinterpreter - Interpreting scikit-learn's decision tree and random forest predictions. bartpy - Bayesian Additive Regression Trees. infiniteboost - Combination of RFs and GBDTs. merf - Mixed Effects Random Forest for Clustering, video rrcf - Robust Random Cut Forest algorithm for anomaly detection on streams.","title":"Decision Tree Models"},{"location":"data_science/#natural-language-processing-nlp-text-processing","text":"talk - nb , nb2 , talk . Text classification Intro , Preprocessing blog post . gensim - NLP, doc2vec, word2vec, text processing, topic modelling (LSA, LDA), Example , Coherence Model for evaluation. Embeddings - GloVe ([ 1 ], [ 2 ]), StarSpace , wikipedia2vec , visualization . magnitude - Vector embedding utility package. pyldavis - Visualization for topic modelling. spaCy - NLP. NTLK - NLP, helpful KMeansClusterer with cosine_distance . pytext - NLP from Facebook. fastText - Efficient text classification and representation learning. annoy - Approximate nearest neighbor search. faiss - Approximate nearest neighbor search. pysparnn - Approximate nearest neighbor search. infomap - Cluster (word-)vectors to find topics, example . datasketch - Probabilistic data structures for large data (MinHash, HyperLogLog). flair - NLP Framework by Zalando. stanfordnlp - NLP Library. Chatistics - Turn Messenger, Hangouts, WhatsApp and Telegram chat logs into DataFrames.","title":"Natural Language Processing (NLP) / Text Processing"},{"location":"data_science/#papers","text":"Search Engine Correlation","title":"Papers"},{"location":"data_science/#biology","text":"","title":"Biology"},{"location":"data_science/#sequencing","text":"scanpy - Analyze single-cell gene expression data, tutorial .","title":"Sequencing"},{"location":"data_science/#image-related","text":"mahotas - Image processing (Bioinformatics), example . imagepy - Software package for bioimage analysis. CellProfiler - Biological image analysis. imglyb - Viewer for large images, talk , slides . microscopium - Unsupervised clustering of images + viewer, talk . cytokit - Analyzing properties of cells in fluorescent microscopy datasets.","title":"Image-related"},{"location":"data_science/#image-processing","text":"Talk cv2 - OpenCV, classical algorithms: Gaussian Filter , Morphological Transformations . scikit-image - Image processing.","title":"Image Processing"},{"location":"data_science/#neural-networks","text":"","title":"Neural Networks"},{"location":"data_science/#tutorials-viewer","text":"Convolutional Neural Networks for Visual Recognition fast.ai course - Lessons 1-7 , Lessons 8-14 Tensorflow without a PhD - Neural Network course by Google. Feature Visualization: Blog , PPT Tensorflow Playground Visualization of optimization algorithms , Another visualization cutouts-explorer - Image Viewer.","title":"Tutorials &amp; Viewer"},{"location":"data_science/#image-related_1","text":"imgaug - More sophisticated image preprocessing. Augmentor - Image augmentation library. keras preprocessing - Preprocess images. albumentations - Wrapper around imgaug and other libraries. augmix - Image augmentation from Google. kornia - Image augmentation, feature extraction and loss functions.","title":"Image Related"},{"location":"data_science/#lossfunction-related","text":"SegLoss - List of loss functions for medical image segmentation.","title":"Lossfunction Related"},{"location":"data_science/#text-related","text":"ktext - Utilities for pre-processing text for deep learning in Keras. textgenrnn - Ready-to-use LSTM for text generation. ctrl - Text generation.","title":"Text Related"},{"location":"data_science/#libs","text":"keras - Neural Networks on top of tensorflow , examples . keras-contrib - Keras community contributions. keras-tuner - Hyperparameter tuning for Keras. hyperas - Keras + Hyperopt: Convenient hyperparameter optimization wrapper. elephas - Distributed Deep learning with Keras Spark. tflearn - Neural Networks on top of tensorflow. tensorlayer - Neural Networks on top of tensorflow, tricks . tensorforce - Tensorflow for applied reinforcement learning. fastai - Neural Networks in pytorch. pytorch-optimizer - Collection of optimizers for pytorch. ignite - Highlevel library for pytorch. skorch - Scikit-learn compatible neural network library that wraps pytorch, talk , slides . autokeras - AutoML for deep learning. PlotNeuralNet - Plot neural networks. lucid - Neural network interpretability, Activation Maps . tcav - Interpretability method. AdaBound - Optimizer that trains as fast as Adam and as good as SGD, alt . foolbox - Adversarial examples that fool neural networks. hiddenlayer - Training metrics. imgclsmob - Pretrained models. netron - Visualizer for deep learning and machine learning models. torchcv - Deep Learning in Computer Vision.","title":"Libs"},{"location":"data_science/#training-related","text":"livelossplot - Live training loss plot in Jupyter Notebook.","title":"Training-related"},{"location":"data_science/#object-detection-instance-segmentation","text":"yolact - Fully convolutional model for real-time instance segmentation. EfficientDet Pytorch , EfficientDet Keras - Scalable and Efficient Object Detection. detectron2 - Object Detection (Mask R-CNN) by Facebook. simpledet - Object Detection and Instance Recognition. CenterNet - Object detection. FCOS - Fully Convolutional One-Stage Object Detection.","title":"Object detection / Instance Segmentation"},{"location":"data_science/#image-classification","text":"efficientnet - Promising neural network architecture.","title":"Image Classification"},{"location":"data_science/#applications-and-snippets","text":"CycleGAN and Pix2pix - Various image-to-image tasks. SPADE - Semantic Image Synthesis. Entity Embeddings of Categorical Variables , code , kaggle Image Super-Resolution - Super-scaling using a Residual Dense Network. Cell Segmentation - Talk , Blog Posts: 1 , 2 deeplearning-models - Deep learning models.","title":"Applications and Snippets"},{"location":"data_science/#gpu","text":"cuML - Run traditional tabular ML tasks on GPUs. thundergbm - GBDTs and Random Forest. thundersvm - Support Vector Machines.","title":"GPU"},{"location":"data_science/#regression","text":"Understanding SVM Regression: slides , forum , paper pyearth - Multivariate Adaptive Regression Splines (MARS), tutorial . pygam - Generalized Additive Models (GAMs), Explanation . GLRM - Generalized Low Rank Models. tweedie - Specialized distribution for zero inflated targets, Talk .","title":"Regression"},{"location":"data_science/#classification","text":"Talk , Notebook Blog post: Probability Scoring All classification metrics DESlib - Dynamic classifier and ensemble selection","title":"Classification"},{"location":"data_science/#clustering","text":"Overview of clustering algorithms applied image data (= Deep Clustering) pyclustering - All sorts of clustering algorithms. GaussianMixture - Generalized k-means clustering using a mixture of Gaussian distributions, video . somoclu - Self-organizing map. hdbscan - Clustering algorithm, talk . nmslib - Similarity search library and toolkit for evaluation of k-NN methods. buckshotpp - Outlier-resistant and scalable clustering algorithm. merf - Mixed Effects Random Forest for Clustering, video tree-SNE - Hierarchical clustering algorithm based on t-SNE.","title":"Clustering"},{"location":"data_science/#interpretable-classifiers-and-regressors","text":"skope-rules - Interpretable classifier, IF-THEN rules. sklearn-expertsys - Interpretable classifiers, Bayesian Rule List classifier.","title":"Interpretable Classifiers and Regressors"},{"location":"data_science/#multi-label-classification","text":"scikit-multilearn - Multi-label classification, talk .","title":"Multi-label classification"},{"location":"data_science/#signal-processing-and-filtering","text":"Stanford Lecture Series on Fourier Transformation , Youtube , Lecture Notes . The Scientist Engineer's Guide to Digital Signal Processing (1999) . Kalman Filter book - Focuses on intuition using Jupyter Notebooks. Includes Baysian and various Kalman filters. Interactive Tool for FIR and IIR filters, Examples . filterpy - Kalman filtering and optimal estimation library.","title":"Signal Processing and Filtering"},{"location":"data_science/#time-series","text":"statsmodels - Time series analysis, seasonal decompose example , SARIMA , granger causality . pyramid , pmdarima - Wrapper for (Auto-) ARIMA. pyflux - Time series prediction algorithms (ARIMA, GARCH, GAS, Bayesian). prophet - Time series prediction library. atspy - Automated Time Series Models. pm-prophet - Time series prediction and decomposition library. htsprophet - Hierarchical Time Series Forecasting using Prophet. nupic - Hierarchical Temporal Memory (HTM) for Time Series Prediction and Anomaly Detection. tensorflow - LSTM and others, examples: link , link , link , Explain LSTM , seq2seq: 1 , 2 , 3 , 4 tspreprocess - Preprocessing: Denoising, Compression, Resampling. tsfresh - Time series feature engineering. thunder - Data structures and algorithms for loading, processing, and analyzing time series data. gatspy - General tools for Astronomical Time Series, talk . gendis - shapelets, example . tslearn - Time series clustering and classification, TimeSeriesKMeans , TimeSeriesKMeans . pastas - Simulation of time series. fastdtw - Dynamic Time Warp Distance. fable - Time Series Forecasting (R package). CausalImpact - Causal Impact Analysis ( R package ). pydlm - Bayesian time series modeling ( R package , Blog post ) PyAF - Automatic Time Series Forecasting. luminol - Anomaly Detection and Correlation library from Linkedin. matrixprofile-ts - Detecting patterns and anomalies, website , ppt , alternative . stumpy - Another matrix profile library. obspy - Seismology package. Useful classic_sta_lta function. RobustSTL - Robust Seasonal-Trend Decomposition. seglearn - Time Series library. pyts - Time series transformation and classification, Imaging time series . Turn time series into images and use Neural Nets: example , example . sktime , sktime-dl - Toolbox for (deep) learning with time series. adtk - Time Series Anomaly Detection.","title":"Time Series"},{"location":"data_science/#time-series-evaluation","text":"TimeSeriesSplit - Sklearn time series split. tscv - Evaluation with gap.","title":"Time Series Evaluation"},{"location":"data_science/#financial-data","text":"pandas-datareader - Read stock data. ffn - Financial functions. bt - Backtesting algorithms. alpaca-trade-api-python - Commission-free trading through API. The Quantopian Stack (some features may require signup on their platform): pyfolio - Portfolio and risk analytics. zipline - Algorithmic trading. alphalens - Performance analysis of predictive stock factors. empyrical - Financial risk metrics.","title":"Financial Data"},{"location":"data_science/#survival-analysis","text":"Time-dependent Cox Model in R . lifelines - Survival analysis, Cox PH Regression, talk , talk2 . scikit-survival - Survival analysis. xgboost - \"objective\": \"survival:cox\" NHANES example survivalstan - Survival analysis, intro . convoys - Analyze time lagged conversions. RandomSurvivalForests (R packages: randomForestSRC, ggRandomForests).","title":"Survival Analysis"},{"location":"data_science/#outlier-detection-anomaly-detection","text":"sklearn - Isolation Forest and others. pyod - Outlier Detection / Anomaly Detection. eif - Extended Isolation Forest. AnomalyDetection - Anomaly detection (R package). luminol - Anomaly Detection and Correlation library from Linkedin. Distances for comparing histograms and detecting outliers - Talk : Kolmogorov-Smirnov , Wasserstein , Energy Distance (Cramer) , Kullback-Leibler divergence . banpei - Anomaly detection library based on singular spectrum transformation. telemanom - Detect anomalies in multivariate time series data using LSTMs.","title":"Outlier Detection &amp; Anomaly Detection"},{"location":"data_science/#ranking","text":"lightning - Large-scale linear classification, regression and ranking.","title":"Ranking"},{"location":"data_science/#scoring","text":"SLIM - Scoring systems for classification, Supersparse linear integer models.","title":"Scoring"},{"location":"data_science/#probabilistic-modeling-and-bayes","text":"Intro , Guide PyMC3 - Baysian modelling, intro pomegranate - Probabilistic modelling, talk . pmlearn - Probabilistic machine learning. arviz - Exploratory analysis of Bayesian models. zhusuan - Bayesian deep learning, generative models. dowhy - Estimate causal effects. edward - Probabilistic modeling, inference, and criticism, Mixture Density Networks (MNDs) , MDN Explanation . Pyro - Deep Universal Probabilistic Programming. tensorflow probability - Deep learning and probabilistic modelling, talk , example . bambi - High-level Bayesian model-building interface on top of PyMC3. neural-tangents - Infinite Neural Networks.","title":"Probabilistic Modeling and Bayes"},{"location":"data_science/#stacking-models-and-ensembles","text":"Model Stacking Blog Post mlxtend - EnsembleVoteClassifier , StackingRegressor , StackingCVRegressor for model stacking. vecstack - Stacking ML models. StackNet - Stacking ML models. mlens - Ensemble learning.","title":"Stacking Models and Ensembles"},{"location":"data_science/#model-evaluation","text":"pycm - Multi-class confusion matrix. pandas_ml - Confusion matrix. Plotting learning curve: link . yellowbrick - Learning curve.","title":"Model Evaluation"},{"location":"data_science/#model-explanation-interpretability-feature-importance","text":"Book , Examples shap - Explain predictions of machine learning models, talk . treeinterpreter - Interpreting scikit-learn's decision tree and random forest predictions. lime - Explaining the predictions of any machine learning classifier, talk , Warning (Myth 7) . lime_xgboost - Create LIMEs for XGBoost. eli5 - Inspecting machine learning classifiers and explaining their predictions. lofo-importance - Leave One Feature Out Importance, talk , examples: 1 , 2 , 3 . pybreakdown - Generate feature contribution plots. FairML - Model explanation, feature importance. pycebox - Individual Conditional Expectation Plot Toolbox. pdpbox - Partial dependence plot toolbox, example . partial_dependence - Visualize and cluster partial dependence. skater - Unified framework to enable model interpretation. anchor - High-Precision Model-Agnostic Explanations for classifiers. l2x - Instancewise feature selection as methodology for model interpretation. contrastive_explanation - Contrastive explanations. DrWhy - Collection of tools for explainable AI. lucid - Neural network interpretability. xai - An eXplainability toolbox for machine learning. innvestigate - A toolbox to investigate neural network predictions. dalex - Explanations for ML models (R package). interpret - Fit interpretable models, explain models (Microsoft). causalml - Causal inference by Uber.","title":"Model Explanation, Interpretability, Feature Importance"},{"location":"data_science/#automated-machine-learning","text":"AdaNet - Automated machine learning based on tensorflow. tpot - Automated machine learning tool, optimizes machine learning pipelines. auto_ml - Automated machine learning for analytics production. autokeras - AutoML for deep learning. nni - Toolkit for neural architecture search and hyper-parameter tuning by Microsoft. automl-gs - Automated machine learning. mljar - Automated machine learning. automl_zero - Automatically discover computer programs that can solve machine learning tasks from Google.","title":"Automated Machine Learning"},{"location":"data_science/#graph-representation-learning","text":"Karate Club - Unsupervised learning on graphs. Pytorch Geometric - Graph representation learning with PyTorch. DLG - Graph representation learning with TensorFlow.","title":"Graph Representation Learning"},{"location":"data_science/#evolutionary-algorithms-optimization","text":"deap - Evolutionary computation framework (Genetic Algorithm, Evolution strategies). evol - DSL for composable evolutionary algorithms, talk . platypus - Multiobjective optimization. autograd - Efficiently computes derivatives of numpy code. nevergrad - Derivation-free optimization. gplearn - Sklearn-like interface for genetic programming. blackbox - Optimization of expensive black-box functions. Optometrist algorithm - paper . DeepSwarm - Neural architecture search.","title":"Evolutionary Algorithms &amp; Optimization"},{"location":"data_science/#hyperparameter-tuning","text":"sklearn - GridSearchCV , RandomizedSearchCV . sklearn-deap - Hyperparameter search using genetic algorithms. hyperopt - Hyperparameter optimization. hyperopt-sklearn - Hyperopt + sklearn. optuna - Hyperparamter optimization, Talk . skopt - BayesSearchCV for Hyperparameter search. tune - Hyperparameter search with a focus on deep learning and deep reinforcement learning. hypergraph - Global optimization methods and hyperparameter optimization. bbopt - Black box hyperparameter optimization. dragonfly - Scalable Bayesian optimisation.","title":"Hyperparameter Tuning"},{"location":"data_science/#incremental-learning-online-learning","text":"sklearn - PassiveAggressiveClassifier , PassiveAggressiveRegressor . creme-ml - Incremental learning framework, talk . Kaggler - Online Learning algorithms.","title":"Incremental Learning, Online Learning"},{"location":"data_science/#active-learning","text":"Talk modAL - Active learning framework.","title":"Active Learning"},{"location":"data_science/#reinforcement-learning","text":"YouTube , YouTube Intro to Monte Carlo Tree Search (MCTS) - 1 , 2 , 3 AlphaZero methodology - 1 , 2 , 3 , Cheat Sheet RLLib - Library for reinforcement learning. Horizon - Facebook RL framework.","title":"Reinforcement Learning"},{"location":"data_science/#deployment-and-lifecycle-management","text":"","title":"Deployment and Lifecycle Management"},{"location":"data_science/#dependency-management","text":"pipreqs - Generate a requirements.txt from import statements. dephell - Dependency management. poetry - Dependency management. pyup - Dependency management. pypi-timemachine - Install packages with pip as if you were in the past.","title":"Dependency Management"},{"location":"data_science/#data-versioning-and-pipelines","text":"dvc - Version control for large files. hangar - Version control for tensor data. kedro - Build data pipelines.","title":"Data Versioning and Pipelines"},{"location":"data_science/#data-science-related","text":"m2cgen - Transpile trained ML models into other languages. sklearn-porter - Transpile trained scikit-learn estimators to C, Java, JavaScript and others. mlflow - Manage the machine learning lifecycle, including experimentation, reproducibility and deployment. modelchimp - Experiment Tracking. skll - Command-line utilities to make it easier to run machine learning experiments. BentoML - Package and deploy machine learning models for serving in production. dagster - Tool with focus on dependency graphs. knockknock - Be notified when your training ends. metaflow - Lifecycle Management Tool by Netflix. cortex - Deploy machine learning models.","title":"Data Science Related"},{"location":"data_science/#math-and-background","text":"All kinds of math and statistics resources Gilbert Strang - Linear Algebra Gilbert Strang - Matrix Methods in Data Analysis, Signal Processing, and Machine Learning","title":"Math and Background"},{"location":"data_science/#other","text":"daft - Render probabilistic graphical models using matplotlib. unyt - Working with units. scrapy - Web scraping library. VowpalWabbit - ML Toolkit from Microsoft. metric-learn - Metric learning.","title":"Other"},{"location":"data_science/#general-python-programming","text":"more_itertools - Extension of itertools. funcy - Fancy and practical functional tools. dateparser - A better date parser. jellyfish - Approximate string matching. coloredlogs - Colored logging output.","title":"General Python Programming"},{"location":"data_science/#resources","text":"Distill.pub - Blog. Machine Learning Videos Data Science Notebooks Recommender Systems (Microsoft) The GAN Zoo - List of Generative Adversarial Networks Datascience Cheatsheets","title":"Resources"},{"location":"data_science/#other-awesome-lists","text":"Awesome Adversarial Machine Learning Awesome AI Booksmarks Awesome AI on Kubernetes Awesome Big Data Awesome Business Machine Learning Awesome Causality Awesome Community Detection Awesome CSV Awesome Data Science with Ruby Awesome Dash Awesome Decision Trees Awesome Deep Learning Awesome ETL Awesome Financial Machine Learning Awesome Fraud Detection Awesome GAN Applications Awesome Graph Classification Awesome Gradient Boosting Awesome Machine Learning Awesome Machine Learning Interpretability Awesome Machine Learning Operations Awesome Monte Carlo Tree Search Awesome Online Machine Learning Awesome Python Awesome Python Data Science Awesome Python Data Science Awesome Python Data Science Awesome Pytorch Awesome Recommender Systems Awesome Semantic Segmentation Awesome Sentence Embedding Awesome Time Series Awesome Time Series Anomaly Detection","title":"Other Awesome Lists"},{"location":"data_science/#things-i-google-a-lot","text":"Frequency codes for time series Date parsing codes Feature Calculators tsfresh","title":"Things I google a lot"},{"location":"data_science/#contributing","text":"Do you know a package that should be on this list? Did you spot a package that is no longer maintained and should be removed from this list? Then feel free to read the contribution guidelines and submit your pull request or create a new issue.","title":"Contributing"},{"location":"data_science/#license","text":"","title":"License"},{"location":"data_science2/","text":"Data Science Table of Contents Doc Open data Data storage Markup Language Languages Workflow/Pipelines tools Dataset Tools Data structure Algorithm Statistics Big Data and Cloud Books Course Misc Doc Three Things About Data Science You Won't Find In the Books Open data awesome-public-datasets - A topic-centric list of high-quality open datasets in public domains. By everyone, for everyone! fivethirtyeight/data - Data and code behind the articles and graphics at FiveThirtyEight https://data.fivethirtyeight.com/ Data storage Ten Simple Rulesfor Digital Data Storage How to share data with a statistician New tech IPFS is the Distributed Web Markup Language YAML \u8bed\u8a00\u6559\u7a0b Languages Data Science with Python R: Dimensionality Reduction and Clustering R vs Python: head to head data analysis | R vs Python\uff1a\u786c\u78b0\u786c\u7684\u6570\u636e\u5206\u6790 DataPyR Choosing R or Python for data analysis? An infographic Workflow/Pipelines tools DSL GNU make , manual , Make \u547d\u4ee4\u6559\u7a0b Common Workflow Language snakemake - Snakemake is a workflow management system that aims to reduce the complexity of creating workflows by providing a fast and comfortable execution environment, together with a clean and modern specification language in python style. Build bioinformatics pipelines with Snakemake nextflow - A DSL for data-driven computational pipelines http://nextflow.io sake - A self-documenting build automation tool Language-dependent toil - A scalable, efficient, cross-platform and easy-to-use workflow engine in pure Python Ruffus - Ruffus is a Computation Pipeline library for python. It is open-sourced, powerful and user-friendly, and widely used in science and bioinformatics. Dataset awesome-public-datasets - An awesome list of (large-scale) public datasets on the Internet. (On-going collection) Tools csvkit - A suite of utilities for converting to and working with CSV, the king of tabular file formats. http://csvkit.rtfd.org/ csvtk - Another cross-platform, efficient, practical and pretty CSV/TSV toolkit in Golang http://shenwei356.github.io/csvtk icdiff - improved colored diff http://www.jefftk.com/icdiff Data structure skizze - A probabilistic data structure service and storage dablooms - A Scalable, Counting, Bloom Filter. Java, Python, Go edition. inbloom - Cross language bloom filter implementation HyperLogLog HyperLogLog++, \u5927\u6570\u636e\u8ba1\u7b97\uff1a\u5982\u4f55\u4ec5\u75281.5KB\u5185\u5b58\u4e3a\u5341\u4ebf\u5bf9\u8c61\u8ba1\u6570 - Hyper LogLog \u7b97\u6cd5 , \u5982\u4f55\u5feb\u901f\u4f30\u8ba1\u5de8\u5927 dataset \u4e2dunique \u5143\u7d20\u7684\u6570\u76ee Algorithm Fast and Easy Levenshtein distance using a Trie Statistics YGC\u7684\u7edf\u8ba1\u7b14\u8bb0 \u300aOn the scalability of statistical procedures: why the p-value bashers just don't get it\u300b by Jeff Leek The Only Probability Cheatsheet You'll Ever Need Where priors come from (some popular distribution) Probabilistic Programming Bayesian Methods for Hackers The Advanced Matrix Factorization Jungle R, STATISTICS, PSYCHOLOGY, OPEN SCIENCE, DATA VISUALIZATION p-value Scientific method: Statistical errors P value ban: small step for a journal, giant leap for science Science Isn\u2019t Broken Odds Are, It's Wrong Big Data and Cloud \u5de6\u8033\u6735\u8017\u5b50\u8c08\u4e91\u8ba1\u7b97\uff1a\u62fc\u7684\u5c31\u662f\u8fd0\u7ef4 \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e8b\u52a1\u5904\u7406 \u3010\u4e13\u6cbb\u4e0d\u660e\u89c9\u5389\u3011\u4e4b\u201c\u5927\u6570\u636e\u201d hadoop\u548c\u5927\u6570\u636e\u7684\u5173\u7cfb\uff1f\u548cspark\u7684\u5173\u7cfb\uff1f\u4e92\u8865\uff1f\u5e76\u884c\uff1f \u540eHadoop\u65f6\u4ee3\u7684\u5927\u6570\u636e\u67b6\u6784 Elements of Scale: Composing and Scaling Data Platforms http://www.analyticsvidhya.com/ \u6570\u636e\u79d1\u5b66\u5bb6\u4fee\u70bc\u5b9d\u5178 Books Mining of Massive Datasets - The book is based on Stanford Computer Science course CS246: Mining Massive Datasets Course Stanford Computer Science course CS246: Mining Massive Datasets algorithms for Big Data - This class will give you a biased sample of techniques for scalable data anslysis Misc What to do with \u201csmall\u201d data? Convert xlsx to csv in linux command line , use ssconvert of Gnumeric","title":"Data_science"},{"location":"data_science2/#data-science","text":"Table of Contents Doc Open data Data storage Markup Language Languages Workflow/Pipelines tools Dataset Tools Data structure Algorithm Statistics Big Data and Cloud Books Course Misc","title":"Data Science"},{"location":"data_science2/#doc","text":"Three Things About Data Science You Won't Find In the Books","title":"Doc"},{"location":"data_science2/#open-data","text":"awesome-public-datasets - A topic-centric list of high-quality open datasets in public domains. By everyone, for everyone! fivethirtyeight/data - Data and code behind the articles and graphics at FiveThirtyEight https://data.fivethirtyeight.com/","title":"Open data"},{"location":"data_science2/#data-storage","text":"Ten Simple Rulesfor Digital Data Storage How to share data with a statistician New tech IPFS is the Distributed Web","title":"Data storage"},{"location":"data_science2/#markup-language","text":"YAML \u8bed\u8a00\u6559\u7a0b","title":"Markup Language"},{"location":"data_science2/#languages","text":"Data Science with Python R: Dimensionality Reduction and Clustering R vs Python: head to head data analysis | R vs Python\uff1a\u786c\u78b0\u786c\u7684\u6570\u636e\u5206\u6790 DataPyR Choosing R or Python for data analysis? An infographic","title":"Languages"},{"location":"data_science2/#workflowpipelines-tools","text":"DSL GNU make , manual , Make \u547d\u4ee4\u6559\u7a0b Common Workflow Language snakemake - Snakemake is a workflow management system that aims to reduce the complexity of creating workflows by providing a fast and comfortable execution environment, together with a clean and modern specification language in python style. Build bioinformatics pipelines with Snakemake nextflow - A DSL for data-driven computational pipelines http://nextflow.io sake - A self-documenting build automation tool Language-dependent toil - A scalable, efficient, cross-platform and easy-to-use workflow engine in pure Python Ruffus - Ruffus is a Computation Pipeline library for python. It is open-sourced, powerful and user-friendly, and widely used in science and bioinformatics.","title":"Workflow/Pipelines tools"},{"location":"data_science2/#dataset","text":"awesome-public-datasets - An awesome list of (large-scale) public datasets on the Internet. (On-going collection)","title":"Dataset"},{"location":"data_science2/#tools","text":"csvkit - A suite of utilities for converting to and working with CSV, the king of tabular file formats. http://csvkit.rtfd.org/ csvtk - Another cross-platform, efficient, practical and pretty CSV/TSV toolkit in Golang http://shenwei356.github.io/csvtk icdiff - improved colored diff http://www.jefftk.com/icdiff","title":"Tools"},{"location":"data_science2/#data-structure","text":"skizze - A probabilistic data structure service and storage dablooms - A Scalable, Counting, Bloom Filter. Java, Python, Go edition. inbloom - Cross language bloom filter implementation HyperLogLog HyperLogLog++, \u5927\u6570\u636e\u8ba1\u7b97\uff1a\u5982\u4f55\u4ec5\u75281.5KB\u5185\u5b58\u4e3a\u5341\u4ebf\u5bf9\u8c61\u8ba1\u6570 - Hyper LogLog \u7b97\u6cd5 , \u5982\u4f55\u5feb\u901f\u4f30\u8ba1\u5de8\u5927 dataset \u4e2dunique \u5143\u7d20\u7684\u6570\u76ee","title":"Data structure"},{"location":"data_science2/#algorithm","text":"Fast and Easy Levenshtein distance using a Trie","title":"Algorithm"},{"location":"data_science2/#statistics","text":"YGC\u7684\u7edf\u8ba1\u7b14\u8bb0 \u300aOn the scalability of statistical procedures: why the p-value bashers just don't get it\u300b by Jeff Leek The Only Probability Cheatsheet You'll Ever Need Where priors come from (some popular distribution) Probabilistic Programming Bayesian Methods for Hackers The Advanced Matrix Factorization Jungle R, STATISTICS, PSYCHOLOGY, OPEN SCIENCE, DATA VISUALIZATION p-value Scientific method: Statistical errors P value ban: small step for a journal, giant leap for science Science Isn\u2019t Broken Odds Are, It's Wrong","title":"Statistics"},{"location":"data_science2/#big-data-and-cloud","text":"\u5de6\u8033\u6735\u8017\u5b50\u8c08\u4e91\u8ba1\u7b97\uff1a\u62fc\u7684\u5c31\u662f\u8fd0\u7ef4 \u5206\u5e03\u5f0f\u7cfb\u7edf\u7684\u4e8b\u52a1\u5904\u7406 \u3010\u4e13\u6cbb\u4e0d\u660e\u89c9\u5389\u3011\u4e4b\u201c\u5927\u6570\u636e\u201d hadoop\u548c\u5927\u6570\u636e\u7684\u5173\u7cfb\uff1f\u548cspark\u7684\u5173\u7cfb\uff1f\u4e92\u8865\uff1f\u5e76\u884c\uff1f \u540eHadoop\u65f6\u4ee3\u7684\u5927\u6570\u636e\u67b6\u6784 Elements of Scale: Composing and Scaling Data Platforms http://www.analyticsvidhya.com/ \u6570\u636e\u79d1\u5b66\u5bb6\u4fee\u70bc\u5b9d\u5178","title":"Big Data and Cloud"},{"location":"data_science2/#books","text":"Mining of Massive Datasets - The book is based on Stanford Computer Science course CS246: Mining Massive Datasets","title":"Books"},{"location":"data_science2/#course","text":"Stanford Computer Science course CS246: Mining Massive Datasets algorithms for Big Data - This class will give you a biased sample of techniques for scalable data anslysis","title":"Course"},{"location":"data_science2/#misc","text":"What to do with \u201csmall\u201d data? Convert xlsx to csv in linux command line , use ssconvert of Gnumeric","title":"Misc"},{"location":"dm_da/","text":"\u6570\u636e\u5206\u6790\u5de5\u7a0b\u5e08\u9762\u8bd5\u4ee5\u53ca\u76f8\u5173\u8d44\u6599\u6536\u96c6 \u7f51\u4e0a\u8d44\u6e90\uff1a Python\u548c\u6570\u636e\u79d1\u5b66 python-data-science-cheatsheet copied\u6570\u636e\u5206\u6790\u9762\u7ecf APP\u7684\u6570\u636e\u5206\u6790 \u4ea7\u54c1\u65e5\u6d3bDAU\u4e0b\u964d\u8be5\u600e\u4e48\u5206\u6790 \u4e92\u8054\u7f51\u4e2d\u4e00\u4e9b\u5e38\u7528\u6307\u6807\uff08PV\u3001UV\u3001\u8e66\u5931\u7387\u3001\u8f6c\u6362\u7387\u3001\u9000\u51fa\u7387) AB test\u5b66\u4e60\u7b14\u8bb0 \u5168\u9762\u7684\u6570\u636e\u6307\u6807\u5206\u6790\u6846\u67b6 \u6570\u636e\u5206\u6790\u5c97_\u9762\u8bd5\u9898\u6574\u7406\u603b\u7ed3 \u5982\u4f55\u5199\u4e00\u4efd\u597d\u7684\u7b80\u5386\uff1a https://mp.weixin.qq.com/s/lD8YELIa6A9So0x6R0K6RA https://github.com/amusi/AI-Job-Resume https://github.com/dyweb/awesome-resume-for-chinese http://cv.ftqq.com/?fr=github# \u8d39\u7c73\u95ee\u9898","title":"DM_DA"},{"location":"dm_da/#_1","text":"\u7f51\u4e0a\u8d44\u6e90\uff1a Python\u548c\u6570\u636e\u79d1\u5b66 python-data-science-cheatsheet copied\u6570\u636e\u5206\u6790\u9762\u7ecf APP\u7684\u6570\u636e\u5206\u6790 \u4ea7\u54c1\u65e5\u6d3bDAU\u4e0b\u964d\u8be5\u600e\u4e48\u5206\u6790 \u4e92\u8054\u7f51\u4e2d\u4e00\u4e9b\u5e38\u7528\u6307\u6807\uff08PV\u3001UV\u3001\u8e66\u5931\u7387\u3001\u8f6c\u6362\u7387\u3001\u9000\u51fa\u7387) AB test\u5b66\u4e60\u7b14\u8bb0 \u5168\u9762\u7684\u6570\u636e\u6307\u6807\u5206\u6790\u6846\u67b6 \u6570\u636e\u5206\u6790\u5c97_\u9762\u8bd5\u9898\u6574\u7406\u603b\u7ed3 \u5982\u4f55\u5199\u4e00\u4efd\u597d\u7684\u7b80\u5386\uff1a https://mp.weixin.qq.com/s/lD8YELIa6A9So0x6R0K6RA https://github.com/amusi/AI-Job-Resume https://github.com/dyweb/awesome-resume-for-chinese http://cv.ftqq.com/?fr=github# \u8d39\u7c73\u95ee\u9898","title":"\u6570\u636e\u5206\u6790\u5de5\u7a0b\u5e08\u9762\u8bd5\u4ee5\u53ca\u76f8\u5173\u8d44\u6599\u6536\u96c6"},{"location":"dm_python/","text":"PythonDataMining \u5728\u7ebf\u6d4f\u89c8 [ \u63a8\u8350 ] \u5728\u5b66\u9662\u7684\u4e66\u67b6\u4e0a\u53d1\u73b0\u4e86\u4e00\u672c\u4e0d\u5e26\u8111\u5b50\u5c31\u80fd\u770b\u61c2\u7684\u4e66\u300aPython\u6570\u636e\u6316\u6398\u4e0e\u5b9e\u6218\u300b pdf \u5728\u5f53\u524d\u76ee\u5f55 ./ \u4e0b\uff0c\u6709 \u9ed1\u767d\u56fe\u4e2d\u6587\u7248 \u548c \u5f69\u8272\u56fe\u8868\u8865\u5145 pdf \u4f53\u79ef\u7565\u5927(8.8Mb)\uff0cgithub \u76f4\u63a5\u6253\u5f00\u6bd4\u8f83\u6162\uff0c\u5efa\u8bae clone \u6216 fork \u968f\u4e66\u9644\u5e26\u7684\u4ee3\u7801\u5728 ./BOOK_CODE \u6587\u4ef6\u5939\u4e0b\u9762\uff0c\u5168\u82f1\u6587 \u539f\u4e66\u6709\u4e9b\u6570\u636e\u96c6\u4e0d\u5305\u542b\u5728\u5185\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6570\u636e\u9700\u8981\u4ece\u5176\u4ed6\u7f51\u7ad9\u4e0a\u4e0b\u8f7d\uff0c\u800c\u7f51\u7ad9\u5df2\u7ecf\u66f4\u65b0\uff0c\u4e66\u7f16\u5199\u65f6\u7684\u6570\u636e\u5f88\u96be\u518d\u627e\u5230 \u6709\u4e9b\u6570\u636e\u8fd8\u8981\u7ffb\u5899\u624d\u62ff\u5f97\u5230\uff0c\u6bd4\u5982\u7b2c\u516d\u7ae0\u201d\u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8fdb\u884c\u793e\u4ea4\u5a92\u4f53\u6316\u6398\u201d\u65f6\uff0c\u6570\u636e\u96c6\u9700\u8981\u901a\u8fc7 twitter \u7684 API \u6765\u83b7\u53d6\uff0c\u8981\u5728\u4ee3\u7801\u91cc\u7ffb\u5899 \u6211\u6839\u636e\u4e66\u4e2d\u7684\u9700\u8981\uff0c\u4e00\u4e2a\u4e00\u4e2a\u91cd\u65b0\u628a\u6570\u636e\u96c6\u627e\u5230\uff0c\u653e\u5230 ./data \u76ee\u5f55\u4e0b\uff08\u5305\u62ec\u4e66\u7f16\u5199\u65f6\u7684\u6570\u636e\u3001\u9700\u8981\u7ffb\u5899\u7684\u6570\u636e\uff09 \u9605\u8bfb\u7b14\u8bb0\u5728\u5f53\u524d\u76ee\u5f55 ./ \u4e0b\uff0c\u8fd9\u662f\u5728\u968f\u4e66\u9644\u5e26\u7684\u4ee3\u7801\u7684\u57fa\u7840\u4e0a\u505a\u7684\u4e2d\u6587\u7248 \u6700\u540e\uff0c\u539a\u7740\u8138\u76ae\u8981 star \uff0c\u5624\u5624\u5624~~ \u672c\u5730\u6d4f\u89c8 \u73af\u5883\uff1apython3.x, jupyter notebook pip install notebook # \u5b89\u88c5\u7b14\u8bb0\u672c jupyter notebook # \u542f\u52a8\u7b14\u8bb0\u672c \u7b14\u8bb0\u76ee\u5f55 \u6570\u636e\u6316\u6398\u6d41\u7a0b\u7b80\u5355\u793a\u4f8b10min.ipynb \u7528\u8fd1\u90bb\u7b97\u6cd5\u5206\u7c7b.ipynb \u7528\u51b3\u7b56\u6811\u9884\u6d4b\u83b7\u80dc\u7403\u961f.ipynb \u7528\u4eb2\u548c\u6027\u5206\u6790\u63a8\u8350\u7535\u5f71.ipynb \u7528\u8f6c\u6362\u5668\u62bd\u53d6\u7279\u5f81.ipynb \u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8fdb\u884c\u793e\u4f1a\u5a92\u4f53\u6316\u6398.ipynb \u7528\u56fe\u6316\u6398\u627e\u5230\u611f\u5174\u8da3\u7684\u4eba.ipynb \u7528\u795e\u7ecf\u7f51\u7edc\u7834\u89e3\u9a8c\u8bc1\u7801.ipynb \u4f5c\u8005\u5f52\u5c5e\u95ee\u9898.ipynb \u65b0\u95fb\u8bed\u6599\u5206\u7c7b.ipynb \u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u56fe\u8c61\u5206\u7c7b.ipynb \u5927\u6570\u636e\u5904\u7406.ipynb \u95ee\u9898\u89e3\u51b3\u8bb0\u5f55 Python Tweepy \u7ffb\u5899\u6293\u53d6Twitter\u4fe1\u606f \u514d\u8d23\u58f0\u660e \u672c\u4ed3\u5e93\u6ca1\u6709\u82f1\u6587\u539f\u7248\u7684\u6587\u4ef6\uff0c\u53ea\u6709\u7f51\u53cb\u7ffb\u8bd1\u7684\u4e2d\u6587\u7248\u6587\u4ef6\u3002\u4e2d\u6587\u7248\u5177\u4f53\u6765\u6e90\u8bb0\u4e0d\u6e05\u4e86\u3002\u5982\u6709\u95ee\u9898\u53ef\u968f\u65f6\u8054\u7cfb\u6211\u5220\u9664\u76f8\u5173\u6587\u4ef6\u3002 License","title":"DM_Python"},{"location":"dm_python/#pythondatamining","text":"\u5728\u5b66\u9662\u7684\u4e66\u67b6\u4e0a\u53d1\u73b0\u4e86\u4e00\u672c\u4e0d\u5e26\u8111\u5b50\u5c31\u80fd\u770b\u61c2\u7684\u4e66\u300aPython\u6570\u636e\u6316\u6398\u4e0e\u5b9e\u6218\u300b pdf \u5728\u5f53\u524d\u76ee\u5f55 ./ \u4e0b\uff0c\u6709 \u9ed1\u767d\u56fe\u4e2d\u6587\u7248 \u548c \u5f69\u8272\u56fe\u8868\u8865\u5145 pdf \u4f53\u79ef\u7565\u5927(8.8Mb)\uff0cgithub \u76f4\u63a5\u6253\u5f00\u6bd4\u8f83\u6162\uff0c\u5efa\u8bae clone \u6216 fork \u968f\u4e66\u9644\u5e26\u7684\u4ee3\u7801\u5728 ./BOOK_CODE \u6587\u4ef6\u5939\u4e0b\u9762\uff0c\u5168\u82f1\u6587 \u539f\u4e66\u6709\u4e9b\u6570\u636e\u96c6\u4e0d\u5305\u542b\u5728\u5185\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6570\u636e\u9700\u8981\u4ece\u5176\u4ed6\u7f51\u7ad9\u4e0a\u4e0b\u8f7d\uff0c\u800c\u7f51\u7ad9\u5df2\u7ecf\u66f4\u65b0\uff0c\u4e66\u7f16\u5199\u65f6\u7684\u6570\u636e\u5f88\u96be\u518d\u627e\u5230 \u6709\u4e9b\u6570\u636e\u8fd8\u8981\u7ffb\u5899\u624d\u62ff\u5f97\u5230\uff0c\u6bd4\u5982\u7b2c\u516d\u7ae0\u201d\u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8fdb\u884c\u793e\u4ea4\u5a92\u4f53\u6316\u6398\u201d\u65f6\uff0c\u6570\u636e\u96c6\u9700\u8981\u901a\u8fc7 twitter \u7684 API \u6765\u83b7\u53d6\uff0c\u8981\u5728\u4ee3\u7801\u91cc\u7ffb\u5899 \u6211\u6839\u636e\u4e66\u4e2d\u7684\u9700\u8981\uff0c\u4e00\u4e2a\u4e00\u4e2a\u91cd\u65b0\u628a\u6570\u636e\u96c6\u627e\u5230\uff0c\u653e\u5230 ./data \u76ee\u5f55\u4e0b\uff08\u5305\u62ec\u4e66\u7f16\u5199\u65f6\u7684\u6570\u636e\u3001\u9700\u8981\u7ffb\u5899\u7684\u6570\u636e\uff09 \u9605\u8bfb\u7b14\u8bb0\u5728\u5f53\u524d\u76ee\u5f55 ./ \u4e0b\uff0c\u8fd9\u662f\u5728\u968f\u4e66\u9644\u5e26\u7684\u4ee3\u7801\u7684\u57fa\u7840\u4e0a\u505a\u7684\u4e2d\u6587\u7248 \u6700\u540e\uff0c\u539a\u7740\u8138\u76ae\u8981 star \uff0c\u5624\u5624\u5624~~","title":"PythonDataMining \u5728\u7ebf\u6d4f\u89c8 [ \u63a8\u8350 ]"},{"location":"dm_python/#_1","text":"\u73af\u5883\uff1apython3.x, jupyter notebook pip install notebook # \u5b89\u88c5\u7b14\u8bb0\u672c jupyter notebook # \u542f\u52a8\u7b14\u8bb0\u672c","title":"\u672c\u5730\u6d4f\u89c8"},{"location":"dm_python/#_2","text":"\u6570\u636e\u6316\u6398\u6d41\u7a0b\u7b80\u5355\u793a\u4f8b10min.ipynb \u7528\u8fd1\u90bb\u7b97\u6cd5\u5206\u7c7b.ipynb \u7528\u51b3\u7b56\u6811\u9884\u6d4b\u83b7\u80dc\u7403\u961f.ipynb \u7528\u4eb2\u548c\u6027\u5206\u6790\u63a8\u8350\u7535\u5f71.ipynb \u7528\u8f6c\u6362\u5668\u62bd\u53d6\u7279\u5f81.ipynb \u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8fdb\u884c\u793e\u4f1a\u5a92\u4f53\u6316\u6398.ipynb \u7528\u56fe\u6316\u6398\u627e\u5230\u611f\u5174\u8da3\u7684\u4eba.ipynb \u7528\u795e\u7ecf\u7f51\u7edc\u7834\u89e3\u9a8c\u8bc1\u7801.ipynb \u4f5c\u8005\u5f52\u5c5e\u95ee\u9898.ipynb \u65b0\u95fb\u8bed\u6599\u5206\u7c7b.ipynb \u7528\u6df1\u5ea6\u5b66\u4e60\u8fdb\u884c\u56fe\u8c61\u5206\u7c7b.ipynb \u5927\u6570\u636e\u5904\u7406.ipynb","title":"\u7b14\u8bb0\u76ee\u5f55"},{"location":"dm_python/#_3","text":"Python Tweepy \u7ffb\u5899\u6293\u53d6Twitter\u4fe1\u606f","title":"\u95ee\u9898\u89e3\u51b3\u8bb0\u5f55"},{"location":"dm_python/#_4","text":"\u672c\u4ed3\u5e93\u6ca1\u6709\u82f1\u6587\u539f\u7248\u7684\u6587\u4ef6\uff0c\u53ea\u6709\u7f51\u53cb\u7ffb\u8bd1\u7684\u4e2d\u6587\u7248\u6587\u4ef6\u3002\u4e2d\u6587\u7248\u5177\u4f53\u6765\u6e90\u8bb0\u4e0d\u6e05\u4e86\u3002\u5982\u6709\u95ee\u9898\u53ef\u968f\u65f6\u8054\u7cfb\u6211\u5220\u9664\u76f8\u5173\u6587\u4ef6\u3002","title":"\u514d\u8d23\u58f0\u660e"},{"location":"dm_python/#license","text":"","title":"License"},{"location":"dm_r/","text":"Summary \u7b80\u4ecb \u6570\u636e\u6316\u6398\u5bfc\u8bba\u548c\u4fe1\u8d37\u6a21\u578b \u56de\u5f52\u6a21\u578b\u548c\u623f\u4ef7\u9884\u6d4b \u611f\u77e5\u673a\u548c\u903b\u8f91\u56de\u5f52 \u51b3\u7b56\u6811\u548c\u96c6\u6210\u5b66\u4e60 \u7279\u5f81\u5de5\u7a0b \u53c2\u6570\u8c03\u4f18 \u65e0\u76d1\u7763\u5b66\u4e60 \u6587\u672c\u6316\u6398 \u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60","title":"DM_R"},{"location":"dm_r/#summary","text":"\u7b80\u4ecb \u6570\u636e\u6316\u6398\u5bfc\u8bba\u548c\u4fe1\u8d37\u6a21\u578b \u56de\u5f52\u6a21\u578b\u548c\u623f\u4ef7\u9884\u6d4b \u611f\u77e5\u673a\u548c\u903b\u8f91\u56de\u5f52 \u51b3\u7b56\u6811\u548c\u96c6\u6210\u5b66\u4e60 \u7279\u5f81\u5de5\u7a0b \u53c2\u6570\u8c03\u4f18 \u65e0\u76d1\u7763\u5b66\u4e60 \u6587\u672c\u6316\u6398 \u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60","title":"Summary"},{"location":"front/","text":"\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u524d\u7aef \ud83d\udc69\ud83c\udffc\ud83d\udcbb \ud83d\udd3a \u4e00\u4efd\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u524d\u7aef\u7684Repo \u5e2e\u52a9\u521d\u5b66\u8005\u4ece\u96f6\u5230\u4e00\u5b66\u4e60\u524d\u7aef\u9879\u76ee \ud83d\udd3a \u5305\u542b\u4e86\u524d\u7aef\u77e5\u8bc6\u4e2d\u6d89\u53ca\u5230\u768414\u4e2a\u6a21\u5757\uff0c\u4ece\u524d\u7aef\u7684\u77e5\u8bc6\u6280\u80fd\u56fe\u8c31\uff0c\u5230\u6709\u8da3\u7684\u6280\u672f\u6587\u6863\u5b66\u4e60\u8d44\u6e90\u4ee5\u53ca\u5728\u524d\u7aef\u524d\u884c\u8def\u4e0a\u957f\u671f\u5173\u6ce8\u7684\u524d\u7aef\u5f00\u53d1\u76f8\u5173\u7684\u7f51\u7ad9\u3001\u535a\u5ba2\u548c\u6d3b\u8dc3\u5f00\u53d1\u8005\u3002 \ud83d\udd3a Github Repo \u25aa\ufe0f Repo Name: Front-End-Alpha-To-Omega \u25aa\ufe0f Repo Link: \u7f51\u9875\u94fe\u63a5 \ud83d\udd3a \u5185\u5bb9 \u25aa\ufe0f Start here \u25aa\ufe0f HTML \u25aa\ufe0f CSS \u25aa\ufe0f JavaScript \u25aa\ufe0f \u5b89\u5168 \u25aa\ufe0f \u6027\u80fd \u25aa\ufe0f SEO \u25aa\ufe0f \u5de5\u5177 \u25aa\ufe0f \u5de5\u7a0b\u5316 \u25aa\ufe0f \u6d4b\u8bd5 \u25aa\ufe0f \u5c0f\u7a0b\u5e8f \ud83d\udc49\ud83c\udffb 2020\u5e74\u5b66\u4e60\u524d\u7aef\u7684\u5b9d\u85cf\u4ed3\u5e93 \u7f51\u9875\u94fe\u63a5 \ud83d\udc49\ud83c\udffb \u6b22\u8fce\u5927\u5bb6\u5728\u4e0b\u65b9\u8bc4\u8bba\u60f3\u77e5\u9053\u7684\u7f16\u7a0b\u76f8\u5173\u6280\u672f\u70b9\u3002 \u8fd9\u4e2a\u524d\u7aef\u9879\u76ee\u4f1a\u6301\u7eed\u66f4\u65b0\uff0c\u66f4\u65b0\u9891\u7387\u6bcf\u5468\u4e00\u6b21\uff0c\u4f1a\u6839\u636e\u5927\u5bb6\u7684\u8bc4\u8bba\u9009\u62e9\u6a21\u5757\u66f4\u65b0\u3002","title":"Front"},{"location":"git/","text":"Git and Github Table of Contents Doc Note Doc Ten Simple Rules for Taking Advantage of Git and GitHub \u5e38\u7528 Git \u547d\u4ee4\u6e05\u5355 \u53f2\u4e0a\u6700\u6d45\u663e\u6613\u61c2\u7684Git\u6559\u7a0b\uff01 by \u5ed6\u96ea\u5cf0 Git \u4f7f\u7528\u89c4\u8303\u6d41\u7a0b \u8ba9\u4f60\u7684Git\u6c34\u5e73\u66f4\u4e0a\u4e00\u5c42\u697c\u768410\u4e2a\u5c0f\u8d34\u58eb \u5982\u4f55\u9ad8\u6548\u5229\u7528GitHub 15 minutes to learn Git \u4f7f\u7528git\u548cgithub\u8fdb\u884c\u534f\u540c\u5f00\u53d1\u6d41\u7a0b GitHub Special: Data Scientists to Follow Best Tutorials on GitHub Pro Git git-cheat-sheet Making Your Code Citable Oh shit, git! - \"So here are some bad situations I've gotten myself into, and how I eventually got myself out of them in plain english.\" e.g. Oh shit, I committed and immediately realized I need to make one small change! git commit --amend Note Configuring a remote for a fork , and then Syncing a fork when upstream goes ahead. Changing a remote's URL Undo pushed commits","title":"Git"},{"location":"git/#git-and-github","text":"Table of Contents Doc Note","title":"Git and Github"},{"location":"git/#doc","text":"Ten Simple Rules for Taking Advantage of Git and GitHub \u5e38\u7528 Git \u547d\u4ee4\u6e05\u5355 \u53f2\u4e0a\u6700\u6d45\u663e\u6613\u61c2\u7684Git\u6559\u7a0b\uff01 by \u5ed6\u96ea\u5cf0 Git \u4f7f\u7528\u89c4\u8303\u6d41\u7a0b \u8ba9\u4f60\u7684Git\u6c34\u5e73\u66f4\u4e0a\u4e00\u5c42\u697c\u768410\u4e2a\u5c0f\u8d34\u58eb \u5982\u4f55\u9ad8\u6548\u5229\u7528GitHub 15 minutes to learn Git \u4f7f\u7528git\u548cgithub\u8fdb\u884c\u534f\u540c\u5f00\u53d1\u6d41\u7a0b GitHub Special: Data Scientists to Follow Best Tutorials on GitHub Pro Git git-cheat-sheet Making Your Code Citable Oh shit, git! - \"So here are some bad situations I've gotten myself into, and how I eventually got myself out of them in plain english.\" e.g. Oh shit, I committed and immediately realized I need to make one small change! git commit --amend","title":"Doc"},{"location":"git/#note","text":"Configuring a remote for a fork , and then Syncing a fork when upstream goes ahead. Changing a remote's URL Undo pushed commits","title":"Note"},{"location":"linux/","text":"Linux Table of Contents resources Doc tools shell resources the-art-of-command-line awesome-shell Doc Linux \u5e38\u89c1\u9ad8\u5371\u64cd\u4f5c \u670d\u52a1\u5668\u64cd\u4f5c\u7cfb\u7edf\u5e94\u8be5\u9009\u62e9 Debian/Ubuntu \u8fd8\u662f CentOS\uff1f (\u767e\u5bb6\u4e89\u9e23) Linux LVM\u7b80\u660e\u6559\u7a0b Linux \u5de5\u5177\u5feb\u901f\u6559\u7a0b Shell alias Linux \u6027\u80fd\u76d1\u6d4b Advanced Scripting Using PBS Environment Variables make\u548cmakefile\uff0c\u591a\u6587\u4ef6\u9879\u76ee\u7ba1\u7406 \u5173\u4e8eCPU Cache -- \u7a0b\u5e8f\u733f\u9700\u8981\u77e5\u9053\u7684\u90a3\u4e9b\u4e8b \u7531\u4e00\u6b21\u78c1\u76d8\u544a\u8b66\u5f15\u53d1\u7684\u8840\u6848 -- du \u548c ls \u7684\u533a\u522b tools Snakemake - reduce the complexity of creating workflows by providing a fast and comfortable execution environment, together with a clean and modern domain specific specification language (DSL) in python style: @bitslife Windows \u5e73\u53f0\u4e0b\u597d\u7528\u7684\u7ec8\u7aef\uff0c MobaXterm \uff0c \u53ef\u4ee5apt-get install \u4e00\u4e9blinux \u4e0b\u5de5\u5177\uff0c\u6bd4\u5982 clang\uff0c \u60f3\u5728Windows\u4e0a\u5b66\u4e60Linux\uff0c\u53ef\u4ee5\u4e0d\u7528 MinGW \u6216\u8005\u5b89\u88c5\u865a\u62df\u673a\u4e86\uff0c Windows\u4e0b\u5b66\u4e60\u751f\u7269\u4fe1\u606f\u7684\u7b52\u5b50\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u4e0b\u4e86\u3002\u4eb2\u6d4b\uff0c\u5f88\u597d\u7528\u3002 command_line_bootcamp Command-line bootcamp adventure in your browser. memusg - A 'time'-like utility for Unix that measures peak memory usage ranger - file manager The ultimate Vim configuration: vimrc trash-cli - Command line interface to the freedesktop.org trashcan. \u907f\u514drm -rf / \u7684\u60b2\u5267\uff0c\u53c2\u8003 \u4e2d\u6587\u4ecb\u7ecd \uff01 Bashmarks is a shell script that allows you to save and jump to commonly used directories dirsize - Summarize size of directories and files in directories\u3002\u6211\u81ea\u5df1\u5199\u7684 brename - Recursively batch rename files and directories by regular expression\u3002\u6211\u81ea\u5df1\u5199\u7684 peco - Simplistic interactive filtering tool httpie - HTTPie is a command line HTTP client, a user-friendly cURL replacement datamash - GNU datamash is a command-line program which performs basic numeric,textual and statistical operations on input textual data files. distribution - Short, simple, direct scripts for creating ASCII graphical histograms in the terminal. fdupes - FDUPES is a program for identifying or deleting duplicate files residing within specified directories. FSlint - FSlint is a utility to find and clean various forms of lint on a filesystem. pigz - A parallel implementation of gzip for modern multi-processor, multi-core machines install font to support emoji : gdouros-symbola-fonts shell The Unix Shell and Extra Unix Shell Material SHELL\u7f16\u7a0b\u4e4b\u8bed\u6cd5\u57fa\u7840 shellcheck - ShellCheck, a static analysis tool for shell scripts http://www.shellcheck.net \u8be6\u89e3 awk \u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5 Linux \u4e2d\u9ad8\u6548\u7f16\u5199 Bash \u811a\u672c\u7684 10 \u4e2a\u6280\u5de7","title":"Linux"},{"location":"linux/#linux","text":"Table of Contents resources Doc tools shell","title":"Linux"},{"location":"linux/#resources","text":"the-art-of-command-line awesome-shell","title":"resources"},{"location":"linux/#doc","text":"Linux \u5e38\u89c1\u9ad8\u5371\u64cd\u4f5c \u670d\u52a1\u5668\u64cd\u4f5c\u7cfb\u7edf\u5e94\u8be5\u9009\u62e9 Debian/Ubuntu \u8fd8\u662f CentOS\uff1f (\u767e\u5bb6\u4e89\u9e23) Linux LVM\u7b80\u660e\u6559\u7a0b Linux \u5de5\u5177\u5feb\u901f\u6559\u7a0b Shell alias Linux \u6027\u80fd\u76d1\u6d4b Advanced Scripting Using PBS Environment Variables make\u548cmakefile\uff0c\u591a\u6587\u4ef6\u9879\u76ee\u7ba1\u7406 \u5173\u4e8eCPU Cache -- \u7a0b\u5e8f\u733f\u9700\u8981\u77e5\u9053\u7684\u90a3\u4e9b\u4e8b \u7531\u4e00\u6b21\u78c1\u76d8\u544a\u8b66\u5f15\u53d1\u7684\u8840\u6848 -- du \u548c ls \u7684\u533a\u522b","title":"Doc"},{"location":"linux/#tools","text":"Snakemake - reduce the complexity of creating workflows by providing a fast and comfortable execution environment, together with a clean and modern domain specific specification language (DSL) in python style: @bitslife Windows \u5e73\u53f0\u4e0b\u597d\u7528\u7684\u7ec8\u7aef\uff0c MobaXterm \uff0c \u53ef\u4ee5apt-get install \u4e00\u4e9blinux \u4e0b\u5de5\u5177\uff0c\u6bd4\u5982 clang\uff0c \u60f3\u5728Windows\u4e0a\u5b66\u4e60Linux\uff0c\u53ef\u4ee5\u4e0d\u7528 MinGW \u6216\u8005\u5b89\u88c5\u865a\u62df\u673a\u4e86\uff0c Windows\u4e0b\u5b66\u4e60\u751f\u7269\u4fe1\u606f\u7684\u7b52\u5b50\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u4e0b\u4e86\u3002\u4eb2\u6d4b\uff0c\u5f88\u597d\u7528\u3002 command_line_bootcamp Command-line bootcamp adventure in your browser. memusg - A 'time'-like utility for Unix that measures peak memory usage ranger - file manager The ultimate Vim configuration: vimrc trash-cli - Command line interface to the freedesktop.org trashcan. \u907f\u514drm -rf / \u7684\u60b2\u5267\uff0c\u53c2\u8003 \u4e2d\u6587\u4ecb\u7ecd \uff01 Bashmarks is a shell script that allows you to save and jump to commonly used directories dirsize - Summarize size of directories and files in directories\u3002\u6211\u81ea\u5df1\u5199\u7684 brename - Recursively batch rename files and directories by regular expression\u3002\u6211\u81ea\u5df1\u5199\u7684 peco - Simplistic interactive filtering tool httpie - HTTPie is a command line HTTP client, a user-friendly cURL replacement datamash - GNU datamash is a command-line program which performs basic numeric,textual and statistical operations on input textual data files. distribution - Short, simple, direct scripts for creating ASCII graphical histograms in the terminal. fdupes - FDUPES is a program for identifying or deleting duplicate files residing within specified directories. FSlint - FSlint is a utility to find and clean various forms of lint on a filesystem. pigz - A parallel implementation of gzip for modern multi-processor, multi-core machines install font to support emoji : gdouros-symbola-fonts","title":"tools"},{"location":"linux/#shell","text":"The Unix Shell and Extra Unix Shell Material SHELL\u7f16\u7a0b\u4e4b\u8bed\u6cd5\u57fa\u7840 shellcheck - ShellCheck, a static analysis tool for shell scripts http://www.shellcheck.net \u8be6\u89e3 awk \u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5 Linux \u4e2d\u9ad8\u6548\u7f16\u5199 Bash \u811a\u672c\u7684 10 \u4e2a\u6280\u5de7","title":"shell"},{"location":"linux_cmd/","text":"Linux Command 550 \u591a\u4e2a Linux \u547d\u4ee4\uff0c\u5185\u5bb9\u5305\u542b Linux \u547d\u4ee4\u624b\u518c\u3001\u8be6\u89e3\u3001\u5b66\u4e60\uff0c\u503c\u5f97\u6536\u85cf\u7684 Linux \u547d\u4ee4\u901f\u67e5\u624b\u518c\u3002\u8bf7\u539f\u8c05\u6211\u5199\u4e86\u4e2a\u722c\u866b\uff0c\u722c\u4e86\u4ed6\u4eec\u5bb6\u7684\u6570\u636e linuxde.net \uff0c\u540c\u65f6\u8fdb\u884c\u4e86\u7f16\u8f91\uff0c\u589e\u52a0\u4e86\u4e00\u4e9b\u6570\u636e\u3002\u5bf9\u4ed6\u4eec\u7684\u8f9b\u52e4\u52b3\u52a8\u8868\u793a\u656c\u610f\uff0c\u4e2a\u4eba\u5e0c\u671b\u80fd\u672c\u5730\u79bb\u7ebf\u641c\u7d22\uff0c\u4e0d\u559c\u6b22\u5e7f\u544a\uff0c\u5e0c\u671b\u5f97\u5230\u5e72\u51c0\u6f02\u4eae\u7684\u9884\u89c8\u754c\u9762\uff0c\u4e1a\u4f59\u548c\u5de5\u4f5c\u65f6\u95f4\u9700\u8981\u4f7f\u7528\u5927\u91cf\u7684\u547d\u4ee4\uff0c\u6240\u4ee5\u5e72\u4e86\u4e00\u4ef6\u770b\u4f3c\u6bd4\u8f83\u611a\u8822\u7684\u4e8b\u60c5\uff0c\u5728\u56de\u5934\u6709\u7a7a\u7684\u65f6\u5019\u4f30\u8ba1\u80fd\u505a\u4e00\u4e2aApp\uff1f\u6216\u8005\u662f\u547d\u4ee4\u884c\u5e2e\u52a9\u5de5\u5177\uff1f\u4f46\u76ee\u524d\u8fd8\u6ca1\u6709\u8ba1\u5212... Web | Dash | Alfred | Krunner | Android | Mac/Win/Linux | Chrome Plugin | \u547d\u4ee4\u884c\u5de5\u5177 \u5f00\u53d1\u4f7f\u7528 \u53ef\u4ee5\u901a\u8fc7 npm \u5b89\u88c5 linux-command \u5305\uff0c\u5305\u542b\u6240\u6709\u547d\u4ee4\u7684 markdown \u6587\u672c\uff0c\u548c\u4e00\u4e2a\u7d22\u5f15\u6587\u4ef6\u3002 npm install linux-command var comm = require( linux-command ); console.log( ---- , comm.ls); var alias = require( linux-command/command/alias.md ); console.log( ---- , alias); // markdown string \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7 CDN \u6765\u8bbf\u95ee\u7d22\u5f15\u6570\u636e\uff0c\u548c\u5bf9\u5e94\u7684\u547d\u4ee4\u8be6\u7ec6\u5185\u5bb9\uff0c\u6211\u5c06\u66f4\u65b0\u5185\u5bb9\u5b9a\u671f\u53d1\u5e03\u7248\u672c\uff0c\u63d0\u4f9b\u5927\u5bb6\u4f7f\u7528\uff0c UNPKG \u5e26\u4e0a\u7248\u672c\u53f7\uff0c\u5c06\u9501\u5b9a\u7248\u672c\u8bbf\u95ee\uff0c\u5220\u9664\u7248\u672c\u53f7\u8bf7\u6c42\u6570\u636e\uff0c\u5c06\u4f1a\u81ea\u52a8\u91cd\u5b9a\u5411\u6700\u65b0\u7248\u672c\u3002 # \u547d\u4ee4\u7d22\u5f15 JSON \u6570\u636e https://unpkg.com/linux-command/dist/data.json # \u5bf9\u5e94\u547d\u4ee4\u8be6\u60c5\uff08Markdown\uff09\u6570\u636e https://unpkg.com/linux-command/command/ \u547d\u4ee4\u540d\u79f0 .md \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7 Github \u7684 Raw \u6765\uff0c\u83b7\u53d6\u6700\u65b0\u7684\u5185\u5bb9 # \u547d\u4ee4\u7d22\u5f15 JSON \u6570\u636e https://raw.githubusercontent.com/jaywcjlove/linux-command/master/dist/data.json # \u5bf9\u5e94\u547d\u4ee4\u8be6\u60c5\uff08Markdown\uff09\u6570\u636e https://raw.githubusercontent.com/jaywcjlove/linux-command/master/command/ \u547d\u4ee4\u540d\u79f0 .md Chrome \u63d2\u4ef6 \u53ef\u5728 \u6e90\u7801\u4ed3\u5e93 \u9884\u89c8\u6548\u679c\uff0c Github\u4e0b\u8f7d crx \u6587\u4ef6\u5b89\u88c5 \u6216\u8005 \u5f00\u6e90\u4e2d\u56fd\u4e0b\u8f7d crx \u6587\u4ef6\u5b89\u88c5 \u4e5f\u53ef\u901a\u8fc7 Chrome Web Store \u4e0b\u8f7d\uff1a Web \u7248\u672c Github Web | OSChina Web | Githack | Statically \u626b\u63cf\u4e8c\u7ef4\u7801\u79fb\u52a8\u7aef\u9884\u89c8\u641c\u7d22\uff0c\u4e5f\u53ef\u901a\u8fc7\u4e8c\u7ef4\u7801\u4e0b\u9762\u94fe\u63a5\u5730\u5740\u6253\u5f00\u4f7f\u7528\u3002 \u9884\u89c8\u641c\u7d22\uff1a https://git.io/linux Alfred \u7248\u672c Alfred \u7248\u672c\u4e0b\u8f7d \uff0c \u4e0b\u56fe\u662f\u754c\u9762\u6548\u679c\u3002 Dash \u7248\u672c \u7531 @SHANG\u6b87 \u63d0\u4f9b\u7684 Dash \u7248\u672c #91 \uff0c\u53ef\u914d\u5408 alfred \u73a9\u800d\uff0c\u4e0b\u8f7d linux-command.docset.zip \u6587\u4ef6\u89e3\u538b\uff0c\u70b9\u51fb\u5b89\u88c5\u5373\u53ef\u3002 \u547d\u4ee4\u884c\u5de5\u5177 @chenjiandongx/how Python \u7248 #129 \uff0c\u7531 @\u9648\u952e\u51ac \u63d0\u4f9b\u3002 @chenjiandongx/pls Golang \u7248 #129 \uff0c\u7531 @\u9648\u952e\u51ac \u63d0\u4f9b\u3002 \u76ee\u5f55 Linux\u547d\u4ee4\u5206\u7c7b \u6587\u4ef6\u7ba1\u7406 File Management \u6587\u4ef6\u4f20\u8f93 File Transfer \u6587\u672c\u5904\u7406 File Editor \u5907\u4efd\u538b\u7f29 File Compression \u7cfb\u7edf\u7ba1\u7406 System Management \u7cfb\u7edf\u8bbe\u7f6e System Settings \u7f51\u7edc\u901a\u8baf Network Communication \u78c1\u76d8\u7ba1\u7406 Disk Management \u78c1\u76d8\u7ef4\u62a4 Disk Maintenance \u8bbe\u5907\u7ba1\u7406 Device Commands \u7535\u5b50\u90ae\u4ef6\u4e0e\u65b0\u95fb\u7ec4 \u5176\u4ed6\u547d\u4ee4 Misc Commands Node\u8c03\u7528 Linux\u5b66\u4e60\u8d44\u6e90\u6574\u7406 \u793e\u533a\u7f51\u7ad9 \u77e5\u8bc6\u76f8\u5173 \u8f6f\u4ef6\u5de5\u5177 \u4e2d\u56fd\u5f00\u6e90\u955c\u50cf\u7ad9\u70b9 \u6e38\u620f\u73a9\u5bb6\u53d1\u884c\u7248 Linux\u547d\u4ee4\u5206\u7c7b \u8fd9\u91cc\u5b58\u653eLinux \u547d\u4ee4\u5927\u5168\u5e76\u4e0d\u5168\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7 linux-command \u6765\u641c\u7d22\uff0c\u5b83\u662f\u628a command \u76ee\u5f55\u91cc\u9762\u641c\u96c6\u7684\u547d\u4ee4\uff0c\u751f\u6210\u4e86\u9759\u6001HTML\u5e76\u63d0\u4f9b\u9884\u89c8\u4ee5\u53ca\u7d22\u5f15\u641c\u7d22\u3002 \u6587\u4ef6\u4f20\u8f93 bye\u3001ftp\u3001ftpcount\u3001ftpshut\u3001ftpwho\u3001ncftp\u3001tftp\u3001uucico\u3001uucp\u3001uupick\u3001uuto\u3001scp \u5907\u4efd\u538b\u7f29 ar\u3001bunzip2\u3001bzip2\u3001bzip2recover\u3001compress\u3001cpio\u3001dump\u3001gunzip\u3001gzexe\u3001gzip\u3001lha\u3001restore\u3001tar\u3001unarj\u3001unzip\u3001zip\u3001zipinfo \u6587\u4ef6\u7ba1\u7406 diff\u3001diffstat\u3001file\u3001find\u3001git\u3001gitview\u3001ln\u3001locate\u3001lsattr\u3001mattrib\u3001mc\u3001mcopy\u3001mdel\u3001mdir\u3001mktemp\u3001mmove\u3001mread\u3001mren\u3001mshowfat\u3001mtools\u3001mtoolstest\u3001mv\u3001od\u3001paste\u3001patch\u3001rcp\u3001rhmask\u3001rm\u3001slocate\u3001split\u3001tee\u3001tmpwatch\u3001touch\u3001umask\u3001whereis\u3001which\u3001cat\u3001chattr\u3001chgrp\u3001chmod\u3001chown\u3001cksum\u3001cmp\u3001cp\u3001cut\u3001indent \u78c1\u76d8\u7ba1\u7406 cd\u3001df\u3001dirs\u3001du\u3001edquota\u3001eject\u3001lndir\u3001ls\u3001mcd\u3001mdeltree\u3001mdu\u3001mkdir\u3001mlabel\u3001mmd\u3001mmount\u3001mrd\u3001mzip\u3001pwd\u3001quota\u3001quotacheck\u3001quotaoff\u3001quotaon\u3001repquota\u3001rmdir\u3001rmt\u3001stat\u3001tree\u3001umount \u78c1\u76d8\u7ef4\u62a4 badblocks\u3001cfdisk\u3001dd\u3001e2fsck\u3001ext2ed\u3001fdisk\u3001fsck.ext2\u3001fsck\u3001fsck.minix\u3001fsconf\u3001hdparm\u3001losetup\u3001mbadblocks\u3001mformat\u3001mkbootdisk\u3001mkdosfs\u3001mke2fs\u3001mkfs.ext2\u3001mkfs\u3001mkfs.minix\u3001mkfs.msdos\u3001mkinitrd\u3001mkisofs\u3001mkswap\u3001mpartition\u3001sfdisk\u3001swapoff\u3001swapon\u3001symlinks\u3001sync \u7cfb\u7edf\u8bbe\u7f6e alias\u3001apmd\u3001aumix\u3001bind\u3001chkconfig\u3001chroot\u3001clock\u3001crontab\u3001declare\u3001depmod\u3001dircolors\u3001dmesg\u3001enable\u3001eval\u3001export\u3001fbset\u3001grpconv\u3001grpunconv\u3001hwclock\u3001insmod\u3001kbdconfig\u3001lilo\u3001liloconfig\u3001lsmod\u3001minfo\u3001mkkickstart\u3001modinfo\u3001modprobe\u3001mouseconfig\u3001ntsysv\u3001passwd\u3001pwconv\u3001pwunconv\u3001rdate\u3001resize\u3001rmmod\u3001rpm\u3001set\u3001setconsole\u3001setenv\u3001setup\u3001sndconfig\u3001SVGAText Mode\u3001timeconfig\u3001ulimit\u3001unalias\u3001unset \u7cfb\u7edf\u7ba1\u7406 adduser\u3001chfn\u3001chsh\u3001date\u3001exit\u3001finger\u3001free\u3001fwhois\u3001gitps\u3001groupdel\u3001groupmod\u3001halt\u3001id\u3001kill\u3001last\u3001lastb\u3001login\u3001logname\u3001logout\u3001logrotate\u3001newgrp\u3001nice\u3001procinfo\u3001ps\u3001pstree\u3001reboot\u3001renice\u3001rlogin\u3001rsh\u3001rwho\u3001screen\u3001shutdown\u3001sliplogin\u3001su\u3001sudo\u3001suspend\u3001swatch\u3001tload\u3001top\u3001uname\u3001useradd\u3001userconf\u3001userdel\u3001usermod\u3001vlock\u3001w\u3001who\u3001whoami\u3001whois \u6587\u672c\u5904\u7406 awk\u3001col\u3001colrm\u3001comm\u3001csplit\u3001ed\u3001egrep\u3001ex\u3001fgrep\u3001fmt\u3001fold\u3001grep\u3001ispell\u3001jed\u3001joe\u3001join\u3001look\u3001mtype\u3001pico\u3001rgrep\u3001sed\u3001sort\u3001spell\u3001tr\u3001uniq\u3001vi\u3001wc \u7f51\u7edc\u901a\u8baf dip\u3001getty\u3001mingetty\u3001ppp-off\u3001smbd(samba daemon)\u3001telnet\u3001uulog\u3001uustat\u3001uux\u3001cu\u3001dnsconf\u3001efax\u3001httpd\u3001ip\u3001ifconfig\u3001mesg\u3001minicom\u3001nc\u3001netconf\u3001netconfig\u3001netstat\u3001ping\u3001pppstats\u3001samba\u3001setserial\u3001shapecfg(shaper configuration)\u3001smbd(samba daemon)\u3001statserial(status ofserial port)\u3001talk\u3001tcpdump\u3001testparm(test parameter)\u3001traceroute\u3001tty(teletypewriter)\u3001uuname\u3001wall(write all)\u3001write\u3001ytalk\u3001arpwatch\u3001apachectl\u3001smbclient(samba client)\u3001pppsetup \u8bbe\u5907\u7ba1\u7406 dumpkeys\u3001loadkeys\u3001MAKEDEV\u3001rdev\u3001setleds \u7535\u5b50\u90ae\u4ef6\u4e0e\u65b0\u95fb\u7ec4 archive\u3001ctlinnd\u3001elm\u3001getlist\u3001inncheck\u3001mail\u3001mailconf\u3001mailq\u3001messages\u3001metamail\u3001mutt\u3001nntpget\u3001pine\u3001slrn\u3001X WINDOWS SYSTEM\u3001reconfig\u3001startx(start X Window)\u3001Xconfigurator\u3001XF86Setup\u3001xlsatoms\u3001xlsclients\u3001xlsfonts \u5176\u4ed6\u547d\u4ee4 yes Linux\u5b66\u4e60\u8d44\u6e90\u6574\u7406 \u793e\u533a\u7f51\u7ad9 Linux\u4e2d\u56fd - \u5404\u79cd\u8d44\u8baf\u3001\u6587\u7ae0\u3001\u6280\u672f \u5b9e\u9a8c\u697c - \u514d\u8d39\u63d0\u4f9b\u4e86Linux\u5728\u7ebf\u73af\u5883\uff0c\u4e0d\u7528\u5728\u81ea\u5df1\u673a\u5b50\u4e0a\u88c5\u7cfb\u7edf\u4e5f\u53ef\u4ee5\u5b66\u4e60Linux\uff0c\u8d85\u65b9\u4fbf\u5b9e\u7528\u3002 \u9e1f\u54e5\u7684linux\u79c1\u623f\u83dc - \u975e\u5e38\u9002\u5408Linux\u5165\u95e8\u521d\u5b66\u8005\u770b\u7684\u6559\u7a0b\u3002 Linux\u516c\u793e - Linux\u76f8\u5173\u7684\u65b0\u95fb\u3001\u6559\u7a0b\u3001\u4e3b\u9898\u3001\u58c1\u7eb8\u90fd\u6709\u3002 Linux Today - Linux\u65b0\u95fb\u8d44\u8baf\u53d1\u5e03\uff0cLinux\u804c\u4e1a\u6280\u672f\u5b66\u4e60\uff01\u3002 \u77e5\u8bc6\u76f8\u5173 Linux\u601d\u7ef4\u5bfc\u56fe\u6574\u7406 Linux\u521d\u5b66\u8005\u8fdb\u9636\u5b66\u4e60\u8d44\u6e90\u6574\u7406 Linux \u57fa\u7840\u5165\u95e8\uff08\u65b0\u7248\uff09 \u3010\u8bd1\u3011Linux\u6982\u5ff5\u67b6\u6784\u7684\u7406\u89e3 En Linux \u5b88\u62a4\u8fdb\u7a0b\u7684\u542f\u52a8\u65b9\u6cd5 Linux\u7f16\u7a0b\u4e4b\u5185\u5b58\u6620\u5c04 Linux\u77e5\u8bc6\u70b9\u5c0f\u7ed3 10\u5927\u767d\u5e3d\u9ed1\u5ba2\u4e13\u7528\u7684 Linux \u64cd\u4f5c\u7cfb\u7edf \u8f6f\u4ef6\u5de5\u5177 \u8d85\u8d5e\u7684Linux\u8f6f\u4ef6 Github\u4ed3\u5e93 Zh En \u7a0b\u5e8f\u5458\u559c\u6b22\u76849\u6b3e\u6700\u4f73\u7684Linux\u6587\u4ef6\u6bd4\u8f83\u5de5\u5177 \u63d0\u9ad8 Linux \u5f00\u53d1\u6548\u7387\u7684 5 \u4e2a\u5de5\u5177 \u4f60\u8981\u4e86\u89e3\u768411\u6b3e\u9762\u5411Linux\u7cfb\u7edf\u7684\u4e00\u6d41\u5907\u4efd\u5b9e\u7528\u5de5\u5177 16\u4e2a\u5f88\u6709\u7528\u7684\u5728\u7ebf\u5de5\u5177 Adobe\u8f6f\u4ef6\u7684\u6700\u4f73\u66ff\u4ee3\u54c1 \u539f\u6587\u5728\u8fd9\u91cc Evince (Adobe Acrobat Reader) \u4e00\u4e2a\u201c\u652f\u6301\u591a\u79cd\u6587\u6863\u683c\u5f0f\u7684\u6587\u6863\u67e5\u770b\u5668\u201d\uff0c\u53ef\u4ee5\u67e5\u770bPDF\uff0c\u8fd8\u652f\u6301\u5404\u79cd\u6f2b\u753b\u4e66\u683c\u5f0f Pixlr (Adobe Photoshop) \u4e00\u4e2a\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u5de5\u5177 Inkscape (Adobe Illustrator) \u4e00\u4e2a\u4e13\u4e1a\u7684\u77e2\u91cf\u56fe\u5f62\u7f16\u8f91\u5668 Pinegrow Web Editor (Adobe Dreamweaver) \u4e00\u4e2a\u53ef\u89c6\u5316\u7f16\u8f91\u5236\u4f5c HTML \u7f51\u7ad9 Scribus (Adobe InDesign) \u4e00\u4e2a\u5f00\u6e90\u7535\u5b50\u6742\u5fd7\u5236\u4f5c\u8f6f\u4ef6 Webflow (Adobe Muse) \u4e00\u6b3e\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u4e0d\u7528\u7f16\u7801\u5c31\u53ef\u4ee5\u5feb\u901f\u521b\u5efa\u7f51\u7ad9\u7684\u8c37\u6b4c\u6d4f\u89c8\u5668\u63d2\u4ef6\u3002 Tupi (Adobe Animate) \u4e00\u6b3e\u53ef\u4ee5\u521b\u5efaHTML5\u52a8\u753b\u7684\u5de5\u5177\u3002 Black Magic Fusion (Adobe After Effects) \u4e00\u6b3e\u5148\u8fdb\u7684\u5408\u6210\u8f6f\u4ef6\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u89c6\u89c9\u7279\u6548\u3001\u5e7f\u7535\u5f71\u89c6\u8bbe\u8ba1\u4ee5\u53ca3D\u52a8\u753b\u8bbe\u8ba1\u7b49\u9886\u57df\u3002 \u4e2d\u56fd\u5f00\u6e90\u955c\u50cf\u7ad9\u70b9 \u963f\u91cc\u4e91\u5f00\u6e90\u955c\u50cf\u7ad9\uff1ahttp://mirrors.aliyun.com/ \u7f51\u6613\u5f00\u6e90\u955c\u50cf\u7ad9\uff1ahttp://mirrors.163.com/ \u641c\u72d0\u5f00\u6e90\u955c\u50cf\u7ad9\uff1ahttp://mirrors.sohu.com/ \u5317\u4eac\u4ea4\u901a\u5927\u5b66\uff1ahttp://mirror.bjtu.edu.cn/ \\ \u6559\u80b2\u7f51\u8350> \u5170\u5dde\u5927\u5b66\uff1ahttp://mirror.lzu.edu.cn/ \\ \u897f\u5317\u9ad8\u6821FTP\u641c\u7d22\u5f15\u64ce> \u53a6\u95e8\u5927\u5b66\uff1ahttp://mirrors.xmu.edu.cn/ \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\uff1ahttp://ftp.sjtu.edu.cn/ \u6e05\u534e\u5927\u5b66\uff1ahttp://mirrors.tuna.tsinghua.edu.cn/ http://mirrors6.tuna.tsinghua.edu.cn/ http://mirrors4.tuna.tsinghua.edu.cn/ \u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66\uff1ahttp://mirrors.ustc.edu.cn/ http://ipv4.ustc.edu.cn/ \\ \u6559\u80b2\u7f51\u3001\u7535\u4fe1> http://ipv6.ustc.edu.cn/ \\ IPv6 only> \u4e1c\u5317\u5927\u5b66\uff1ahttp://mirror.neu.edu.cn/ \u6d59\u6c5f\u5927\u5b66\uff1ahttp://mirrors.zju.edu.cn/ \u4e1c\u8f6f\u4fe1\u606f\u5b66\u9662\uff1ahttp://mirrors.neusoft.edu.cn/ \u6e38\u620f\u73a9\u5bb6\u53d1\u884c\u7248 \u9762\u5411\u6e38\u620f\u73a9\u5bb6\u7684\u516b\u6b3e\u6700\u4f73 Linux \u53d1\u884c\u7248\uff0c\u672c\u6587\u7531\u5f00\u6e90\u4e2d\u56fd\u6574\u7406\uff0c \u539f\u6587\u5728\u8fd9\u91cc \u3002 SteamOS \u5b98\u65b9\u6587\u6863 \u955c\u50cf\u4e0b\u8f7d Ubuntu GamePack \u4e0b\u8f7d\u5730\u5740 Fedora \u2013 Games Spin \u4e0b\u8f7d\u5730\u5740 SparkyLinux \u2013 GameOver Edition \u4e0b\u8f7d\u5730\u5740 Lakka \u4e0b\u8f7d\u5730\u5740 Game Drift Linux \u4e0b\u8f7d\u5730\u5740 Solus \u4e0b\u8f7d\u5730\u5740 Manjaro Gaming Edition (mGAMe) \u4e0b\u8f7d\u5730\u5740 Team \u5c0f\u5f1f\u8c03\u8c03\u2122 ZhuangZhu-74 Huck Huang","title":"Linux_cmd"},{"location":"linux_cmd/#linux-command","text":"550 \u591a\u4e2a Linux \u547d\u4ee4\uff0c\u5185\u5bb9\u5305\u542b Linux \u547d\u4ee4\u624b\u518c\u3001\u8be6\u89e3\u3001\u5b66\u4e60\uff0c\u503c\u5f97\u6536\u85cf\u7684 Linux \u547d\u4ee4\u901f\u67e5\u624b\u518c\u3002\u8bf7\u539f\u8c05\u6211\u5199\u4e86\u4e2a\u722c\u866b\uff0c\u722c\u4e86\u4ed6\u4eec\u5bb6\u7684\u6570\u636e linuxde.net \uff0c\u540c\u65f6\u8fdb\u884c\u4e86\u7f16\u8f91\uff0c\u589e\u52a0\u4e86\u4e00\u4e9b\u6570\u636e\u3002\u5bf9\u4ed6\u4eec\u7684\u8f9b\u52e4\u52b3\u52a8\u8868\u793a\u656c\u610f\uff0c\u4e2a\u4eba\u5e0c\u671b\u80fd\u672c\u5730\u79bb\u7ebf\u641c\u7d22\uff0c\u4e0d\u559c\u6b22\u5e7f\u544a\uff0c\u5e0c\u671b\u5f97\u5230\u5e72\u51c0\u6f02\u4eae\u7684\u9884\u89c8\u754c\u9762\uff0c\u4e1a\u4f59\u548c\u5de5\u4f5c\u65f6\u95f4\u9700\u8981\u4f7f\u7528\u5927\u91cf\u7684\u547d\u4ee4\uff0c\u6240\u4ee5\u5e72\u4e86\u4e00\u4ef6\u770b\u4f3c\u6bd4\u8f83\u611a\u8822\u7684\u4e8b\u60c5\uff0c\u5728\u56de\u5934\u6709\u7a7a\u7684\u65f6\u5019\u4f30\u8ba1\u80fd\u505a\u4e00\u4e2aApp\uff1f\u6216\u8005\u662f\u547d\u4ee4\u884c\u5e2e\u52a9\u5de5\u5177\uff1f\u4f46\u76ee\u524d\u8fd8\u6ca1\u6709\u8ba1\u5212... Web | Dash | Alfred | Krunner | Android | Mac/Win/Linux | Chrome Plugin | \u547d\u4ee4\u884c\u5de5\u5177","title":"Linux Command"},{"location":"linux_cmd/#_1","text":"\u53ef\u4ee5\u901a\u8fc7 npm \u5b89\u88c5 linux-command \u5305\uff0c\u5305\u542b\u6240\u6709\u547d\u4ee4\u7684 markdown \u6587\u672c\uff0c\u548c\u4e00\u4e2a\u7d22\u5f15\u6587\u4ef6\u3002 npm install linux-command var comm = require( linux-command ); console.log( ---- , comm.ls); var alias = require( linux-command/command/alias.md ); console.log( ---- , alias); // markdown string \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7 CDN \u6765\u8bbf\u95ee\u7d22\u5f15\u6570\u636e\uff0c\u548c\u5bf9\u5e94\u7684\u547d\u4ee4\u8be6\u7ec6\u5185\u5bb9\uff0c\u6211\u5c06\u66f4\u65b0\u5185\u5bb9\u5b9a\u671f\u53d1\u5e03\u7248\u672c\uff0c\u63d0\u4f9b\u5927\u5bb6\u4f7f\u7528\uff0c UNPKG \u5e26\u4e0a\u7248\u672c\u53f7\uff0c\u5c06\u9501\u5b9a\u7248\u672c\u8bbf\u95ee\uff0c\u5220\u9664\u7248\u672c\u53f7\u8bf7\u6c42\u6570\u636e\uff0c\u5c06\u4f1a\u81ea\u52a8\u91cd\u5b9a\u5411\u6700\u65b0\u7248\u672c\u3002 # \u547d\u4ee4\u7d22\u5f15 JSON \u6570\u636e https://unpkg.com/linux-command/dist/data.json # \u5bf9\u5e94\u547d\u4ee4\u8be6\u60c5\uff08Markdown\uff09\u6570\u636e https://unpkg.com/linux-command/command/ \u547d\u4ee4\u540d\u79f0 .md \u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7 Github \u7684 Raw \u6765\uff0c\u83b7\u53d6\u6700\u65b0\u7684\u5185\u5bb9 # \u547d\u4ee4\u7d22\u5f15 JSON \u6570\u636e https://raw.githubusercontent.com/jaywcjlove/linux-command/master/dist/data.json # \u5bf9\u5e94\u547d\u4ee4\u8be6\u60c5\uff08Markdown\uff09\u6570\u636e https://raw.githubusercontent.com/jaywcjlove/linux-command/master/command/ \u547d\u4ee4\u540d\u79f0 .md","title":"\u5f00\u53d1\u4f7f\u7528"},{"location":"linux_cmd/#chrome","text":"\u53ef\u5728 \u6e90\u7801\u4ed3\u5e93 \u9884\u89c8\u6548\u679c\uff0c Github\u4e0b\u8f7d crx \u6587\u4ef6\u5b89\u88c5 \u6216\u8005 \u5f00\u6e90\u4e2d\u56fd\u4e0b\u8f7d crx \u6587\u4ef6\u5b89\u88c5 \u4e5f\u53ef\u901a\u8fc7 Chrome Web Store \u4e0b\u8f7d\uff1a","title":"Chrome \u63d2\u4ef6"},{"location":"linux_cmd/#web","text":"Github Web | OSChina Web | Githack | Statically \u626b\u63cf\u4e8c\u7ef4\u7801\u79fb\u52a8\u7aef\u9884\u89c8\u641c\u7d22\uff0c\u4e5f\u53ef\u901a\u8fc7\u4e8c\u7ef4\u7801\u4e0b\u9762\u94fe\u63a5\u5730\u5740\u6253\u5f00\u4f7f\u7528\u3002 \u9884\u89c8\u641c\u7d22\uff1a https://git.io/linux","title":"Web \u7248\u672c"},{"location":"linux_cmd/#alfred","text":"Alfred \u7248\u672c\u4e0b\u8f7d \uff0c \u4e0b\u56fe\u662f\u754c\u9762\u6548\u679c\u3002","title":"Alfred \u7248\u672c"},{"location":"linux_cmd/#dash","text":"\u7531 @SHANG\u6b87 \u63d0\u4f9b\u7684 Dash \u7248\u672c #91 \uff0c\u53ef\u914d\u5408 alfred \u73a9\u800d\uff0c\u4e0b\u8f7d linux-command.docset.zip \u6587\u4ef6\u89e3\u538b\uff0c\u70b9\u51fb\u5b89\u88c5\u5373\u53ef\u3002","title":"Dash \u7248\u672c"},{"location":"linux_cmd/#_2","text":"@chenjiandongx/how Python \u7248 #129 \uff0c\u7531 @\u9648\u952e\u51ac \u63d0\u4f9b\u3002 @chenjiandongx/pls Golang \u7248 #129 \uff0c\u7531 @\u9648\u952e\u51ac \u63d0\u4f9b\u3002","title":"\u547d\u4ee4\u884c\u5de5\u5177"},{"location":"linux_cmd/#_3","text":"Linux\u547d\u4ee4\u5206\u7c7b \u6587\u4ef6\u7ba1\u7406 File Management \u6587\u4ef6\u4f20\u8f93 File Transfer \u6587\u672c\u5904\u7406 File Editor \u5907\u4efd\u538b\u7f29 File Compression \u7cfb\u7edf\u7ba1\u7406 System Management \u7cfb\u7edf\u8bbe\u7f6e System Settings \u7f51\u7edc\u901a\u8baf Network Communication \u78c1\u76d8\u7ba1\u7406 Disk Management \u78c1\u76d8\u7ef4\u62a4 Disk Maintenance \u8bbe\u5907\u7ba1\u7406 Device Commands \u7535\u5b50\u90ae\u4ef6\u4e0e\u65b0\u95fb\u7ec4 \u5176\u4ed6\u547d\u4ee4 Misc Commands Node\u8c03\u7528 Linux\u5b66\u4e60\u8d44\u6e90\u6574\u7406 \u793e\u533a\u7f51\u7ad9 \u77e5\u8bc6\u76f8\u5173 \u8f6f\u4ef6\u5de5\u5177 \u4e2d\u56fd\u5f00\u6e90\u955c\u50cf\u7ad9\u70b9 \u6e38\u620f\u73a9\u5bb6\u53d1\u884c\u7248","title":"\u76ee\u5f55"},{"location":"linux_cmd/#linux","text":"\u8fd9\u91cc\u5b58\u653eLinux \u547d\u4ee4\u5927\u5168\u5e76\u4e0d\u5168\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7 linux-command \u6765\u641c\u7d22\uff0c\u5b83\u662f\u628a command \u76ee\u5f55\u91cc\u9762\u641c\u96c6\u7684\u547d\u4ee4\uff0c\u751f\u6210\u4e86\u9759\u6001HTML\u5e76\u63d0\u4f9b\u9884\u89c8\u4ee5\u53ca\u7d22\u5f15\u641c\u7d22\u3002","title":"Linux\u547d\u4ee4\u5206\u7c7b"},{"location":"linux_cmd/#_4","text":"bye\u3001ftp\u3001ftpcount\u3001ftpshut\u3001ftpwho\u3001ncftp\u3001tftp\u3001uucico\u3001uucp\u3001uupick\u3001uuto\u3001scp","title":"\u6587\u4ef6\u4f20\u8f93"},{"location":"linux_cmd/#_5","text":"ar\u3001bunzip2\u3001bzip2\u3001bzip2recover\u3001compress\u3001cpio\u3001dump\u3001gunzip\u3001gzexe\u3001gzip\u3001lha\u3001restore\u3001tar\u3001unarj\u3001unzip\u3001zip\u3001zipinfo","title":"\u5907\u4efd\u538b\u7f29"},{"location":"linux_cmd/#_6","text":"diff\u3001diffstat\u3001file\u3001find\u3001git\u3001gitview\u3001ln\u3001locate\u3001lsattr\u3001mattrib\u3001mc\u3001mcopy\u3001mdel\u3001mdir\u3001mktemp\u3001mmove\u3001mread\u3001mren\u3001mshowfat\u3001mtools\u3001mtoolstest\u3001mv\u3001od\u3001paste\u3001patch\u3001rcp\u3001rhmask\u3001rm\u3001slocate\u3001split\u3001tee\u3001tmpwatch\u3001touch\u3001umask\u3001whereis\u3001which\u3001cat\u3001chattr\u3001chgrp\u3001chmod\u3001chown\u3001cksum\u3001cmp\u3001cp\u3001cut\u3001indent","title":"\u6587\u4ef6\u7ba1\u7406"},{"location":"linux_cmd/#_7","text":"cd\u3001df\u3001dirs\u3001du\u3001edquota\u3001eject\u3001lndir\u3001ls\u3001mcd\u3001mdeltree\u3001mdu\u3001mkdir\u3001mlabel\u3001mmd\u3001mmount\u3001mrd\u3001mzip\u3001pwd\u3001quota\u3001quotacheck\u3001quotaoff\u3001quotaon\u3001repquota\u3001rmdir\u3001rmt\u3001stat\u3001tree\u3001umount","title":"\u78c1\u76d8\u7ba1\u7406"},{"location":"linux_cmd/#_8","text":"badblocks\u3001cfdisk\u3001dd\u3001e2fsck\u3001ext2ed\u3001fdisk\u3001fsck.ext2\u3001fsck\u3001fsck.minix\u3001fsconf\u3001hdparm\u3001losetup\u3001mbadblocks\u3001mformat\u3001mkbootdisk\u3001mkdosfs\u3001mke2fs\u3001mkfs.ext2\u3001mkfs\u3001mkfs.minix\u3001mkfs.msdos\u3001mkinitrd\u3001mkisofs\u3001mkswap\u3001mpartition\u3001sfdisk\u3001swapoff\u3001swapon\u3001symlinks\u3001sync","title":"\u78c1\u76d8\u7ef4\u62a4"},{"location":"linux_cmd/#_9","text":"alias\u3001apmd\u3001aumix\u3001bind\u3001chkconfig\u3001chroot\u3001clock\u3001crontab\u3001declare\u3001depmod\u3001dircolors\u3001dmesg\u3001enable\u3001eval\u3001export\u3001fbset\u3001grpconv\u3001grpunconv\u3001hwclock\u3001insmod\u3001kbdconfig\u3001lilo\u3001liloconfig\u3001lsmod\u3001minfo\u3001mkkickstart\u3001modinfo\u3001modprobe\u3001mouseconfig\u3001ntsysv\u3001passwd\u3001pwconv\u3001pwunconv\u3001rdate\u3001resize\u3001rmmod\u3001rpm\u3001set\u3001setconsole\u3001setenv\u3001setup\u3001sndconfig\u3001SVGAText Mode\u3001timeconfig\u3001ulimit\u3001unalias\u3001unset","title":"\u7cfb\u7edf\u8bbe\u7f6e"},{"location":"linux_cmd/#_10","text":"adduser\u3001chfn\u3001chsh\u3001date\u3001exit\u3001finger\u3001free\u3001fwhois\u3001gitps\u3001groupdel\u3001groupmod\u3001halt\u3001id\u3001kill\u3001last\u3001lastb\u3001login\u3001logname\u3001logout\u3001logrotate\u3001newgrp\u3001nice\u3001procinfo\u3001ps\u3001pstree\u3001reboot\u3001renice\u3001rlogin\u3001rsh\u3001rwho\u3001screen\u3001shutdown\u3001sliplogin\u3001su\u3001sudo\u3001suspend\u3001swatch\u3001tload\u3001top\u3001uname\u3001useradd\u3001userconf\u3001userdel\u3001usermod\u3001vlock\u3001w\u3001who\u3001whoami\u3001whois","title":"\u7cfb\u7edf\u7ba1\u7406"},{"location":"linux_cmd/#_11","text":"awk\u3001col\u3001colrm\u3001comm\u3001csplit\u3001ed\u3001egrep\u3001ex\u3001fgrep\u3001fmt\u3001fold\u3001grep\u3001ispell\u3001jed\u3001joe\u3001join\u3001look\u3001mtype\u3001pico\u3001rgrep\u3001sed\u3001sort\u3001spell\u3001tr\u3001uniq\u3001vi\u3001wc","title":"\u6587\u672c\u5904\u7406"},{"location":"linux_cmd/#_12","text":"dip\u3001getty\u3001mingetty\u3001ppp-off\u3001smbd(samba daemon)\u3001telnet\u3001uulog\u3001uustat\u3001uux\u3001cu\u3001dnsconf\u3001efax\u3001httpd\u3001ip\u3001ifconfig\u3001mesg\u3001minicom\u3001nc\u3001netconf\u3001netconfig\u3001netstat\u3001ping\u3001pppstats\u3001samba\u3001setserial\u3001shapecfg(shaper configuration)\u3001smbd(samba daemon)\u3001statserial(status ofserial port)\u3001talk\u3001tcpdump\u3001testparm(test parameter)\u3001traceroute\u3001tty(teletypewriter)\u3001uuname\u3001wall(write all)\u3001write\u3001ytalk\u3001arpwatch\u3001apachectl\u3001smbclient(samba client)\u3001pppsetup","title":"\u7f51\u7edc\u901a\u8baf"},{"location":"linux_cmd/#_13","text":"dumpkeys\u3001loadkeys\u3001MAKEDEV\u3001rdev\u3001setleds","title":"\u8bbe\u5907\u7ba1\u7406"},{"location":"linux_cmd/#_14","text":"archive\u3001ctlinnd\u3001elm\u3001getlist\u3001inncheck\u3001mail\u3001mailconf\u3001mailq\u3001messages\u3001metamail\u3001mutt\u3001nntpget\u3001pine\u3001slrn\u3001X WINDOWS SYSTEM\u3001reconfig\u3001startx(start X Window)\u3001Xconfigurator\u3001XF86Setup\u3001xlsatoms\u3001xlsclients\u3001xlsfonts","title":"\u7535\u5b50\u90ae\u4ef6\u4e0e\u65b0\u95fb\u7ec4"},{"location":"linux_cmd/#_15","text":"yes","title":"\u5176\u4ed6\u547d\u4ee4"},{"location":"linux_cmd/#linux_1","text":"","title":"Linux\u5b66\u4e60\u8d44\u6e90\u6574\u7406"},{"location":"linux_cmd/#_16","text":"Linux\u4e2d\u56fd - \u5404\u79cd\u8d44\u8baf\u3001\u6587\u7ae0\u3001\u6280\u672f \u5b9e\u9a8c\u697c - \u514d\u8d39\u63d0\u4f9b\u4e86Linux\u5728\u7ebf\u73af\u5883\uff0c\u4e0d\u7528\u5728\u81ea\u5df1\u673a\u5b50\u4e0a\u88c5\u7cfb\u7edf\u4e5f\u53ef\u4ee5\u5b66\u4e60Linux\uff0c\u8d85\u65b9\u4fbf\u5b9e\u7528\u3002 \u9e1f\u54e5\u7684linux\u79c1\u623f\u83dc - \u975e\u5e38\u9002\u5408Linux\u5165\u95e8\u521d\u5b66\u8005\u770b\u7684\u6559\u7a0b\u3002 Linux\u516c\u793e - Linux\u76f8\u5173\u7684\u65b0\u95fb\u3001\u6559\u7a0b\u3001\u4e3b\u9898\u3001\u58c1\u7eb8\u90fd\u6709\u3002 Linux Today - Linux\u65b0\u95fb\u8d44\u8baf\u53d1\u5e03\uff0cLinux\u804c\u4e1a\u6280\u672f\u5b66\u4e60\uff01\u3002","title":"\u793e\u533a\u7f51\u7ad9"},{"location":"linux_cmd/#_17","text":"Linux\u601d\u7ef4\u5bfc\u56fe\u6574\u7406 Linux\u521d\u5b66\u8005\u8fdb\u9636\u5b66\u4e60\u8d44\u6e90\u6574\u7406 Linux \u57fa\u7840\u5165\u95e8\uff08\u65b0\u7248\uff09 \u3010\u8bd1\u3011Linux\u6982\u5ff5\u67b6\u6784\u7684\u7406\u89e3 En Linux \u5b88\u62a4\u8fdb\u7a0b\u7684\u542f\u52a8\u65b9\u6cd5 Linux\u7f16\u7a0b\u4e4b\u5185\u5b58\u6620\u5c04 Linux\u77e5\u8bc6\u70b9\u5c0f\u7ed3 10\u5927\u767d\u5e3d\u9ed1\u5ba2\u4e13\u7528\u7684 Linux \u64cd\u4f5c\u7cfb\u7edf","title":"\u77e5\u8bc6\u76f8\u5173"},{"location":"linux_cmd/#_18","text":"\u8d85\u8d5e\u7684Linux\u8f6f\u4ef6 Github\u4ed3\u5e93 Zh En \u7a0b\u5e8f\u5458\u559c\u6b22\u76849\u6b3e\u6700\u4f73\u7684Linux\u6587\u4ef6\u6bd4\u8f83\u5de5\u5177 \u63d0\u9ad8 Linux \u5f00\u53d1\u6548\u7387\u7684 5 \u4e2a\u5de5\u5177 \u4f60\u8981\u4e86\u89e3\u768411\u6b3e\u9762\u5411Linux\u7cfb\u7edf\u7684\u4e00\u6d41\u5907\u4efd\u5b9e\u7528\u5de5\u5177 16\u4e2a\u5f88\u6709\u7528\u7684\u5728\u7ebf\u5de5\u5177 Adobe\u8f6f\u4ef6\u7684\u6700\u4f73\u66ff\u4ee3\u54c1 \u539f\u6587\u5728\u8fd9\u91cc Evince (Adobe Acrobat Reader) \u4e00\u4e2a\u201c\u652f\u6301\u591a\u79cd\u6587\u6863\u683c\u5f0f\u7684\u6587\u6863\u67e5\u770b\u5668\u201d\uff0c\u53ef\u4ee5\u67e5\u770bPDF\uff0c\u8fd8\u652f\u6301\u5404\u79cd\u6f2b\u753b\u4e66\u683c\u5f0f Pixlr (Adobe Photoshop) \u4e00\u4e2a\u5f3a\u5927\u7684\u56fe\u50cf\u7f16\u8f91\u5de5\u5177 Inkscape (Adobe Illustrator) \u4e00\u4e2a\u4e13\u4e1a\u7684\u77e2\u91cf\u56fe\u5f62\u7f16\u8f91\u5668 Pinegrow Web Editor (Adobe Dreamweaver) \u4e00\u4e2a\u53ef\u89c6\u5316\u7f16\u8f91\u5236\u4f5c HTML \u7f51\u7ad9 Scribus (Adobe InDesign) \u4e00\u4e2a\u5f00\u6e90\u7535\u5b50\u6742\u5fd7\u5236\u4f5c\u8f6f\u4ef6 Webflow (Adobe Muse) \u4e00\u6b3e\u53ef\u4ee5\u5e2e\u52a9\u7528\u6237\u4e0d\u7528\u7f16\u7801\u5c31\u53ef\u4ee5\u5feb\u901f\u521b\u5efa\u7f51\u7ad9\u7684\u8c37\u6b4c\u6d4f\u89c8\u5668\u63d2\u4ef6\u3002 Tupi (Adobe Animate) \u4e00\u6b3e\u53ef\u4ee5\u521b\u5efaHTML5\u52a8\u753b\u7684\u5de5\u5177\u3002 Black Magic Fusion (Adobe After Effects) \u4e00\u6b3e\u5148\u8fdb\u7684\u5408\u6210\u8f6f\u4ef6\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u89c6\u89c9\u7279\u6548\u3001\u5e7f\u7535\u5f71\u89c6\u8bbe\u8ba1\u4ee5\u53ca3D\u52a8\u753b\u8bbe\u8ba1\u7b49\u9886\u57df\u3002","title":"\u8f6f\u4ef6\u5de5\u5177"},{"location":"linux_cmd/#_19","text":"\u963f\u91cc\u4e91\u5f00\u6e90\u955c\u50cf\u7ad9\uff1ahttp://mirrors.aliyun.com/ \u7f51\u6613\u5f00\u6e90\u955c\u50cf\u7ad9\uff1ahttp://mirrors.163.com/ \u641c\u72d0\u5f00\u6e90\u955c\u50cf\u7ad9\uff1ahttp://mirrors.sohu.com/ \u5317\u4eac\u4ea4\u901a\u5927\u5b66\uff1ahttp://mirror.bjtu.edu.cn/ \\ \u6559\u80b2\u7f51\u8350> \u5170\u5dde\u5927\u5b66\uff1ahttp://mirror.lzu.edu.cn/ \\ \u897f\u5317\u9ad8\u6821FTP\u641c\u7d22\u5f15\u64ce> \u53a6\u95e8\u5927\u5b66\uff1ahttp://mirrors.xmu.edu.cn/ \u4e0a\u6d77\u4ea4\u901a\u5927\u5b66\uff1ahttp://ftp.sjtu.edu.cn/ \u6e05\u534e\u5927\u5b66\uff1ahttp://mirrors.tuna.tsinghua.edu.cn/ http://mirrors6.tuna.tsinghua.edu.cn/ http://mirrors4.tuna.tsinghua.edu.cn/ \u4e2d\u56fd\u79d1\u5b66\u6280\u672f\u5927\u5b66\uff1ahttp://mirrors.ustc.edu.cn/ http://ipv4.ustc.edu.cn/ \\ \u6559\u80b2\u7f51\u3001\u7535\u4fe1> http://ipv6.ustc.edu.cn/ \\ IPv6 only> \u4e1c\u5317\u5927\u5b66\uff1ahttp://mirror.neu.edu.cn/ \u6d59\u6c5f\u5927\u5b66\uff1ahttp://mirrors.zju.edu.cn/ \u4e1c\u8f6f\u4fe1\u606f\u5b66\u9662\uff1ahttp://mirrors.neusoft.edu.cn/","title":"\u4e2d\u56fd\u5f00\u6e90\u955c\u50cf\u7ad9\u70b9"},{"location":"linux_cmd/#_20","text":"\u9762\u5411\u6e38\u620f\u73a9\u5bb6\u7684\u516b\u6b3e\u6700\u4f73 Linux \u53d1\u884c\u7248\uff0c\u672c\u6587\u7531\u5f00\u6e90\u4e2d\u56fd\u6574\u7406\uff0c \u539f\u6587\u5728\u8fd9\u91cc \u3002 SteamOS \u5b98\u65b9\u6587\u6863 \u955c\u50cf\u4e0b\u8f7d Ubuntu GamePack \u4e0b\u8f7d\u5730\u5740 Fedora \u2013 Games Spin \u4e0b\u8f7d\u5730\u5740 SparkyLinux \u2013 GameOver Edition \u4e0b\u8f7d\u5730\u5740 Lakka \u4e0b\u8f7d\u5730\u5740 Game Drift Linux \u4e0b\u8f7d\u5730\u5740 Solus \u4e0b\u8f7d\u5730\u5740 Manjaro Gaming Edition (mGAMe) \u4e0b\u8f7d\u5730\u5740","title":"\u6e38\u620f\u73a9\u5bb6\u53d1\u884c\u7248"},{"location":"linux_cmd/#team","text":"\u5c0f\u5f1f\u8c03\u8c03\u2122 ZhuangZhu-74 Huck Huang","title":"Team"},{"location":"machine_learning/","text":"Marchine Learning Table of Contents Doc Random Forest Gradient boosting Dimensionality reduction Clustering benchmark Neural Network Deep Learning Optimization Doc Very Brief Introduction to Machine Learning for AI Markov Chains What are the advantages of different classification algorithms? VisuAlgo - visualising data structures and algorithms through animation https://oj.leetcode.com/problems/ My recommendations \u2013 SlideShare Presentations on Data Science \u673a\u5668\u5b66\u4e60\u5e38\u89c1\u7b97\u6cd5\u5206\u7c7b\u6c47\u603b A Tour of Machine Learning Algorithms | \u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e4b\u65c5 dive-into-machine-learning Machine Learning Roadmap: Your Self-Study Guide to Machine Learning \u5982\u4f55\u9009\u62e9\u673a\u5668\u5b66\u4e60\u7b97\u6cd5 \u673a\u5668\u5b66\u4e60\u7cfb\u5217(4)_\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e00\u89c8\uff0c\u5e94\u7528\u5efa\u8bae\u4e0e\u89e3\u51b3\u601d\u8def Detecting diabetic retinopathy in eye images Cheatsheet \u2013 Python R codes for common Machine Learning Algorithms 10 \u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u8981\u70b9\uff08\u9644 Python \u548c R \u4ee3\u7801\uff09 | Essentials of Machine Learning Algorithms (with Python and R Codes) useR-machine-learning-tutorial What is feature engineering? Learning \u770b\u673a\u5668\u5b66\u4e60\u8bba\u6587\u65f6\uff0c\u770b\u4e0d\u61c2\u6570\u5b66\u516c\u5f0f\u600e\u4e48\u529e\uff1f 5 Techniques To Understand Machine Learning Algorithms Without the Background in Mathematics machine-learning-for-software-engineers What If I Am Not Good At Mathematics Courses \u673a\u5668\u5b66\u4e60\u901f\u6210\u8bfe\u7a0b from Google \u6df1\u5ea6\u5b66\u4e60\u5165\u95e8\u7cfb\u5217 - \u7f51\u6613\u4e91\u8bfe\u5802 \u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\u7b2c\u4e00\u5b63\u8bfe\u7a0b\u6c47\u603b Random Forest Powerful Guide to learn Random Forest (with codes in R Python) , Tuning the parameters of your Random Forest model awesome-random-forest A Complete Tutorial on Tree Based Modeling from Scratch (in R Python) gradient boosting CatBoost - CatBoost is an open-source gradient boosting on decision trees library with categorical features support out of the box for Python, R https://catboost.yandex LightGBM - A fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It is under the umbrella of the DMTK(http://github.com/microsoft/dmtk) project of Microsoft. XGboost - An optimized general purpose gradient boosting library. The library is parallelized, and also provides an optimized distributed version. A Guide to Gradient Boosted Trees with XGBoost in Python CatBoost vs. Light GBM vs. XGBoost , \u5165\u95e8 | \u4ece\u7ed3\u6784\u5230\u6027\u80fd\uff0c\u4e00\u6587\u6982\u8ff0XGBoost\u3001Light GBM\u548cCatBoost\u7684\u540c\u4e0e\u4e0d\u540c Dimensionality reduction Comprehensive Guide on t-SNE algorithm with implementation in R Python Clustering annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk pysparnn - Approximate Nearest Neighbor Search for Sparse Data in Python! benchmark benchm-ml - A minimal benchmark for scalability, speed and accuracy of commonly used open source implementations (R packages, Python scikit-learn, H2O, xgboost, Spark MLlib etc.) of the top machine learning algorithms for binary classification (random forests, gradient boosted trees, deep neural networks etc.). Neural Network Learning How To Code Neural Networks A Neural Network in 11 lines of Python (Part 1) [\u4eba\u4eba\u90fd\u80fd\u7528Python\u5199\u51faLSTM-RNN\u7684\u4ee3\u7801\uff01[\u4f60\u7684\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6700\u4f73\u8d77\u6b65]]\u3002 English Deep Learning DeepLearning.TV \u6df1\u5ea6\u5b66\u4e60\u7b80\u660e\u4ecb\u7ecd A Deep Learning Tutorial: From Perceptrons to Deep Networks | \u6df1\u5ea6\u5b66\u4e60\u6982\u8ff0\uff1a\u4ece\u611f\u77e5\u673a\u5230\u6df1\u5ea6\u7f51\u7edc UFLDL Tutorial Deep Learning Tutorials \u673a\u5668\u5b66\u4e60\u524d\u6cbf\u70ed\u70b9\u2013Deep Learning 7 Steps for becoming Deep Learning Expert \u7ffb\u8bd1 Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei) packages Evaluation of Deep Learning Frameworks Deep Machine Learning libraries and frameworks Deep Learning Libraries by Language mxnet - Lightweight, Portable, Flexible Distributed Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, and more \u5229\u7528Theano\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u2014\u2014Logistic Regression \uff0c \u5229\u7528Theano\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u2014\u2014Multilayer Perceptron \uff0c \u5229\u7528Theano\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u2014\u2014Convolutional Neural Networks hyperas - Keras + Hyperopt: A very simple wrapper for convenient hyperparameter optimization http://maxpumperla.github.io/hyperas/ TensorFlow TensorFlow\u5165\u95e8\uff1a\u7ebf\u6027\u56de\u5f52 Optimization Spearmint - Spearmint Bayesian optimization codebase","title":"ML"},{"location":"machine_learning/#marchine-learning","text":"Table of Contents Doc Random Forest Gradient boosting Dimensionality reduction Clustering benchmark Neural Network Deep Learning Optimization","title":"Marchine Learning"},{"location":"machine_learning/#doc","text":"Very Brief Introduction to Machine Learning for AI Markov Chains What are the advantages of different classification algorithms? VisuAlgo - visualising data structures and algorithms through animation https://oj.leetcode.com/problems/ My recommendations \u2013 SlideShare Presentations on Data Science \u673a\u5668\u5b66\u4e60\u5e38\u89c1\u7b97\u6cd5\u5206\u7c7b\u6c47\u603b A Tour of Machine Learning Algorithms | \u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e4b\u65c5 dive-into-machine-learning Machine Learning Roadmap: Your Self-Study Guide to Machine Learning \u5982\u4f55\u9009\u62e9\u673a\u5668\u5b66\u4e60\u7b97\u6cd5 \u673a\u5668\u5b66\u4e60\u7cfb\u5217(4)_\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4e00\u89c8\uff0c\u5e94\u7528\u5efa\u8bae\u4e0e\u89e3\u51b3\u601d\u8def Detecting diabetic retinopathy in eye images Cheatsheet \u2013 Python R codes for common Machine Learning Algorithms 10 \u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u8981\u70b9\uff08\u9644 Python \u548c R \u4ee3\u7801\uff09 | Essentials of Machine Learning Algorithms (with Python and R Codes) useR-machine-learning-tutorial What is feature engineering? Learning \u770b\u673a\u5668\u5b66\u4e60\u8bba\u6587\u65f6\uff0c\u770b\u4e0d\u61c2\u6570\u5b66\u516c\u5f0f\u600e\u4e48\u529e\uff1f 5 Techniques To Understand Machine Learning Algorithms Without the Background in Mathematics machine-learning-for-software-engineers What If I Am Not Good At Mathematics","title":"Doc"},{"location":"machine_learning/#courses","text":"\u673a\u5668\u5b66\u4e60\u901f\u6210\u8bfe\u7a0b from Google \u6df1\u5ea6\u5b66\u4e60\u5165\u95e8\u7cfb\u5217 - \u7f51\u6613\u4e91\u8bfe\u5802 \u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\u7b2c\u4e00\u5b63\u8bfe\u7a0b\u6c47\u603b","title":"Courses"},{"location":"machine_learning/#random-forest","text":"Powerful Guide to learn Random Forest (with codes in R Python) , Tuning the parameters of your Random Forest model awesome-random-forest A Complete Tutorial on Tree Based Modeling from Scratch (in R Python)","title":"Random Forest"},{"location":"machine_learning/#gradient-boosting","text":"CatBoost - CatBoost is an open-source gradient boosting on decision trees library with categorical features support out of the box for Python, R https://catboost.yandex LightGBM - A fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It is under the umbrella of the DMTK(http://github.com/microsoft/dmtk) project of Microsoft. XGboost - An optimized general purpose gradient boosting library. The library is parallelized, and also provides an optimized distributed version. A Guide to Gradient Boosted Trees with XGBoost in Python CatBoost vs. Light GBM vs. XGBoost , \u5165\u95e8 | \u4ece\u7ed3\u6784\u5230\u6027\u80fd\uff0c\u4e00\u6587\u6982\u8ff0XGBoost\u3001Light GBM\u548cCatBoost\u7684\u540c\u4e0e\u4e0d\u540c","title":"gradient boosting"},{"location":"machine_learning/#dimensionality-reduction","text":"Comprehensive Guide on t-SNE algorithm with implementation in R Python","title":"Dimensionality reduction"},{"location":"machine_learning/#clustering","text":"annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk pysparnn - Approximate Nearest Neighbor Search for Sparse Data in Python!","title":"Clustering"},{"location":"machine_learning/#benchmark","text":"benchm-ml - A minimal benchmark for scalability, speed and accuracy of commonly used open source implementations (R packages, Python scikit-learn, H2O, xgboost, Spark MLlib etc.) of the top machine learning algorithms for binary classification (random forests, gradient boosted trees, deep neural networks etc.).","title":"benchmark"},{"location":"machine_learning/#neural-network","text":"Learning How To Code Neural Networks A Neural Network in 11 lines of Python (Part 1) [\u4eba\u4eba\u90fd\u80fd\u7528Python\u5199\u51faLSTM-RNN\u7684\u4ee3\u7801\uff01[\u4f60\u7684\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u6700\u4f73\u8d77\u6b65]]\u3002 English","title":"Neural Network"},{"location":"machine_learning/#deep-learning","text":"DeepLearning.TV \u6df1\u5ea6\u5b66\u4e60\u7b80\u660e\u4ecb\u7ecd A Deep Learning Tutorial: From Perceptrons to Deep Networks | \u6df1\u5ea6\u5b66\u4e60\u6982\u8ff0\uff1a\u4ece\u611f\u77e5\u673a\u5230\u6df1\u5ea6\u7f51\u7edc UFLDL Tutorial Deep Learning Tutorials \u673a\u5668\u5b66\u4e60\u524d\u6cbf\u70ed\u70b9\u2013Deep Learning 7 Steps for becoming Deep Learning Expert \u7ffb\u8bd1 Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei) packages Evaluation of Deep Learning Frameworks Deep Machine Learning libraries and frameworks Deep Learning Libraries by Language mxnet - Lightweight, Portable, Flexible Distributed Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Go, and more \u5229\u7528Theano\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u2014\u2014Logistic Regression \uff0c \u5229\u7528Theano\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u2014\u2014Multilayer Perceptron \uff0c \u5229\u7528Theano\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u2014\u2014Convolutional Neural Networks hyperas - Keras + Hyperopt: A very simple wrapper for convenient hyperparameter optimization http://maxpumperla.github.io/hyperas/ TensorFlow TensorFlow\u5165\u95e8\uff1a\u7ebf\u6027\u56de\u5f52","title":"Deep Learning"},{"location":"machine_learning/#optimization","text":"Spearmint - Spearmint Bayesian optimization codebase","title":"Optimization"},{"location":"macos_app/","text":"Awesome macOS open source applications Support Hey friend! Help me out for a couple of :beers:! List of awesome open source applications for macOS. This list contains a lot of native, and cross-platform apps. The main goal of this repository is to find free open source apps and start contributing. Feel free to contribute to the list, any suggestions are welcome! Languages You can see in which language an app is written. Currently there are following languages: ![c_icon] - C language. ![cpp_icon] - C++ language. ![c_sharp_icon] - C# language. ![clojure_icon] - Clojure language. ![coffee_script_icon] - CoffeeScript language. ![css_icon] - CSS language. ![go_icon] - Go language. ![elm_icon] - Elm language. ![haskell_icon] - Haskell language. ![javascript_icon] - JavaScript language. ![lua_icon] - Lua language. ![objective_c_icon] - Objective-C language. ![python_icon] - Python language. ![ruby_icon] - Ruby language. ![rust_icon] - Rust language. ![swift_icon] - Swift language. ![type_script_icon] - TypeScript language. Contents Audio Backup Browser Chat Cryptocurrency Database Development Git iOS / macOS JSON Parsing Web development Other development Downloader Editors CSV JSON Markdown TeX Text Extensions Finder Games Graphics IDE Images Keyboard Mail Menubar Music News Notes Other Podcast Productivity Screensaver Security Sharing Files Social Networking Streaming System Terminal Touch Bar Utilities VPN Proxy Video Wallpaper Window Management Applications Audio AUHost - Application which hosts AudioUnits v3 using AVFoundation API. ![swift_icon] Aural Player - Aural Player is a audio player application for the macOS platform. Inspired by the classic Winamp player for Windows, it is designed to be to-the-point and easy to use. ![swift_icon] AutoMute - Automatically mute the sound when headphones disconnect / Mac awake from sleep. ![objective_c_icon] Background Music - Background Music, a macOS audio utility: automatically pause your music, set individual apps' volumes and record system audio. ![cpp_icon] BlackHole - BlackHole is a modern macOS virtual audio driver that allows applications to pass audio to other applications with zero additional latency. ![c_icon] CAM - macOS camera recording using ffmpeg ![javascript_icon] Clementine - Clementine is a modern music player and library organizer for Windows, Linux and macOS. ![cpp_icon] Cog - Cog is an open source audio player for macOS. The basic layout is a single-paned playlist interface with two retractable drawers, one for navigating the user's music folders and another for viewing audio file properties, like bitrate. ![objective_c_icon] LocalRadio - LocalRadio is software for listening to \"Software-Defined Radio\" on your Mac and mobile devices. ![objective_c_icon] Lyricism - macOS app to show you lyric what currently iTunes or Spotify is playing. ![objective_c_icon] ![swift_icon] LyricsX - Lyrics for iTunes, Spotify and Vox. ![swift_icon] Mous Player - Simple yet powerful audio player for BSD/Linux/macOS. ![cpp_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] ShazamScrobbler - Scrobble vinyl, radios, movies to Last.fm. ![objective_c_icon] Sonora - Minimal, beautifully designed music player for macOS. ![objective_c_icon] SpotMenu - Spotify and iTunes in your menu bar. ![objective_c_icon] ![swift_icon] SpotSpot - Spotify mini-player for macOS. ![javascript_icon] Suohai - Audio input/output source lock for macOS. ![swift_icon] Tickeys - Instant audio feedback for typing. macOS version. ![rust_icon] [Un]MuteMic - macOS app to mute unmute the input volume of your microphone. Perfect for podcasters. ![objective_c_icon] ![c_icon] eqMac2 - System-Wide Equalizer for the Mac. ![cpp_icon] fre:ac - The fre:ac audio converter project. ![cpp_icon] iTunes-Volume-Control - This app allows you to control the iTunes volume using volume up and volume down hotkeys. ![objective_c_icon] jmc - jmc is new macOS media organizer. ![swift_icon] shairport-sync - macOS/Linux/FreeBSD/OpenBSD Airplay audio receiver. ![c_icon] ![cpp_icon] waveSDR - macOS native desktop Software Defined Radio application using the RTL-SDR USB device. ![swift_icon] Backup Mackup - Keep your application settings in sync (macOS/Linux). ![python_icon] UrBackup - UrBackup is Client/Server network backup for Windows, macOS and Linux. ![cpp_icon] ![c_icon] shallow-backup - Easily create lightweight documentation of installed applications, dotfiles, and more. ![python_icon] Browser Beaker Browser - Beaker is an experimental peer-to-peer Web browser. ![javascript_icon] Brave Browser - Brave browser for Desktop and Laptop computers running Windows, macOS, and Linux. ![javascript_icon] Finicky - Always opens the right browser. ![swift_icon] Helium - Floating browser window for macOS. ![objective_c_icon] ![swift_icon] Kaktus Browser - Experimental web browser with minimalistic design. Running Windows, macOS and Linux. ![javascript_icon] Pennywise - Pennywise opens any website or media in a small floating window that remains on top of all other applications. It's a great alternative to Helium. ![javascript_icon] Plash - Make any website your desktop wallpaper. ![swift_icon] browserosaurus - macOS tool that prompts you to choose a browser when opening a link. ![javascript_icon] otter-browser - Otter Browser aims to recreate the best aspects of the classic Opera (12.x) UI using Qt5. ![cpp_icon] seb-mac - Safe Exam Browser for macOS and iOS. ![c_icon] Chat Beagle IM - Powerful XMPP client with support for file transfer, VoIP and end-to-end encryption. ![swift_icon] ChitChat - Native Mac app wrapper for WhatsApp Web. ![objective_c_icon] Electronic WeChat - Better WeChat on macOS and Linux. ![javascript_icon] Franz - Franz is messaging application for services like WhatsApp, Slack, Messenger and many more. ![javascript_icon] Google Allo for Desktop - Native macOS Windows desktop app for Google Allo. ![javascript_icon] GroupMe - Unofficial GroupMe App. ![javascript_icon] ![css_icon] MessagesHistoryBrowser - macOS application to comfortably browse and search through your Messages.app history. ![swift_icon] Riot.im - Riot.im is a collaboration app (currently Electron) for the Matrix protocol. ![javascript_icon] Seaglass - A truly native Matrix client for macOS. ![swift_icon] Signal Desktop - Electron app that links with your Signal Android or Signal iOS app. ![javascript_icon] Telegram - Source code of Telegram for macOS on Swift. ![swift_icon] Telegram Desktop - Telegram Desktop messaging app. ![cpp_icon] Textual - Textual is an IRC client for macOS. ![objective_c_icon] Torchat-Mac - TorChat for Mac is a macOS native and unofficial port of torchat. ![objective_c_icon] WhatsAppBar - Send WhatsApp message from menu bar. ![swift_icon] Wire Desktop - Standalone Electron app for the chatapp Wire. ![javascript_icon] Cryptocurrency Balance Open - App for all the world\u2019s currencies. ![swift_icon] CoinBar - macOS menu bar application for tracking crypto coin prices. ![swift_icon] Copay - A secure bitcoin wallet platform for both desktop and mobile devices. ![type_script_icon] Crypto Bar - macOS menu bar application built with Electron. ![javascript_icon] Float coin - Native menu bar app with floating window and support for many Exchanges. ![swift_icon] Database DB Browser for SQLite - SQLite database management GUI. ![cpp_icon] DBeaver - Universal database tool and SQL client. ![java_icon] Medis - \ud83d\udcbb Medis is a beautiful, easy-to-use Mac database management application for Redis. ![javascript_icon] MongoHub - Add another lightweight Mac Native MongoDB client. ![objective_c_icon] ![c_icon] Postbird - PostgreSQL GUI client for macOS. ![javascript_icon] Postgres.app - The easiest way to get started with PostgreSQL on the Mac. ![swift_icon] Redis.app - The easiest way to get started with Redis on the Mac. ![swift_icon] Robo 3T - Robo 3T (formerly Robomongo) is the free lightweight GUI for MongoDB enthusiasts. ![cpp_icon] Sequel Pro - MySQL/MariaDB database management for macOS. ![objective_c_icon] mongoDB.app - The easiest way to get started with mongoDB on the Mac. ![swift_icon] Development Git Cashew - Cashew macOS Github Issue Tracker. ![objective_c_icon] ![c_icon] GPM - macOS application for easily operating GitHub Projects. ![swift_icon] Git Interactive Rebase Tool - Full feature terminal based sequence editor for interactive rebase. ![rust_icon] GitHub Desktop - Simple collaboration from your desktop. ![type_script_icon] GitSync - Minimalistic Git client for Mac. ![swift_icon] GitUp - The Git interface you've been missing all your life has finally arrived. ![objective_c_icon] GitX - Graphical client for the git version control system. ![objective_c_icon] Gitee - Gitee, macOS status bar application for Github. ![objective_c_icon] ![swift_icon] Github contributions - GitHub contributions app, for iOS, WatchOS, and macOS. ![swift_icon] GithubListener - Simple app that will notify about new commits to watched repositories. ![swift_icon] GithubNotify - Simple macOS app to alert you when you have unread GitHub notifications. ![swift_icon] Streaker - GitHub contribution streak tracking menubar app. ![javascript_icon] TeamStatus-for-GitHub - macOS status bar application for tracking code review process within the team. ![swift_icon] Trailer - Managing Pull Requests and Issues For GitHub GitHub Enterprise. ![swift_icon] Xit - Xit is a graphical tool for working with git repositories. ![swift_icon] JSON Parsing JSON Mapper - Simple macOS app to generate Swift Object Mapper classes from JSON. ![swift_icon] JSONExport - Desktop application for macOS which enables you to export JSON objects as model classes with their associated constructors, utility methods, setters and getters in your favorite language. ![swift_icon] j2s - macOS app to convert JSON objects into Swift structs (currently targets Swift 4 and Codable). ![swift_icon] Other Development ChefInspector - Node and Attribute viewer for Chef ![swift_icon] MQTTX - An elegant Cross-platform MQTT 5.0 desktop client. ![javascript_icon] ![type_script_icon] macho-browser - Browser for macOS Mach-O binaries. ![objective_c_icon] vegvisir - Browser based GUI for LLDB Debugger. ![javascript_icon] Web Development CoreOS VM - CoreOS VM is macOS status bar app which allows in an easy way to control CoreOS VM on your Mac. ![objective_c_icon] Corectl App for macOS - Corectl App is a macOS Status bar App which works like a wrapper around the corectl command line tool corectld to control the server runtime process. ![swift_icon] HTTP Toolkit - HTTP Toolkit is a cross-platform tool to intercept, debug mock HTTP. ![type_script_icon] Insomnia - Insomnia is a cross-platform REST client, built on top of Electron. ![javascript_icon] KubeMonitor - KubeMonitor is a macOS app that displays information about your active Kubernetes cluster in your menu bar. ![swift_icon] KubeSwitch - KubeSwitch lists the available kubernetes cluster contexts on the mac, in Mac's Menu bar. ![swift_icon] Lantern - Dedicated Mac app for website auditing and crawling. ![swift_icon] LocalSites - Simple Menu Bar (Status Bar) App for macOS listing local Bonjour websites (as Safari 11 no longer has Bonjour Bookmarks). ![swift_icon] Mockup Generator - Mockup Generator is a macOS app built with AngularJS/Electron that sits in your menu bar allowing you to capture screenshots of your favourite websites and wrap them in device mock-ups. ![javascript_icon] Now Desktop - Create deployments right from the tray menu. ![javascript_icon] aws-s3-uploader - Simple macOS app for uploading files to Amazon Web Services. ![javascript_icon] iTunesConnect - macOS app to let you access iTunesConnect. ![swift_icon] ndm - Npm desktop GUI. ![javascript_icon] nodeScratchpad - Evaluate Nodejs/JS code snippets from Menubar. ![swift_icon] stts - macOS app for monitoring the status of cloud services. ![swift_icon] iOS / macOS AVXCAssets Generator - Takes path for your assets images and creates appiconset and imageset for you in just one click. ![swift_icon] AppBox - Tool for iOS developers to build and deploy Development, Ad-Hoc and In-house (Enterprise) applications directly to the devices from your Dropbox account. ![objective_c_icon] AppIcons - Tool for generating icons in all sizes as required by macOS and iOS apps. ![swift_icon] AppStoreReviewTimes - Gives you indication about the average iOS / macOS app stores review times. ![swift_icon] AppleTrace - Trace tool for iOS/macOS. ![objective_c_icon] Asset Catalog Tinkerer - App that lets you open .car files and browse/extract their images. ![objective_c_icon] ![swift_icon] Assets - Assets is a macOS app that manages assets for your development projects (Xcode, web, etc). ![swift_icon] Attabench - Attabench is a microbenchmarking app for macOS, designed to measure and visualize the performance of Swift code. ![swift_icon] Board For GitHub - Small application to monitor your GitHub project web page in a native macOS app :octocat:! ![objective_c_icon] Brisk - macOS app for submitting radars. ![swift_icon] Cleaner for Xcode - Cleaner for Xcode.app built with react-native-macOS. ![objective_c_icon] CocoaRestClient - Native Apple macOS app for testing HTTP/REST endpoints. ![objective_c_icon] FilterShop - macOS App to explore CoreImage Filters. ![swift_icon] IconGenerator - macOS app to generate app icons. ![javascript_icon] Iconizer - Create Xcode image catalogs (xcassets) on the fly. ![swift_icon] Icons.app - App for macOS which is designed to generate consistent sized icons of an existing application in various states, jiggling (shaking) etc. ![objective_c_icon] InjectionIII - overdue Swift rewrite of Injection. ![objective_c_icon] ![swift_icon] Knuff - The debug application for Apple Push Notification Service (APNs). ![objective_c_icon] LayerX - Intuitive app to display transparent images on screen. ![swift_icon] Localizable.strings - Mac app to localize your iOS and macOS projects. ![swift_icon] Localization Editor - Simple macOS editor app to help you manage iOS app localizations by allowing you to edit all the translations side by side. ![swift_icon] Localizations - Localizations is an macOS app that manages your Xcode project localization files (.strings). ![swift_icon] Menubar Colors - macOS app for convenient access to the system color panel. ![swift_icon] Notarize - Notarization status monitoring tool for macOS, supporting multiple developer accounts ![swift_icon] PodsUpdater - macOS app which helps you manage dependency releases in your Podfile. ![swift_icon] ProfilesManager - Apple iOS/macOS Provisioning Profiles management,.provisionprofile, .mobileprovision files manager tool for mac. ![objective_c_icon] PushNotifications - macOS app to test push notifications on iOS and Android. ![javascript_icon] ResignTool - This is an app for macOS that can (re)sign apps and bundle them into ipa files that are ready to be installed on an iOS device. ![objective_c_icon] Resizr - MacOS application for creating AppIcon for iOS and Android apps. ![swift_icon] SmartPush - iOS Push Notification Debug App. ![objective_c_icon] TransporterPad - iOS/Android app deployment tool for macOS. ![swift_icon] WWDC - Unofficial WWDC app for macOS. ![swift_icon] WWDC.srt - Powerful app for downloading subtitle for each WWDC session video since 2013 in (srt) format. ![swift_icon] calabash-launcher - iOS Calabash Launcher is a macOS app that helps you run and manage Calabash tests on your Mac. ![swift_icon] iOS Images Extractor - iOS Images Extractor is a Mac app to normalize, decode, and extract images from iOS apps. ![objective_c_icon] iSimulator - iSimulator is a GUI utility to control the Simulator and manage the app installed on the simulator. ![objective_c_icon] xib2Storyboard - Tool to convert Xcode .xib to .storyboard files. ![objective_c_icon] Downloader App Downloader - Easily search and download macOS apps from the huge homebrew cask app catalog. ![swift_icon] Get It - Native macOS video/audio downloader. Think of it as a youtube downloader that works on many sites. ![swift_icon] Motrix - A full-featured download manager. ![javascript_icon] Pillager - macOS Video Downloader written in Swift and Objective-C. ![objective_c_icon] ![swift_icon] YouTube Downloader for macOS - Simple menu bar app to download YouTube movies on your Mac. I wrote this as a test project to learn more about app development on macOS. ![swift_icon] udemy-downloader-gui - desktop application for downloading Udemy Courses. ![javascript_icon] Editors CSV TableTool - simple CSV editor for the macOS. ![objective_c_icon] JSON JSON-Splora - GUI for editing, visualizing, and manipulating JSON data. ![javascript_icon] Markdown Gingko - Tree-structured markdown editor for macOS, Windows, and Linux. ![elm_icon] MacDown - Markdown editor for macOS. ![objective_c_icon] Mark Text - Realtime preview markdown editor for macOS Windows and Linux. ![javascript_icon] Pine - A modern MacOS markdown editor. ![swift_icon] QOwnNotes - Plain-text file notepad and todo-list manager with markdown support and ownCloud / Nextcloud integration. ![cpp_icon] TeX Qilin Editor - Text editor for exact sciences with built-in KaTeX/AsciiMath support. ![javascript_icon] Text CotEditor - Lightweight Plain-Text Editor for macOS. ![swift_icon] MacVim - Text editor for macOS. ![c_icon] Noto - Plain text editor for macOS with customizable themes. ![swift_icon] SubEthaEdit - General purpose plain text editor for macOS. Widely known for its live collaboration feature. ![objective_c_icon] TextMate - TextMate is a graphical text editor for macOS. ![objective_c_icon] VimR - Refined Neovim experience for macOS. ![swift_icon] Extensions BetterPiP - Use native picture-in-picture with browsers such as Google Chrome for HTML5 videos. ![swift_icon] Middleclick - Emulate a scroll wheel click with three finger Click or Tap on MacBook trackpad and Magic Mouse ![c_icon] PageExtender - Extend pages with your own CSS and JS files. ![swift_icon] ![javascript_icon] PiPTool - Add the Picture-in-Picture Functionality to YouTube, Netflix, Plex and other video broadcasting services in macOS. ![javascript_icon] PiPifier - PiPifier is a native macOS 10.12 Safari extension that lets you use every HTML5 video in Picture in Picture mode. ![swift_icon] Sessions - Safari extension to save your working sessions ![swift_icon] nef - This Xcode extension enables you to make a code selection and export it to a snippets. Available on Mac AppStore. ![swift_icon] Finder Clipy - Clipy is a Clipboard extension app for macOS. ![swift_icon] CopyQ - Clipboard manager with advanced features ![cpp_icon] FiScript - Execute custom scripts from the MacOS context menu (CTRL+click) in Finder. ![swift_icon] Finder Go - macOS app and Finder Sync Extension to open Terminal, iTerm, Hyper from Finder. ![swift_icon] OpenInCode - Finder toolbar app to open current folder in Visual Studio Code. ![objective_c_icon] OpenInTerminal - \u2728 Finder Toolbar app for macOS to open the current directory in Terminal, iTerm or Hyper. ![swift_icon] Quick Look plugins - List of useful Quick Look plugins for developers. ![objective_c_icon] ![c_icon] cd to... - Finder Toolbar app to open the current directory in the Terminal ![objective_c_icon] Games Battle for Wesnoth - Turn-based tactical strategy game, featuring both single-player and online multiplayer combat. ![cpp_icon] Boxer - The DOS game emulator that\u2019s fit for your Mac. ![cpp_icon] ![objective_c_icon] Dolphin - Powerful emulator for Nintendo GameCube and Wii games. ![cpp_icon] OpenEmu - Retro video game emulation for macOS. ![objective_c_icon] OpenRCT2 - Re-implementation of RollerCoaster Tycoon 2. ![cpp_icon] Screentendo - Turn your screen into a playable level of Mario. ![objective_c_icon] Stockfish - Beautiful, powerful chess application. ![cpp_icon] ![objective_c_icon] Graphics Aseprite - Animated sprite editor pixel art tool (Windows, macOS, Linux). ![cpp_icon] ![c_icon] CaptuocrToy - Tool to capture screenshot and recognize text by online ocr apis. ![swift_icon] GifCapture - Gif capture app for macOS. ![swift_icon] Gifcurry - Video to GIF maker with a graphical interface capable of cropping, adding text, seeking, and trimming. ![haskell_icon] Gifski - Convert videos to high-quality GIFs. ![swift_icon] InfiniteCanvas - Proof of concept Mac drawing application. ![swift_icon] Material Colors Native - Choose your Material colours and copy the hex code. ![objective_c_icon] Pencil2D Animation - Pencil2D is an animation/drawing software for macOS, Windows, and Linux. It lets you create traditional hand-drawn animation (cartoon) using both bitmap and vector graphics. ![cpp_icon] ScreenToLayers for macOS - ScreenToLayers is a macOS application to easily capture your screen as a layered PSD file. ![objective_c_icon] ![css_icon] macSVG - macOS application for designing HTML5 SVG (Scalable Vector Graphics) art and animation with a WebKit web view. ![objective_c_icon] IDE Atom - The hackable text editor. ![javascript_icon] LiveCode - Cross-platform development IDE. ![c_icon] Oni - Oni is a modern take on modal editing code editor focused on developer productivity. ![javascript_icon] ![type_script_icon] Visual Studio Code - Code editor developed by Microsoft. ![type_script_icon] ZeroBraneStudio - ZeroBrane Studio is a lightweight cross-platform Lua IDE with code completion, syntax highlighting, remote debugger, code analyzer, live coding, and debugging support for various Lua engines. ![lua_icon] Images APNGb - macOS app which assembles and disassembles animated png files. ![swift_icon] Crunch - Insane(ly slow but wicked good) PNG image optimization. ![python_icon] ExifCleaner - Remove image metadata with drag and drop, multi-core batch processing, and dark mode. ![javascript_icon] Freehand - macOS Status Bar App for quick sketch. ![swift_icon] Gimp - Gimp is GNU Image Manipulation Program. ![c_icon] Iconology - Edit Icons and then Export to Xcode, Icns, Ico, Favicon, Mac Iconset, or a Custom List of Sizes. ![swift_icon] ImageAlpha - Mac GUI for pngquant, pngnq and posterizer. ![objective_c_icon] ![python_icon] Imagine - Imagine is a desktop app for compression of PNG and JPEG, with a modern and friendly UI. ![type_script_icon] InVesalius - 3D medical imaging reconstruction software ![python_icon] Katana - Katana is a simple screenshot utility for macOS that lives in your menubar. ![javascript_icon] ![css_icon] PhotoMiner - macOS app for finding and lost forgotten photos on your disks. ![swift_icon] Screenbar - macOS menubar app for automating screenshots. ![swift_icon] WebPonize - WebPonize is a macOS App for converting PNG, JPEG, animated (or not) GIF images into WebP. ![swift_icon] ![c_icon] Keyboard AnnePro-mac - macOS application for controlling AnnePro keyboard over bluetooth. ![swift_icon] Fluor - Handy tool for macOS allowing you to switch Fn keys' mode based on active application. ![swift_icon] GokuRakuJoudo - Karabiner-Elements configuration manager, rescue to bloated karabiner.json ![clojure_icon] Karabiner - Karabiner (KeyRemap4MacBook) is a powerful utility for keyboard customization. ![cpp_icon] ![objective_c_icon] Karabiner-Elements - Karabiner-Elements is a powerful utility for keyboard customization on macOS Sierra (10.12) or later. ![cpp_icon] ![objective_c_icon] Kawa - Better input source switcher for macOS. ![swift_icon] Thor - Switch the right application ASAP. ![swift_icon] Unshaky - A software attempt to address the \"double key press\" issue on Apple's butterfly keyboard ![swift_icon] Mail Correo - Menubar/taskbar Gmail App for Windows and macOS. ![javascript_icon] ElectronMail - Unofficial desktop app for ProtonMail and Tutanota end-to-end encrypted email providers. ![type_script_icon] Mailspring - \ud83d\udc8c A beautiful, fast and maintained fork of @nylas Mail by one of the original authors ![javascript_icon] Rambox - Cross Platform messaging and emailing app that combines common web applications into one. ![javascript_icon] ![css_icon] dejalu - Fast and Simple Email Client. ![cpp_icon] ![objective_c_icon] Medical InVesalius - 3D medical imaging reconstruction software ![python_icon] Menubar Airpass - Status bar Mac application to overcome time constrained WiFi networks. ![javascript_icon] AnyBar - macOS menubar status indicator. ![objective_c_icon] BitBar - Put the output from any script or program in your macOS Menu Bar. ![objective_c_icon] CloudyTabs - Simple menu bar macOS application for displaying lists of your iCloud Tabs and Reading List. ![swift_icon] DatWeatherDoe - Simple menu bar weather app for macOS written in Swift. ![swift_icon] DisplayMenu - Simple (bare-bones) macOS menubar extra to apply display presets. ![swift_icon] Dozer - Hide MacOS menubar items. ![swift_icon] KubeContext - import, manage and switch between your Kubernetes contexts on Mac. ![swift_icon] MenuMeters - CPU, memory, disk, and network monitoring tools for macOS. ![objective_c_icon] Menubar Brightness - macOS app to change the screen brightness on the menubar. ![javascript_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] Night Shift Control - Night Shift Control is a simple macOS menubar app for controlling Night Shift. It's aim is to bring features from f.lux which are missing from Night Shift such as disabling Night Shift for certain apps. ![swift_icon] PSIBar - Quickly hacked up PSI macOS status bar app. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] Quickeys - A mac menu bar app that provides note taking functionality though a quick dropdown menu. ![swift_icon] SensibleSideButtons - Small menu bar utility that lets you use your third-party mouse's side buttons for navigation across a variety of apps. ![objective_c_icon] ![c_icon] Shifty - macOS menu bar app that gives you more control over Night Shift. ![swift_icon] gSwitch - macOS status bar app that allows control over the gpu on dual gpu macbooks. ![swift_icon] iGlance - macOS System Monitor (cpu, memory, network, fan and battery) for the Status Bar. ![swift_icon] Music Audacity - Free, open source, cross-platform audio software ![c_icon] Carol - A minimal and beautiful lyrics app that stays in the menu bar of macOS. ![c_sharp_icon] ChordDetector - Tiny menu bar app that listens iTunes and Spotify to detect chords of songs! ![swift_icon] DeezPlayer - Deezer Desktop app for Windows, Linux and macOS. ![coffee_script_icon] Lilypond UI - Create beautiful musical scores with LilyPond. ![javascript_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] SoundCleod - SoundCloud for macOS and Windows. ![javascript_icon] Spotify-Cli-Mac - Control Spotify without leaving your terminal. :notes: ![javascript_icon] YouTube-Music - macOS wrapper for music.youtube.com. ![swift_icon] iTunes Graphs - macOS app to visualise your iTunes library as graphs. ![swift_icon] lyricsify - Simple Spotify lyrics viewer menu bar app for macOS in Swift. ![swift_icon] News Diurna - Basic/Classic Hacker News app, used as a Cocoa Swift learning platform. ![swift_icon] NetNewsWire - Feed reader for macOS. ![swift_icon] Vienna - Vienna is a RSS/Atom newsreader for macOS. ![objective_c_icon] hacker-menu - Hacker News Delivered to Desktop. ![javascript_icon] Notes Boostnote - Note-taking application made for programmers just like you. ![javascript_icon] Dnote - A simple command line notebook with multi-device sync and web interface. ![go_icon] ![type_script_icon] FSNotes - Notes manager for macOS/iOS: modern notational velocity (nvALT) on steroids. ![swift_icon] FromScratch - Little app that you can use as a quick note taking or todo app. ![javascript_icon] ![css_icon] Jupyter Notebook Viewer - Notebook viewer for macOS. ![swift_icon] NoteTaker - Simple note taking app for macOS and iOS which uses Realm and CloudKit for syncing. ![swift_icon] Notes - Notes is a macOS application built to create notes, using text amongst other formats: images, videos, contacts, and etc. ![swift_icon] QOwnNotes - Plain-text file notepad and todo-list manager with markdown support and ownCloud / Nextcloud integration. ![cpp_icon] Simplenote - Simplest way to keep notes. ![objective_c_icon] Standard Notes - Safe place for your notes, thoughts, and life's work. ![javascript_icon] ![css_icon] Tusk - Unofficial, third-party, community driven Evernote app with a handful of useful features. ![javascript_icon] ![css_icon] joplin - Note taking and to-do application with synchronization capabilities for Windows, macOS, Linux, Android and iOS. ![javascript_icon] notable - Simple note taking application. ![javascript_icon] tmpNote - Very simple macOS app to make temporary notes. ![swift_icon] Other Cakebrew - Manage your Homebrew formulas with style using Cakebrew. ![objective_c_icon] DevDocs for macOS - An unofficial DevDocs API documentation viewer. ![swift_icon] Gas Mask - Hosts file manager for macOS. ![objective_c_icon] Hosts - Cocoa GUI for /etc/hosts. ![objective_c_icon] ImageOptim - GUI image optimizer for Mac. ![objective_c_icon] Keyframes Player - Simple macOS app to preview animations created with Facebook's keyframes framework. ![swift_icon] Lepton - Democratizing Code Snippets Management (macOS/Win/Linux). ![javascript_icon] Letters - Teach your kids the alphabet and how to type. ![swift_icon] Platypus - Mac developer tool that creates application bundles from command line scripts. ![objective_c_icon] QorumLogs - Swift Logging Utility for Xcode Google Docs. ![swift_icon] React Native Debugger - Desktop app for inspecting your React Native projects. macOS, Linux, and Windows. ![javascript_icon] Reactotron - Desktop app for inspecting your React JS and React Native projects. macOS, Linux, and Windows. ![javascript_icon] RktMachine - Menu bar macOS app for running rkt in a macOS hypervisor CoreOS VM. ![swift_icon] Ruby.app - macOS app that contains a full Ruby installation (for use with Ruby/Gosu). ![ruby_icon] Shuttle - Simple SSH shortcut menu for macOS. ![objective_c_icon] SwiftyBeaver - Convenient logging during development release in Swift. ![swift_icon] Unused - Mac app for checking Xcode projects for unused resources. ![objective_c_icon] Vagrant Manager - Manage your vagrant machines in one place with Vagrant Manager for macOS. ![objective_c_icon] macGist - Simple app to send pasteboard items to GitHub's Gist. ![swift_icon] syncthing-macosx - Frugal nativemacOS macOS Syncthing application bundle. ![objective_c_icon] Player IINA - The modern video player for macOS. ![swift_icon] MPlayerX - Media player on macOS. ![objective_c_icon] MacMorpheus - 3D 180/360 video player for macOS for PSVR with head tracking. ![objective_c_icon] Movie Monad - Desktop video player built with Haskell that uses GStreamer and GTK+. ![haskell_icon] mpv - Video player based on MPlayer/mplayer2. ![c_icon] Podcast Cumulonimbus - Simple, beautiful podcast app. ![javascript_icon] Doughnut - Podcast player and library for mac ![swift_icon] PodcastMenu - PodcastMenu is a simple app which puts Overcast on your Mac's menu bar so you can listen to your favorite podcasts while you work. ![swift_icon] Podlive for macOS - macOS client to listen to live streaming podcasts (only). It currently supports all livestreams broadcasting via Ultraschall with Studio Link On Air . ![objective_c_icon] mkchromecast - Cast macOS and Linux Audio/Video to your Google Cast and Sonos Devices. ![python_icon] Productivity Ao - Elegant Microsoft To-Do desktop app. ![javascript_icon] ![css_icon] Calculeta - Calculator for macOS which working on statusbar. ![swift_icon] Cerebro - Cross-platform launcher app. ![javascript_icon] ClipMenu - Clipboard manager for macOS. ![objective_c_icon] Clocker - macOS app to plan and organize through timezones. ![objective_c_icon] ControlPlane - Automate running tasks based on where you are or what you do. ![objective_c_icon] Flycut - Clean and simple clipboard manager for developers. ![objective_c_icon] KeyHolder - Record shortcuts in macOS, like Alfred.app. ![swift_icon] Kiwix - Kiwix for iOS and macOS, build on Swift. ![swift_icon] Linked Ideas - macOS application to write down and connect ideas. ![swift_icon] Maccy - Lightweight search-as-you-type clipboard manager. ![swift_icon] Manta - Flexible invoicing desktop app with beautiful customizable templates. ![javascript_icon] Middleclick - Emulate a scroll wheel click with three finger Click or Tap on MacBook trackpad and Magic Mouse ![c_icon] PDF Archiver - Nice tool for tagging and archiving tasks. ![swift_icon] Paperless Desktop - Desktop app that uses the paperless API to manage your document scans. ![javascript_icon] Pennywise - Pennywise opens any website or media in a small floating window that remains on top of all other applications. It's a great alternative to Helium. ![javascript_icon] Pomodoro Cycle - Pomodoro Cycle for macOS ![type_script_icon] QOwnNotes - Plain-text file notepad and todo-list manager with markdown support and ownCloud / Nextcloud integration. ![cpp_icon] Quicksilver - Quicksilver is a fast macOS productivity application that gives you the power to control your Mac quickly and elegantly. ![objective_c_icon] Quickwords - Write anything in a matter of seconds. Create snippets that can substitute text, execute tedious tasks and more. ![javascript_icon] ![css_icon] SelfControl - macOS app to block your own access to distracting websites etc for a predetermined period of time. It can not be undone by the app or by a restart \u2013 you must wait for the timer to run out. ![objective_c_icon] Sessions - Safari extension to save your working sessions ![swift_icon] Speed Reader - Read faster with the power of silencing vocalization with SpeedReader. ![swift_icon] StickyNotes - A Windows 10-esque Sticky Notes app implemented in AppKit. ![swift_icon] Strategr - No-fuss time management. ![cpp_icon] ![objective_c_icon] Super Productivity - Free to do list time tracker for programmers designers with Jira integration. ![type_script_icon] ![javascript_icon] Thyme - The task timer for OS X. ![objective_c_icon] Timer - Simple Timer app for Mac. ![swift_icon] Toggl Desktop - Toggl Desktop app for Windows, Mac and Linux. ![cpp_icon] TomatoBar - Pomodoro Technique Timer for macOS with Touch Bar support. ![swift_icon] TrelloApp - Unofficial wrapper application for Trello.com written in Swift. This is almost a \"Hello World\" for a site specific browser. ![swift_icon] Watson - A CLI application for time tracking. ![python_icon] Whale - Unofficial Trello app. ![javascript_icon] Yomu - Manga reader app for macOS. ![swift_icon] espanso - Cross-platform Text Expander, a powerful replacement for Alfred Snippets ![rust_icon] macOrganizer - macOS app for organizing files or removing unnecessary files. ![swift_icon] status-bar-todo - Simple macOS app to keep TODO-list in status bar. ![swift_icon] stretchly - Cross-platform electron app that reminds you to take breaks when working with computer. ![javascript_icon] Screensaver Aerial - Apple TV Aerial Screensaver for macOS. ![swift_icon] Image-As-Wallpaper - Utility application helps with selection of images for using as desktop wallpaper or in screensaver on Mac computers. ![swift_icon] Irvue - Screensaver for macOS. ![objective_c_icon] Life Saver - An abstract screensaver based on Conway's Game of Life implemented with SpriteKit ![swift_icon] MusaicFM - iTunes Screensaver Clone for Spotify and Last.fm ![objective_c_icon] Predator - A predator-inspired clock screensaver for macOS ![swift_icon] The GitHub Matrix Screensaver - The GitHub Matrix Screensaver for macOS. ![javascript_icon] Security Cloaker - simple drag-and-drop, password-based file encryption. ![rust_icon] Cryptomator - Multi-platform transparent client-side encryption of your files in the cloud. ![java_icon] LuLu - LuLu is macOS firewall application that aims to block unauthorized (outgoing) network traffic. ![objective_c_icon] Swifty - Free and offline password manager. ![javascript_icon] stronghold - Easily configure macOS security settings from the terminal. ![python_icon] Sharing Files Deluge - Lightweight cross-platform BitTorrent client. ![python_icon] NitroShare - Transferring files from one device to another ![cpp_icon] Rhea - macOS status bar app for quickly sharing files and URLs. ![objective_c_icon] Transmission - Official Transmission BitTorrent client repository. ![objective_c_icon] ![c_icon] Tribler - Privacy enhanced BitTorrent client with P2P content discovery. ![python_icon] mac2imgur - Simple Mac app designed to make uploading images and screenshots to Imgur quick and effortless. ![swift_icon] qBittorrent - BitTorrent client in Qt. ![cpp_icon] Social Networking Caprine - Elegant Facebook Messenger desktop app. ![javascript_icon] ![css_icon] Goofy - Unofficial Facebook Messenger client. ![javascript_icon] Leviathan - Leviathan is a iOS and macOS client application for the Mastodon social network. ![swift_icon] Messenger - macOS app wrapping Facebook's Messenger for desktop. ![objective_c_icon] Product Hunt - share and discover your favorite new products and applications. ![swift_icon] Quail - Unofficial esa app. ![javascript_icon] Ramme - Unofficial Instagram Desktop App. ![javascript_icon] ![css_icon] Simpo - macOS menubar app to post status quickly. ![ruby_icon] appear.in - Unofficial appear.in app. ![javascript_icon] Streaming Galeri - Perpetual artwork streaming app. ![javascript_icon] OBS Studio - Free and open source software for live streaming and screen recording. ![cpp_icon] System AppPolice - App for macOS with a minimalistic UI which lets you quickly throttle down the CPU usage of any running process. ![objective_c_icon] Apple Juice - Advanced battery gauge for macOS. ![swift_icon] Clean-Me - Small macOS app that acts as a system cleaner (logs, cache, ...). ![swift_icon] Diagnostics - Diagnostics is an application displaying the diagnostic reports from applications on macOS. ![swift_icon] DisableMonitor - Easily disable or enable a monitor on your Mac. ![objective_c_icon] EtreCheck - EtreCheck is an easy-to-use macOS app to display important details of your system configuration and allow you to copy that information to the Clipboard. ![objective_c_icon] Fanny - Monitor your Mac's fan speed and CPU temperature from your Notification Center. ![objective_c_icon] HoRNDIS - Android USB tethering driver for macOS. ![cpp_icon] Juice - Make your battery information a bit more interesting. ![swift_icon] KeepingYouAwake - Prevents your Mac from going to sleep. ![objective_c_icon] Latest - Small utility app for macOS that makes sure you know about all the latest updates to the apps you use. ![swift_icon] Loading - Simple network activity monitor for macOS. ![objective_c_icon] Overkill - Stop iTunes from opening when you connect your iPhone. ![swift_icon] ProfileCreator - macOS Application to create standard or customized configuration profiles. ![objective_c_icon] Sloth - Sloth is an macOS application that displays a list of all open files and sockets in use by all running applications on your system. ![objective_c_icon] Turbo Boost Switcher - Turbo Boost Switcher is a little application for Mac computers that allows to enable and/or disable the Turbo Boost feature. ![objective_c_icon] VerticalBar - macOS application to add a vertical bar to Dock. ![swift_icon] macOSLucidaGrande - A small utility to set Lucida Grande as your Mac's system UI font. ![objective_c_icon] Terminal Alacritty - Cross-platform, GPU-accelerated terminal emulator. ![rust_icon] Bifrost - A tiny terminal emulator for serial port communication (macOS/Linux). ![go_icon] Console - macOS console application. ![swift_icon] Finder Go - macOS app and Finder Sync Extension to open Terminal, iTerm, Hyper from Finder. ![swift_icon] Hyper - Terminal built on web technologies. ![javascript_icon] ![css_icon] Kitty - Cross-platform, fast, feature full, GPU based terminal emulator. ![python_icon] ![c_icon] OpenInTerminal - \u2728 Finder Toolbar app for macOS to open the current directory in Terminal, iTerm or Hyper. ![swift_icon] OpenTerminal - App for macOS that opens a new Finder window and changes the current directory to the folder launched by the app. ![swift_icon] Upterm - A terminal emulator for the 21st century (formerly Black Screen). ![javascript_icon] cd to... - Finder Toolbar app to open the current directory in the Terminal ![objective_c_icon] iTerm 2 - Terminal emulator for macOS that does amazing things. ![objective_c_icon] wallpapper - wallpapper is a console application for creating dynamic wallpapers for Mojave. ![swift_icon] Touch Bar Muse - Spotify controller with TouchBar support. ![swift_icon] MyTouchbarMyRules - App to customize your Touch Bar as you want. ![swift_icon] Pock - Display macOS Dock in Touch Bar. ![swift_icon] Touch Bar Preview - Small application to display your designs on the Touch Bar of the new MacBook Pro. ![swift_icon] Touch Bar Simulator - Use the Touch Bar on any Mac. ![swift_icon] Touch Emoji - Emoji picker for MacBook Pro Touch Bar. ![swift_icon] Utilities Android tool for Mac - One-click screenshots, video recordings, app installation for iOS and Android ![swift_icon] ArchiveMounter - Mounts archives like disk images. ![swift_icon] BeardedSpice - Control web based media players with the media keys found on Mac keyboards. ![objective_c_icon] Bitwarden - Cross-platform password management solutions for individuals, teams, and business organizations. ![type_script_icon] Buttercup Desktop - Secure password manager for mac and other platforms. ![javascript_icon] Calculeta - Calculator for macOS which working on statusbar. ![swift_icon] Catch - Catch: Broadcatching made easy. ![swift_icon] Clear Clipboard Text Format - Easily clear the format of your clipboard text with Clear Clipboard Text Format. ![objective_c_icon] CornerCal - Simple, clean calendar and clock app for macOS. ![swift_icon] Crypter - Crypter is an innovative, convenient and secure cross-platform crypto app that simplifies secure password generation and management by requiring you to only remember one bit, the MasterPass. ![javascript_icon] ECheck - Small tool to validate epub files for macOS. ![swift_icon] Flying Carpet - cross-platform file transfer over ad-hoc wifi, like AirDrop but for Mac/Windows/Linux. ![go_icon] Funky - Easily toggle the function key on your Mac on a per app basis. ![objective_c_icon] Gray - Pick between the light appearance and the dark appearance on a per-app basis with the click of a button ![swift_icon] Kap - Screen recorder application built with web technology. ![javascript_icon] KeePassXC - Cross-platform community-driven port of the Windows application \"Keepass Password Safe\" ![cpp_icon] KeeWeb - Cross-platform password manager compatible with KeePass. ![javascript_icon] Kyapchar - Simple screen and microphone audio recorder for macOS. ![swift_icon] Life-Calendar - Life Calendar. ![swift_icon] Lunar - Intelligent adaptive brightness for your external displays. ![swift_icon] MQTTX - An elegant Cross-platform MQTT 5.0 desktop client. ![javascript_icon] ![type_script_icon] MacPass - Native macOS KeePass client. ![objective_c_icon] Maria - macOS native app/widget for aria2 download tool. ![swift_icon] Meme Maker - Meme Maker macOS application for meme creation. ![swift_icon] Middleclick - Emulate a scroll wheel click with three finger Click or Tap on MacBook trackpad and Magic Mouse ![c_icon] Monolingual - Remove unnecessary language resources from macOS ![swift_icon] Mos - Smooth your mouse's scrolling and reverse the mouse scroll direction ![swift_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] Noti - Receive Android notifications on your mac (with PushBullet). ![swift_icon] Numi - A handy calculator with natural language parsing. ![javascript_icon] PB for Desktop - Receive native push notifications on macOS, Windows and Linux. ![javascript_icon] Padlock - A minimal, open source password manager for macOS. ![javascript_icon] PercentCalculator - A menu bar application that calculates parcents. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] PowerShell - PowerShell is a cross-platform automation and configuration tool/framework that works well with your existing tools. ![c_sharp_icon] ScreenCat - ScreenCat is a screen sharing + remote collaboration application. ![javascript_icon] ![css_icon] SlowQuitApps - Add a global delay to Command-Q to stop accidental app quits. ![objective_c_icon] Super Productivity - Free to do list time tracker for programmers designers with Jira integration. ![type_script_icon] ![javascript_icon] Telephone - SIP softphone for macOS. ![objective_c_icon] ![swift_icon] The Blockstack Browser - Blockstack is an internet for decentralized apps where users own their data. The Blockstack Browser allows you to explore the Blockstack internet. ![javascript_icon] The Unarchiver - The Unarchiver is an Objective-C application for uncompressing archive files. ![objective_c_icon] ToTheTop - Small macOS application to help you scroll to the top. ![swift_icon] calibre - cross platform e-book manager. ![python_icon] fselect - Command-line tool to search files with SQL syntax. ![rust_icon] homebrew-cask - A CLI workflow for the administration of macOS applications distributed as binaries ![ruby_icon] iOScanX - Cocoa application for semi-automated iOS app analysis and evaluation. ![objective_c_icon] ![c_icon] mac-sound-fix - Mac Sound Re-Enabler. ![swift_icon] wechsel - manage bluetooth connections with your keyboard. ![swift_icon] \u00dcbersicht - Keep an eye on what's happening on your machine and in the world. ![objective_c_icon] VPN Proxy ShadowsocksX-NG - Next Generation of ShadowsocksX. ![swift_icon] Specht - Rule-based proxy app built with Network Extension for macOS. ![swift_icon] SpechtLite - Rule-based proxy app for macOS. ![swift_icon] Tunnelblick - Tunnelblick is a graphic user interface for OpenVPN on macOS. ![objective_c_icon] clashX - A rule based custom proxy with GUI for Mac base on clash. ![swift_icon] rvc-mac - Ribose VPN Client macOS Menu App. ![swift_icon] Video Acid.Cam.v2.OSX - Acid Cam v2 for macOS distorts video to create art. ![cpp_icon] AppleEvents - Unofficial Apple Events app for macOS. ![objective_c_icon] Conferences.digital - Best way to watch the latest and greatest videos from your favourite developer conferences for free on your Mac. ![swift_icon] Datamosh - Datamosh your videos on macOS. ![swift_icon] Face Data - macOS application used to auto-annotate landmarks from a video. ![swift_icon] GNU Gatekeeper - Video conferencing server for H.323 terminals. ![cpp_icon] Gifted - Turn any short video into an animated GIF quickly and easily. ![objective_c_icon] HandBrake - HandBrake is a video transcoder available for Linux, Mac, and Windows. ![c_icon] MenuTube - Catch YouTube into your macOS menu bar! ![javascript_icon] OpenShot - Easy to use, quick to learn, and surprisingly powerful video editor. ![python_icon] Quick Caption - Transcribe and generate caption files (SRT, ASS and FCPXML) without manually entering time codes. ![swift_icon] QuickLook Video - This package allows macOS Finder to display thumbnails, static QuickLook previews, cover art and metadata for most types of video files. ![objective_c_icon] Subler - Subler is an macOS app created to mux and tag mp4 files. ![objective_c_icon] VLC - VLC is a free and open source cross-platform multimedia player ![c_icon] Vid Quiz Creator - macOS application to insert quizzes within video playback and play those videos to receiving devices using the LISNR API. ![swift_icon] WebTorrent Desktop - Streaming torrent app. For Mac, Windows, and Linux. ![javascript_icon] Yoda - Nifty macOS application which enables you to browse and download videos from YouTube. ![javascript_icon] Wallpaper 500-mac-wallpaper - Simple macOS app for the status bar to automatically download photos from 500px.com to a local folder that can be set as a source of wallpapers. ![swift_icon] ArtWall - ArtStation set as wallpapers from artwork.rss . ![objective_c_icon] Artify - A macOS application for bringing dedicatedly 18th century Arts to everyone ![swift_icon] BingPaper - Use Bing daily photo as your wallpaper on macOS. ![swift_icon] Desktop Wallpaper Switcher - Win / Linux / macOS tool for managing and cycling desktop wallpapers. ![cpp_icon] Muzei - Muzei wallpaper app for macOS. ![swift_icon] Plash - Make any website your desktop wallpaper. ![swift_icon] Satellite Eyes - macOS app to automatically set your desktop wallpaper to the satellite view overhead. ![objective_c_icon] Sunscreen - Sunscreen is a fun, lightweight application that changes your desktop wallpaper based on sunrise and sunset. ![swift_icon] WallpaperMenu - macOS menubar application for navigation through beautiful pictures on the web and set them up as your desktop image. ![ruby_icon] earth-wallpapers - Simple macOS menubar app which fetches latest photos from a subreddit. ![javascript_icon] pyDailyChanger - pyDailyChanger is a program that changes your wallpaper daily. ![python_icon] Window Management Amethyst - Automatic tiling window manager for macOS. ![swift_icon] AppGrid - Grid-based keyboard window manager for macOS. ![objective_c_icon] Desktop Profiles - An innovative desktop/window manager for macOS ![swift_icon] Hammerspoon - Staggeringly powerful macOS desktop automation with Lua. ![lua_icon] ![objective_c_icon] Phoenix - Lightweight macOS window and app manager scriptable with JavaScript. ![objective_c_icon] Rectangle - Rectangle is a window manager heavily based on Spectacle, written in Swift. ![swift_icon] ShiftIt - Managing windows size and position. ![objective_c_icon] Slate - Window management application (replacement for Divvy/SizeUp/ShiftIt). ![objective_c_icon] Spectacle - Spectacle allows you to organize your windows without using a mouse. ![objective_c_icon] chunkwm - Tiling window manager for macOS that uses a plugin architecture. ![cpp_icon] Contributors Thanks to all the people who contribute:","title":"Macos_app"},{"location":"macos_app/#awesome-macos-open-source-applications","text":"","title":"Awesome macOS open source applications"},{"location":"macos_app/#support","text":"Hey friend! Help me out for a couple of :beers:! List of awesome open source applications for macOS. This list contains a lot of native, and cross-platform apps. The main goal of this repository is to find free open source apps and start contributing. Feel free to contribute to the list, any suggestions are welcome!","title":"Support"},{"location":"macos_app/#languages","text":"You can see in which language an app is written. Currently there are following languages: ![c_icon] - C language. ![cpp_icon] - C++ language. ![c_sharp_icon] - C# language. ![clojure_icon] - Clojure language. ![coffee_script_icon] - CoffeeScript language. ![css_icon] - CSS language. ![go_icon] - Go language. ![elm_icon] - Elm language. ![haskell_icon] - Haskell language. ![javascript_icon] - JavaScript language. ![lua_icon] - Lua language. ![objective_c_icon] - Objective-C language. ![python_icon] - Python language. ![ruby_icon] - Ruby language. ![rust_icon] - Rust language. ![swift_icon] - Swift language. ![type_script_icon] - TypeScript language.","title":"Languages"},{"location":"macos_app/#contents","text":"Audio Backup Browser Chat Cryptocurrency Database Development Git iOS / macOS JSON Parsing Web development Other development Downloader Editors CSV JSON Markdown TeX Text Extensions Finder Games Graphics IDE Images Keyboard Mail Menubar Music News Notes Other Podcast Productivity Screensaver Security Sharing Files Social Networking Streaming System Terminal Touch Bar Utilities VPN Proxy Video Wallpaper Window Management","title":"Contents"},{"location":"macos_app/#applications","text":"","title":"Applications"},{"location":"macos_app/#audio","text":"AUHost - Application which hosts AudioUnits v3 using AVFoundation API. ![swift_icon] Aural Player - Aural Player is a audio player application for the macOS platform. Inspired by the classic Winamp player for Windows, it is designed to be to-the-point and easy to use. ![swift_icon] AutoMute - Automatically mute the sound when headphones disconnect / Mac awake from sleep. ![objective_c_icon] Background Music - Background Music, a macOS audio utility: automatically pause your music, set individual apps' volumes and record system audio. ![cpp_icon] BlackHole - BlackHole is a modern macOS virtual audio driver that allows applications to pass audio to other applications with zero additional latency. ![c_icon] CAM - macOS camera recording using ffmpeg ![javascript_icon] Clementine - Clementine is a modern music player and library organizer for Windows, Linux and macOS. ![cpp_icon] Cog - Cog is an open source audio player for macOS. The basic layout is a single-paned playlist interface with two retractable drawers, one for navigating the user's music folders and another for viewing audio file properties, like bitrate. ![objective_c_icon] LocalRadio - LocalRadio is software for listening to \"Software-Defined Radio\" on your Mac and mobile devices. ![objective_c_icon] Lyricism - macOS app to show you lyric what currently iTunes or Spotify is playing. ![objective_c_icon] ![swift_icon] LyricsX - Lyrics for iTunes, Spotify and Vox. ![swift_icon] Mous Player - Simple yet powerful audio player for BSD/Linux/macOS. ![cpp_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] ShazamScrobbler - Scrobble vinyl, radios, movies to Last.fm. ![objective_c_icon] Sonora - Minimal, beautifully designed music player for macOS. ![objective_c_icon] SpotMenu - Spotify and iTunes in your menu bar. ![objective_c_icon] ![swift_icon] SpotSpot - Spotify mini-player for macOS. ![javascript_icon] Suohai - Audio input/output source lock for macOS. ![swift_icon] Tickeys - Instant audio feedback for typing. macOS version. ![rust_icon] [Un]MuteMic - macOS app to mute unmute the input volume of your microphone. Perfect for podcasters. ![objective_c_icon] ![c_icon] eqMac2 - System-Wide Equalizer for the Mac. ![cpp_icon] fre:ac - The fre:ac audio converter project. ![cpp_icon] iTunes-Volume-Control - This app allows you to control the iTunes volume using volume up and volume down hotkeys. ![objective_c_icon] jmc - jmc is new macOS media organizer. ![swift_icon] shairport-sync - macOS/Linux/FreeBSD/OpenBSD Airplay audio receiver. ![c_icon] ![cpp_icon] waveSDR - macOS native desktop Software Defined Radio application using the RTL-SDR USB device. ![swift_icon]","title":"Audio"},{"location":"macos_app/#backup","text":"Mackup - Keep your application settings in sync (macOS/Linux). ![python_icon] UrBackup - UrBackup is Client/Server network backup for Windows, macOS and Linux. ![cpp_icon] ![c_icon] shallow-backup - Easily create lightweight documentation of installed applications, dotfiles, and more. ![python_icon]","title":"Backup"},{"location":"macos_app/#browser","text":"Beaker Browser - Beaker is an experimental peer-to-peer Web browser. ![javascript_icon] Brave Browser - Brave browser for Desktop and Laptop computers running Windows, macOS, and Linux. ![javascript_icon] Finicky - Always opens the right browser. ![swift_icon] Helium - Floating browser window for macOS. ![objective_c_icon] ![swift_icon] Kaktus Browser - Experimental web browser with minimalistic design. Running Windows, macOS and Linux. ![javascript_icon] Pennywise - Pennywise opens any website or media in a small floating window that remains on top of all other applications. It's a great alternative to Helium. ![javascript_icon] Plash - Make any website your desktop wallpaper. ![swift_icon] browserosaurus - macOS tool that prompts you to choose a browser when opening a link. ![javascript_icon] otter-browser - Otter Browser aims to recreate the best aspects of the classic Opera (12.x) UI using Qt5. ![cpp_icon] seb-mac - Safe Exam Browser for macOS and iOS. ![c_icon]","title":"Browser"},{"location":"macos_app/#chat","text":"Beagle IM - Powerful XMPP client with support for file transfer, VoIP and end-to-end encryption. ![swift_icon] ChitChat - Native Mac app wrapper for WhatsApp Web. ![objective_c_icon] Electronic WeChat - Better WeChat on macOS and Linux. ![javascript_icon] Franz - Franz is messaging application for services like WhatsApp, Slack, Messenger and many more. ![javascript_icon] Google Allo for Desktop - Native macOS Windows desktop app for Google Allo. ![javascript_icon] GroupMe - Unofficial GroupMe App. ![javascript_icon] ![css_icon] MessagesHistoryBrowser - macOS application to comfortably browse and search through your Messages.app history. ![swift_icon] Riot.im - Riot.im is a collaboration app (currently Electron) for the Matrix protocol. ![javascript_icon] Seaglass - A truly native Matrix client for macOS. ![swift_icon] Signal Desktop - Electron app that links with your Signal Android or Signal iOS app. ![javascript_icon] Telegram - Source code of Telegram for macOS on Swift. ![swift_icon] Telegram Desktop - Telegram Desktop messaging app. ![cpp_icon] Textual - Textual is an IRC client for macOS. ![objective_c_icon] Torchat-Mac - TorChat for Mac is a macOS native and unofficial port of torchat. ![objective_c_icon] WhatsAppBar - Send WhatsApp message from menu bar. ![swift_icon] Wire Desktop - Standalone Electron app for the chatapp Wire. ![javascript_icon]","title":"Chat"},{"location":"macos_app/#cryptocurrency","text":"Balance Open - App for all the world\u2019s currencies. ![swift_icon] CoinBar - macOS menu bar application for tracking crypto coin prices. ![swift_icon] Copay - A secure bitcoin wallet platform for both desktop and mobile devices. ![type_script_icon] Crypto Bar - macOS menu bar application built with Electron. ![javascript_icon] Float coin - Native menu bar app with floating window and support for many Exchanges. ![swift_icon]","title":"Cryptocurrency"},{"location":"macos_app/#database","text":"DB Browser for SQLite - SQLite database management GUI. ![cpp_icon] DBeaver - Universal database tool and SQL client. ![java_icon] Medis - \ud83d\udcbb Medis is a beautiful, easy-to-use Mac database management application for Redis. ![javascript_icon] MongoHub - Add another lightweight Mac Native MongoDB client. ![objective_c_icon] ![c_icon] Postbird - PostgreSQL GUI client for macOS. ![javascript_icon] Postgres.app - The easiest way to get started with PostgreSQL on the Mac. ![swift_icon] Redis.app - The easiest way to get started with Redis on the Mac. ![swift_icon] Robo 3T - Robo 3T (formerly Robomongo) is the free lightweight GUI for MongoDB enthusiasts. ![cpp_icon] Sequel Pro - MySQL/MariaDB database management for macOS. ![objective_c_icon] mongoDB.app - The easiest way to get started with mongoDB on the Mac. ![swift_icon]","title":"Database"},{"location":"macos_app/#development","text":"","title":"Development"},{"location":"macos_app/#git","text":"Cashew - Cashew macOS Github Issue Tracker. ![objective_c_icon] ![c_icon] GPM - macOS application for easily operating GitHub Projects. ![swift_icon] Git Interactive Rebase Tool - Full feature terminal based sequence editor for interactive rebase. ![rust_icon] GitHub Desktop - Simple collaboration from your desktop. ![type_script_icon] GitSync - Minimalistic Git client for Mac. ![swift_icon] GitUp - The Git interface you've been missing all your life has finally arrived. ![objective_c_icon] GitX - Graphical client for the git version control system. ![objective_c_icon] Gitee - Gitee, macOS status bar application for Github. ![objective_c_icon] ![swift_icon] Github contributions - GitHub contributions app, for iOS, WatchOS, and macOS. ![swift_icon] GithubListener - Simple app that will notify about new commits to watched repositories. ![swift_icon] GithubNotify - Simple macOS app to alert you when you have unread GitHub notifications. ![swift_icon] Streaker - GitHub contribution streak tracking menubar app. ![javascript_icon] TeamStatus-for-GitHub - macOS status bar application for tracking code review process within the team. ![swift_icon] Trailer - Managing Pull Requests and Issues For GitHub GitHub Enterprise. ![swift_icon] Xit - Xit is a graphical tool for working with git repositories. ![swift_icon]","title":"Git"},{"location":"macos_app/#json-parsing","text":"JSON Mapper - Simple macOS app to generate Swift Object Mapper classes from JSON. ![swift_icon] JSONExport - Desktop application for macOS which enables you to export JSON objects as model classes with their associated constructors, utility methods, setters and getters in your favorite language. ![swift_icon] j2s - macOS app to convert JSON objects into Swift structs (currently targets Swift 4 and Codable). ![swift_icon]","title":"JSON Parsing"},{"location":"macos_app/#other-development","text":"ChefInspector - Node and Attribute viewer for Chef ![swift_icon] MQTTX - An elegant Cross-platform MQTT 5.0 desktop client. ![javascript_icon] ![type_script_icon] macho-browser - Browser for macOS Mach-O binaries. ![objective_c_icon] vegvisir - Browser based GUI for LLDB Debugger. ![javascript_icon]","title":"Other Development"},{"location":"macos_app/#web-development","text":"CoreOS VM - CoreOS VM is macOS status bar app which allows in an easy way to control CoreOS VM on your Mac. ![objective_c_icon] Corectl App for macOS - Corectl App is a macOS Status bar App which works like a wrapper around the corectl command line tool corectld to control the server runtime process. ![swift_icon] HTTP Toolkit - HTTP Toolkit is a cross-platform tool to intercept, debug mock HTTP. ![type_script_icon] Insomnia - Insomnia is a cross-platform REST client, built on top of Electron. ![javascript_icon] KubeMonitor - KubeMonitor is a macOS app that displays information about your active Kubernetes cluster in your menu bar. ![swift_icon] KubeSwitch - KubeSwitch lists the available kubernetes cluster contexts on the mac, in Mac's Menu bar. ![swift_icon] Lantern - Dedicated Mac app for website auditing and crawling. ![swift_icon] LocalSites - Simple Menu Bar (Status Bar) App for macOS listing local Bonjour websites (as Safari 11 no longer has Bonjour Bookmarks). ![swift_icon] Mockup Generator - Mockup Generator is a macOS app built with AngularJS/Electron that sits in your menu bar allowing you to capture screenshots of your favourite websites and wrap them in device mock-ups. ![javascript_icon] Now Desktop - Create deployments right from the tray menu. ![javascript_icon] aws-s3-uploader - Simple macOS app for uploading files to Amazon Web Services. ![javascript_icon] iTunesConnect - macOS app to let you access iTunesConnect. ![swift_icon] ndm - Npm desktop GUI. ![javascript_icon] nodeScratchpad - Evaluate Nodejs/JS code snippets from Menubar. ![swift_icon] stts - macOS app for monitoring the status of cloud services. ![swift_icon]","title":"Web Development"},{"location":"macos_app/#ios-macos","text":"AVXCAssets Generator - Takes path for your assets images and creates appiconset and imageset for you in just one click. ![swift_icon] AppBox - Tool for iOS developers to build and deploy Development, Ad-Hoc and In-house (Enterprise) applications directly to the devices from your Dropbox account. ![objective_c_icon] AppIcons - Tool for generating icons in all sizes as required by macOS and iOS apps. ![swift_icon] AppStoreReviewTimes - Gives you indication about the average iOS / macOS app stores review times. ![swift_icon] AppleTrace - Trace tool for iOS/macOS. ![objective_c_icon] Asset Catalog Tinkerer - App that lets you open .car files and browse/extract their images. ![objective_c_icon] ![swift_icon] Assets - Assets is a macOS app that manages assets for your development projects (Xcode, web, etc). ![swift_icon] Attabench - Attabench is a microbenchmarking app for macOS, designed to measure and visualize the performance of Swift code. ![swift_icon] Board For GitHub - Small application to monitor your GitHub project web page in a native macOS app :octocat:! ![objective_c_icon] Brisk - macOS app for submitting radars. ![swift_icon] Cleaner for Xcode - Cleaner for Xcode.app built with react-native-macOS. ![objective_c_icon] CocoaRestClient - Native Apple macOS app for testing HTTP/REST endpoints. ![objective_c_icon] FilterShop - macOS App to explore CoreImage Filters. ![swift_icon] IconGenerator - macOS app to generate app icons. ![javascript_icon] Iconizer - Create Xcode image catalogs (xcassets) on the fly. ![swift_icon] Icons.app - App for macOS which is designed to generate consistent sized icons of an existing application in various states, jiggling (shaking) etc. ![objective_c_icon] InjectionIII - overdue Swift rewrite of Injection. ![objective_c_icon] ![swift_icon] Knuff - The debug application for Apple Push Notification Service (APNs). ![objective_c_icon] LayerX - Intuitive app to display transparent images on screen. ![swift_icon] Localizable.strings - Mac app to localize your iOS and macOS projects. ![swift_icon] Localization Editor - Simple macOS editor app to help you manage iOS app localizations by allowing you to edit all the translations side by side. ![swift_icon] Localizations - Localizations is an macOS app that manages your Xcode project localization files (.strings). ![swift_icon] Menubar Colors - macOS app for convenient access to the system color panel. ![swift_icon] Notarize - Notarization status monitoring tool for macOS, supporting multiple developer accounts ![swift_icon] PodsUpdater - macOS app which helps you manage dependency releases in your Podfile. ![swift_icon] ProfilesManager - Apple iOS/macOS Provisioning Profiles management,.provisionprofile, .mobileprovision files manager tool for mac. ![objective_c_icon] PushNotifications - macOS app to test push notifications on iOS and Android. ![javascript_icon] ResignTool - This is an app for macOS that can (re)sign apps and bundle them into ipa files that are ready to be installed on an iOS device. ![objective_c_icon] Resizr - MacOS application for creating AppIcon for iOS and Android apps. ![swift_icon] SmartPush - iOS Push Notification Debug App. ![objective_c_icon] TransporterPad - iOS/Android app deployment tool for macOS. ![swift_icon] WWDC - Unofficial WWDC app for macOS. ![swift_icon] WWDC.srt - Powerful app for downloading subtitle for each WWDC session video since 2013 in (srt) format. ![swift_icon] calabash-launcher - iOS Calabash Launcher is a macOS app that helps you run and manage Calabash tests on your Mac. ![swift_icon] iOS Images Extractor - iOS Images Extractor is a Mac app to normalize, decode, and extract images from iOS apps. ![objective_c_icon] iSimulator - iSimulator is a GUI utility to control the Simulator and manage the app installed on the simulator. ![objective_c_icon] xib2Storyboard - Tool to convert Xcode .xib to .storyboard files. ![objective_c_icon]","title":"iOS / macOS"},{"location":"macos_app/#downloader","text":"App Downloader - Easily search and download macOS apps from the huge homebrew cask app catalog. ![swift_icon] Get It - Native macOS video/audio downloader. Think of it as a youtube downloader that works on many sites. ![swift_icon] Motrix - A full-featured download manager. ![javascript_icon] Pillager - macOS Video Downloader written in Swift and Objective-C. ![objective_c_icon] ![swift_icon] YouTube Downloader for macOS - Simple menu bar app to download YouTube movies on your Mac. I wrote this as a test project to learn more about app development on macOS. ![swift_icon] udemy-downloader-gui - desktop application for downloading Udemy Courses. ![javascript_icon]","title":"Downloader"},{"location":"macos_app/#editors","text":"","title":"Editors"},{"location":"macos_app/#csv","text":"TableTool - simple CSV editor for the macOS. ![objective_c_icon]","title":"CSV"},{"location":"macos_app/#json","text":"JSON-Splora - GUI for editing, visualizing, and manipulating JSON data. ![javascript_icon]","title":"JSON"},{"location":"macos_app/#markdown","text":"Gingko - Tree-structured markdown editor for macOS, Windows, and Linux. ![elm_icon] MacDown - Markdown editor for macOS. ![objective_c_icon] Mark Text - Realtime preview markdown editor for macOS Windows and Linux. ![javascript_icon] Pine - A modern MacOS markdown editor. ![swift_icon] QOwnNotes - Plain-text file notepad and todo-list manager with markdown support and ownCloud / Nextcloud integration. ![cpp_icon]","title":"Markdown"},{"location":"macos_app/#tex","text":"Qilin Editor - Text editor for exact sciences with built-in KaTeX/AsciiMath support. ![javascript_icon]","title":"TeX"},{"location":"macos_app/#text","text":"CotEditor - Lightweight Plain-Text Editor for macOS. ![swift_icon] MacVim - Text editor for macOS. ![c_icon] Noto - Plain text editor for macOS with customizable themes. ![swift_icon] SubEthaEdit - General purpose plain text editor for macOS. Widely known for its live collaboration feature. ![objective_c_icon] TextMate - TextMate is a graphical text editor for macOS. ![objective_c_icon] VimR - Refined Neovim experience for macOS. ![swift_icon]","title":"Text"},{"location":"macos_app/#extensions","text":"BetterPiP - Use native picture-in-picture with browsers such as Google Chrome for HTML5 videos. ![swift_icon] Middleclick - Emulate a scroll wheel click with three finger Click or Tap on MacBook trackpad and Magic Mouse ![c_icon] PageExtender - Extend pages with your own CSS and JS files. ![swift_icon] ![javascript_icon] PiPTool - Add the Picture-in-Picture Functionality to YouTube, Netflix, Plex and other video broadcasting services in macOS. ![javascript_icon] PiPifier - PiPifier is a native macOS 10.12 Safari extension that lets you use every HTML5 video in Picture in Picture mode. ![swift_icon] Sessions - Safari extension to save your working sessions ![swift_icon] nef - This Xcode extension enables you to make a code selection and export it to a snippets. Available on Mac AppStore. ![swift_icon]","title":"Extensions"},{"location":"macos_app/#finder","text":"Clipy - Clipy is a Clipboard extension app for macOS. ![swift_icon] CopyQ - Clipboard manager with advanced features ![cpp_icon] FiScript - Execute custom scripts from the MacOS context menu (CTRL+click) in Finder. ![swift_icon] Finder Go - macOS app and Finder Sync Extension to open Terminal, iTerm, Hyper from Finder. ![swift_icon] OpenInCode - Finder toolbar app to open current folder in Visual Studio Code. ![objective_c_icon] OpenInTerminal - \u2728 Finder Toolbar app for macOS to open the current directory in Terminal, iTerm or Hyper. ![swift_icon] Quick Look plugins - List of useful Quick Look plugins for developers. ![objective_c_icon] ![c_icon] cd to... - Finder Toolbar app to open the current directory in the Terminal ![objective_c_icon]","title":"Finder"},{"location":"macos_app/#games","text":"Battle for Wesnoth - Turn-based tactical strategy game, featuring both single-player and online multiplayer combat. ![cpp_icon] Boxer - The DOS game emulator that\u2019s fit for your Mac. ![cpp_icon] ![objective_c_icon] Dolphin - Powerful emulator for Nintendo GameCube and Wii games. ![cpp_icon] OpenEmu - Retro video game emulation for macOS. ![objective_c_icon] OpenRCT2 - Re-implementation of RollerCoaster Tycoon 2. ![cpp_icon] Screentendo - Turn your screen into a playable level of Mario. ![objective_c_icon] Stockfish - Beautiful, powerful chess application. ![cpp_icon] ![objective_c_icon]","title":"Games"},{"location":"macos_app/#graphics","text":"Aseprite - Animated sprite editor pixel art tool (Windows, macOS, Linux). ![cpp_icon] ![c_icon] CaptuocrToy - Tool to capture screenshot and recognize text by online ocr apis. ![swift_icon] GifCapture - Gif capture app for macOS. ![swift_icon] Gifcurry - Video to GIF maker with a graphical interface capable of cropping, adding text, seeking, and trimming. ![haskell_icon] Gifski - Convert videos to high-quality GIFs. ![swift_icon] InfiniteCanvas - Proof of concept Mac drawing application. ![swift_icon] Material Colors Native - Choose your Material colours and copy the hex code. ![objective_c_icon] Pencil2D Animation - Pencil2D is an animation/drawing software for macOS, Windows, and Linux. It lets you create traditional hand-drawn animation (cartoon) using both bitmap and vector graphics. ![cpp_icon] ScreenToLayers for macOS - ScreenToLayers is a macOS application to easily capture your screen as a layered PSD file. ![objective_c_icon] ![css_icon] macSVG - macOS application for designing HTML5 SVG (Scalable Vector Graphics) art and animation with a WebKit web view. ![objective_c_icon]","title":"Graphics"},{"location":"macos_app/#ide","text":"Atom - The hackable text editor. ![javascript_icon] LiveCode - Cross-platform development IDE. ![c_icon] Oni - Oni is a modern take on modal editing code editor focused on developer productivity. ![javascript_icon] ![type_script_icon] Visual Studio Code - Code editor developed by Microsoft. ![type_script_icon] ZeroBraneStudio - ZeroBrane Studio is a lightweight cross-platform Lua IDE with code completion, syntax highlighting, remote debugger, code analyzer, live coding, and debugging support for various Lua engines. ![lua_icon]","title":"IDE"},{"location":"macos_app/#images","text":"APNGb - macOS app which assembles and disassembles animated png files. ![swift_icon] Crunch - Insane(ly slow but wicked good) PNG image optimization. ![python_icon] ExifCleaner - Remove image metadata with drag and drop, multi-core batch processing, and dark mode. ![javascript_icon] Freehand - macOS Status Bar App for quick sketch. ![swift_icon] Gimp - Gimp is GNU Image Manipulation Program. ![c_icon] Iconology - Edit Icons and then Export to Xcode, Icns, Ico, Favicon, Mac Iconset, or a Custom List of Sizes. ![swift_icon] ImageAlpha - Mac GUI for pngquant, pngnq and posterizer. ![objective_c_icon] ![python_icon] Imagine - Imagine is a desktop app for compression of PNG and JPEG, with a modern and friendly UI. ![type_script_icon] InVesalius - 3D medical imaging reconstruction software ![python_icon] Katana - Katana is a simple screenshot utility for macOS that lives in your menubar. ![javascript_icon] ![css_icon] PhotoMiner - macOS app for finding and lost forgotten photos on your disks. ![swift_icon] Screenbar - macOS menubar app for automating screenshots. ![swift_icon] WebPonize - WebPonize is a macOS App for converting PNG, JPEG, animated (or not) GIF images into WebP. ![swift_icon] ![c_icon]","title":"Images"},{"location":"macos_app/#keyboard","text":"AnnePro-mac - macOS application for controlling AnnePro keyboard over bluetooth. ![swift_icon] Fluor - Handy tool for macOS allowing you to switch Fn keys' mode based on active application. ![swift_icon] GokuRakuJoudo - Karabiner-Elements configuration manager, rescue to bloated karabiner.json ![clojure_icon] Karabiner - Karabiner (KeyRemap4MacBook) is a powerful utility for keyboard customization. ![cpp_icon] ![objective_c_icon] Karabiner-Elements - Karabiner-Elements is a powerful utility for keyboard customization on macOS Sierra (10.12) or later. ![cpp_icon] ![objective_c_icon] Kawa - Better input source switcher for macOS. ![swift_icon] Thor - Switch the right application ASAP. ![swift_icon] Unshaky - A software attempt to address the \"double key press\" issue on Apple's butterfly keyboard ![swift_icon]","title":"Keyboard"},{"location":"macos_app/#mail","text":"Correo - Menubar/taskbar Gmail App for Windows and macOS. ![javascript_icon] ElectronMail - Unofficial desktop app for ProtonMail and Tutanota end-to-end encrypted email providers. ![type_script_icon] Mailspring - \ud83d\udc8c A beautiful, fast and maintained fork of @nylas Mail by one of the original authors ![javascript_icon] Rambox - Cross Platform messaging and emailing app that combines common web applications into one. ![javascript_icon] ![css_icon] dejalu - Fast and Simple Email Client. ![cpp_icon] ![objective_c_icon]","title":"Mail"},{"location":"macos_app/#medical","text":"InVesalius - 3D medical imaging reconstruction software ![python_icon]","title":"Medical"},{"location":"macos_app/#menubar","text":"Airpass - Status bar Mac application to overcome time constrained WiFi networks. ![javascript_icon] AnyBar - macOS menubar status indicator. ![objective_c_icon] BitBar - Put the output from any script or program in your macOS Menu Bar. ![objective_c_icon] CloudyTabs - Simple menu bar macOS application for displaying lists of your iCloud Tabs and Reading List. ![swift_icon] DatWeatherDoe - Simple menu bar weather app for macOS written in Swift. ![swift_icon] DisplayMenu - Simple (bare-bones) macOS menubar extra to apply display presets. ![swift_icon] Dozer - Hide MacOS menubar items. ![swift_icon] KubeContext - import, manage and switch between your Kubernetes contexts on Mac. ![swift_icon] MenuMeters - CPU, memory, disk, and network monitoring tools for macOS. ![objective_c_icon] Menubar Brightness - macOS app to change the screen brightness on the menubar. ![javascript_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] Night Shift Control - Night Shift Control is a simple macOS menubar app for controlling Night Shift. It's aim is to bring features from f.lux which are missing from Night Shift such as disabling Night Shift for certain apps. ![swift_icon] PSIBar - Quickly hacked up PSI macOS status bar app. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] Quickeys - A mac menu bar app that provides note taking functionality though a quick dropdown menu. ![swift_icon] SensibleSideButtons - Small menu bar utility that lets you use your third-party mouse's side buttons for navigation across a variety of apps. ![objective_c_icon] ![c_icon] Shifty - macOS menu bar app that gives you more control over Night Shift. ![swift_icon] gSwitch - macOS status bar app that allows control over the gpu on dual gpu macbooks. ![swift_icon] iGlance - macOS System Monitor (cpu, memory, network, fan and battery) for the Status Bar. ![swift_icon]","title":"Menubar"},{"location":"macos_app/#music","text":"Audacity - Free, open source, cross-platform audio software ![c_icon] Carol - A minimal and beautiful lyrics app that stays in the menu bar of macOS. ![c_sharp_icon] ChordDetector - Tiny menu bar app that listens iTunes and Spotify to detect chords of songs! ![swift_icon] DeezPlayer - Deezer Desktop app for Windows, Linux and macOS. ![coffee_script_icon] Lilypond UI - Create beautiful musical scores with LilyPond. ![javascript_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] SoundCleod - SoundCloud for macOS and Windows. ![javascript_icon] Spotify-Cli-Mac - Control Spotify without leaving your terminal. :notes: ![javascript_icon] YouTube-Music - macOS wrapper for music.youtube.com. ![swift_icon] iTunes Graphs - macOS app to visualise your iTunes library as graphs. ![swift_icon] lyricsify - Simple Spotify lyrics viewer menu bar app for macOS in Swift. ![swift_icon]","title":"Music"},{"location":"macos_app/#news","text":"Diurna - Basic/Classic Hacker News app, used as a Cocoa Swift learning platform. ![swift_icon] NetNewsWire - Feed reader for macOS. ![swift_icon] Vienna - Vienna is a RSS/Atom newsreader for macOS. ![objective_c_icon] hacker-menu - Hacker News Delivered to Desktop. ![javascript_icon]","title":"News"},{"location":"macos_app/#notes","text":"Boostnote - Note-taking application made for programmers just like you. ![javascript_icon] Dnote - A simple command line notebook with multi-device sync and web interface. ![go_icon] ![type_script_icon] FSNotes - Notes manager for macOS/iOS: modern notational velocity (nvALT) on steroids. ![swift_icon] FromScratch - Little app that you can use as a quick note taking or todo app. ![javascript_icon] ![css_icon] Jupyter Notebook Viewer - Notebook viewer for macOS. ![swift_icon] NoteTaker - Simple note taking app for macOS and iOS which uses Realm and CloudKit for syncing. ![swift_icon] Notes - Notes is a macOS application built to create notes, using text amongst other formats: images, videos, contacts, and etc. ![swift_icon] QOwnNotes - Plain-text file notepad and todo-list manager with markdown support and ownCloud / Nextcloud integration. ![cpp_icon] Simplenote - Simplest way to keep notes. ![objective_c_icon] Standard Notes - Safe place for your notes, thoughts, and life's work. ![javascript_icon] ![css_icon] Tusk - Unofficial, third-party, community driven Evernote app with a handful of useful features. ![javascript_icon] ![css_icon] joplin - Note taking and to-do application with synchronization capabilities for Windows, macOS, Linux, Android and iOS. ![javascript_icon] notable - Simple note taking application. ![javascript_icon] tmpNote - Very simple macOS app to make temporary notes. ![swift_icon]","title":"Notes"},{"location":"macos_app/#other","text":"Cakebrew - Manage your Homebrew formulas with style using Cakebrew. ![objective_c_icon] DevDocs for macOS - An unofficial DevDocs API documentation viewer. ![swift_icon] Gas Mask - Hosts file manager for macOS. ![objective_c_icon] Hosts - Cocoa GUI for /etc/hosts. ![objective_c_icon] ImageOptim - GUI image optimizer for Mac. ![objective_c_icon] Keyframes Player - Simple macOS app to preview animations created with Facebook's keyframes framework. ![swift_icon] Lepton - Democratizing Code Snippets Management (macOS/Win/Linux). ![javascript_icon] Letters - Teach your kids the alphabet and how to type. ![swift_icon] Platypus - Mac developer tool that creates application bundles from command line scripts. ![objective_c_icon] QorumLogs - Swift Logging Utility for Xcode Google Docs. ![swift_icon] React Native Debugger - Desktop app for inspecting your React Native projects. macOS, Linux, and Windows. ![javascript_icon] Reactotron - Desktop app for inspecting your React JS and React Native projects. macOS, Linux, and Windows. ![javascript_icon] RktMachine - Menu bar macOS app for running rkt in a macOS hypervisor CoreOS VM. ![swift_icon] Ruby.app - macOS app that contains a full Ruby installation (for use with Ruby/Gosu). ![ruby_icon] Shuttle - Simple SSH shortcut menu for macOS. ![objective_c_icon] SwiftyBeaver - Convenient logging during development release in Swift. ![swift_icon] Unused - Mac app for checking Xcode projects for unused resources. ![objective_c_icon] Vagrant Manager - Manage your vagrant machines in one place with Vagrant Manager for macOS. ![objective_c_icon] macGist - Simple app to send pasteboard items to GitHub's Gist. ![swift_icon] syncthing-macosx - Frugal nativemacOS macOS Syncthing application bundle. ![objective_c_icon]","title":"Other"},{"location":"macos_app/#player","text":"IINA - The modern video player for macOS. ![swift_icon] MPlayerX - Media player on macOS. ![objective_c_icon] MacMorpheus - 3D 180/360 video player for macOS for PSVR with head tracking. ![objective_c_icon] Movie Monad - Desktop video player built with Haskell that uses GStreamer and GTK+. ![haskell_icon] mpv - Video player based on MPlayer/mplayer2. ![c_icon]","title":"Player"},{"location":"macos_app/#podcast","text":"Cumulonimbus - Simple, beautiful podcast app. ![javascript_icon] Doughnut - Podcast player and library for mac ![swift_icon] PodcastMenu - PodcastMenu is a simple app which puts Overcast on your Mac's menu bar so you can listen to your favorite podcasts while you work. ![swift_icon] Podlive for macOS - macOS client to listen to live streaming podcasts (only). It currently supports all livestreams broadcasting via Ultraschall with Studio Link On Air . ![objective_c_icon] mkchromecast - Cast macOS and Linux Audio/Video to your Google Cast and Sonos Devices. ![python_icon]","title":"Podcast"},{"location":"macos_app/#productivity","text":"Ao - Elegant Microsoft To-Do desktop app. ![javascript_icon] ![css_icon] Calculeta - Calculator for macOS which working on statusbar. ![swift_icon] Cerebro - Cross-platform launcher app. ![javascript_icon] ClipMenu - Clipboard manager for macOS. ![objective_c_icon] Clocker - macOS app to plan and organize through timezones. ![objective_c_icon] ControlPlane - Automate running tasks based on where you are or what you do. ![objective_c_icon] Flycut - Clean and simple clipboard manager for developers. ![objective_c_icon] KeyHolder - Record shortcuts in macOS, like Alfred.app. ![swift_icon] Kiwix - Kiwix for iOS and macOS, build on Swift. ![swift_icon] Linked Ideas - macOS application to write down and connect ideas. ![swift_icon] Maccy - Lightweight search-as-you-type clipboard manager. ![swift_icon] Manta - Flexible invoicing desktop app with beautiful customizable templates. ![javascript_icon] Middleclick - Emulate a scroll wheel click with three finger Click or Tap on MacBook trackpad and Magic Mouse ![c_icon] PDF Archiver - Nice tool for tagging and archiving tasks. ![swift_icon] Paperless Desktop - Desktop app that uses the paperless API to manage your document scans. ![javascript_icon] Pennywise - Pennywise opens any website or media in a small floating window that remains on top of all other applications. It's a great alternative to Helium. ![javascript_icon] Pomodoro Cycle - Pomodoro Cycle for macOS ![type_script_icon] QOwnNotes - Plain-text file notepad and todo-list manager with markdown support and ownCloud / Nextcloud integration. ![cpp_icon] Quicksilver - Quicksilver is a fast macOS productivity application that gives you the power to control your Mac quickly and elegantly. ![objective_c_icon] Quickwords - Write anything in a matter of seconds. Create snippets that can substitute text, execute tedious tasks and more. ![javascript_icon] ![css_icon] SelfControl - macOS app to block your own access to distracting websites etc for a predetermined period of time. It can not be undone by the app or by a restart \u2013 you must wait for the timer to run out. ![objective_c_icon] Sessions - Safari extension to save your working sessions ![swift_icon] Speed Reader - Read faster with the power of silencing vocalization with SpeedReader. ![swift_icon] StickyNotes - A Windows 10-esque Sticky Notes app implemented in AppKit. ![swift_icon] Strategr - No-fuss time management. ![cpp_icon] ![objective_c_icon] Super Productivity - Free to do list time tracker for programmers designers with Jira integration. ![type_script_icon] ![javascript_icon] Thyme - The task timer for OS X. ![objective_c_icon] Timer - Simple Timer app for Mac. ![swift_icon] Toggl Desktop - Toggl Desktop app for Windows, Mac and Linux. ![cpp_icon] TomatoBar - Pomodoro Technique Timer for macOS with Touch Bar support. ![swift_icon] TrelloApp - Unofficial wrapper application for Trello.com written in Swift. This is almost a \"Hello World\" for a site specific browser. ![swift_icon] Watson - A CLI application for time tracking. ![python_icon] Whale - Unofficial Trello app. ![javascript_icon] Yomu - Manga reader app for macOS. ![swift_icon] espanso - Cross-platform Text Expander, a powerful replacement for Alfred Snippets ![rust_icon] macOrganizer - macOS app for organizing files or removing unnecessary files. ![swift_icon] status-bar-todo - Simple macOS app to keep TODO-list in status bar. ![swift_icon] stretchly - Cross-platform electron app that reminds you to take breaks when working with computer. ![javascript_icon]","title":"Productivity"},{"location":"macos_app/#screensaver","text":"Aerial - Apple TV Aerial Screensaver for macOS. ![swift_icon] Image-As-Wallpaper - Utility application helps with selection of images for using as desktop wallpaper or in screensaver on Mac computers. ![swift_icon] Irvue - Screensaver for macOS. ![objective_c_icon] Life Saver - An abstract screensaver based on Conway's Game of Life implemented with SpriteKit ![swift_icon] MusaicFM - iTunes Screensaver Clone for Spotify and Last.fm ![objective_c_icon] Predator - A predator-inspired clock screensaver for macOS ![swift_icon] The GitHub Matrix Screensaver - The GitHub Matrix Screensaver for macOS. ![javascript_icon]","title":"Screensaver"},{"location":"macos_app/#security","text":"Cloaker - simple drag-and-drop, password-based file encryption. ![rust_icon] Cryptomator - Multi-platform transparent client-side encryption of your files in the cloud. ![java_icon] LuLu - LuLu is macOS firewall application that aims to block unauthorized (outgoing) network traffic. ![objective_c_icon] Swifty - Free and offline password manager. ![javascript_icon] stronghold - Easily configure macOS security settings from the terminal. ![python_icon]","title":"Security"},{"location":"macos_app/#sharing-files","text":"Deluge - Lightweight cross-platform BitTorrent client. ![python_icon] NitroShare - Transferring files from one device to another ![cpp_icon] Rhea - macOS status bar app for quickly sharing files and URLs. ![objective_c_icon] Transmission - Official Transmission BitTorrent client repository. ![objective_c_icon] ![c_icon] Tribler - Privacy enhanced BitTorrent client with P2P content discovery. ![python_icon] mac2imgur - Simple Mac app designed to make uploading images and screenshots to Imgur quick and effortless. ![swift_icon] qBittorrent - BitTorrent client in Qt. ![cpp_icon]","title":"Sharing Files"},{"location":"macos_app/#social-networking","text":"Caprine - Elegant Facebook Messenger desktop app. ![javascript_icon] ![css_icon] Goofy - Unofficial Facebook Messenger client. ![javascript_icon] Leviathan - Leviathan is a iOS and macOS client application for the Mastodon social network. ![swift_icon] Messenger - macOS app wrapping Facebook's Messenger for desktop. ![objective_c_icon] Product Hunt - share and discover your favorite new products and applications. ![swift_icon] Quail - Unofficial esa app. ![javascript_icon] Ramme - Unofficial Instagram Desktop App. ![javascript_icon] ![css_icon] Simpo - macOS menubar app to post status quickly. ![ruby_icon] appear.in - Unofficial appear.in app. ![javascript_icon]","title":"Social Networking"},{"location":"macos_app/#streaming","text":"Galeri - Perpetual artwork streaming app. ![javascript_icon] OBS Studio - Free and open source software for live streaming and screen recording. ![cpp_icon]","title":"Streaming"},{"location":"macos_app/#system","text":"AppPolice - App for macOS with a minimalistic UI which lets you quickly throttle down the CPU usage of any running process. ![objective_c_icon] Apple Juice - Advanced battery gauge for macOS. ![swift_icon] Clean-Me - Small macOS app that acts as a system cleaner (logs, cache, ...). ![swift_icon] Diagnostics - Diagnostics is an application displaying the diagnostic reports from applications on macOS. ![swift_icon] DisableMonitor - Easily disable or enable a monitor on your Mac. ![objective_c_icon] EtreCheck - EtreCheck is an easy-to-use macOS app to display important details of your system configuration and allow you to copy that information to the Clipboard. ![objective_c_icon] Fanny - Monitor your Mac's fan speed and CPU temperature from your Notification Center. ![objective_c_icon] HoRNDIS - Android USB tethering driver for macOS. ![cpp_icon] Juice - Make your battery information a bit more interesting. ![swift_icon] KeepingYouAwake - Prevents your Mac from going to sleep. ![objective_c_icon] Latest - Small utility app for macOS that makes sure you know about all the latest updates to the apps you use. ![swift_icon] Loading - Simple network activity monitor for macOS. ![objective_c_icon] Overkill - Stop iTunes from opening when you connect your iPhone. ![swift_icon] ProfileCreator - macOS Application to create standard or customized configuration profiles. ![objective_c_icon] Sloth - Sloth is an macOS application that displays a list of all open files and sockets in use by all running applications on your system. ![objective_c_icon] Turbo Boost Switcher - Turbo Boost Switcher is a little application for Mac computers that allows to enable and/or disable the Turbo Boost feature. ![objective_c_icon] VerticalBar - macOS application to add a vertical bar to Dock. ![swift_icon] macOSLucidaGrande - A small utility to set Lucida Grande as your Mac's system UI font. ![objective_c_icon]","title":"System"},{"location":"macos_app/#terminal","text":"Alacritty - Cross-platform, GPU-accelerated terminal emulator. ![rust_icon] Bifrost - A tiny terminal emulator for serial port communication (macOS/Linux). ![go_icon] Console - macOS console application. ![swift_icon] Finder Go - macOS app and Finder Sync Extension to open Terminal, iTerm, Hyper from Finder. ![swift_icon] Hyper - Terminal built on web technologies. ![javascript_icon] ![css_icon] Kitty - Cross-platform, fast, feature full, GPU based terminal emulator. ![python_icon] ![c_icon] OpenInTerminal - \u2728 Finder Toolbar app for macOS to open the current directory in Terminal, iTerm or Hyper. ![swift_icon] OpenTerminal - App for macOS that opens a new Finder window and changes the current directory to the folder launched by the app. ![swift_icon] Upterm - A terminal emulator for the 21st century (formerly Black Screen). ![javascript_icon] cd to... - Finder Toolbar app to open the current directory in the Terminal ![objective_c_icon] iTerm 2 - Terminal emulator for macOS that does amazing things. ![objective_c_icon] wallpapper - wallpapper is a console application for creating dynamic wallpapers for Mojave. ![swift_icon]","title":"Terminal"},{"location":"macos_app/#touch-bar","text":"Muse - Spotify controller with TouchBar support. ![swift_icon] MyTouchbarMyRules - App to customize your Touch Bar as you want. ![swift_icon] Pock - Display macOS Dock in Touch Bar. ![swift_icon] Touch Bar Preview - Small application to display your designs on the Touch Bar of the new MacBook Pro. ![swift_icon] Touch Bar Simulator - Use the Touch Bar on any Mac. ![swift_icon] Touch Emoji - Emoji picker for MacBook Pro Touch Bar. ![swift_icon]","title":"Touch Bar"},{"location":"macos_app/#utilities","text":"Android tool for Mac - One-click screenshots, video recordings, app installation for iOS and Android ![swift_icon] ArchiveMounter - Mounts archives like disk images. ![swift_icon] BeardedSpice - Control web based media players with the media keys found on Mac keyboards. ![objective_c_icon] Bitwarden - Cross-platform password management solutions for individuals, teams, and business organizations. ![type_script_icon] Buttercup Desktop - Secure password manager for mac and other platforms. ![javascript_icon] Calculeta - Calculator for macOS which working on statusbar. ![swift_icon] Catch - Catch: Broadcatching made easy. ![swift_icon] Clear Clipboard Text Format - Easily clear the format of your clipboard text with Clear Clipboard Text Format. ![objective_c_icon] CornerCal - Simple, clean calendar and clock app for macOS. ![swift_icon] Crypter - Crypter is an innovative, convenient and secure cross-platform crypto app that simplifies secure password generation and management by requiring you to only remember one bit, the MasterPass. ![javascript_icon] ECheck - Small tool to validate epub files for macOS. ![swift_icon] Flying Carpet - cross-platform file transfer over ad-hoc wifi, like AirDrop but for Mac/Windows/Linux. ![go_icon] Funky - Easily toggle the function key on your Mac on a per app basis. ![objective_c_icon] Gray - Pick between the light appearance and the dark appearance on a per-app basis with the click of a button ![swift_icon] Kap - Screen recorder application built with web technology. ![javascript_icon] KeePassXC - Cross-platform community-driven port of the Windows application \"Keepass Password Safe\" ![cpp_icon] KeeWeb - Cross-platform password manager compatible with KeePass. ![javascript_icon] Kyapchar - Simple screen and microphone audio recorder for macOS. ![swift_icon] Life-Calendar - Life Calendar. ![swift_icon] Lunar - Intelligent adaptive brightness for your external displays. ![swift_icon] MQTTX - An elegant Cross-platform MQTT 5.0 desktop client. ![javascript_icon] ![type_script_icon] MacPass - Native macOS KeePass client. ![objective_c_icon] Maria - macOS native app/widget for aria2 download tool. ![swift_icon] Meme Maker - Meme Maker macOS application for meme creation. ![swift_icon] Middleclick - Emulate a scroll wheel click with three finger Click or Tap on MacBook trackpad and Magic Mouse ![c_icon] Monolingual - Remove unnecessary language resources from macOS ![swift_icon] Mos - Smooth your mouse's scrolling and reverse the mouse scroll direction ![swift_icon] Music Bar - Music Bar is macOS application that places music controls right in your menu bar. ![swift_icon] Noti - Receive Android notifications on your mac (with PushBullet). ![swift_icon] Numi - A handy calculator with natural language parsing. ![javascript_icon] PB for Desktop - Receive native push notifications on macOS, Windows and Linux. ![javascript_icon] Padlock - A minimal, open source password manager for macOS. ![javascript_icon] PercentCalculator - A menu bar application that calculates parcents. ![swift_icon] PlayStatus - PlayStatus is a macOS app that allows the control of Spotify and iTunes music playback from the menu bar. ![swift_icon] PowerShell - PowerShell is a cross-platform automation and configuration tool/framework that works well with your existing tools. ![c_sharp_icon] ScreenCat - ScreenCat is a screen sharing + remote collaboration application. ![javascript_icon] ![css_icon] SlowQuitApps - Add a global delay to Command-Q to stop accidental app quits. ![objective_c_icon] Super Productivity - Free to do list time tracker for programmers designers with Jira integration. ![type_script_icon] ![javascript_icon] Telephone - SIP softphone for macOS. ![objective_c_icon] ![swift_icon] The Blockstack Browser - Blockstack is an internet for decentralized apps where users own their data. The Blockstack Browser allows you to explore the Blockstack internet. ![javascript_icon] The Unarchiver - The Unarchiver is an Objective-C application for uncompressing archive files. ![objective_c_icon] ToTheTop - Small macOS application to help you scroll to the top. ![swift_icon] calibre - cross platform e-book manager. ![python_icon] fselect - Command-line tool to search files with SQL syntax. ![rust_icon] homebrew-cask - A CLI workflow for the administration of macOS applications distributed as binaries ![ruby_icon] iOScanX - Cocoa application for semi-automated iOS app analysis and evaluation. ![objective_c_icon] ![c_icon] mac-sound-fix - Mac Sound Re-Enabler. ![swift_icon] wechsel - manage bluetooth connections with your keyboard. ![swift_icon] \u00dcbersicht - Keep an eye on what's happening on your machine and in the world. ![objective_c_icon]","title":"Utilities"},{"location":"macos_app/#vpn-proxy","text":"ShadowsocksX-NG - Next Generation of ShadowsocksX. ![swift_icon] Specht - Rule-based proxy app built with Network Extension for macOS. ![swift_icon] SpechtLite - Rule-based proxy app for macOS. ![swift_icon] Tunnelblick - Tunnelblick is a graphic user interface for OpenVPN on macOS. ![objective_c_icon] clashX - A rule based custom proxy with GUI for Mac base on clash. ![swift_icon] rvc-mac - Ribose VPN Client macOS Menu App. ![swift_icon]","title":"VPN &amp; Proxy"},{"location":"macos_app/#video","text":"Acid.Cam.v2.OSX - Acid Cam v2 for macOS distorts video to create art. ![cpp_icon] AppleEvents - Unofficial Apple Events app for macOS. ![objective_c_icon] Conferences.digital - Best way to watch the latest and greatest videos from your favourite developer conferences for free on your Mac. ![swift_icon] Datamosh - Datamosh your videos on macOS. ![swift_icon] Face Data - macOS application used to auto-annotate landmarks from a video. ![swift_icon] GNU Gatekeeper - Video conferencing server for H.323 terminals. ![cpp_icon] Gifted - Turn any short video into an animated GIF quickly and easily. ![objective_c_icon] HandBrake - HandBrake is a video transcoder available for Linux, Mac, and Windows. ![c_icon] MenuTube - Catch YouTube into your macOS menu bar! ![javascript_icon] OpenShot - Easy to use, quick to learn, and surprisingly powerful video editor. ![python_icon] Quick Caption - Transcribe and generate caption files (SRT, ASS and FCPXML) without manually entering time codes. ![swift_icon] QuickLook Video - This package allows macOS Finder to display thumbnails, static QuickLook previews, cover art and metadata for most types of video files. ![objective_c_icon] Subler - Subler is an macOS app created to mux and tag mp4 files. ![objective_c_icon] VLC - VLC is a free and open source cross-platform multimedia player ![c_icon] Vid Quiz Creator - macOS application to insert quizzes within video playback and play those videos to receiving devices using the LISNR API. ![swift_icon] WebTorrent Desktop - Streaming torrent app. For Mac, Windows, and Linux. ![javascript_icon] Yoda - Nifty macOS application which enables you to browse and download videos from YouTube. ![javascript_icon]","title":"Video"},{"location":"macos_app/#wallpaper","text":"500-mac-wallpaper - Simple macOS app for the status bar to automatically download photos from 500px.com to a local folder that can be set as a source of wallpapers. ![swift_icon] ArtWall - ArtStation set as wallpapers from artwork.rss . ![objective_c_icon] Artify - A macOS application for bringing dedicatedly 18th century Arts to everyone ![swift_icon] BingPaper - Use Bing daily photo as your wallpaper on macOS. ![swift_icon] Desktop Wallpaper Switcher - Win / Linux / macOS tool for managing and cycling desktop wallpapers. ![cpp_icon] Muzei - Muzei wallpaper app for macOS. ![swift_icon] Plash - Make any website your desktop wallpaper. ![swift_icon] Satellite Eyes - macOS app to automatically set your desktop wallpaper to the satellite view overhead. ![objective_c_icon] Sunscreen - Sunscreen is a fun, lightweight application that changes your desktop wallpaper based on sunrise and sunset. ![swift_icon] WallpaperMenu - macOS menubar application for navigation through beautiful pictures on the web and set them up as your desktop image. ![ruby_icon] earth-wallpapers - Simple macOS menubar app which fetches latest photos from a subreddit. ![javascript_icon] pyDailyChanger - pyDailyChanger is a program that changes your wallpaper daily. ![python_icon]","title":"Wallpaper"},{"location":"macos_app/#window-management","text":"Amethyst - Automatic tiling window manager for macOS. ![swift_icon] AppGrid - Grid-based keyboard window manager for macOS. ![objective_c_icon] Desktop Profiles - An innovative desktop/window manager for macOS ![swift_icon] Hammerspoon - Staggeringly powerful macOS desktop automation with Lua. ![lua_icon] ![objective_c_icon] Phoenix - Lightweight macOS window and app manager scriptable with JavaScript. ![objective_c_icon] Rectangle - Rectangle is a window manager heavily based on Spectacle, written in Swift. ![swift_icon] ShiftIt - Managing windows size and position. ![objective_c_icon] Slate - Window management application (replacement for Divvy/SizeUp/ShiftIt). ![objective_c_icon] Spectacle - Spectacle allows you to organize your windows without using a mouse. ![objective_c_icon] chunkwm - Tiling window manager for macOS that uses a plugin architecture. ![cpp_icon]","title":"Window Management"},{"location":"macos_app/#contributors","text":"Thanks to all the people who contribute:","title":"Contributors"},{"location":"math/","text":"math Linear Algebra Where to start learning Linear Algebra? Key Machine Learning PreReq: Viewing Linear Algebra through the right lenses Linear Algebra - A free linear algebra textbook and online resource written by David Cherney, Tom Denton, Rohit Thomas and Andrew Waldron Linear Algebra - A Free text for a standard US undergraduate course A First Course in Linear Algebra Linear Algebra, Theory And Applications Elementary Linear Algebra","title":"Math"},{"location":"math/#math","text":"","title":"math"},{"location":"math/#linear-algebra","text":"Where to start learning Linear Algebra? Key Machine Learning PreReq: Viewing Linear Algebra through the right lenses Linear Algebra - A free linear algebra textbook and online resource written by David Cherney, Tom Denton, Rohit Thomas and Andrew Waldron Linear Algebra - A Free text for a standard US undergraduate course A First Course in Linear Algebra Linear Algebra, Theory And Applications Elementary Linear Algebra","title":"Linear Algebra"},{"location":"ml_tutor/","text":"Deeplearning Algorithms tutorial \u6700\u8fd1\u4ee5\u6765\u4e00\u76f4\u5728\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u548c\u7b97\u6cd5\uff0c\u7136\u540e\u81ea\u5df1\u5c31\u5728\u4e0d\u65ad\u603b\u7ed3\u548c\u5199\u7b14\u8bb0\uff0c\u8bb0\u5f55\u4e0b\u81ea\u5df1\u7684\u5b66\u4e60AI\u4e0e\u7b97\u6cd5\u5386\u7a0b\u3002 \u673a\u5668\u5b66\u4e60(Machine Learning, ML)\u662f\u4e00\u95e8\u591a\u9886\u57df\u4ea4\u53c9\u5b66\u79d1\uff0c\u6d89\u53ca\u6982\u7387\u8bba\u3001\u7edf\u8ba1\u5b66\u3001\u903c\u8fd1\u8bba\u3001\u51f8\u5206\u6790\u3001\u7b97\u6cd5\u590d\u6742\u5ea6\u7406\u8bba\u7b49\u591a\u95e8\u5b66\u79d1\u3002\u4e13\u95e8\u7814\u7a76\u8ba1\u7b97\u673a\u600e\u6837\u6a21\u62df\u6216\u5b9e\u73b0\u4eba\u7c7b\u7684\u5b66\u4e60\u884c\u4e3a\uff0c\u4ee5\u83b7\u53d6\u65b0\u7684\u77e5\u8bc6\u6216\u6280\u80fd\uff0c\u91cd\u65b0\u7ec4\u7ec7\u5df2\u6709\u7684\u77e5\u8bc6\u7ed3\u6784\u4f7f\u4e4b\u4e0d\u65ad\u6539\u5584\u81ea\u8eab\u7684\u6027\u80fd\u3002 \u673a\u5668\u5b66\u4e60\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u673a\u5668\u5b66\u4e60\u9010\u6e10\u53d1\u5c55\u6210\u6a21\u5f0f\u8bc6\u522b\u548c\u8ba1\u7b97\u79d1\u5b66\u7406\u8bba\u7684\u7814\u7a76\u3002 \u673a\u5668\u5b66\u4e60\uff1a\u591a\u9886\u57df\u4ea4\u53c9\u5b66\u79d1\uff0c\u6d89\u53ca\u6982\u7387\u8bba\u7edf\u8ba1\u5b66\uff0c\u903c\u8fd1\u8bba\uff0c\u51f8\u5206\u6790\uff0c\u7b97\u6cd5\u590d\u6742\u5ea6\u7406\u8bba\u7b49\u591a\u95e8\u5b66\u79d1\u3002 \u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\uff1a\u8bed\u97f3\u8bc6\u522b\uff0c\u81ea\u52a8\u9a7e\u9a76\uff0c\u8bed\u8a00\u7ffb\u8bd1\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u63a8\u8350\u7cfb\u7edf\uff0c\u65e0\u4eba\u673a\uff0c\u8bc6\u522b\u5783\u573e\u90ae\u4ef6\uff0c\u4eba\u8138\u8bc6\u522b\uff0c\u7535\u5546\u63a8\u8350\u7cfb\u7edf\u3002 \u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u6982\u5ff5\uff1a\u8bad\u7ec3\u96c6\uff0c\u6d4b\u8bd5\u96c6\uff0c\u7279\u5f81\u503c\uff0c\u76d1\u7763\u5b66\u4e60\uff0c\u975e\u76d1\u7763\u5b66\u4e60\uff0c\u5206\u7c7b\uff0c\u56de\u5f52 \u76ee\u524d\u56fd\u5185\u5728AI\u611f\u77e5\u5c42\u9762\u5e94\u7528\u5df2\u7ecf\u767e\u82b1\u9f50\u653e\uff0c\u4e3b\u8981\u662f\u65e0\u4eba\u9a7e\u9a76\u3001\u667a\u80fd\u97f3\u7bb1\u3001\u5d4c\u5165\u5f0f\u3002\u4f46\u5728\u8ba4\u77e5\u5c42\u9762\u8fd8\u662f\u6bd4\u8f83\u7f3a\u4e4f\uff0c\u6240\u4ee5\u65b0\u5165\u884c\u7684AI\u5e94\u7528\u56e2\u961f\u53ef\u4ee5\u653e\u5728\u8ba4\u77e5\u5c42\u3002\u5982\u5f00\u5934\u6240\u8ff0\uff0c\u8ba4\u77e5\u5c42\u6700\u91cd\u8981\u7684\u662f\u7b97\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u9605\u8bfbNature\u4e0a\u6700\u9886\u5148\u7684\u7b97\u6cd5\u516c\u53f8DeepMind\u7684\u51e0\u7bc7\u5927\u4f5c\uff0c\u5982\u4e0b\uff1a 2016.01.Mastering the game of Go with deep neural networks and tree search 2016.10.Hybrid computing using a neural network with dynamic external memory 2017.10.Mastering the game of Go without human knowledge \u673a\u5668\u5b66\u4e60\u6b65\u9aa4\u6846\u67b6 \u628a\u6570\u636e\u62c6\u5206\u4e3a\u8bad\u7ec3\u96c6\u5408\u6d4b\u8bd5\u96c6 \u7528\u8bad\u7ec3\u96c6\u5408\u8bad\u7ec3\u96c6\u7684\u7279\u5f81\u5411\u91cf\u6765\u8bad\u7ec3\u7b97\u6cd5 \u7528\u5b66\u4e60\u6765\u7684\u7b97\u6cd5\u8fd0\u7528\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7d2f\u8bc4\u4f30\u7b97\u6cd5(\u53ef\u80fd\u8981\u8bbe\u8ba1\u5230\u8c03\u6574\u53c2\u6570(parameter tuning) \u7528\u6765\u9a8c\u8bc1\u96c6(validation set)) \u673a\u5668\u5b66\u4e60 \u673a\u5668\u5b66\u4e60\uff1a\u673a\u5668\u5b66\u4e60\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u673a\u5668\u5b66\u4e60\u9010\u6e10\u53d1\u5c55\u6210\u6a21\u5f0f\u8bc6\u522b\u548c\u8ba1\u7b97\u79d1\u5b66\u7406\u8bba\u7684\u7814\u7a76\u3002\u901a\u4fd7\u7684\u8bb2\u673a\u5668\u5b66\u4e60\u662f\u4e00\u79cd\u80fd\u591f\u8d4b\u4e88\u673a\u5668\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u8ba9\u5b83\u5b8c\u6210\u76f4\u63a5\u7f16\u7a0b\u65e0\u6cd5\u5b8c\u6210\u7684\u529f\u80fd\u7684\u65b9\u6cd5\u3002\u4f46\u4ece\u5b9e\u8df5\u7684\u610f\u4e49\u4e0a\u6765\u8bf4\uff0c\u673a\u5668\u5b66\u4e60\u662f\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u6570\u636e\uff0c\u8bad\u7ec3\u51fa\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u7684\u4e00\u79cd\u65b9\u6cd5\u3002 \u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\uff1a\u673a\u5668\u5b66\u4e60\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u6316\u6398\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u751f\u7269\u7279\u5f81\u8bc6\u522b\u3001\u641c\u7d22\u5f15\u64ce\u3001\u533b\u5b66\u8bca\u65ad\u3001\u68c0\u6d4b\u4fe1\u7528\u5361\u6b3a\u8bc8\u3001\u8bc1\u5238\u5e02\u573a\u5206\u6790\u3001DNA\u5e8f\u5217\u6d4b\u5e8f\u3001\u8bed\u97f3\u548c\u624b\u5199\u8bc6\u522b\u3001\u6218\u7565\u6e38\u620f\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u3002\u4e0b\u9762\u5f00\u542f\u6211\u4eec\u7684\u673a\u5668\u5b66\u4e60\uff01 \u673a\u5668\u5b66\u4e60\u73af\u5883\u7684\u642d\u5efa \u673a\u5668\u5b66\u4e60\u7684\u5165\u95e8 \u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6 \u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\u6848\u4f8b \u6df1\u5ea6\u5b66\u4e60 \u6df1\u5ea6\u5b66\u4e60\uff1a\u6df1\u5ea6\u5b66\u4e60\u662f\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u5ef6\u4f38\u51fa\u6765\u7684\u4e00\u4e2a\u65b0\u7684\u9886\u57df\uff0c\u7531\u4ee5\u4eba\u5927\u8111\u7ed3\u6784\u4e3a\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u4e3a\u8d77\u6e90\u52a0\u4e4b\u6a21\u578b\u7ed3\u6784\u6df1\u5ea6\u7684\u589e\u52a0\u53d1\u5c55\uff0c\u5e76\u4f34\u968f\u5927\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u9ad8\u800c\u4ea7\u751f\u7684\u4e00\u7cfb\u5217\u65b0\u7684\u7b97\u6cd5\u3002 \u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u5411\uff1a\u88ab\u5e94\u7528\u5728\u56fe\u50cf\u5904\u7406\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4ee5\u53ca\u8bed\u97f3\u8bc6\u522b\u7b49\u9886\u57df\u3002 \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60\u7684\u5165\u95e8 \u6df1\u5ea6\u5b66\u4e60\u73af\u5883\u7684\u642d\u5efa \u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\u6848\u4f8b \u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6982\u89c8 \u4ece2016\u5e74\u8d77\uff0c\u673a\u5668\u5b66\u4e60\u6709\u4e86\u65b0\u7684\u7a81\u7834\u548c\u53d1\u5c55\u3002\u4f46\u662f\uff0c\u6709\u6548\u7684\u673a\u5668\u5b66\u4e60\u662f\u56f0\u96be\u7684\uff0c\u56e0\u4e3a\u673a\u5668\u5b66\u4e60\u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u4ea4\u53c9\u5b66\u79d1\uff0c\u6ca1\u6709\u79d1\u5b66\u7684\u65b9\u6cd5\u53ca\u4e00\u5b9a\u7684\u79ef\u7d2f\u5f88\u96be\u5165\u95e8\u3002 \u4ece2017\u5e7410\u670819\u65e5\uff0cNature\u4e0a\u53d1\u8868\u4e86\u65b0\u4e00\u4ee3AlphaGo\u7248\u672cAlphaGo Zero\u7684\u6280\u672f\u8bba\u6587\u3002\u6307\u51fa\u4e00\u79cd\u4ec5\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\uff0cAlphaGo Zero\u4e0d\u4f7f\u7528\u4eba\u7c7b\u7684\u6570\u636e\u3001\u6307\u5bfc\u6216\u89c4\u5219\u4ee5\u5916\u7684\u9886\u57df\u77e5\u8bc6\u6210\u4e86\u81ea\u5df1\u7684\u8001\u5e08\u3002DeepMind\u4ee3\u8868\u4e86\u76ee\u524d\u4eba\u5de5\u667a\u80fd\u9886\u57df\u6700\u5f3a\u7684\u6280\u672f\uff0c\u5176\u6838\u5fc3\u662f\u4e24\u4e2a\u5b57\uff1a\u7b97\u6cd5\u3002 \u5f88\u591a\u4eba\u90fd\u60f3\u6210\u4e3a\u4e00\u4e2aAI\u5f00\u53d1\u8005\uff0c\u4e0d\u4ec5\u662f\u56e0\u4e3aAI\u5f00\u53d1\u7684\u85aa\u8d44\u9ad8\uff0c\u66f4\u4e3b\u8981\u662f\u56e0\u4e3aAI\u8fd9\u51e0\u5e74\u7684\u5feb\u901f\u53d1\u5c55,\u4f46\u662f\u56e0\u4e3aAI\u672c\u8eab\u7684\u95e8\u69db\u5c31\u6bd4\u8f83\u9ad8,\u5f88\u591a\u4eba\u53ef\u80fd\u5c31\u4f1a\u6bd4\u8f83\u5f98\u5f8a\uff0c\u56e0\u800c\u60f3\u628a\u81ea\u5df1\u5b66\u4e60AI\u7684\u8fc7\u7a0b\u5199\u6210\u672c\u4e66,\u4f9b\u5927\u5bb6\u53c2\u8003\u548c\u5b66\u4e60\uff01 BP\u795e\u7ecf\u7f51\u7edc RBF\u7b97\u6cd5 SOM\u795e\u7ecf\u7f51\u7edc ART\u795e\u7ecf\u7f51\u7edc \u8d1d\u53f6\u65af\u7f51\u7edc \u7c97\u7cd9\u96c6 \u5b64\u7acb\u70b9\u5206\u6790 CART EM FP\u2014tree GSP\u5e8f\u5217 \u534f\u540c\u8fc7\u6ee4 BIRCH Prefixspan PageRank AdaBoost CBA KNN Hopfield\u795e\u7ecf\u7f51\u7edc \u51b3\u7b56\u6811 \u805a\u7c7b\u5206\u6790 \u5173\u8054\u89c4\u5219 \u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09 \u540e\u9762\u7684\u7b97\u6cd5\u548c\u6211\u4eec\u7684\u7b97\u6cd5\u6a21\u578b\uff0c\u6211\u4f1a\u6301\u7eed\u66f4\u65b0\u6574\u7406\uff0c\u540e\u7eed\u7684\u7b97\u6cd5\u7ae0\u8282\u4f1a\u4e0d\u65ad\u7684\u8865\u4e0a\uff0c\u5e0c\u671b\u53ef\u4ee5\u5bf9\u65b0\u5165\u95e8\u5b66\u4e60AI\u5f00\u53d1\u548c\u7b97\u6cd5\u7684\u5f00\u53d1\u8005\u6709\u6240\u5e2e\u52a9\uff01 \u7b97\u6cd5\u6a21\u578b \u56de\u5f52\u7b97\u6cd5 \u7ebf\u6027\u56de\u5f52 \u903b\u8f91\u56de\u5f52 \u591a\u5143\u81ea\u9002\u5e94\u56de\u5f52(MARS) \u672c\u5730\u6563\u70b9\u5e73\u6ed1\u4f30\u8ba1(LOESS) \u57fa\u4e8e\u5b9e\u4f8b\u7684\u5b66\u4e60\u7b97\u6cd5 K-\u8fd1\u90bb\u7b97\u6cd5\uff08KNN\uff09 \u5b66\u4e60\u77e2\u91cf\u5316\uff08LVQ\uff09 \u81ea\u7ec4\u7ec7\u6620\u5c04\u7b97\u6cd5\uff08SOM\uff09 \u5c40\u90e8\u52a0\u6743\u5b66\u4e60\u7b97\u6cd5\uff08LWR\uff09 \u6b63\u5219\u5316\u7b97\u6cd5 \u5cad\u56de\u5f52\uff08Ridge Regression\uff09 LASSO\uff08Least Absolute Shrinkage and Selection Operator\uff09 \u5f39\u6027\u7f51\u7edc(Elastic Net) \u6700\u5c0f\u89d2\u56de\u5f52(LARS) \u51b3\u7b56\u6811\u7b97\u6cd5 \u5206\u7c7b\u548c\u56de\u5f52\u6811\uff08CART\uff09 ID3\u7b97\u6cd5(Iterative Dichotomiser 3) CHAID\uff08Chi-squared Automatic Interaction Detection) \u968f\u673a\u68ee\u6797\uff08Random Forest\uff09 \u591a\u5143\u81ea\u9002\u5e94\u56de\u5f52\u6837\u6761\uff08MARS\uff09 \u68af\u5ea6\u63a8\u8fdb\u673a\uff08Gradient Boosting Machine,GBM\uff09 \u8d1d\u53f6\u65af\u7b97\u6cd5 \u6734\u7d20\u8d1d\u53f6\u65af(Naive Bayes) \u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af(Gaussian Naive Bayes) \u591a\u9879\u5f0f\u6734\u7d20\u8d1d\u53f6\u65af(Multinomial Naive Bayes) \u5e73\u5747\u5355\u4f9d\u8d56\u4f30\u8ba1\u91cf\uff08Averaged One-Dependence Estimators\uff09 \u8d1d\u53f6\u65af\u7f51\u7edc\uff08Bayesian Belief Network\uff09 \u57fa\u4e8e\u6838\u7684\u7b97\u6cd5 \u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09 \u5f84\u5411\u57fa\u51fd\u6570\uff08Radial Basis Function \uff0cRBF) \u7ebf\u6027\u5224\u522b\u5206\u6790\uff08Linear Discriminate Analysis \uff0cLDA) \u805a\u7c7b\u7b97\u6cd5(Cluster analysis) K-\u5747\u503c(K-Means Algorithm) \u6a21\u7ccac-\u5747\u503c\u805a\u7c7b\u7b97\u6cd5(Fuzzy C-means Algorithm) \u671f\u671b\u6700\u5927\u5316(Expectation-Maximization) \u805a\u7c7b\u5206\u6790(Cluster Analysis) \u5173\u8054\u89c4\u5219\u5b66\u4e60(Association Rule Learning) \u5148\u9a8c\u7b97\u6cd5(Apriori Algorithm) Eclat\u7b97\u6cd5(Eclat Algorithm) FP-growth\u7b97\u6cd5(FP-Growth Algorithm) \u4eba\u5de5\u795e\u7ecf\u7f51\u7edc(Artificial Neural Network) \u81ea\u52a8\u7f16\u7801\u5668(Autoencoder) \u53cd\u5411\u4f20\u64ad(Backpropagation) \u9012\u5f52\u795e\u7ecf\u7f51\u7edc(Recurrent Neural Network) ) \u591a\u5c42\u611f\u77e5\u5668(Multilayer Perceptron) \u73bb\u5c14\u5179\u66fc\u673a(Boltzmann Machine) \u5377\u79ef\u795e\u7ecf\u7f51\u7edc(Convolutional Neural Network) Hopfield\u7f51\u7edc(Hopfield Network) \u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc(Radial Basis Function Network) \u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a(Restricted Boltzmann Machine) \u81ea\u7ec4\u7ec7\u6620\u5c04(Self-Organizing Map) \u8109\u51b2\u795e\u7ecf\u7f51\u7edc(Spiking Neural Network) \u6df1\u5ea6\u5b66\u4e60(Deep Learning) \u6df1\u5ea6\u4fe1\u5ff5\u7f51\u7edc(Deep Belief Machines) \u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(Deep Convolutional Neural Networks) \u6df1\u5ea6\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(Deep Recurrent Neural Networks) \u5206\u5c42\u65f6\u95f4\u8bb0\u5fc6(Hierarchical Temporal Memory) \u5806\u53e0\u81ea\u52a8\u7f16\u7801\u5668(Stacked AutoEncoder) \u751f\u6210\u5f0f\u5bf9\u6297\u7f51\u7edc(Generative Adversarial Networks) \u964d\u7ef4\u7b97\u6cd5(Dimensionality Reduction Algorithm) \u4e3b\u6210\u5206\u5206\u6790\u6cd5(Principal Component Analysis) \u591a\u7ef4\u7f29\u653e(Mutiple Dimensional Scaling) \u7ebf\u6027\u5224\u522b\u5206\u6790(Linear Discriminant Analysis) \u7b49\u5ea6\u91cf\u6620\u5c04(IsometricMapping,Isomap) \u5c40\u90e8\u7ebf\u6027\u5d4c\u5165(Locally Linear Embedding) \u62c9\u666e\u62c9\u65af\u7279\u5f81\u6620\u5c04(Laplacian Eigenmaps) t\uff0d\u5206\u5e03\u968f\u673a\u8fd1\u90bb\u5d4c\u5165(t-SNE) \u6df1\u5ea6\u81ea\u52a8\u7f16\u7801\u5668(Deep Autoencoder Networks) \u96c6\u6210\u7b97\u6cd5(Ensemble Learning) Boosting Bagging AdaBoost \u5806\u53e0\u6cdb\u5316\uff08\u6df7\u5408\uff09 GBM \u7b97\u6cd5 GBRT \u7b97\u6cd5 \u968f\u673a\u68ee\u6797 \u5176\u4ed6\u7b97\u6cd5 \u7279\u5f81\u9009\u62e9\u7b97\u6cd5 \u6027\u80fd\u8bc4\u4f30\u7b97\u6cd5 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8ba1\u7b97\u673a\u89c6\u89c9 \u63a8\u8350\u7cfb\u7edf \u5f3a\u5316\u5b66\u4e60 \u8fc1\u79fb\u5b66\u4e60 \u7b97\u6cd5\u6a21\u578b\u7684\u6574\u4f53\u57fa\u672c\u5c31\u662f\u8fd9\u6837\u76ee\u5f55\uff0c\u540e\u7eed\u7684\u7b97\u6cd5\u6a21\u578b\u6211\u4f1a\u4e0d\u65ad\u5b8c\u5584\u548c\u8865\u5145\uff0c\u66f4\u65b0\uff01 \u673a\u5668\u5b66\u4e60\u7684\u57fa\u7840 \u673a\u5668\u5b66\u4e60\u9700\u8981\u7684\u7406\u8bba\u57fa\u7840\uff1a\u6570\u5b66\uff0c\u7ebf\u6027\u4ee3\u6570\uff0c\u6570\u7406\u7edf\u8ba1\uff0c\u6982\u7387\u8bba\uff0c\u9ad8\u7b49\u6570\u5b66\u3001\u51f8\u4f18\u5316\u7406\u8bba\uff0c\u5f62\u5f0f\u903b\u8f91\u7b49 \u53c2\u8003\u4e66\u7c4d \u540c\u6d4e\u7ebf\u6027\u4ee3\u6570\u6559\u6750 \u540c\u6d4e\u9ad8\u7b49\u6570\u5b66\u7b2c\u516d\u7248\u4e0a\u4e0b\u518c \u6982\u7387\u8bba\u4e0e\u6570\u7406\u7edf\u8ba1\u540c\u6d4e\u5927\u5b66 \u51f8\u4f18\u5316\u7406\u8bba \u673a\u5668\u5b66\u4e60-\u5468\u5fd7\u534e \u9762\u5411\u673a\u5668\u667a\u80fd\u7684TensorFlow\u5b9e\u8df5 \u673a\u5668\u5b66\u4e60 \u6570\u5b66\u4e4b\u7f8e \u6df1\u5ea6\u5b66\u4e60 \u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5b66\u4e60 \u68af\u5ea6\u4e0b\u964d \u65e0\u76d1\u7763\u795e\u7ecf\u5143 Tensorflow\u5b9e\u8df5 Artificial Intelligence Tensorflow\u65b0\u624b\u5165\u95e8 \u673a\u5668\u5b66\u4e60 \u89c9\u5f97\u6b64\u6587\u7ae0\u4e0d\u9519\uff0c\u652f\u6301\u6211\u7684\u8bdd\u53ef\u4ee5\u7ed9\u6211star \uff0c:star:\uff01\u5982\u679c\u6709\u95ee\u9898\u53ef\u4ee5\u52a0\u6211\u7684\u5fae\u4fe1 Sen0676 ,\u52a0\u5165\u6211\u4eec\u7684\u4ea4\u6d41\u7fa4\u4e00\u8d77\u4ea4\u6d41\u673a\u5668\u5b66\u4e60\uff01 License This is free software distributed under the terms of the MIT license","title":"ML_tutor"},{"location":"ml_tutor/#deeplearning-algorithms-tutorial","text":"\u6700\u8fd1\u4ee5\u6765\u4e00\u76f4\u5728\u5b66\u4e60\u673a\u5668\u5b66\u4e60\u548c\u7b97\u6cd5\uff0c\u7136\u540e\u81ea\u5df1\u5c31\u5728\u4e0d\u65ad\u603b\u7ed3\u548c\u5199\u7b14\u8bb0\uff0c\u8bb0\u5f55\u4e0b\u81ea\u5df1\u7684\u5b66\u4e60AI\u4e0e\u7b97\u6cd5\u5386\u7a0b\u3002 \u673a\u5668\u5b66\u4e60(Machine Learning, ML)\u662f\u4e00\u95e8\u591a\u9886\u57df\u4ea4\u53c9\u5b66\u79d1\uff0c\u6d89\u53ca\u6982\u7387\u8bba\u3001\u7edf\u8ba1\u5b66\u3001\u903c\u8fd1\u8bba\u3001\u51f8\u5206\u6790\u3001\u7b97\u6cd5\u590d\u6742\u5ea6\u7406\u8bba\u7b49\u591a\u95e8\u5b66\u79d1\u3002\u4e13\u95e8\u7814\u7a76\u8ba1\u7b97\u673a\u600e\u6837\u6a21\u62df\u6216\u5b9e\u73b0\u4eba\u7c7b\u7684\u5b66\u4e60\u884c\u4e3a\uff0c\u4ee5\u83b7\u53d6\u65b0\u7684\u77e5\u8bc6\u6216\u6280\u80fd\uff0c\u91cd\u65b0\u7ec4\u7ec7\u5df2\u6709\u7684\u77e5\u8bc6\u7ed3\u6784\u4f7f\u4e4b\u4e0d\u65ad\u6539\u5584\u81ea\u8eab\u7684\u6027\u80fd\u3002 \u673a\u5668\u5b66\u4e60\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u673a\u5668\u5b66\u4e60\u9010\u6e10\u53d1\u5c55\u6210\u6a21\u5f0f\u8bc6\u522b\u548c\u8ba1\u7b97\u79d1\u5b66\u7406\u8bba\u7684\u7814\u7a76\u3002 \u673a\u5668\u5b66\u4e60\uff1a\u591a\u9886\u57df\u4ea4\u53c9\u5b66\u79d1\uff0c\u6d89\u53ca\u6982\u7387\u8bba\u7edf\u8ba1\u5b66\uff0c\u903c\u8fd1\u8bba\uff0c\u51f8\u5206\u6790\uff0c\u7b97\u6cd5\u590d\u6742\u5ea6\u7406\u8bba\u7b49\u591a\u95e8\u5b66\u79d1\u3002 \u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\uff1a\u8bed\u97f3\u8bc6\u522b\uff0c\u81ea\u52a8\u9a7e\u9a76\uff0c\u8bed\u8a00\u7ffb\u8bd1\uff0c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u63a8\u8350\u7cfb\u7edf\uff0c\u65e0\u4eba\u673a\uff0c\u8bc6\u522b\u5783\u573e\u90ae\u4ef6\uff0c\u4eba\u8138\u8bc6\u522b\uff0c\u7535\u5546\u63a8\u8350\u7cfb\u7edf\u3002 \u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u6982\u5ff5\uff1a\u8bad\u7ec3\u96c6\uff0c\u6d4b\u8bd5\u96c6\uff0c\u7279\u5f81\u503c\uff0c\u76d1\u7763\u5b66\u4e60\uff0c\u975e\u76d1\u7763\u5b66\u4e60\uff0c\u5206\u7c7b\uff0c\u56de\u5f52 \u76ee\u524d\u56fd\u5185\u5728AI\u611f\u77e5\u5c42\u9762\u5e94\u7528\u5df2\u7ecf\u767e\u82b1\u9f50\u653e\uff0c\u4e3b\u8981\u662f\u65e0\u4eba\u9a7e\u9a76\u3001\u667a\u80fd\u97f3\u7bb1\u3001\u5d4c\u5165\u5f0f\u3002\u4f46\u5728\u8ba4\u77e5\u5c42\u9762\u8fd8\u662f\u6bd4\u8f83\u7f3a\u4e4f\uff0c\u6240\u4ee5\u65b0\u5165\u884c\u7684AI\u5e94\u7528\u56e2\u961f\u53ef\u4ee5\u653e\u5728\u8ba4\u77e5\u5c42\u3002\u5982\u5f00\u5934\u6240\u8ff0\uff0c\u8ba4\u77e5\u5c42\u6700\u91cd\u8981\u7684\u662f\u7b97\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u9605\u8bfbNature\u4e0a\u6700\u9886\u5148\u7684\u7b97\u6cd5\u516c\u53f8DeepMind\u7684\u51e0\u7bc7\u5927\u4f5c\uff0c\u5982\u4e0b\uff1a 2016.01.Mastering the game of Go with deep neural networks and tree search 2016.10.Hybrid computing using a neural network with dynamic external memory 2017.10.Mastering the game of Go without human knowledge","title":"Deeplearning Algorithms tutorial"},{"location":"ml_tutor/#_1","text":"\u628a\u6570\u636e\u62c6\u5206\u4e3a\u8bad\u7ec3\u96c6\u5408\u6d4b\u8bd5\u96c6 \u7528\u8bad\u7ec3\u96c6\u5408\u8bad\u7ec3\u96c6\u7684\u7279\u5f81\u5411\u91cf\u6765\u8bad\u7ec3\u7b97\u6cd5 \u7528\u5b66\u4e60\u6765\u7684\u7b97\u6cd5\u8fd0\u7528\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7d2f\u8bc4\u4f30\u7b97\u6cd5(\u53ef\u80fd\u8981\u8bbe\u8ba1\u5230\u8c03\u6574\u53c2\u6570(parameter tuning) \u7528\u6765\u9a8c\u8bc1\u96c6(validation set))","title":"\u673a\u5668\u5b66\u4e60\u6b65\u9aa4\u6846\u67b6"},{"location":"ml_tutor/#_2","text":"\u673a\u5668\u5b66\u4e60\uff1a\u673a\u5668\u5b66\u4e60\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff0c\u5728\u4eba\u5de5\u667a\u80fd\u9886\u57df\uff0c\u673a\u5668\u5b66\u4e60\u9010\u6e10\u53d1\u5c55\u6210\u6a21\u5f0f\u8bc6\u522b\u548c\u8ba1\u7b97\u79d1\u5b66\u7406\u8bba\u7684\u7814\u7a76\u3002\u901a\u4fd7\u7684\u8bb2\u673a\u5668\u5b66\u4e60\u662f\u4e00\u79cd\u80fd\u591f\u8d4b\u4e88\u673a\u5668\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u53ef\u4ee5\u8ba9\u5b83\u5b8c\u6210\u76f4\u63a5\u7f16\u7a0b\u65e0\u6cd5\u5b8c\u6210\u7684\u529f\u80fd\u7684\u65b9\u6cd5\u3002\u4f46\u4ece\u5b9e\u8df5\u7684\u610f\u4e49\u4e0a\u6765\u8bf4\uff0c\u673a\u5668\u5b66\u4e60\u662f\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u6570\u636e\uff0c\u8bad\u7ec3\u51fa\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u7684\u4e00\u79cd\u65b9\u6cd5\u3002 \u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\uff1a\u673a\u5668\u5b66\u4e60\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6570\u636e\u6316\u6398\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u751f\u7269\u7279\u5f81\u8bc6\u522b\u3001\u641c\u7d22\u5f15\u64ce\u3001\u533b\u5b66\u8bca\u65ad\u3001\u68c0\u6d4b\u4fe1\u7528\u5361\u6b3a\u8bc8\u3001\u8bc1\u5238\u5e02\u573a\u5206\u6790\u3001DNA\u5e8f\u5217\u6d4b\u5e8f\u3001\u8bed\u97f3\u548c\u624b\u5199\u8bc6\u522b\u3001\u6218\u7565\u6e38\u620f\u548c\u673a\u5668\u4eba\u7b49\u9886\u57df\u3002\u4e0b\u9762\u5f00\u542f\u6211\u4eec\u7684\u673a\u5668\u5b66\u4e60\uff01 \u673a\u5668\u5b66\u4e60\u73af\u5883\u7684\u642d\u5efa \u673a\u5668\u5b66\u4e60\u7684\u5165\u95e8 \u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6 \u673a\u5668\u5b66\u4e60\u7684\u5e94\u7528\u6848\u4f8b","title":"\u673a\u5668\u5b66\u4e60"},{"location":"ml_tutor/#_3","text":"\u6df1\u5ea6\u5b66\u4e60\uff1a\u6df1\u5ea6\u5b66\u4e60\u662f\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u5ef6\u4f38\u51fa\u6765\u7684\u4e00\u4e2a\u65b0\u7684\u9886\u57df\uff0c\u7531\u4ee5\u4eba\u5927\u8111\u7ed3\u6784\u4e3a\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u4e3a\u8d77\u6e90\u52a0\u4e4b\u6a21\u578b\u7ed3\u6784\u6df1\u5ea6\u7684\u589e\u52a0\u53d1\u5c55\uff0c\u5e76\u4f34\u968f\u5927\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\u7684\u63d0\u9ad8\u800c\u4ea7\u751f\u7684\u4e00\u7cfb\u5217\u65b0\u7684\u7b97\u6cd5\u3002 \u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u5411\uff1a\u88ab\u5e94\u7528\u5728\u56fe\u50cf\u5904\u7406\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4ee5\u53ca\u8bed\u97f3\u8bc6\u522b\u7b49\u9886\u57df\u3002 \u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60\u7684\u5165\u95e8 \u6df1\u5ea6\u5b66\u4e60\u73af\u5883\u7684\u642d\u5efa \u6df1\u5ea6\u5b66\u4e60\u7684\u5e94\u7528\u6848\u4f8b","title":"\u6df1\u5ea6\u5b66\u4e60"},{"location":"ml_tutor/#_4","text":"\u4ece2016\u5e74\u8d77\uff0c\u673a\u5668\u5b66\u4e60\u6709\u4e86\u65b0\u7684\u7a81\u7834\u548c\u53d1\u5c55\u3002\u4f46\u662f\uff0c\u6709\u6548\u7684\u673a\u5668\u5b66\u4e60\u662f\u56f0\u96be\u7684\uff0c\u56e0\u4e3a\u673a\u5668\u5b66\u4e60\u672c\u8eab\u5c31\u662f\u4e00\u4e2a\u4ea4\u53c9\u5b66\u79d1\uff0c\u6ca1\u6709\u79d1\u5b66\u7684\u65b9\u6cd5\u53ca\u4e00\u5b9a\u7684\u79ef\u7d2f\u5f88\u96be\u5165\u95e8\u3002 \u4ece2017\u5e7410\u670819\u65e5\uff0cNature\u4e0a\u53d1\u8868\u4e86\u65b0\u4e00\u4ee3AlphaGo\u7248\u672cAlphaGo Zero\u7684\u6280\u672f\u8bba\u6587\u3002\u6307\u51fa\u4e00\u79cd\u4ec5\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7b97\u6cd5\uff0cAlphaGo Zero\u4e0d\u4f7f\u7528\u4eba\u7c7b\u7684\u6570\u636e\u3001\u6307\u5bfc\u6216\u89c4\u5219\u4ee5\u5916\u7684\u9886\u57df\u77e5\u8bc6\u6210\u4e86\u81ea\u5df1\u7684\u8001\u5e08\u3002DeepMind\u4ee3\u8868\u4e86\u76ee\u524d\u4eba\u5de5\u667a\u80fd\u9886\u57df\u6700\u5f3a\u7684\u6280\u672f\uff0c\u5176\u6838\u5fc3\u662f\u4e24\u4e2a\u5b57\uff1a\u7b97\u6cd5\u3002 \u5f88\u591a\u4eba\u90fd\u60f3\u6210\u4e3a\u4e00\u4e2aAI\u5f00\u53d1\u8005\uff0c\u4e0d\u4ec5\u662f\u56e0\u4e3aAI\u5f00\u53d1\u7684\u85aa\u8d44\u9ad8\uff0c\u66f4\u4e3b\u8981\u662f\u56e0\u4e3aAI\u8fd9\u51e0\u5e74\u7684\u5feb\u901f\u53d1\u5c55,\u4f46\u662f\u56e0\u4e3aAI\u672c\u8eab\u7684\u95e8\u69db\u5c31\u6bd4\u8f83\u9ad8,\u5f88\u591a\u4eba\u53ef\u80fd\u5c31\u4f1a\u6bd4\u8f83\u5f98\u5f8a\uff0c\u56e0\u800c\u60f3\u628a\u81ea\u5df1\u5b66\u4e60AI\u7684\u8fc7\u7a0b\u5199\u6210\u672c\u4e66,\u4f9b\u5927\u5bb6\u53c2\u8003\u548c\u5b66\u4e60\uff01 BP\u795e\u7ecf\u7f51\u7edc RBF\u7b97\u6cd5 SOM\u795e\u7ecf\u7f51\u7edc ART\u795e\u7ecf\u7f51\u7edc \u8d1d\u53f6\u65af\u7f51\u7edc \u7c97\u7cd9\u96c6 \u5b64\u7acb\u70b9\u5206\u6790 CART EM FP\u2014tree GSP\u5e8f\u5217 \u534f\u540c\u8fc7\u6ee4 BIRCH Prefixspan PageRank AdaBoost CBA KNN Hopfield\u795e\u7ecf\u7f51\u7edc \u51b3\u7b56\u6811 \u805a\u7c7b\u5206\u6790 \u5173\u8054\u89c4\u5219 \u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09 \u540e\u9762\u7684\u7b97\u6cd5\u548c\u6211\u4eec\u7684\u7b97\u6cd5\u6a21\u578b\uff0c\u6211\u4f1a\u6301\u7eed\u66f4\u65b0\u6574\u7406\uff0c\u540e\u7eed\u7684\u7b97\u6cd5\u7ae0\u8282\u4f1a\u4e0d\u65ad\u7684\u8865\u4e0a\uff0c\u5e0c\u671b\u53ef\u4ee5\u5bf9\u65b0\u5165\u95e8\u5b66\u4e60AI\u5f00\u53d1\u548c\u7b97\u6cd5\u7684\u5f00\u53d1\u8005\u6709\u6240\u5e2e\u52a9\uff01","title":"\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6982\u89c8"},{"location":"ml_tutor/#_5","text":"\u56de\u5f52\u7b97\u6cd5 \u7ebf\u6027\u56de\u5f52 \u903b\u8f91\u56de\u5f52 \u591a\u5143\u81ea\u9002\u5e94\u56de\u5f52(MARS) \u672c\u5730\u6563\u70b9\u5e73\u6ed1\u4f30\u8ba1(LOESS) \u57fa\u4e8e\u5b9e\u4f8b\u7684\u5b66\u4e60\u7b97\u6cd5 K-\u8fd1\u90bb\u7b97\u6cd5\uff08KNN\uff09 \u5b66\u4e60\u77e2\u91cf\u5316\uff08LVQ\uff09 \u81ea\u7ec4\u7ec7\u6620\u5c04\u7b97\u6cd5\uff08SOM\uff09 \u5c40\u90e8\u52a0\u6743\u5b66\u4e60\u7b97\u6cd5\uff08LWR\uff09 \u6b63\u5219\u5316\u7b97\u6cd5 \u5cad\u56de\u5f52\uff08Ridge Regression\uff09 LASSO\uff08Least Absolute Shrinkage and Selection Operator\uff09 \u5f39\u6027\u7f51\u7edc(Elastic Net) \u6700\u5c0f\u89d2\u56de\u5f52(LARS) \u51b3\u7b56\u6811\u7b97\u6cd5 \u5206\u7c7b\u548c\u56de\u5f52\u6811\uff08CART\uff09 ID3\u7b97\u6cd5(Iterative Dichotomiser 3) CHAID\uff08Chi-squared Automatic Interaction Detection) \u968f\u673a\u68ee\u6797\uff08Random Forest\uff09 \u591a\u5143\u81ea\u9002\u5e94\u56de\u5f52\u6837\u6761\uff08MARS\uff09 \u68af\u5ea6\u63a8\u8fdb\u673a\uff08Gradient Boosting Machine,GBM\uff09 \u8d1d\u53f6\u65af\u7b97\u6cd5 \u6734\u7d20\u8d1d\u53f6\u65af(Naive Bayes) \u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af(Gaussian Naive Bayes) \u591a\u9879\u5f0f\u6734\u7d20\u8d1d\u53f6\u65af(Multinomial Naive Bayes) \u5e73\u5747\u5355\u4f9d\u8d56\u4f30\u8ba1\u91cf\uff08Averaged One-Dependence Estimators\uff09 \u8d1d\u53f6\u65af\u7f51\u7edc\uff08Bayesian Belief Network\uff09 \u57fa\u4e8e\u6838\u7684\u7b97\u6cd5 \u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09 \u5f84\u5411\u57fa\u51fd\u6570\uff08Radial Basis Function \uff0cRBF) \u7ebf\u6027\u5224\u522b\u5206\u6790\uff08Linear Discriminate Analysis \uff0cLDA) \u805a\u7c7b\u7b97\u6cd5(Cluster analysis) K-\u5747\u503c(K-Means Algorithm) \u6a21\u7ccac-\u5747\u503c\u805a\u7c7b\u7b97\u6cd5(Fuzzy C-means Algorithm) \u671f\u671b\u6700\u5927\u5316(Expectation-Maximization) \u805a\u7c7b\u5206\u6790(Cluster Analysis) \u5173\u8054\u89c4\u5219\u5b66\u4e60(Association Rule Learning) \u5148\u9a8c\u7b97\u6cd5(Apriori Algorithm) Eclat\u7b97\u6cd5(Eclat Algorithm) FP-growth\u7b97\u6cd5(FP-Growth Algorithm) \u4eba\u5de5\u795e\u7ecf\u7f51\u7edc(Artificial Neural Network) \u81ea\u52a8\u7f16\u7801\u5668(Autoencoder) \u53cd\u5411\u4f20\u64ad(Backpropagation) \u9012\u5f52\u795e\u7ecf\u7f51\u7edc(Recurrent Neural Network) ) \u591a\u5c42\u611f\u77e5\u5668(Multilayer Perceptron) \u73bb\u5c14\u5179\u66fc\u673a(Boltzmann Machine) \u5377\u79ef\u795e\u7ecf\u7f51\u7edc(Convolutional Neural Network) Hopfield\u7f51\u7edc(Hopfield Network) \u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc(Radial Basis Function Network) \u53d7\u9650\u73bb\u5c14\u5179\u66fc\u673a(Restricted Boltzmann Machine) \u81ea\u7ec4\u7ec7\u6620\u5c04(Self-Organizing Map) \u8109\u51b2\u795e\u7ecf\u7f51\u7edc(Spiking Neural Network) \u6df1\u5ea6\u5b66\u4e60(Deep Learning) \u6df1\u5ea6\u4fe1\u5ff5\u7f51\u7edc(Deep Belief Machines) \u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(Deep Convolutional Neural Networks) \u6df1\u5ea6\u9012\u5f52\u795e\u7ecf\u7f51\u7edc(Deep Recurrent Neural Networks) \u5206\u5c42\u65f6\u95f4\u8bb0\u5fc6(Hierarchical Temporal Memory) \u5806\u53e0\u81ea\u52a8\u7f16\u7801\u5668(Stacked AutoEncoder) \u751f\u6210\u5f0f\u5bf9\u6297\u7f51\u7edc(Generative Adversarial Networks) \u964d\u7ef4\u7b97\u6cd5(Dimensionality Reduction Algorithm) \u4e3b\u6210\u5206\u5206\u6790\u6cd5(Principal Component Analysis) \u591a\u7ef4\u7f29\u653e(Mutiple Dimensional Scaling) \u7ebf\u6027\u5224\u522b\u5206\u6790(Linear Discriminant Analysis) \u7b49\u5ea6\u91cf\u6620\u5c04(IsometricMapping,Isomap) \u5c40\u90e8\u7ebf\u6027\u5d4c\u5165(Locally Linear Embedding) \u62c9\u666e\u62c9\u65af\u7279\u5f81\u6620\u5c04(Laplacian Eigenmaps) t\uff0d\u5206\u5e03\u968f\u673a\u8fd1\u90bb\u5d4c\u5165(t-SNE) \u6df1\u5ea6\u81ea\u52a8\u7f16\u7801\u5668(Deep Autoencoder Networks) \u96c6\u6210\u7b97\u6cd5(Ensemble Learning) Boosting Bagging AdaBoost \u5806\u53e0\u6cdb\u5316\uff08\u6df7\u5408\uff09 GBM \u7b97\u6cd5 GBRT \u7b97\u6cd5 \u968f\u673a\u68ee\u6797 \u5176\u4ed6\u7b97\u6cd5 \u7279\u5f81\u9009\u62e9\u7b97\u6cd5 \u6027\u80fd\u8bc4\u4f30\u7b97\u6cd5 \u81ea\u7136\u8bed\u8a00\u5904\u7406 \u8ba1\u7b97\u673a\u89c6\u89c9 \u63a8\u8350\u7cfb\u7edf \u5f3a\u5316\u5b66\u4e60 \u8fc1\u79fb\u5b66\u4e60 \u7b97\u6cd5\u6a21\u578b\u7684\u6574\u4f53\u57fa\u672c\u5c31\u662f\u8fd9\u6837\u76ee\u5f55\uff0c\u540e\u7eed\u7684\u7b97\u6cd5\u6a21\u578b\u6211\u4f1a\u4e0d\u65ad\u5b8c\u5584\u548c\u8865\u5145\uff0c\u66f4\u65b0\uff01","title":"\u7b97\u6cd5\u6a21\u578b"},{"location":"ml_tutor/#_6","text":"\u673a\u5668\u5b66\u4e60\u9700\u8981\u7684\u7406\u8bba\u57fa\u7840\uff1a\u6570\u5b66\uff0c\u7ebf\u6027\u4ee3\u6570\uff0c\u6570\u7406\u7edf\u8ba1\uff0c\u6982\u7387\u8bba\uff0c\u9ad8\u7b49\u6570\u5b66\u3001\u51f8\u4f18\u5316\u7406\u8bba\uff0c\u5f62\u5f0f\u903b\u8f91\u7b49","title":"\u673a\u5668\u5b66\u4e60\u7684\u57fa\u7840"},{"location":"ml_tutor/#_7","text":"\u540c\u6d4e\u7ebf\u6027\u4ee3\u6570\u6559\u6750 \u540c\u6d4e\u9ad8\u7b49\u6570\u5b66\u7b2c\u516d\u7248\u4e0a\u4e0b\u518c \u6982\u7387\u8bba\u4e0e\u6570\u7406\u7edf\u8ba1\u540c\u6d4e\u5927\u5b66 \u51f8\u4f18\u5316\u7406\u8bba \u673a\u5668\u5b66\u4e60-\u5468\u5fd7\u534e \u9762\u5411\u673a\u5668\u667a\u80fd\u7684TensorFlow\u5b9e\u8df5 \u673a\u5668\u5b66\u4e60 \u6570\u5b66\u4e4b\u7f8e \u6df1\u5ea6\u5b66\u4e60 \u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u5b66\u4e60 \u68af\u5ea6\u4e0b\u964d \u65e0\u76d1\u7763\u795e\u7ecf\u5143 Tensorflow\u5b9e\u8df5 Artificial Intelligence Tensorflow\u65b0\u624b\u5165\u95e8","title":"\u53c2\u8003\u4e66\u7c4d"},{"location":"ml_tutor/#_8","text":"\u89c9\u5f97\u6b64\u6587\u7ae0\u4e0d\u9519\uff0c\u652f\u6301\u6211\u7684\u8bdd\u53ef\u4ee5\u7ed9\u6211star \uff0c:star:\uff01\u5982\u679c\u6709\u95ee\u9898\u53ef\u4ee5\u52a0\u6211\u7684\u5fae\u4fe1 Sen0676 ,\u52a0\u5165\u6211\u4eec\u7684\u4ea4\u6d41\u7fa4\u4e00\u8d77\u4ea4\u6d41\u673a\u5668\u5b66\u4e60\uff01","title":"\u673a\u5668\u5b66\u4e60"},{"location":"ml_tutor/#license","text":"This is free software distributed under the terms of the MIT license","title":"License"},{"location":"mysql_chefs/","text":"MySQL Cookbook The MySQL Cookbook is a library cookbook that provides resource primitives (LWRPs) for use in recipes. It is designed to be a reference example for creating highly reusable cross-platform cookbooks. Scope This cookbook is concerned with the \"MySQL Community Server\", particularly those shipped with F/OSS Unix and Linux distributions. It does not address forks or value-added repackaged MySQL distributions like MariaDB or Percona. Maintainers This cookbook is maintained by the Sous Chefs. The Sous Chefs are a community of Chef cookbook maintainers working together to maintain important cookbooks. If you\u2019d like to know more please visit sous-chefs.org or come chat with us on the Chef Community Slack in #sous-chefs . Requirements Chef 12.7 or higher Network accessible package repositories 'recipe[selinux::disabled]' on RHEL platforms Platform Support The following platforms have been tested with Test Kitchen: OS 5.1 5.5 5.6 5.7 debian-8 X ubuntu-14.04 X X ubuntu-16.04 X centos-6 X X X X centos-7 X X X fedora X X openSUSE Leap X Cookbook Dependencies There are no hard coupled dependencies. However, there is a loose dependency on yum-mysql-community for RHEL/CentOS platforms. As of the 8.0 version of this cookbook, configuration of the package repos is now the responsibility of the user. Usage Place a dependency on the mysql cookbook in your cookbook's metadata.rb depends 'mysql', '~ 8.0' Then, in a recipe: mysql_service 'foo' do port '3306' version '5.5' initial_root_password 'change me' action [:create, :start] end The service name on the OS is mysql-foo . You can manually start and stop it with service mysql-foo start and service mysql-foo stop . If you use default as the name the service name will be mysql instead of mysql-default . The configuration file is at /etc/mysql-foo/my.cnf . It contains the minimum options to get the service running. It looks like this. # Chef generated my.cnf for instance mysql-foo [client] default-character-set = utf8 port = 3306 socket = /var/run/mysql-foo/mysqld.sock [mysql] default-character-set = utf8 [mysqld] user = mysql pid-file = /var/run/mysql-foo/mysqld.pid socket = /var/run/mysql-foo/mysqld.sock port = 3306 datadir = /var/lib/mysql-foo tmpdir = /tmp log-error = /var/log/mysql-foo/error.log !includedir /etc/mysql-foo/conf.d [mysqld_safe] socket = /var/run/mysql-foo/mysqld.sock You can put extra configuration into the conf.d directory by using the mysql_config resource, like this: mysql_service 'foo' do port '3306' version '5.5' initial_root_password 'change me' action [:create, :start] end mysql_config 'foo' do source 'my_extra_settings.erb' instance 'foo' notifies :restart, 'mysql_service[foo]' action :create end You are responsible for providing my_extra_settings.erb in your own cookbook's templates folder. The name of the mysql service instance must be provided in mysql config as this defaults to 'default'. Connecting with the mysql CLI command Logging into the machine and typing mysql with no extra arguments will fail. You need to explicitly connect over the socket with mysql -S /var/run/mysql-foo/mysqld.sock , or over the network with mysql -h 127.0.0.1 Upgrading from older version of the mysql cookbook It is strongly recommended that you rebuild the machine from scratch. This is easy if you have your data_dir on a dedicated mount point. If you must upgrade in-place, follow the instructions below. The 6.x series supports multiple service instances on a single machine. It dynamically names the support directories and service names. /etc/mysql becomes /etc/mysql-instance_name . Other support directories in /var /run etc work the same way. Make sure to specify the data_dir property on the mysql_service resource to point to the old /var/lib/mysql directory. Resources Overview mysql_service The mysql_service resource manages the basic plumbing needed to get a MySQL server instance running with minimal configuration. The :create action handles package installation, support directories, socket files, and other operating system level concerns. The internal configuration file contains just enough to get the service up and running, then loads extra configuration from a conf.d directory. Further configurations are managed with the mysql_config resource. If the data_dir is empty, a database will be initialized, and a root user will be set up with initial_root_password . If this directory already contains database files, no action will be taken. The :start action starts the service on the machine using the appropriate provider for the platform. The :start action should be omitted when used in recipes designed to build containers. Example mysql_service 'default' do version '5.7' bind_address '0.0.0.0' port '3306' data_dir '/data' initial_root_password 'Ch4ng3me' action [:create, :start] end Please note that when using notifies or subscribes , the resource to reference is mysql_service[name] , not service[mysql] . Parameters charset - specifies the default character set. Defaults to utf8 . data_dir - determines where the actual data files are kept on the machine. This is useful when mounting external storage. When omitted, it will default to the platform's native location. error_log - Tunable location of the error_log initial_root_password - allows the user to specify the initial root password for mysql when initializing new databases. This can be set explicitly in a recipe, driven from a node attribute, or from data_bags. When omitted, it defaults to ilikerandompasswords . Please be sure to change it. instance - A string to identify the MySQL service. By convention, to allow for multiple instances of the mysql_service , directories and files on disk are named mysql- instance_name . Defaults to the resource name. package_name - Defaults to a value looked up in an internal map. package_version - Specific version of the package to install,passed onto the underlying package manager. Defaults to nil . bind_address - determines the listen IP address for the mysqld service. When omitted, it will be determined by MySQL. If the address is \"regular\" IPv4/IPv6address (e.g 127.0.0.1 or ::1), the server accepts TCP/IP connections only for that particular address. If the address is \"0.0.0.0\" (IPv4) or \"::\" (IPv6), the server accepts TCP/IP connections on all IPv4 or IPv6 interfaces. mysqld_options - A key value hash of options to be rendered into the main my.cnf. WARNING - It is highly recommended that you use the mysql_config resource instead of sending extra config into a mysql_service resource. This will allow you to set up notifications and subscriptions between the service and its configuration. That being said, this can be useful for adding extra options needed for database initialization at first run. port - determines the listen port for the mysqld service. When omitted, it will default to '3306'. run_group - The name of the system group the mysql_service should run as. Defaults to 'mysql'. run_user - The name of the system user the mysql_service should run as. Defaults to 'mysql'. pid_file - Tunable location of the pid file. socket - determines where to write the socket file for the mysql_service instance. Useful when configuring clients on the same machine to talk over socket and skip the networking stack. Defaults to a calculated value based on platform and instance name. tmp_dir - Tunable location of the tmp_dir. version - allows the user to select from the versions available for the platform, where applicable. When omitted, it will install the default MySQL version for the target platform. Available version numbers are 5.0 , 5.1 , 5.5 , 5.6 , and 5.7 , depending on platform. Actions :create - Configures everything but the underlying operating system service. :delete - Removes everything but the package and data_dir. :start - Starts the underlying operating system service. :stop - Stops the underlying operating system service. :restart - Restarts the underlying operating system service. :reload - Reloads the underlying operating system service. Providers Chef selects the appropriate provider based on platform and version, but you can specify one if your platform support it. mysql_service[instance-1] do port '1234' data_dir '/mnt/lottadisk' provider Chef::Provider::MysqlServiceSysvinit action [:create, :start] end Chef::Provider::MysqlServiceBase - Configures everything needed to run a MySQL service except the platform service facility. This provider should never be used directly. The :start , :stop , :restart , and :reload actions are stubs meant to be overridden by the providers below. Chef::Provider::MysqlServiceSmf - Starts a mysql_service using the Service Management Facility, used by Solaris and Illumos. Manages the FMRI and method script. Chef::Provider::MysqlServiceSystemd - Starts a mysql_service using SystemD. Manages the unit file and activation state Chef::Provider::MysqlServiceSysvinit - Starts a mysql_service using SysVinit. Manages the init script and status. Chef::Provider::MysqlServiceUpstart - Starts a mysql_service using Upstart. Manages job definitions and status. mysql_config The mysql_config resource is a wrapper around the core Chef template resource. Instead of a path parameter, it uses the instance parameter to calculate the path on the filesystem where file is rendered. Example mysql_config[default] do source 'site.cnf.erb' action :create end Parameters config_name - The base name of the configuration file to be rendered into the conf.d directory on disk. Defaults to the resource name. cookbook - The name of the cookbook to look for the template source. Defaults to nil group - System group for file ownership. Defaults to 'mysql'. instance - Name of the mysql_service instance the config is meant for. Defaults to 'default'. owner - System user for file ownership. Defaults to 'mysql'. source - Template in cookbook to be rendered. variables - Variables to be passed to the underlying template resource. version - Version of the mysql_service instance the config is meant for. Used to calculate path. Only necessary when using packages with unique configuration paths, such as RHEL Software Collections or OmniOS. Defaults to 'nil' Actions :create - Renders the template to disk at a path calculated using the instance parameter. :delete - Deletes the file from the conf.d directory calculated using the instance parameter. More Examples mysql_service 'instance-1' do action [:create, :start] end mysql_service 'instance-2' do action [:create, :start] end mysql_config 'logging' do instance 'instance-1' source 'logging.cnf.erb' action :create notifies :restart, 'mysql_service[instance-1]' end mysql_config 'security settings for instance-2' do config_name 'security' instance 'instance-2' source 'security_stuff.cnf.erb' variables(:foo = 'bar') action :create notifies :restart, 'mysql_service[instance-2]' end mysql_client The mysql_client resource manages the MySQL client binaries and development libraries. It is an example of a \"singleton\" resource. Declaring two mysql_client resources on a machine usually won't yield two separate copies of the client binaries, except for platforms that support multiple versions (RHEL SCL, OmniOS). Example mysql_client 'default' do action :create end Properties package_name - An array of packages to be installed. Defaults to a value looked up in an internal map. package_version - Specific versions of the package to install, passed onto the underlying package manager. Defaults to nil . version - Major MySQL version number of client packages. Only valid on for platforms that support multiple versions, such as RHEL via Software Collections and OmniOS. Actions :create - Installs the client software :delete - Removes the client software Advanced Usage Examples There are a number of configuration scenarios supported by the use of resource primitives in recipes. For example, you might want to run multiple MySQL services, as different users, and mount block devices that contain pre-existing databases. Multiple Instances as Different Users # instance-1 user 'alice' do action :create end directory '/mnt/data/mysql/instance-1' do owner 'alice' action :create end mount '/mnt/data/mysql/instance-1' do device '/dev/sdb1' fstype 'ext4' action [:mount, :enable] end mysql_service 'instance-1' do port '3307' run_user 'alice' data_dir '/mnt/data/mysql/instance-1' action [:create, :start] end mysql_config 'site config for instance-1' do instance 'instance-1' source 'instance-1.cnf.erb' notifies :restart, 'mysql_service[instance-1]' end # instance-2 user 'bob' do action :create end directory '/mnt/data/mysql/instance-2' do owner 'bob' action :create end mount '/mnt/data/mysql/instance-2' do device '/dev/sdc1' fstype 'ext3' action [:mount, :enable] end mysql_service 'instance-2' do port '3308' run_user 'bob' data_dir '/mnt/data/mysql/instance-2' action [:create, :start] end mysql_config 'site config for instance-2' do instance 'instance-2' source 'instance-2.cnf.erb' notifies :restart, 'mysql_service[instance-2]' end Replication Testing Use multiple mysql_service instances to test a replication setup. This particular example serves as a smoke test in Test Kitchen because it exercises different resources and requires service restarts. https://github.com/chef-cookbooks/mysql/blob/master/test/fixtures/cookbooks/mysql_replication_test/recipes/default.rb Frequently Asked Questions How do I run this behind my firewall On Linux, the mysql_service resource uses the platform's underlying package manager to install software. For this to work behind firewalls, you'll need to either: Configure the system yum/apt utilities to use a proxy server that can reach the Internet Host a package repository on a network that the machine can talk to On the RHEL platform_family, applying the yum::default recipe will allow you to drive the yum_globalconfig resource with attributes to change the global yum proxy settings. If hosting repository mirrors, applying one of the following recipes and adjust the settings with node attributes. recipe[yum-centos::default] from the Supermarket https://supermarket.chef.io/cookbooks/yum-centos https://github.com/chef-cookbooks/yum-centos recipe[yum-mysql-community::default] from the Supermarket https://supermarket.chef.io/cookbooks/yum-mysql-community https://github.com/chef-cookbooks/yum-mysql-community The mysql command line doesn't work If you log into the machine and type mysql , you may see an error like this one: Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' This is because MySQL is hardcoded to read the defined default my.cnf file, typically at /etc/my.cnf, and this LWRP deletes it to prevent overlap among multiple MySQL configurations. To connect to the socket from the command line, check the socket in the relevant my.cnf file and use something like this: mysql -S /var/run/mysql-foo/mysqld.sock -Pwhatever Or to connect over the network, use something like this: connect over the network.. mysql -h 127.0.0.1 -Pwhatever These network or socket ssettings can also be put in you $HOME/.my.cnf, if preferred. What about MariaDB, Percona, etc MySQL forks are purposefully out of scope for this cookbook. This is mostly to reduce the testing matrix to a manageable size. Cookbooks for these technologies can easily be created by copying and adapting this cookbook. However, there will be differences. Package repository locations, package version names, software major version numbers, supported platform matrices, and the availability of software such as XtraDB and Galera are the main reasons that creating multiple cookbooks to make sense. There are existing cookbooks to carter for these forks, check them out on the supermarket Contributors This project exists thanks to all the people who contribute. Backers Thank you to all our backers! Sponsors Support this project by becoming a sponsor. Your logo will show up here with a link to your website.","title":"Mysql_chefs"},{"location":"mysql_chefs/#mysql-cookbook","text":"The MySQL Cookbook is a library cookbook that provides resource primitives (LWRPs) for use in recipes. It is designed to be a reference example for creating highly reusable cross-platform cookbooks.","title":"MySQL Cookbook"},{"location":"mysql_chefs/#scope","text":"This cookbook is concerned with the \"MySQL Community Server\", particularly those shipped with F/OSS Unix and Linux distributions. It does not address forks or value-added repackaged MySQL distributions like MariaDB or Percona.","title":"Scope"},{"location":"mysql_chefs/#maintainers","text":"This cookbook is maintained by the Sous Chefs. The Sous Chefs are a community of Chef cookbook maintainers working together to maintain important cookbooks. If you\u2019d like to know more please visit sous-chefs.org or come chat with us on the Chef Community Slack in #sous-chefs .","title":"Maintainers"},{"location":"mysql_chefs/#requirements","text":"Chef 12.7 or higher Network accessible package repositories 'recipe[selinux::disabled]' on RHEL platforms","title":"Requirements"},{"location":"mysql_chefs/#platform-support","text":"The following platforms have been tested with Test Kitchen: OS 5.1 5.5 5.6 5.7 debian-8 X ubuntu-14.04 X X ubuntu-16.04 X centos-6 X X X X centos-7 X X X fedora X X openSUSE Leap X","title":"Platform Support"},{"location":"mysql_chefs/#cookbook-dependencies","text":"There are no hard coupled dependencies. However, there is a loose dependency on yum-mysql-community for RHEL/CentOS platforms. As of the 8.0 version of this cookbook, configuration of the package repos is now the responsibility of the user.","title":"Cookbook Dependencies"},{"location":"mysql_chefs/#usage","text":"Place a dependency on the mysql cookbook in your cookbook's metadata.rb depends 'mysql', '~ 8.0' Then, in a recipe: mysql_service 'foo' do port '3306' version '5.5' initial_root_password 'change me' action [:create, :start] end The service name on the OS is mysql-foo . You can manually start and stop it with service mysql-foo start and service mysql-foo stop . If you use default as the name the service name will be mysql instead of mysql-default . The configuration file is at /etc/mysql-foo/my.cnf . It contains the minimum options to get the service running. It looks like this. # Chef generated my.cnf for instance mysql-foo [client] default-character-set = utf8 port = 3306 socket = /var/run/mysql-foo/mysqld.sock [mysql] default-character-set = utf8 [mysqld] user = mysql pid-file = /var/run/mysql-foo/mysqld.pid socket = /var/run/mysql-foo/mysqld.sock port = 3306 datadir = /var/lib/mysql-foo tmpdir = /tmp log-error = /var/log/mysql-foo/error.log !includedir /etc/mysql-foo/conf.d [mysqld_safe] socket = /var/run/mysql-foo/mysqld.sock You can put extra configuration into the conf.d directory by using the mysql_config resource, like this: mysql_service 'foo' do port '3306' version '5.5' initial_root_password 'change me' action [:create, :start] end mysql_config 'foo' do source 'my_extra_settings.erb' instance 'foo' notifies :restart, 'mysql_service[foo]' action :create end You are responsible for providing my_extra_settings.erb in your own cookbook's templates folder. The name of the mysql service instance must be provided in mysql config as this defaults to 'default'.","title":"Usage"},{"location":"mysql_chefs/#connecting-with-the-mysql-cli-command","text":"Logging into the machine and typing mysql with no extra arguments will fail. You need to explicitly connect over the socket with mysql -S /var/run/mysql-foo/mysqld.sock , or over the network with mysql -h 127.0.0.1","title":"Connecting with the mysql CLI command"},{"location":"mysql_chefs/#upgrading-from-older-version-of-the-mysql-cookbook","text":"It is strongly recommended that you rebuild the machine from scratch. This is easy if you have your data_dir on a dedicated mount point. If you must upgrade in-place, follow the instructions below. The 6.x series supports multiple service instances on a single machine. It dynamically names the support directories and service names. /etc/mysql becomes /etc/mysql-instance_name . Other support directories in /var /run etc work the same way. Make sure to specify the data_dir property on the mysql_service resource to point to the old /var/lib/mysql directory.","title":"Upgrading from older version of the mysql cookbook"},{"location":"mysql_chefs/#resources-overview","text":"","title":"Resources Overview"},{"location":"mysql_chefs/#mysql_service","text":"The mysql_service resource manages the basic plumbing needed to get a MySQL server instance running with minimal configuration. The :create action handles package installation, support directories, socket files, and other operating system level concerns. The internal configuration file contains just enough to get the service up and running, then loads extra configuration from a conf.d directory. Further configurations are managed with the mysql_config resource. If the data_dir is empty, a database will be initialized, and a root user will be set up with initial_root_password . If this directory already contains database files, no action will be taken. The :start action starts the service on the machine using the appropriate provider for the platform. The :start action should be omitted when used in recipes designed to build containers.","title":"mysql_service"},{"location":"mysql_chefs/#example","text":"mysql_service 'default' do version '5.7' bind_address '0.0.0.0' port '3306' data_dir '/data' initial_root_password 'Ch4ng3me' action [:create, :start] end Please note that when using notifies or subscribes , the resource to reference is mysql_service[name] , not service[mysql] .","title":"Example"},{"location":"mysql_chefs/#parameters","text":"charset - specifies the default character set. Defaults to utf8 . data_dir - determines where the actual data files are kept on the machine. This is useful when mounting external storage. When omitted, it will default to the platform's native location. error_log - Tunable location of the error_log initial_root_password - allows the user to specify the initial root password for mysql when initializing new databases. This can be set explicitly in a recipe, driven from a node attribute, or from data_bags. When omitted, it defaults to ilikerandompasswords . Please be sure to change it. instance - A string to identify the MySQL service. By convention, to allow for multiple instances of the mysql_service , directories and files on disk are named mysql- instance_name . Defaults to the resource name. package_name - Defaults to a value looked up in an internal map. package_version - Specific version of the package to install,passed onto the underlying package manager. Defaults to nil . bind_address - determines the listen IP address for the mysqld service. When omitted, it will be determined by MySQL. If the address is \"regular\" IPv4/IPv6address (e.g 127.0.0.1 or ::1), the server accepts TCP/IP connections only for that particular address. If the address is \"0.0.0.0\" (IPv4) or \"::\" (IPv6), the server accepts TCP/IP connections on all IPv4 or IPv6 interfaces. mysqld_options - A key value hash of options to be rendered into the main my.cnf. WARNING - It is highly recommended that you use the mysql_config resource instead of sending extra config into a mysql_service resource. This will allow you to set up notifications and subscriptions between the service and its configuration. That being said, this can be useful for adding extra options needed for database initialization at first run. port - determines the listen port for the mysqld service. When omitted, it will default to '3306'. run_group - The name of the system group the mysql_service should run as. Defaults to 'mysql'. run_user - The name of the system user the mysql_service should run as. Defaults to 'mysql'. pid_file - Tunable location of the pid file. socket - determines where to write the socket file for the mysql_service instance. Useful when configuring clients on the same machine to talk over socket and skip the networking stack. Defaults to a calculated value based on platform and instance name. tmp_dir - Tunable location of the tmp_dir. version - allows the user to select from the versions available for the platform, where applicable. When omitted, it will install the default MySQL version for the target platform. Available version numbers are 5.0 , 5.1 , 5.5 , 5.6 , and 5.7 , depending on platform.","title":"Parameters"},{"location":"mysql_chefs/#actions","text":":create - Configures everything but the underlying operating system service. :delete - Removes everything but the package and data_dir. :start - Starts the underlying operating system service. :stop - Stops the underlying operating system service. :restart - Restarts the underlying operating system service. :reload - Reloads the underlying operating system service.","title":"Actions"},{"location":"mysql_chefs/#providers","text":"Chef selects the appropriate provider based on platform and version, but you can specify one if your platform support it. mysql_service[instance-1] do port '1234' data_dir '/mnt/lottadisk' provider Chef::Provider::MysqlServiceSysvinit action [:create, :start] end Chef::Provider::MysqlServiceBase - Configures everything needed to run a MySQL service except the platform service facility. This provider should never be used directly. The :start , :stop , :restart , and :reload actions are stubs meant to be overridden by the providers below. Chef::Provider::MysqlServiceSmf - Starts a mysql_service using the Service Management Facility, used by Solaris and Illumos. Manages the FMRI and method script. Chef::Provider::MysqlServiceSystemd - Starts a mysql_service using SystemD. Manages the unit file and activation state Chef::Provider::MysqlServiceSysvinit - Starts a mysql_service using SysVinit. Manages the init script and status. Chef::Provider::MysqlServiceUpstart - Starts a mysql_service using Upstart. Manages job definitions and status.","title":"Providers"},{"location":"mysql_chefs/#mysql_config","text":"The mysql_config resource is a wrapper around the core Chef template resource. Instead of a path parameter, it uses the instance parameter to calculate the path on the filesystem where file is rendered.","title":"mysql_config"},{"location":"mysql_chefs/#example_1","text":"mysql_config[default] do source 'site.cnf.erb' action :create end","title":"Example"},{"location":"mysql_chefs/#parameters_1","text":"config_name - The base name of the configuration file to be rendered into the conf.d directory on disk. Defaults to the resource name. cookbook - The name of the cookbook to look for the template source. Defaults to nil group - System group for file ownership. Defaults to 'mysql'. instance - Name of the mysql_service instance the config is meant for. Defaults to 'default'. owner - System user for file ownership. Defaults to 'mysql'. source - Template in cookbook to be rendered. variables - Variables to be passed to the underlying template resource. version - Version of the mysql_service instance the config is meant for. Used to calculate path. Only necessary when using packages with unique configuration paths, such as RHEL Software Collections or OmniOS. Defaults to 'nil'","title":"Parameters"},{"location":"mysql_chefs/#actions_1","text":":create - Renders the template to disk at a path calculated using the instance parameter. :delete - Deletes the file from the conf.d directory calculated using the instance parameter.","title":"Actions"},{"location":"mysql_chefs/#more-examples","text":"mysql_service 'instance-1' do action [:create, :start] end mysql_service 'instance-2' do action [:create, :start] end mysql_config 'logging' do instance 'instance-1' source 'logging.cnf.erb' action :create notifies :restart, 'mysql_service[instance-1]' end mysql_config 'security settings for instance-2' do config_name 'security' instance 'instance-2' source 'security_stuff.cnf.erb' variables(:foo = 'bar') action :create notifies :restart, 'mysql_service[instance-2]' end","title":"More Examples"},{"location":"mysql_chefs/#mysql_client","text":"The mysql_client resource manages the MySQL client binaries and development libraries. It is an example of a \"singleton\" resource. Declaring two mysql_client resources on a machine usually won't yield two separate copies of the client binaries, except for platforms that support multiple versions (RHEL SCL, OmniOS).","title":"mysql_client"},{"location":"mysql_chefs/#example_2","text":"mysql_client 'default' do action :create end","title":"Example"},{"location":"mysql_chefs/#properties","text":"package_name - An array of packages to be installed. Defaults to a value looked up in an internal map. package_version - Specific versions of the package to install, passed onto the underlying package manager. Defaults to nil . version - Major MySQL version number of client packages. Only valid on for platforms that support multiple versions, such as RHEL via Software Collections and OmniOS.","title":"Properties"},{"location":"mysql_chefs/#actions_2","text":":create - Installs the client software :delete - Removes the client software","title":"Actions"},{"location":"mysql_chefs/#advanced-usage-examples","text":"There are a number of configuration scenarios supported by the use of resource primitives in recipes. For example, you might want to run multiple MySQL services, as different users, and mount block devices that contain pre-existing databases.","title":"Advanced Usage Examples"},{"location":"mysql_chefs/#multiple-instances-as-different-users","text":"# instance-1 user 'alice' do action :create end directory '/mnt/data/mysql/instance-1' do owner 'alice' action :create end mount '/mnt/data/mysql/instance-1' do device '/dev/sdb1' fstype 'ext4' action [:mount, :enable] end mysql_service 'instance-1' do port '3307' run_user 'alice' data_dir '/mnt/data/mysql/instance-1' action [:create, :start] end mysql_config 'site config for instance-1' do instance 'instance-1' source 'instance-1.cnf.erb' notifies :restart, 'mysql_service[instance-1]' end # instance-2 user 'bob' do action :create end directory '/mnt/data/mysql/instance-2' do owner 'bob' action :create end mount '/mnt/data/mysql/instance-2' do device '/dev/sdc1' fstype 'ext3' action [:mount, :enable] end mysql_service 'instance-2' do port '3308' run_user 'bob' data_dir '/mnt/data/mysql/instance-2' action [:create, :start] end mysql_config 'site config for instance-2' do instance 'instance-2' source 'instance-2.cnf.erb' notifies :restart, 'mysql_service[instance-2]' end","title":"Multiple Instances as Different Users"},{"location":"mysql_chefs/#replication-testing","text":"Use multiple mysql_service instances to test a replication setup. This particular example serves as a smoke test in Test Kitchen because it exercises different resources and requires service restarts. https://github.com/chef-cookbooks/mysql/blob/master/test/fixtures/cookbooks/mysql_replication_test/recipes/default.rb","title":"Replication Testing"},{"location":"mysql_chefs/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"mysql_chefs/#how-do-i-run-this-behind-my-firewall","text":"On Linux, the mysql_service resource uses the platform's underlying package manager to install software. For this to work behind firewalls, you'll need to either: Configure the system yum/apt utilities to use a proxy server that can reach the Internet Host a package repository on a network that the machine can talk to On the RHEL platform_family, applying the yum::default recipe will allow you to drive the yum_globalconfig resource with attributes to change the global yum proxy settings. If hosting repository mirrors, applying one of the following recipes and adjust the settings with node attributes. recipe[yum-centos::default] from the Supermarket https://supermarket.chef.io/cookbooks/yum-centos https://github.com/chef-cookbooks/yum-centos recipe[yum-mysql-community::default] from the Supermarket https://supermarket.chef.io/cookbooks/yum-mysql-community https://github.com/chef-cookbooks/yum-mysql-community","title":"How do I run this behind my firewall"},{"location":"mysql_chefs/#the-mysql-command-line-doesnt-work","text":"If you log into the machine and type mysql , you may see an error like this one: Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' This is because MySQL is hardcoded to read the defined default my.cnf file, typically at /etc/my.cnf, and this LWRP deletes it to prevent overlap among multiple MySQL configurations. To connect to the socket from the command line, check the socket in the relevant my.cnf file and use something like this: mysql -S /var/run/mysql-foo/mysqld.sock -Pwhatever Or to connect over the network, use something like this: connect over the network.. mysql -h 127.0.0.1 -Pwhatever These network or socket ssettings can also be put in you $HOME/.my.cnf, if preferred.","title":"The mysql command line doesn't work"},{"location":"mysql_chefs/#what-about-mariadb-percona-etc","text":"MySQL forks are purposefully out of scope for this cookbook. This is mostly to reduce the testing matrix to a manageable size. Cookbooks for these technologies can easily be created by copying and adapting this cookbook. However, there will be differences. Package repository locations, package version names, software major version numbers, supported platform matrices, and the availability of software such as XtraDB and Galera are the main reasons that creating multiple cookbooks to make sense. There are existing cookbooks to carter for these forks, check them out on the supermarket","title":"What about MariaDB, Percona, etc"},{"location":"mysql_chefs/#contributors","text":"This project exists thanks to all the people who contribute.","title":"Contributors"},{"location":"mysql_chefs/#backers","text":"Thank you to all our backers!","title":"Backers"},{"location":"mysql_chefs/#sponsors","text":"Support this project by becoming a sponsor. Your logo will show up here with a link to your website.","title":"Sponsors"},{"location":"p_books/","text":"\u76ee\u5f55 \u8bed\u8a00\u65e0\u5173 IDE Web WEB\u670d\u52a1\u5668 \u5176\u5b83 \u51fd\u6570\u5f0f\u6982\u5ff5 \u5206\u5e03\u5f0f\u7cfb\u7edf \u5728\u7ebf\u6559\u80b2 \u5927\u6570\u636e \u64cd\u4f5c\u7cfb\u7edf \u6570\u636e\u5e93 \u667a\u80fd\u7cfb\u7edf \u6b63\u5219\u8868\u8fbe\u5f0f \u7248\u672c\u63a7\u5236 \u7a0b\u5e8f\u5458\u6742\u8c08 \u7ba1\u7406\u548c\u76d1\u63a7 \u7f16\u7a0b\u827a\u672f \u7f16\u8bd1\u539f\u7406 \u7f16\u8f91\u5668 \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66 \u8bbe\u8ba1\u6a21\u5f0f \u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5 \u9879\u76ee\u76f8\u5173 \u8bed\u8a00\u76f8\u5173 Android AWK C C# C++ CoffeeScript Dart Elasticsearch Elixir Erlang Fortran Golang Haskell HTML / CSS HTTP iOS Java JavaScript AngularJS Backbone.js D3.js ExtJS impress.js jQuery Node.js React.js Vue.js Zepto.js LaTeX LISP Lua Markdown MySQL NoSQL Perl PHP PostgreSQL Python Django R reStructuredText Ruby Rust Scala Scheme Scratch Shell Swift TypeScript VBA Vim Visual Prolog WebAssembly \u8bed\u8a00\u65e0\u5173 IDE IntelliJ IDEA \u7b80\u4f53\u4e2d\u6587\u4e13\u9898\u6559\u7a0b Web 3 Web Designs in 3 Weeks Chrome \u5f00\u53d1\u8005\u5de5\u5177\u4e2d\u6587\u624b\u518c Chrome\u6269\u5c55\u53ca\u5e94\u7528\u5f00\u53d1 Chrome\u6269\u5c55\u5f00\u53d1\u6587\u6863 Growth: \u5168\u6808\u589e\u957f\u5de5\u7a0b\u5e08\u6307\u5357 Grunt\u4e2d\u6587\u6587\u6863 Gulp \u5165\u95e8\u6307\u5357 gulp\u4e2d\u6587\u6587\u6863 HTTP \u63a5\u53e3\u8bbe\u8ba1\u6307\u5317 HTTP/2.0 \u4e2d\u6587\u7ffb\u8bd1 http2\u8bb2\u89e3 JSON\u98ce\u683c\u6307\u5357 Wireshark\u7528\u6237\u624b\u518c \u4e00\u7ad9\u5f0f\u5b66\u4e60Wireshark \u5173\u4e8e\u6d4f\u89c8\u5668\u548c\u7f51\u7edc\u7684 20 \u9879\u987b\u77e5 \u524d\u7aef\u4ee3\u7801\u89c4\u8303 \u53ca \u6700\u4f73\u5b9e\u8df5 \u524d\u7aef\u5f00\u53d1\u4f53\u7cfb\u5efa\u8bbe\u65e5\u8bb0 \u524d\u7aef\u8d44\u6e90\u5206\u4eab\uff08\u4e00\uff09 \u524d\u7aef\u8d44\u6e90\u5206\u4eab\uff08\u4e8c\uff09 \u6b63\u5219\u8868\u8fbe\u5f0f30\u5206\u949f\u5165\u95e8\u6559\u7a0b \u6d4f\u89c8\u5668\u5f00\u53d1\u5de5\u5177\u7684\u79d8\u5bc6 \u79fb\u52a8Web\u524d\u7aef\u77e5\u8bc6\u5e93 \u79fb\u52a8\u524d\u7aef\u5f00\u53d1\u6536\u85cf\u5939 WEB\u670d\u52a1\u5668 Apache \u4e2d\u6587\u624b\u518c Nginx\u5f00\u53d1\u4ece\u5165\u95e8\u5230\u7cbe\u901a - \u6dd8\u5b9d\u56e2\u961f Nginx\u6559\u7a0b\u4ece\u5165\u95e8\u5230\u7cbe\u901a - \u8fd0\u7ef4\u751f\u5b58\u65f6\u95f4 (PDF) \u5176\u5b83 OpenWrt\u667a\u80fd\u3001\u81ea\u52a8\u3001\u900f\u660e\u7ffb\u5899\u8def\u7531\u5668\u6559\u7a0b SAN \u7ba1\u7406\u5165\u95e8\u7cfb\u5217 Sketch \u4e2d\u6587\u624b\u518c \u6df1\u5165\u7406\u89e3\u5e76\u884c\u7f16\u7a0b \u51fd\u6570\u5f0f\u6982\u5ff5 \u50bb\u74dc\u51fd\u6570\u7f16\u7a0b \u5206\u5e03\u5f0f\u7cfb\u7edf \u8d70\u5411\u5206\u5e03\u5f0f (PDF) \u5728\u7ebf\u6559\u80b2 51CTO\u5b66\u9662 Codecademy CodeSchool Coursera Learn X in Y minutes shiyanlou TeamTreeHouse Udacity xuetangX \u6155\u8bfe\u7f51 \u6781\u5ba2\u5b66\u9662 \u6c47\u667a\u7f51 \u8ba1\u849c\u5ba2 \u5927\u6570\u636e Spark \u7f16\u7a0b\u6307\u5357\u7b80\u4f53\u4e2d\u6587\u7248 \u5927\u578b\u96c6\u7fa4\u4e0a\u7684\u5feb\u901f\u548c\u901a\u7528\u6570\u636e\u5904\u7406\u67b6\u6784 \u6570\u636e\u6316\u6398\u4e2d\u7ecf\u5178\u7684\u7b97\u6cd5\u5b9e\u73b0\u548c\u8be6\u7ec6\u7684\u6ce8\u91ca \u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357 \u64cd\u4f5c\u7cfb\u7edf Debian \u53c2\u8003\u624b\u518c Docker \u2014\u2014 \u4ece\u5165\u95e8\u5230\u5b9e\u8df5 Docker\u4e2d\u6587\u6307\u5357 Docker\u5165\u95e8\u5b9e\u6218 FreeBSD \u4f7f\u7528\u624b\u518c Linux Documentation (\u4e2d\u6587\u7248) Linux Guide for Complete Beginners Linux \u6784\u5efa\u6307\u5357 Linux \u7cfb\u7edf\u9ad8\u7ea7\u7f16\u7a0b Linux\u5de5\u5177\u5feb\u901f\u6559\u7a0b Mac \u5f00\u53d1\u914d\u7f6e\u624b\u518c Operating Systems: Three Easy Pieces The Linux Command Line Ubuntu \u53c2\u8003\u624b\u518c uCore Lab: Operating System Course in Tsinghua University UNIX TOOLBOX \u547d\u4ee4\u884c\u7684\u827a\u672f \u5d4c\u5165\u5f0f Linux \u77e5\u8bc6\u5e93 (eLinux.org \u4e2d\u6587\u7248) \u5f00\u6e90\u4e16\u754c\u65c5\u884c\u624b\u518c \u7406\u89e3Linux\u8fdb\u7a0b \u9e1f\u54e5\u7684 Linux \u79c1\u623f\u83dc \u57fa\u7840\u5b66\u4e60\u7bc7 \u9e1f\u54e5\u7684 Linux \u79c1\u623f\u83dc \u670d\u52a1\u5668\u67b6\u8bbe\u7bc7 \u6570\u636e\u5e93 \u667a\u80fd\u7cfb\u7edf \u4e00\u6b65\u6b65\u642d\u5efa\u7269\u8054\u7f51\u7cfb\u7edf \u6b63\u5219\u8868\u8fbe\u5f0f \u6b63\u5219\u8868\u8fbe\u5f0f-\u83dc\u9e1f\u6559\u7a0b \u6b63\u5219\u8868\u8fbe\u5f0f30\u5206\u949f\u5165\u95e8\u6559\u7a0b \u7248\u672c\u63a7\u5236 Git - \u7b80\u6613\u6307\u5357 Git-Cheat-Sheet - flyhigher139 Git Community Book \u4e2d\u6587\u7248 git-flow \u5907\u5fd8\u6e05\u5355 Git magic Git Magic Git \u53c2\u8003\u624b\u518c Github\u5e2e\u52a9\u6587\u6863 GitHub\u79d8\u7c4d Git\u6559\u7a0b - \u5ed6\u96ea\u5cf0 Got GitHub GotGitHub HgInit (\u4e2d\u6587\u7248) Mercurial \u4f7f\u7528\u6559\u7a0b Pro Git Pro Git \u4e2d\u6587\u7248 Pro Git \u7b2c\u4e8c\u7248 \u4e2d\u6587\u7248 - Bingo Huang svn \u624b\u518c \u5b66\u4e60 Git \u5206\u652f \u6c89\u6d78\u5f0f\u5b66 Git \u7334\u5b50\u90fd\u80fd\u61c2\u7684GIT\u5165\u95e8 \u7a0b\u5e8f\u5458\u6742\u8c08 \u7a0b\u5e8f\u5458\u7684\u81ea\u6211\u4fee\u517b \u7ba1\u7406\u548c\u76d1\u63a7 ElasticSearch \u6743\u5a01\u6307\u5357 Elasticsearch \u6743\u5a01\u6307\u5357\uff08\u4e2d\u6587\u7248\uff09 ELKstack \u4e2d\u6587\u6307\u5357 Logstash \u6700\u4f73\u5b9e\u8df5 Mastering Elasticsearch(\u4e2d\u6587\u7248) Puppet 2.7 Cookbook \u4e2d\u6587\u7248 \u7f16\u7a0b\u827a\u672f \u6bcf\u4e2a\u7a0b\u5e8f\u5458\u90fd\u5e94\u8be5\u4e86\u89e3\u7684\u5185\u5b58\u77e5\u8bc6 (\u7b2c\u4e00\u90e8\u5206) \u7a0b\u5e8f\u5458\u7f16\u7a0b\u827a\u672f \u7f16\u7a0b\u5165\u95e8\u6307\u5357 \u7f16\u8bd1\u539f\u7406 \u300a\u8ba1\u7b97\u673a\u7a0b\u5e8f\u7684\u7ed3\u6784\u548c\u89e3\u91ca\u300b\u516c\u5f00\u8bfe \u7ffb\u8bd1\u9879\u76ee \u7f16\u8f91\u5668 exvim--vim \u6539\u826f\u6210IDE\u9879\u76ee Vim\u4e2d\u6587\u6587\u6863 \u6240\u9700\u5373\u6240\u83b7\uff1a\u50cf IDE \u4e00\u6837\u4f7f\u7528 vim \u7b28\u65b9\u6cd5\u5b66Vimscript \u4e2d\u8bd1\u672c \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66 LearnOpenGL CN OpenGL \u6559\u7a0b \u8bbe\u8ba1\u6a21\u5f0f \u53f2\u4e0a\u6700\u5168\u8bbe\u8ba1\u6a21\u5f0f\u5bfc\u5b66\u76ee\u5f55 \u56fe\u8bf4\u8bbe\u8ba1\u6a21\u5f0f \u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5 \u50bb\u74dc\u51fd\u6570\u7f16\u7a0b (\u300aFunctional Programming For The Rest of Us\u300b\u4e2d\u6587\u7248) \u785d\u70df\u4e2d\u7684 Scrum \u548c XP \u9879\u76ee\u76f8\u5173 GNU make \u6307\u5357 Gradle 2 \u7528\u6237\u6307\u5357 Gradle \u4e2d\u6587\u4f7f\u7528\u6587\u6863 Joel\u8c08\u8f6f\u4ef6 selenium \u4e2d\u6587\u6587\u6863 \u5f00\u6e90\u8f6f\u4ef6\u67b6\u6784 \u7d04\u8033\u8ac7\u8edf\u9ad4(Joel on Software) (\u7e41\u4f53\u4e2d\u6587) \u7f16\u7801\u89c4\u8303 \u8ba9\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u5217\u4e13\u680f \u8ffd\u6c42\u4ee3\u7801\u8d28\u91cf \u8bed\u8a00\u76f8\u5173 Android Android Design(\u4e2d\u6587\u7248) Android Note(\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u79ef\u7d2f\u7684\u77e5\u8bc6\u70b9) Android6.0\u65b0\u7279\u6027\u8be6\u89e3 Android\u5b66\u4e60\u4e4b\u8def Android\u5f00\u53d1\u6280\u672f\u524d\u7ebf(android-tech-frontier) Google Android\u5b98\u65b9\u57f9\u8bad\u8bfe\u7a0b\u4e2d\u6587\u7248 Google Material Design \u6b63\u9ad4\u4e2d\u6587\u7248 ( \u8bd1\u672c\u4e00 \uff0c \u8bd1\u672c\u4e8c ) Material Design \u4e2d\u6587\u7248 Point-of-Android AWK awk\u4e2d\u6587\u6307\u5357 awk\u7a0b\u5e8f\u8bbe\u8ba1\u8bed\u8a00 C C \u8bed\u8a00\u5e38\u89c1\u95ee\u9898\u96c6 C/C++ \u5b66\u4e60\u6559\u7a0b Linux C \u7f16\u7a0b\u4e00\u7ad9\u5f0f\u5b66\u4e60 \u65b0\u6982\u5ff5 C \u8bed\u8a00\u6559\u7a0b C Sharp \u7cbe\u901aC#(\u7b2c6\u7248) C++ 100\u4e2agcc\u5c0f\u6280\u5de7 100\u4e2agdb\u5c0f\u6280\u5de7 C \u8bed\u8a00\u7f16\u7a0b\u900f\u89c6 C/C++ Primer - andycai C++ FAQ LITE(\u4e2d\u6587\u7248) C++ Primer 5th Answers C++ Template \u8fdb\u9636\u6307\u5357 C++ \u57fa\u7840\u6559\u7a0b C++ \u5e76\u53d1\u7f16\u7a0b(\u57fa\u4e8eC++11) C++ \u5e76\u53d1\u7f16\u7a0b\u6307\u5357 CGDB\u4e2d\u6587\u624b\u518c Cmake \u5b9e\u8df5 (PDF) GNU make \u6307\u5357 Google C++ \u98ce\u683c\u6307\u5357 QT \u6559\u7a0b ZMQ \u6307\u5357 \u50cf\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4e00\u6837\u601d\u8003\uff08C++\u7248) (\u300aHow To Think Like a Computer Scientist: C++ Version\u300b\u4e2d\u6587\u7248) \u7b80\u5355\u6613\u61c2\u7684C\u9b54\u6cd5 \u8ddf\u6211\u4e00\u8d77\u5199Makefile (PDF) CoffeeScript CoffeeScript \u4e2d\u6587 CoffeeScript \u7f16\u7801\u98ce\u683c\u6307\u5357 CoffeeScript \u7f16\u7a0b\u98ce\u683c\u6307\u5357 Dart Dart \u8bed\u8a00\u5bfc\u89c8 Elasticsearch Elasticsearch \u6743\u5a01\u6307\u5357 \uff08\u300aElasticsearch the definitive guide\u300b\u4e2d\u6587\u7248\uff09 Mastering Elasticsearch(\u4e2d\u6587\u7248) Elixir Elixir Getting Started \u4e2d\u6587\u7ffb\u8bd1 Elixir \u7f16\u7a0b\u8bed\u8a00\u6559\u7a0b (Elixir School) Elixir\u5143\u7f16\u7a0b\u4e0eDSL \u4e2d\u6587\u7ffb\u8bd1 Phoenix \u6846\u67b6\u4e2d\u6587\u6587\u6863 Erlang Erlang \u5e76\u53d1\u7f16\u7a0b (\u300aConcurrent Programming in Erlang (Part I)\u300b\u4e2d\u6587\u7248) Fortran Fortran77\u548c90/95\u7f16\u7a0b\u5165\u95e8 Golang Go Web \u7f16\u7a0b Go \u5165\u95e8\u6307\u5357 (\u300aThe Way to Go\u300b\u4e2d\u6587\u7248) Go \u5b98\u65b9\u6587\u6863\u7ffb\u8bd1 Go \u6307\u5357 (\u300aA Tour of Go\u300b\u4e2d\u6587\u7248) Go \u7b80\u6613\u6559\u7a0b (\u300a The Little Go Book \u300b\u4e2d\u6587\u7248) Go \u7f16\u7a0b\u57fa\u7840 Go \u8bed\u8a00\u5b9e\u6218\u7b14\u8bb0 Go \u8bed\u8a00\u6807\u51c6\u5e93 Go \u8bed\u8a00\u9ad8\u7ea7\u7f16\u7a0b\uff08Advanced Go Programming\uff09 Go\u547d\u4ee4\u6559\u7a0b Go\u5b9e\u6218\u5f00\u53d1 Go\u8bed\u8a00\u535a\u5ba2\u5b9e\u8df5 Java\u7a0b\u5e8f\u5458\u7684Golang\u5165\u95e8\u6307\u5357 Network programming with Go \u4e2d\u6587\u7ffb\u8bd1\u7248\u672c Revel \u6846\u67b6\u624b\u518c \u5b66\u4e60Go\u8bed\u8a00 \u795e\u5947\u7684 Go \u8bed\u8a00 Groovy \u5b9e\u6218 Groovy \u7cfb\u5217 Haskell Haskell \u8da3\u5b66\u6307\u5357 Real World Haskell \u4e2d\u6587\u7248 HTML / CSS CSS3 Tutorial \u300aCSS3 \u6559\u7a0b\u300b CSS\u53c2\u8003\u624b\u518c Emmet \u6587\u6863 HTML5 \u6559\u7a0b HTML\u548cCSS\u7f16\u7801\u89c4\u8303 Sass Guidelines \u4e2d\u6587 \u524d\u7aef\u4ee3\u7801\u89c4\u8303 - \u817e\u8bafAlloyTeam\u56e2\u961f \u5b66\u4e60CSS\u5e03\u5c40 \u901a\u7528 CSS \u7b14\u8bb0\u3001\u5efa\u8bae\u4e0e\u6307\u5bfc iOS Apple Watch\u5f00\u53d1\u521d\u63a2 Google Objective-C Style Guide \u4e2d\u6587\u7248 iOS7\u4eba\u673a\u754c\u9762\u6307\u5357 iOS\u5f00\u53d160\u5206\u949f\u5165\u95e8 iPhone 6 \u5c4f\u5e55\u63ed\u79d8 \u7f51\u6613\u65af\u5766\u798f\u5927\u5b66\u516c\u5f00\u8bfe\uff1aiOS 7\u5e94\u7528\u5f00\u53d1\u5b57\u5e55\u6587\u4ef6 Java Activiti 5.x \u7528\u6237\u6307\u5357 Apache MINA 2 \u7528\u6237\u6307\u5357 Apache Shiro \u7528\u6237\u6307\u5357 Google Java\u7f16\u7a0b\u98ce\u683c\u6307\u5357 H2 Database \u6559\u7a0b Java Servlet 3.1 \u89c4\u8303 Java \u7f16\u7801\u89c4\u8303 Java \u7f16\u7a0b\u601d\u60f3 - quanke Jersey 2.x \u7528\u6237\u6307\u5357 JSSE \u53c2\u8003\u6307\u5357 MyBatis\u4e2d\u6587\u6587\u6863 Netty 4.x \u7528\u6237\u6307\u5357 Netty \u5b9e\u6218(\u7cbe\u9ad3) Nutz-book Nutz\u70f9\u8c03\u5411\u5bfc Nutz\u6587\u6863 REST \u5b9e\u6218 Spring Boot\u53c2\u8003\u6307\u5357 (:construction: \u7ffb\u8bd1\u4e2d ) Spring Framework 4.x\u53c2\u8003\u6587\u6863 \u7528jersey\u6784\u5efaREST\u670d\u52a1 Javascript Airbnb JavaScript \u89c4\u8303 ECMAScript 6 \u5165\u95e8 - \u962e\u4e00\u5cf0 Google JavaScript \u4ee3\u7801\u98ce\u683c\u6307\u5357 JavaScript Promise\u8ff7\u4f60\u4e66 Javascript \u539f\u7406 JavaScript \u6807\u51c6\u53c2\u8003\u6559\u7a0b\uff08alpha\uff09 \u300aJavaScript \u6a21\u5f0f\u300b (\u300aJavaScript patterns\u300b\u8bd1\u672c) javascript \u7684 12 \u4e2a\u602a\u7656 JavaScript \u79d8\u5bc6\u82b1\u56ed JavaScript\u6838\u5fc3\u6982\u5ff5\u53ca\u5b9e\u8df5 (PDF) Javascript\u7f16\u7a0b\u6307\u5357 ( \u6e90\u7801 ) \u4f60\u4e0d\u77e5\u9053\u7684Javascript \u547d\u540d\u51fd\u6570\u8868\u8fbe\u5f0f\u63a2\u79d8 - kangax\u3001\u4e3a\u4e4b\u6f2b\u7b14(\u7ffb\u8bd1) (\u539f\u59cb\u5730\u5740\u65e0\u6cd5\u6253\u5f00\uff0c\u6240\u4ee5\u6b64\u5904\u5730\u5740\u4e3ajustjavac\u535a\u5ba2\u4e0a\u7684\u5907\u4efd) \u5b66\u7528 JavaScript \u8bbe\u8ba1\u6a21\u5f0f - \u5f00\u6e90\u4e2d\u56fd \u6df1\u5165\u7406\u89e3JavaScript\u7cfb\u5217 AngularJS AngularJS\u5165\u95e8\u6559\u7a0b AngularJS\u6700\u4f73\u5b9e\u8df5\u548c\u98ce\u683c\u6307\u5357 \u5728Windows\u73af\u5883\u4e0b\u7528Yeoman\u6784\u5efaAngularJS\u9879\u76ee \u6784\u5efa\u81ea\u5df1\u7684AngularJS Backbone.js Backbone.js\u4e2d\u6587\u6587\u6863 Backbone.js\u5165\u95e8\u6559\u7a0b (PDF) Backbone.js\u5165\u95e8\u6559\u7a0b\u7b2c\u4e8c\u7248 D3.js Learning D3.JS - \u5341\u4e8c\u6708\u5496\u5561\u9986 \u5b98\u65b9API\u6587\u6863 \u5f20\u5929\u65ed\u7684D3\u6559\u7a0b \u695a\u72c2\u4eba\u7684D3\u6559\u7a0b ExtJS Ext4.1.0 \u4e2d\u6587\u6587\u6863 impress.js impress.js\u7684\u4e2d\u6587\u6559\u7a0b jQuery How to write jQuery plugin \u7b80\u5355\u6613\u61c2\u7684JQuery\u9b54\u6cd5 Node.js express.js \u4e2d\u6587\u6587\u6863 Express\u6846\u67b6 koa \u4e2d\u6587\u6587\u6863 Learn You The Node.js For Much Win! (\u4e2d\u6587\u7248) Node debug \u4e09\u6cd5\u4e09\u4f8b Node.js Fullstack\u300a\u5f9e\u96f6\u5230\u4e00\u7684\u9032\u6483\u300b Node.js \u5305\u6559\u4e0d\u5305\u4f1a Nodejs Wiki Book (\u7e41\u4f53\u4e2d\u6587) nodejs\u4e2d\u6587\u6587\u6863 Node\u5165\u95e8 The NodeJS \u4e2d\u6587\u6587\u6863 - \u793e\u533a\u7ffb\u8bd1 \u4e03\u5929\u5b66\u4f1aNodeJS - \u963f\u91cc\u56e2\u961f \u4f7f\u7528 Express + MongoDB \u642d\u5efa\u591a\u4eba\u535a\u5ba2 * JavaScript\u5168\u6808\u5de5\u7a0b\u5e08\u57f9\u8bad\u6750\u6599 React.js Learn React Webpack by building the Hacker News front page React-Bits \u4e2d\u6587\u6587\u6863 React Native \u4e2d\u6587\u6587\u6863(\u542b\u6700\u65b0Android\u5185\u5bb9) React webpack-cookbook React.js \u4e2d\u6587\u6587\u6863 React.js \u5165\u95e8\u6559\u7a0b Vue.js Vue.js\u4e2d\u6587\u6587\u6863 Zepto.js Zepto.js \u4e2d\u6587\u6587\u6863 LaTeX LaTeX \u7b14\u8bb0 \u4e00\u4efd\u4e0d\u592a\u7b80\u77ed\u7684 LaTeX2\u03b5 \u4ecb\u7ecd \u5927\u5bb6\u4f86\u5b78 LaTeX (PDF) LISP ANSI Common Lisp \u4e2d\u6587\u7ffb\u8bd1\u7248 Common Lisp \u9ad8\u7ea7\u7f16\u7a0b\u6280\u672f (\u300aOn Lisp\u300b\u4e2d\u6587\u7248) Lua Lua 5.3 \u53c2\u8003\u624b\u518c Markdown Markdown \u7b80\u660e\u6559\u7a0b \u732e\u7ed9\u5199\u4f5c\u8005\u7684 Markdown \u65b0\u624b\u6307\u5357 MySQL 21\u5206\u949fMySQL\u5165\u95e8\u6559\u7a0b MySQL\u7d22\u5f15\u80cc\u540e\u7684\u6570\u636e\u7ed3\u6784\u53ca\u7b97\u6cd5\u539f\u7406 NoSQL Disque \u4f7f\u7528\u6559\u7a0b Redis \u547d\u4ee4\u53c2\u8003 Redis \u8bbe\u8ba1\u4e0e\u5b9e\u73b0 The Little MongoDB Book The Little Redis Book \u5e26\u6709\u8be6\u7ec6\u6ce8\u91ca\u7684 Redis 2.6 \u4ee3\u7801 \u5e26\u6709\u8be6\u7ec6\u6ce8\u91ca\u7684 Redis 3.0 \u4ee3\u7801 Perl Master Perl Today Perl 5 \u6559\u7a0b Perl \u6559\u7a0b PHP Composer\u4e2d\u6587\u6587\u6863 Laravel5.4\u4e2d\u6587\u6587\u6863 Phalcon7\u4e2d\u6587\u6587\u6863 PHP \u4e4b\u9053 PHP\u4e2d\u6587\u624b\u518c PHP\u6807\u51c6\u89c4\u8303\u4e2d\u6587\u7248 Symfony2 \u5b9e\u4f8b\u6559\u7a0b Yii2\u4e2d\u6587\u6587\u6863 \u6df1\u5165\u7406\u89e3 PHP \u5185\u6838 PostgreSQL PostgreSQL 8.2.3 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.3.1 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.4.4 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.5.3 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.6.0 \u4e2d\u6587\u6587\u6863 Python Django 1.11.6 \u4e2d\u6587\u6587\u6863 Django 2.2.1 \u4e2d\u6587\u6587\u6863 - (\u8fd9\u4e2a\u5f88\u65b0\uff0c\u4e5f\u5f88\u5168\uff0cOnline) Django book 2.0 Matplotlib 3.0.3 \u4e2d\u6587\u6587\u6863 - (Online) Numpy 1.16 \u4e2d\u6587\u6587\u6863 - (Online) Python 3 \u6587\u6863(\u7b80\u4f53\u4e2d\u6587) 3.2.2 documentation Python 3.8.0a3\u4e2d\u6587\u6587\u6863 - (\u76ee\u524d\u5728\u7ebf\u6700\u5168\u7684\u4e2d\u6587\u6587\u6863\u4e86\uff0cOnline) Python Cookbook\u7b2c\u4e09\u7248 - David Beazley\u3001Brian K.Jones\u3001\u718a\u80fd(\u7ffb\u8bd1) Python \u4e2d\u6587\u5b66\u4e60\u5927\u672c\u8425 Python\u4e4b\u65c5 - Ethan Python\u6559\u7a0b - \u5ed6\u96ea\u5cf0\u7684\u5b98\u65b9\u7f51\u7ad9 Tornado 6.1 \u4e2d\u6587\u6587\u6863 - (\u7f51\u7edc\u4e0a\u5176\u4ed6\u7684\u90fd\u662f\u8f83\u65e7\u7248\u672c\u7684\uff0cOnline) \u50cf\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4e00\u6837\u601d\u8003Python - Allen B. Downey\u3001\u5927\u80d6\u54e5(\u7ffb\u8bd1) \u6df1\u5165 Python 3 \u7b28\u529e\u6cd5\u5b66 Python \u7b80\u660e Python \u6559\u7a0b - Swaroop C H\u3001\u6c88\u6d01\u5143(\u7ffb\u8bd1)\u3001\u6f20\u4f26(\u7ffb\u8bd1) Django Django Girls \u6559\u7a0b (1.11) (HTML) Django \u642d\u5efa\u4e2a\u4eba\u535a\u5ba2\u6559\u7a0b (2.1) - (\u675c\u8d5b) (HTML) R 153\u5206\u949f\u5b66\u4f1a R (PDF) R \u5bfc\u8bba (\u300aAn Introduction to R\u300b\u4e2d\u6587\u7248) (PDF) \u7528 R \u6784\u5efa Shiny \u5e94\u7528\u7a0b\u5e8f (\u300aBuilding 'Shiny' Applications with R\u300b\u4e2d\u6587\u7248) \u7edf\u8ba1\u5b66\u4e0e R \u8bfb\u4e66\u7b14\u8bb0 (PDF) reStructuredText reStructuredText \u5165\u95e8 Ruby Rails \u98ce\u683c\u6307\u5357 Ruby on Rails \u5b9e\u6218\u5723\u7ecf Ruby on Rails \u6307\u5357 Ruby \u98ce\u683c\u6307\u5357 Sinatra \u7b28\u65b9\u6cd5\u5b66 Ruby Rust Rust \u5b98\u65b9\u6559\u7a0b Rust \u8bed\u8a00\u5b66\u4e60\u7b14\u8bb0 RustPrimer \u901a\u8fc7\u4f8b\u5b50\u5b66\u4e60 Rust Scala Effective Scala Scala \u521d\u5b66\u8005\u6307\u5357 (\u300aThe Neophyte's Guide to Scala\u300b\u4e2d\u6587\u7248) Scala \u8bfe\u5802 (Twitter\u7684Scala\u4e2d\u6587\u6559\u7a0b) Scheme Scheme \u5165\u95e8\u6559\u7a0b (\u300aYet Another Scheme Tutorial\u300b\u4e2d\u6587\u7248) Scratch \u521b\u610f\u8ba1\u7b97\u8bfe\u7a0b\u6307\u5357 Shell shell-book Shell \u7f16\u7a0b\u57fa\u7840 Shell \u7f16\u7a0b\u8303\u4f8b - \u6cf0\u6653\u79d1\u6280 Shell \u811a\u672c\u7f16\u7a0b30\u5206\u949f\u5165\u95e8 The Linux Command Line \u4e2d\u6587\u7248 Swift \u300aThe Swift Programming Language\u300b\u4e2d\u6587\u7248 TypeScript TypeScript Deep Dive \u4e2d\u6587\u7248 TypeScript \u4e2d\u6587\u7f51 TypeScript \u5165\u95e8\u6559\u7a0b VBA (Microsoft Visual Basic Applications) \u7b80\u660eExcel VBA Vim Vim Manual(\u4e2d\u6587\u7248) \u5927\u5bb6\u4f86\u5b78 VIM Visual Prolog Visual Prolog 7\u521d\u5b66\u6307\u5357 Visual Prolog 7\u8fb9\u7ec3\u8fb9\u5b66 WebAssembly C/C++\u9762\u5411WebAssembly\u7f16\u7a0b","title":"P_books"},{"location":"p_books/#_1","text":"\u8bed\u8a00\u65e0\u5173 IDE Web WEB\u670d\u52a1\u5668 \u5176\u5b83 \u51fd\u6570\u5f0f\u6982\u5ff5 \u5206\u5e03\u5f0f\u7cfb\u7edf \u5728\u7ebf\u6559\u80b2 \u5927\u6570\u636e \u64cd\u4f5c\u7cfb\u7edf \u6570\u636e\u5e93 \u667a\u80fd\u7cfb\u7edf \u6b63\u5219\u8868\u8fbe\u5f0f \u7248\u672c\u63a7\u5236 \u7a0b\u5e8f\u5458\u6742\u8c08 \u7ba1\u7406\u548c\u76d1\u63a7 \u7f16\u7a0b\u827a\u672f \u7f16\u8bd1\u539f\u7406 \u7f16\u8f91\u5668 \u8ba1\u7b97\u673a\u56fe\u5f62\u5b66 \u8bbe\u8ba1\u6a21\u5f0f \u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5 \u9879\u76ee\u76f8\u5173 \u8bed\u8a00\u76f8\u5173 Android AWK C C# C++ CoffeeScript Dart Elasticsearch Elixir Erlang Fortran Golang Haskell HTML / CSS HTTP iOS Java JavaScript AngularJS Backbone.js D3.js ExtJS impress.js jQuery Node.js React.js Vue.js Zepto.js LaTeX LISP Lua Markdown MySQL NoSQL Perl PHP PostgreSQL Python Django R reStructuredText Ruby Rust Scala Scheme Scratch Shell Swift TypeScript VBA Vim Visual Prolog WebAssembly","title":"\u76ee\u5f55"},{"location":"p_books/#_2","text":"","title":"\u8bed\u8a00\u65e0\u5173"},{"location":"p_books/#ide","text":"IntelliJ IDEA \u7b80\u4f53\u4e2d\u6587\u4e13\u9898\u6559\u7a0b","title":"IDE"},{"location":"p_books/#web","text":"3 Web Designs in 3 Weeks Chrome \u5f00\u53d1\u8005\u5de5\u5177\u4e2d\u6587\u624b\u518c Chrome\u6269\u5c55\u53ca\u5e94\u7528\u5f00\u53d1 Chrome\u6269\u5c55\u5f00\u53d1\u6587\u6863 Growth: \u5168\u6808\u589e\u957f\u5de5\u7a0b\u5e08\u6307\u5357 Grunt\u4e2d\u6587\u6587\u6863 Gulp \u5165\u95e8\u6307\u5357 gulp\u4e2d\u6587\u6587\u6863 HTTP \u63a5\u53e3\u8bbe\u8ba1\u6307\u5317 HTTP/2.0 \u4e2d\u6587\u7ffb\u8bd1 http2\u8bb2\u89e3 JSON\u98ce\u683c\u6307\u5357 Wireshark\u7528\u6237\u624b\u518c \u4e00\u7ad9\u5f0f\u5b66\u4e60Wireshark \u5173\u4e8e\u6d4f\u89c8\u5668\u548c\u7f51\u7edc\u7684 20 \u9879\u987b\u77e5 \u524d\u7aef\u4ee3\u7801\u89c4\u8303 \u53ca \u6700\u4f73\u5b9e\u8df5 \u524d\u7aef\u5f00\u53d1\u4f53\u7cfb\u5efa\u8bbe\u65e5\u8bb0 \u524d\u7aef\u8d44\u6e90\u5206\u4eab\uff08\u4e00\uff09 \u524d\u7aef\u8d44\u6e90\u5206\u4eab\uff08\u4e8c\uff09 \u6b63\u5219\u8868\u8fbe\u5f0f30\u5206\u949f\u5165\u95e8\u6559\u7a0b \u6d4f\u89c8\u5668\u5f00\u53d1\u5de5\u5177\u7684\u79d8\u5bc6 \u79fb\u52a8Web\u524d\u7aef\u77e5\u8bc6\u5e93 \u79fb\u52a8\u524d\u7aef\u5f00\u53d1\u6536\u85cf\u5939","title":"Web"},{"location":"p_books/#web_1","text":"Apache \u4e2d\u6587\u624b\u518c Nginx\u5f00\u53d1\u4ece\u5165\u95e8\u5230\u7cbe\u901a - \u6dd8\u5b9d\u56e2\u961f Nginx\u6559\u7a0b\u4ece\u5165\u95e8\u5230\u7cbe\u901a - \u8fd0\u7ef4\u751f\u5b58\u65f6\u95f4 (PDF)","title":"WEB\u670d\u52a1\u5668"},{"location":"p_books/#_3","text":"OpenWrt\u667a\u80fd\u3001\u81ea\u52a8\u3001\u900f\u660e\u7ffb\u5899\u8def\u7531\u5668\u6559\u7a0b SAN \u7ba1\u7406\u5165\u95e8\u7cfb\u5217 Sketch \u4e2d\u6587\u624b\u518c \u6df1\u5165\u7406\u89e3\u5e76\u884c\u7f16\u7a0b","title":"\u5176\u5b83"},{"location":"p_books/#_4","text":"\u50bb\u74dc\u51fd\u6570\u7f16\u7a0b","title":"\u51fd\u6570\u5f0f\u6982\u5ff5"},{"location":"p_books/#_5","text":"\u8d70\u5411\u5206\u5e03\u5f0f (PDF)","title":"\u5206\u5e03\u5f0f\u7cfb\u7edf"},{"location":"p_books/#_6","text":"51CTO\u5b66\u9662 Codecademy CodeSchool Coursera Learn X in Y minutes shiyanlou TeamTreeHouse Udacity xuetangX \u6155\u8bfe\u7f51 \u6781\u5ba2\u5b66\u9662 \u6c47\u667a\u7f51 \u8ba1\u849c\u5ba2","title":"\u5728\u7ebf\u6559\u80b2"},{"location":"p_books/#_7","text":"Spark \u7f16\u7a0b\u6307\u5357\u7b80\u4f53\u4e2d\u6587\u7248 \u5927\u578b\u96c6\u7fa4\u4e0a\u7684\u5feb\u901f\u548c\u901a\u7528\u6570\u636e\u5904\u7406\u67b6\u6784 \u6570\u636e\u6316\u6398\u4e2d\u7ecf\u5178\u7684\u7b97\u6cd5\u5b9e\u73b0\u548c\u8be6\u7ec6\u7684\u6ce8\u91ca \u9762\u5411\u7a0b\u5e8f\u5458\u7684\u6570\u636e\u6316\u6398\u6307\u5357","title":"\u5927\u6570\u636e"},{"location":"p_books/#_8","text":"Debian \u53c2\u8003\u624b\u518c Docker \u2014\u2014 \u4ece\u5165\u95e8\u5230\u5b9e\u8df5 Docker\u4e2d\u6587\u6307\u5357 Docker\u5165\u95e8\u5b9e\u6218 FreeBSD \u4f7f\u7528\u624b\u518c Linux Documentation (\u4e2d\u6587\u7248) Linux Guide for Complete Beginners Linux \u6784\u5efa\u6307\u5357 Linux \u7cfb\u7edf\u9ad8\u7ea7\u7f16\u7a0b Linux\u5de5\u5177\u5feb\u901f\u6559\u7a0b Mac \u5f00\u53d1\u914d\u7f6e\u624b\u518c Operating Systems: Three Easy Pieces The Linux Command Line Ubuntu \u53c2\u8003\u624b\u518c uCore Lab: Operating System Course in Tsinghua University UNIX TOOLBOX \u547d\u4ee4\u884c\u7684\u827a\u672f \u5d4c\u5165\u5f0f Linux \u77e5\u8bc6\u5e93 (eLinux.org \u4e2d\u6587\u7248) \u5f00\u6e90\u4e16\u754c\u65c5\u884c\u624b\u518c \u7406\u89e3Linux\u8fdb\u7a0b \u9e1f\u54e5\u7684 Linux \u79c1\u623f\u83dc \u57fa\u7840\u5b66\u4e60\u7bc7 \u9e1f\u54e5\u7684 Linux \u79c1\u623f\u83dc \u670d\u52a1\u5668\u67b6\u8bbe\u7bc7","title":"\u64cd\u4f5c\u7cfb\u7edf"},{"location":"p_books/#_9","text":"","title":"\u6570\u636e\u5e93"},{"location":"p_books/#_10","text":"\u4e00\u6b65\u6b65\u642d\u5efa\u7269\u8054\u7f51\u7cfb\u7edf","title":"\u667a\u80fd\u7cfb\u7edf"},{"location":"p_books/#_11","text":"\u6b63\u5219\u8868\u8fbe\u5f0f-\u83dc\u9e1f\u6559\u7a0b \u6b63\u5219\u8868\u8fbe\u5f0f30\u5206\u949f\u5165\u95e8\u6559\u7a0b","title":"\u6b63\u5219\u8868\u8fbe\u5f0f"},{"location":"p_books/#_12","text":"Git - \u7b80\u6613\u6307\u5357 Git-Cheat-Sheet - flyhigher139 Git Community Book \u4e2d\u6587\u7248 git-flow \u5907\u5fd8\u6e05\u5355 Git magic Git Magic Git \u53c2\u8003\u624b\u518c Github\u5e2e\u52a9\u6587\u6863 GitHub\u79d8\u7c4d Git\u6559\u7a0b - \u5ed6\u96ea\u5cf0 Got GitHub GotGitHub HgInit (\u4e2d\u6587\u7248) Mercurial \u4f7f\u7528\u6559\u7a0b Pro Git Pro Git \u4e2d\u6587\u7248 Pro Git \u7b2c\u4e8c\u7248 \u4e2d\u6587\u7248 - Bingo Huang svn \u624b\u518c \u5b66\u4e60 Git \u5206\u652f \u6c89\u6d78\u5f0f\u5b66 Git \u7334\u5b50\u90fd\u80fd\u61c2\u7684GIT\u5165\u95e8","title":"\u7248\u672c\u63a7\u5236"},{"location":"p_books/#_13","text":"\u7a0b\u5e8f\u5458\u7684\u81ea\u6211\u4fee\u517b","title":"\u7a0b\u5e8f\u5458\u6742\u8c08"},{"location":"p_books/#_14","text":"ElasticSearch \u6743\u5a01\u6307\u5357 Elasticsearch \u6743\u5a01\u6307\u5357\uff08\u4e2d\u6587\u7248\uff09 ELKstack \u4e2d\u6587\u6307\u5357 Logstash \u6700\u4f73\u5b9e\u8df5 Mastering Elasticsearch(\u4e2d\u6587\u7248) Puppet 2.7 Cookbook \u4e2d\u6587\u7248","title":"\u7ba1\u7406\u548c\u76d1\u63a7"},{"location":"p_books/#_15","text":"\u6bcf\u4e2a\u7a0b\u5e8f\u5458\u90fd\u5e94\u8be5\u4e86\u89e3\u7684\u5185\u5b58\u77e5\u8bc6 (\u7b2c\u4e00\u90e8\u5206) \u7a0b\u5e8f\u5458\u7f16\u7a0b\u827a\u672f \u7f16\u7a0b\u5165\u95e8\u6307\u5357","title":"\u7f16\u7a0b\u827a\u672f"},{"location":"p_books/#_16","text":"\u300a\u8ba1\u7b97\u673a\u7a0b\u5e8f\u7684\u7ed3\u6784\u548c\u89e3\u91ca\u300b\u516c\u5f00\u8bfe \u7ffb\u8bd1\u9879\u76ee","title":"\u7f16\u8bd1\u539f\u7406"},{"location":"p_books/#_17","text":"exvim--vim \u6539\u826f\u6210IDE\u9879\u76ee Vim\u4e2d\u6587\u6587\u6863 \u6240\u9700\u5373\u6240\u83b7\uff1a\u50cf IDE \u4e00\u6837\u4f7f\u7528 vim \u7b28\u65b9\u6cd5\u5b66Vimscript \u4e2d\u8bd1\u672c","title":"\u7f16\u8f91\u5668"},{"location":"p_books/#_18","text":"LearnOpenGL CN OpenGL \u6559\u7a0b","title":"\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66"},{"location":"p_books/#_19","text":"\u53f2\u4e0a\u6700\u5168\u8bbe\u8ba1\u6a21\u5f0f\u5bfc\u5b66\u76ee\u5f55 \u56fe\u8bf4\u8bbe\u8ba1\u6a21\u5f0f","title":"\u8bbe\u8ba1\u6a21\u5f0f"},{"location":"p_books/#_20","text":"\u50bb\u74dc\u51fd\u6570\u7f16\u7a0b (\u300aFunctional Programming For The Rest of Us\u300b\u4e2d\u6587\u7248) \u785d\u70df\u4e2d\u7684 Scrum \u548c XP","title":"\u8f6f\u4ef6\u5f00\u53d1\u65b9\u6cd5"},{"location":"p_books/#_21","text":"GNU make \u6307\u5357 Gradle 2 \u7528\u6237\u6307\u5357 Gradle \u4e2d\u6587\u4f7f\u7528\u6587\u6863 Joel\u8c08\u8f6f\u4ef6 selenium \u4e2d\u6587\u6587\u6863 \u5f00\u6e90\u8f6f\u4ef6\u67b6\u6784 \u7d04\u8033\u8ac7\u8edf\u9ad4(Joel on Software) (\u7e41\u4f53\u4e2d\u6587) \u7f16\u7801\u89c4\u8303 \u8ba9\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u5217\u4e13\u680f \u8ffd\u6c42\u4ee3\u7801\u8d28\u91cf","title":"\u9879\u76ee\u76f8\u5173"},{"location":"p_books/#_22","text":"","title":"\u8bed\u8a00\u76f8\u5173"},{"location":"p_books/#android","text":"Android Design(\u4e2d\u6587\u7248) Android Note(\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u79ef\u7d2f\u7684\u77e5\u8bc6\u70b9) Android6.0\u65b0\u7279\u6027\u8be6\u89e3 Android\u5b66\u4e60\u4e4b\u8def Android\u5f00\u53d1\u6280\u672f\u524d\u7ebf(android-tech-frontier) Google Android\u5b98\u65b9\u57f9\u8bad\u8bfe\u7a0b\u4e2d\u6587\u7248 Google Material Design \u6b63\u9ad4\u4e2d\u6587\u7248 ( \u8bd1\u672c\u4e00 \uff0c \u8bd1\u672c\u4e8c ) Material Design \u4e2d\u6587\u7248 Point-of-Android","title":"Android"},{"location":"p_books/#awk","text":"awk\u4e2d\u6587\u6307\u5357 awk\u7a0b\u5e8f\u8bbe\u8ba1\u8bed\u8a00","title":"AWK"},{"location":"p_books/#c","text":"C \u8bed\u8a00\u5e38\u89c1\u95ee\u9898\u96c6 C/C++ \u5b66\u4e60\u6559\u7a0b Linux C \u7f16\u7a0b\u4e00\u7ad9\u5f0f\u5b66\u4e60 \u65b0\u6982\u5ff5 C \u8bed\u8a00\u6559\u7a0b","title":"C"},{"location":"p_books/#c-sharp","text":"\u7cbe\u901aC#(\u7b2c6\u7248)","title":"C Sharp"},{"location":"p_books/#c_1","text":"100\u4e2agcc\u5c0f\u6280\u5de7 100\u4e2agdb\u5c0f\u6280\u5de7 C \u8bed\u8a00\u7f16\u7a0b\u900f\u89c6 C/C++ Primer - andycai C++ FAQ LITE(\u4e2d\u6587\u7248) C++ Primer 5th Answers C++ Template \u8fdb\u9636\u6307\u5357 C++ \u57fa\u7840\u6559\u7a0b C++ \u5e76\u53d1\u7f16\u7a0b(\u57fa\u4e8eC++11) C++ \u5e76\u53d1\u7f16\u7a0b\u6307\u5357 CGDB\u4e2d\u6587\u624b\u518c Cmake \u5b9e\u8df5 (PDF) GNU make \u6307\u5357 Google C++ \u98ce\u683c\u6307\u5357 QT \u6559\u7a0b ZMQ \u6307\u5357 \u50cf\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4e00\u6837\u601d\u8003\uff08C++\u7248) (\u300aHow To Think Like a Computer Scientist: C++ Version\u300b\u4e2d\u6587\u7248) \u7b80\u5355\u6613\u61c2\u7684C\u9b54\u6cd5 \u8ddf\u6211\u4e00\u8d77\u5199Makefile (PDF)","title":"C++"},{"location":"p_books/#coffeescript","text":"CoffeeScript \u4e2d\u6587 CoffeeScript \u7f16\u7801\u98ce\u683c\u6307\u5357 CoffeeScript \u7f16\u7a0b\u98ce\u683c\u6307\u5357","title":"CoffeeScript"},{"location":"p_books/#dart","text":"Dart \u8bed\u8a00\u5bfc\u89c8","title":"Dart"},{"location":"p_books/#elasticsearch","text":"Elasticsearch \u6743\u5a01\u6307\u5357 \uff08\u300aElasticsearch the definitive guide\u300b\u4e2d\u6587\u7248\uff09 Mastering Elasticsearch(\u4e2d\u6587\u7248)","title":"Elasticsearch"},{"location":"p_books/#elixir","text":"Elixir Getting Started \u4e2d\u6587\u7ffb\u8bd1 Elixir \u7f16\u7a0b\u8bed\u8a00\u6559\u7a0b (Elixir School) Elixir\u5143\u7f16\u7a0b\u4e0eDSL \u4e2d\u6587\u7ffb\u8bd1 Phoenix \u6846\u67b6\u4e2d\u6587\u6587\u6863","title":"Elixir"},{"location":"p_books/#erlang","text":"Erlang \u5e76\u53d1\u7f16\u7a0b (\u300aConcurrent Programming in Erlang (Part I)\u300b\u4e2d\u6587\u7248)","title":"Erlang"},{"location":"p_books/#fortran","text":"Fortran77\u548c90/95\u7f16\u7a0b\u5165\u95e8","title":"Fortran"},{"location":"p_books/#golang","text":"Go Web \u7f16\u7a0b Go \u5165\u95e8\u6307\u5357 (\u300aThe Way to Go\u300b\u4e2d\u6587\u7248) Go \u5b98\u65b9\u6587\u6863\u7ffb\u8bd1 Go \u6307\u5357 (\u300aA Tour of Go\u300b\u4e2d\u6587\u7248) Go \u7b80\u6613\u6559\u7a0b (\u300a The Little Go Book \u300b\u4e2d\u6587\u7248) Go \u7f16\u7a0b\u57fa\u7840 Go \u8bed\u8a00\u5b9e\u6218\u7b14\u8bb0 Go \u8bed\u8a00\u6807\u51c6\u5e93 Go \u8bed\u8a00\u9ad8\u7ea7\u7f16\u7a0b\uff08Advanced Go Programming\uff09 Go\u547d\u4ee4\u6559\u7a0b Go\u5b9e\u6218\u5f00\u53d1 Go\u8bed\u8a00\u535a\u5ba2\u5b9e\u8df5 Java\u7a0b\u5e8f\u5458\u7684Golang\u5165\u95e8\u6307\u5357 Network programming with Go \u4e2d\u6587\u7ffb\u8bd1\u7248\u672c Revel \u6846\u67b6\u624b\u518c \u5b66\u4e60Go\u8bed\u8a00 \u795e\u5947\u7684 Go \u8bed\u8a00","title":"Golang"},{"location":"p_books/#groovy","text":"\u5b9e\u6218 Groovy \u7cfb\u5217","title":"Groovy"},{"location":"p_books/#haskell","text":"Haskell \u8da3\u5b66\u6307\u5357 Real World Haskell \u4e2d\u6587\u7248","title":"Haskell"},{"location":"p_books/#html-css","text":"CSS3 Tutorial \u300aCSS3 \u6559\u7a0b\u300b CSS\u53c2\u8003\u624b\u518c Emmet \u6587\u6863 HTML5 \u6559\u7a0b HTML\u548cCSS\u7f16\u7801\u89c4\u8303 Sass Guidelines \u4e2d\u6587 \u524d\u7aef\u4ee3\u7801\u89c4\u8303 - \u817e\u8bafAlloyTeam\u56e2\u961f \u5b66\u4e60CSS\u5e03\u5c40 \u901a\u7528 CSS \u7b14\u8bb0\u3001\u5efa\u8bae\u4e0e\u6307\u5bfc","title":"HTML / CSS"},{"location":"p_books/#ios","text":"Apple Watch\u5f00\u53d1\u521d\u63a2 Google Objective-C Style Guide \u4e2d\u6587\u7248 iOS7\u4eba\u673a\u754c\u9762\u6307\u5357 iOS\u5f00\u53d160\u5206\u949f\u5165\u95e8 iPhone 6 \u5c4f\u5e55\u63ed\u79d8 \u7f51\u6613\u65af\u5766\u798f\u5927\u5b66\u516c\u5f00\u8bfe\uff1aiOS 7\u5e94\u7528\u5f00\u53d1\u5b57\u5e55\u6587\u4ef6","title":"iOS"},{"location":"p_books/#java","text":"Activiti 5.x \u7528\u6237\u6307\u5357 Apache MINA 2 \u7528\u6237\u6307\u5357 Apache Shiro \u7528\u6237\u6307\u5357 Google Java\u7f16\u7a0b\u98ce\u683c\u6307\u5357 H2 Database \u6559\u7a0b Java Servlet 3.1 \u89c4\u8303 Java \u7f16\u7801\u89c4\u8303 Java \u7f16\u7a0b\u601d\u60f3 - quanke Jersey 2.x \u7528\u6237\u6307\u5357 JSSE \u53c2\u8003\u6307\u5357 MyBatis\u4e2d\u6587\u6587\u6863 Netty 4.x \u7528\u6237\u6307\u5357 Netty \u5b9e\u6218(\u7cbe\u9ad3) Nutz-book Nutz\u70f9\u8c03\u5411\u5bfc Nutz\u6587\u6863 REST \u5b9e\u6218 Spring Boot\u53c2\u8003\u6307\u5357 (:construction: \u7ffb\u8bd1\u4e2d ) Spring Framework 4.x\u53c2\u8003\u6587\u6863 \u7528jersey\u6784\u5efaREST\u670d\u52a1","title":"Java"},{"location":"p_books/#javascript","text":"Airbnb JavaScript \u89c4\u8303 ECMAScript 6 \u5165\u95e8 - \u962e\u4e00\u5cf0 Google JavaScript \u4ee3\u7801\u98ce\u683c\u6307\u5357 JavaScript Promise\u8ff7\u4f60\u4e66 Javascript \u539f\u7406 JavaScript \u6807\u51c6\u53c2\u8003\u6559\u7a0b\uff08alpha\uff09 \u300aJavaScript \u6a21\u5f0f\u300b (\u300aJavaScript patterns\u300b\u8bd1\u672c) javascript \u7684 12 \u4e2a\u602a\u7656 JavaScript \u79d8\u5bc6\u82b1\u56ed JavaScript\u6838\u5fc3\u6982\u5ff5\u53ca\u5b9e\u8df5 (PDF) Javascript\u7f16\u7a0b\u6307\u5357 ( \u6e90\u7801 ) \u4f60\u4e0d\u77e5\u9053\u7684Javascript \u547d\u540d\u51fd\u6570\u8868\u8fbe\u5f0f\u63a2\u79d8 - kangax\u3001\u4e3a\u4e4b\u6f2b\u7b14(\u7ffb\u8bd1) (\u539f\u59cb\u5730\u5740\u65e0\u6cd5\u6253\u5f00\uff0c\u6240\u4ee5\u6b64\u5904\u5730\u5740\u4e3ajustjavac\u535a\u5ba2\u4e0a\u7684\u5907\u4efd) \u5b66\u7528 JavaScript \u8bbe\u8ba1\u6a21\u5f0f - \u5f00\u6e90\u4e2d\u56fd \u6df1\u5165\u7406\u89e3JavaScript\u7cfb\u5217","title":"Javascript"},{"location":"p_books/#angularjs","text":"AngularJS\u5165\u95e8\u6559\u7a0b AngularJS\u6700\u4f73\u5b9e\u8df5\u548c\u98ce\u683c\u6307\u5357 \u5728Windows\u73af\u5883\u4e0b\u7528Yeoman\u6784\u5efaAngularJS\u9879\u76ee \u6784\u5efa\u81ea\u5df1\u7684AngularJS","title":"AngularJS"},{"location":"p_books/#backbonejs","text":"Backbone.js\u4e2d\u6587\u6587\u6863 Backbone.js\u5165\u95e8\u6559\u7a0b (PDF) Backbone.js\u5165\u95e8\u6559\u7a0b\u7b2c\u4e8c\u7248","title":"Backbone.js"},{"location":"p_books/#d3js","text":"Learning D3.JS - \u5341\u4e8c\u6708\u5496\u5561\u9986 \u5b98\u65b9API\u6587\u6863 \u5f20\u5929\u65ed\u7684D3\u6559\u7a0b \u695a\u72c2\u4eba\u7684D3\u6559\u7a0b","title":"D3.js"},{"location":"p_books/#extjs","text":"Ext4.1.0 \u4e2d\u6587\u6587\u6863","title":"ExtJS"},{"location":"p_books/#impressjs","text":"impress.js\u7684\u4e2d\u6587\u6559\u7a0b","title":"impress.js"},{"location":"p_books/#jquery","text":"How to write jQuery plugin \u7b80\u5355\u6613\u61c2\u7684JQuery\u9b54\u6cd5","title":"jQuery"},{"location":"p_books/#nodejs","text":"express.js \u4e2d\u6587\u6587\u6863 Express\u6846\u67b6 koa \u4e2d\u6587\u6587\u6863 Learn You The Node.js For Much Win! (\u4e2d\u6587\u7248) Node debug \u4e09\u6cd5\u4e09\u4f8b Node.js Fullstack\u300a\u5f9e\u96f6\u5230\u4e00\u7684\u9032\u6483\u300b Node.js \u5305\u6559\u4e0d\u5305\u4f1a Nodejs Wiki Book (\u7e41\u4f53\u4e2d\u6587) nodejs\u4e2d\u6587\u6587\u6863 Node\u5165\u95e8 The NodeJS \u4e2d\u6587\u6587\u6863 - \u793e\u533a\u7ffb\u8bd1 \u4e03\u5929\u5b66\u4f1aNodeJS - \u963f\u91cc\u56e2\u961f \u4f7f\u7528 Express + MongoDB \u642d\u5efa\u591a\u4eba\u535a\u5ba2 * JavaScript\u5168\u6808\u5de5\u7a0b\u5e08\u57f9\u8bad\u6750\u6599","title":"Node.js"},{"location":"p_books/#reactjs","text":"Learn React Webpack by building the Hacker News front page React-Bits \u4e2d\u6587\u6587\u6863 React Native \u4e2d\u6587\u6587\u6863(\u542b\u6700\u65b0Android\u5185\u5bb9) React webpack-cookbook React.js \u4e2d\u6587\u6587\u6863 React.js \u5165\u95e8\u6559\u7a0b","title":"React.js"},{"location":"p_books/#vuejs","text":"Vue.js\u4e2d\u6587\u6587\u6863","title":"Vue.js"},{"location":"p_books/#zeptojs","text":"Zepto.js \u4e2d\u6587\u6587\u6863","title":"Zepto.js"},{"location":"p_books/#latex","text":"LaTeX \u7b14\u8bb0 \u4e00\u4efd\u4e0d\u592a\u7b80\u77ed\u7684 LaTeX2\u03b5 \u4ecb\u7ecd \u5927\u5bb6\u4f86\u5b78 LaTeX (PDF)","title":"LaTeX"},{"location":"p_books/#lisp","text":"ANSI Common Lisp \u4e2d\u6587\u7ffb\u8bd1\u7248 Common Lisp \u9ad8\u7ea7\u7f16\u7a0b\u6280\u672f (\u300aOn Lisp\u300b\u4e2d\u6587\u7248)","title":"LISP"},{"location":"p_books/#lua","text":"Lua 5.3 \u53c2\u8003\u624b\u518c","title":"Lua"},{"location":"p_books/#markdown","text":"Markdown \u7b80\u660e\u6559\u7a0b \u732e\u7ed9\u5199\u4f5c\u8005\u7684 Markdown \u65b0\u624b\u6307\u5357","title":"Markdown"},{"location":"p_books/#mysql","text":"21\u5206\u949fMySQL\u5165\u95e8\u6559\u7a0b MySQL\u7d22\u5f15\u80cc\u540e\u7684\u6570\u636e\u7ed3\u6784\u53ca\u7b97\u6cd5\u539f\u7406","title":"MySQL"},{"location":"p_books/#nosql","text":"Disque \u4f7f\u7528\u6559\u7a0b Redis \u547d\u4ee4\u53c2\u8003 Redis \u8bbe\u8ba1\u4e0e\u5b9e\u73b0 The Little MongoDB Book The Little Redis Book \u5e26\u6709\u8be6\u7ec6\u6ce8\u91ca\u7684 Redis 2.6 \u4ee3\u7801 \u5e26\u6709\u8be6\u7ec6\u6ce8\u91ca\u7684 Redis 3.0 \u4ee3\u7801","title":"NoSQL"},{"location":"p_books/#perl","text":"Master Perl Today Perl 5 \u6559\u7a0b Perl \u6559\u7a0b","title":"Perl"},{"location":"p_books/#php","text":"Composer\u4e2d\u6587\u6587\u6863 Laravel5.4\u4e2d\u6587\u6587\u6863 Phalcon7\u4e2d\u6587\u6587\u6863 PHP \u4e4b\u9053 PHP\u4e2d\u6587\u624b\u518c PHP\u6807\u51c6\u89c4\u8303\u4e2d\u6587\u7248 Symfony2 \u5b9e\u4f8b\u6559\u7a0b Yii2\u4e2d\u6587\u6587\u6863 \u6df1\u5165\u7406\u89e3 PHP \u5185\u6838","title":"PHP"},{"location":"p_books/#postgresql","text":"PostgreSQL 8.2.3 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.3.1 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.4.4 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.5.3 \u4e2d\u6587\u6587\u6863 PostgreSQL 9.6.0 \u4e2d\u6587\u6587\u6863","title":"PostgreSQL"},{"location":"p_books/#python","text":"Django 1.11.6 \u4e2d\u6587\u6587\u6863 Django 2.2.1 \u4e2d\u6587\u6587\u6863 - (\u8fd9\u4e2a\u5f88\u65b0\uff0c\u4e5f\u5f88\u5168\uff0cOnline) Django book 2.0 Matplotlib 3.0.3 \u4e2d\u6587\u6587\u6863 - (Online) Numpy 1.16 \u4e2d\u6587\u6587\u6863 - (Online) Python 3 \u6587\u6863(\u7b80\u4f53\u4e2d\u6587) 3.2.2 documentation Python 3.8.0a3\u4e2d\u6587\u6587\u6863 - (\u76ee\u524d\u5728\u7ebf\u6700\u5168\u7684\u4e2d\u6587\u6587\u6863\u4e86\uff0cOnline) Python Cookbook\u7b2c\u4e09\u7248 - David Beazley\u3001Brian K.Jones\u3001\u718a\u80fd(\u7ffb\u8bd1) Python \u4e2d\u6587\u5b66\u4e60\u5927\u672c\u8425 Python\u4e4b\u65c5 - Ethan Python\u6559\u7a0b - \u5ed6\u96ea\u5cf0\u7684\u5b98\u65b9\u7f51\u7ad9 Tornado 6.1 \u4e2d\u6587\u6587\u6863 - (\u7f51\u7edc\u4e0a\u5176\u4ed6\u7684\u90fd\u662f\u8f83\u65e7\u7248\u672c\u7684\uff0cOnline) \u50cf\u8ba1\u7b97\u673a\u79d1\u5b66\u5bb6\u4e00\u6837\u601d\u8003Python - Allen B. Downey\u3001\u5927\u80d6\u54e5(\u7ffb\u8bd1) \u6df1\u5165 Python 3 \u7b28\u529e\u6cd5\u5b66 Python \u7b80\u660e Python \u6559\u7a0b - Swaroop C H\u3001\u6c88\u6d01\u5143(\u7ffb\u8bd1)\u3001\u6f20\u4f26(\u7ffb\u8bd1)","title":"Python"},{"location":"p_books/#django","text":"Django Girls \u6559\u7a0b (1.11) (HTML) Django \u642d\u5efa\u4e2a\u4eba\u535a\u5ba2\u6559\u7a0b (2.1) - (\u675c\u8d5b) (HTML)","title":"Django"},{"location":"p_books/#r","text":"153\u5206\u949f\u5b66\u4f1a R (PDF) R \u5bfc\u8bba (\u300aAn Introduction to R\u300b\u4e2d\u6587\u7248) (PDF) \u7528 R \u6784\u5efa Shiny \u5e94\u7528\u7a0b\u5e8f (\u300aBuilding 'Shiny' Applications with R\u300b\u4e2d\u6587\u7248) \u7edf\u8ba1\u5b66\u4e0e R \u8bfb\u4e66\u7b14\u8bb0 (PDF)","title":"R"},{"location":"p_books/#restructuredtext","text":"reStructuredText \u5165\u95e8","title":"reStructuredText"},{"location":"p_books/#ruby","text":"Rails \u98ce\u683c\u6307\u5357 Ruby on Rails \u5b9e\u6218\u5723\u7ecf Ruby on Rails \u6307\u5357 Ruby \u98ce\u683c\u6307\u5357 Sinatra \u7b28\u65b9\u6cd5\u5b66 Ruby","title":"Ruby"},{"location":"p_books/#rust","text":"Rust \u5b98\u65b9\u6559\u7a0b Rust \u8bed\u8a00\u5b66\u4e60\u7b14\u8bb0 RustPrimer \u901a\u8fc7\u4f8b\u5b50\u5b66\u4e60 Rust","title":"Rust"},{"location":"p_books/#scala","text":"Effective Scala Scala \u521d\u5b66\u8005\u6307\u5357 (\u300aThe Neophyte's Guide to Scala\u300b\u4e2d\u6587\u7248) Scala \u8bfe\u5802 (Twitter\u7684Scala\u4e2d\u6587\u6559\u7a0b)","title":"Scala"},{"location":"p_books/#scheme","text":"Scheme \u5165\u95e8\u6559\u7a0b (\u300aYet Another Scheme Tutorial\u300b\u4e2d\u6587\u7248)","title":"Scheme"},{"location":"p_books/#scratch","text":"\u521b\u610f\u8ba1\u7b97\u8bfe\u7a0b\u6307\u5357","title":"Scratch"},{"location":"p_books/#shell","text":"shell-book Shell \u7f16\u7a0b\u57fa\u7840 Shell \u7f16\u7a0b\u8303\u4f8b - \u6cf0\u6653\u79d1\u6280 Shell \u811a\u672c\u7f16\u7a0b30\u5206\u949f\u5165\u95e8 The Linux Command Line \u4e2d\u6587\u7248","title":"Shell"},{"location":"p_books/#swift","text":"\u300aThe Swift Programming Language\u300b\u4e2d\u6587\u7248","title":"Swift"},{"location":"p_books/#typescript","text":"TypeScript Deep Dive \u4e2d\u6587\u7248 TypeScript \u4e2d\u6587\u7f51 TypeScript \u5165\u95e8\u6559\u7a0b","title":"TypeScript"},{"location":"p_books/#vba-microsoft-visual-basic-applications","text":"\u7b80\u660eExcel VBA","title":"VBA (Microsoft Visual Basic Applications)"},{"location":"p_books/#vim","text":"Vim Manual(\u4e2d\u6587\u7248) \u5927\u5bb6\u4f86\u5b78 VIM","title":"Vim"},{"location":"p_books/#visual-prolog","text":"Visual Prolog 7\u521d\u5b66\u6307\u5357 Visual Prolog 7\u8fb9\u7ec3\u8fb9\u5b66","title":"Visual Prolog"},{"location":"p_books/#webassembly","text":"C/C++\u9762\u5411WebAssembly\u7f16\u7a0b","title":"WebAssembly"},{"location":"python/","text":"Python Table of Contents Tutorials Comparison to R Coding Functional Programming Concurrence Programming Data science IDE Performance Bigdata Database File formats Others Kits Utils CLI Data Science Statistical and visualization NumPy Pandas matplotlib Plots Map GUI Data structure and Algorithm Machine Learning NLP Clustering Optimization Deep learning Image GEO Web Flask Comparison Others Fun Finance More Misc Tutorials Python\u6559\u7a0b ( Python3\u7248\u672c ) by \u5ed6\u96ea\u5cf0, python\u8fdb\u9636 \u96f6\u57fa\u7840\u5b66python Intermediate Python Python Practice Book pymbook Full Stack Python Migrating to Python 3 with pleasure / \u5728Python 2.7\u5373\u5c06\u505c\u6b62\u652f\u6301\u65f6\uff0c\u6211\u4eec\u4e3a\u4f60\u51c6\u5907\u4e86\u4e00\u4efd3.x\u8fc1\u79fb\u6307\u5357 Writing Python 2-3 compatible code | \u7f16\u5199\u517c\u5bb9 Python 2 \u548c Python 3 \u7684\u4ee3\u7801 Python Cookbook more books good-to-great-python-reads/ Python \u5165\u95e8\u6307\u5357 Useful functions, tutorials, and other Python-related things Training Material | Addfor s.r.l. Notebook Gallery Python Notes Collections Automate the Boring Stuff with Python Practical Programming for Total Beginners More A COLLECTION OF PYTHON MUST READS 10 python blogs worth following Python for Data Science vs Python for Web Development Computational Statistics in Python Comparison to R Data munging in R with dplyr and in Python with Pandas: basic structures and going from one to the other Coding python-patterns - A collection of design patterns/idioms in Python Code Style | Python\u4ee3\u7801\u98ce\u683c Python \u53d8\u91cf\u547d\u540d\u89c4\u8303 Python\u5b89\u5168\u7f16\u7801\u4e0e\u4ee3\u7801\u5ba1\u8ba1 Python\u547d\u540d\u7a7a\u95f4\u548c\u4f5c\u7528\u57df\u7aa5\u63a2 regular-expressions , Python\u6b63\u5219\u8868\u8fbe\u5f0f\u6307\u5357 , FuzzyFinder - in 10 lines of Python Python's Hidden Regular Expression Gems Python Tips and Traps Python best practices A random collection of useful Python snippets pyperform - An easy and convienent way to performance test python code. \u50cfPython\u4e13\u5bb6\u4e00\u6837\u7f16\u7a0b: \u9053\u5730\u7684Python , \u521d\u5b66\u8005\u5fc5\u77e5\u7684Python\u4e2d\u4f18\u96c5\u7684\u7528\u6cd5 The Most Diabolical Python Antipattern The Little Book of Python Anti-Patterns , The Most Diabolical Python Antipattern Potential Pythonic Pitfalls Python Patterns - An Optimization Anecdote Python\u4e2d\u7684\u53d8\u91cf\u3001\u5f15\u7528\u3001\u62f7\u8d1d\u548c\u4f5c\u7528\u57df \u6df1\u5165\u7406\u89e3Python\u4e2d\u7684\u751f\u6210\u5668 Understanding Python metaclasses , python-metaclasses-by-example , \u6df1\u523b\u7406\u89e3Python\u4e2d\u7684\u5143\u7c7b(metaclass) , \u4f7f\u7528\u5143\u7c7b Python: Tips, Tricks and Idioms , part2 PythonDecoratorLibrary , more about decorator Python\u4e2d\u7684\u7c7b\u548c\u5bf9\u8c61\uff08\u4e2d\u7ea7\uff09 Python\u81ea\u7701\uff08\u53cd\u5c04\uff09\u6307\u5357 Properties Python Multiple Inheritance: call super on all Python\u4e2d\u7684\u9ed8\u8ba4\u53c2\u6570 dict and slot \uff0c SAVING 9 GB OF RAM WITH PYTHON\u2019S SLOTS Python\u7684\u51e0\u4e2a\u7279\u522b\u7684__\u5f00\u5934\u7684\u65b9\u6cd5 [\u8bd1] \u4e0e Python \u65e0\u7f1d\u96c6\u6210\u2014\u2014\u57fa\u672c\u7279\u6b8a\u65b9\u6cd5 1 Using % and .format() for great good! Optimize Python with Closures A simple example of Python OOP development (with TDD) - Part 1 Python 3 OOP Notebooks 5 Simple Rules For Building Great Python Packages | \u6784\u5efa\u5065\u58ee Python \u5305\u7684 5 \u4e2a\u7b80\u5355\u89c4\u5219 Python \u4e2d\u7684\u8fdb\u7a0b\u3001\u7ebf\u7a0b\u3001\u534f\u7a0b\u3001\u540c\u6b65\u3001\u5f02\u6b65\u3001\u56de\u8c03 Inner Functions - What Are They Good For? __init__() \u65b9\u6cd5 1 , 2 , 3 , 4 Good logging practice in Python | \u6bcf\u4e2a Python \u7a0b\u5e8f\u5458\u90fd\u8981\u77e5\u9053\u7684\u65e5\u5fd7\u5b9e\u8df5 Why Doesn't Python Have Switch/Case? python\u7f16\u7a0b\u4e2d\u5e38\u7528\u768412\u79cd\u57fa\u7840\u77e5\u8bc6\u603b\u7ed3 Python Examples - Get python examples for popular modules. 30 Python Language Features and Tricks You May Not Know About | 30\u4e2a\u6709\u5173Python\u7684\u5c0f\u6280\u5de7 yapf - A formatter for Python files Python error check graph Python\u4e2deval\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669 \u5173\u4e8ePython\u7684\u9762\u8bd5\u9898 Functional Programming Functional Programming HOWTO A practical introduction to functional programming | \u51fd\u6570\u5f0f\u7f16\u7a0b\u5b9e\u6218\u6559\u7a0b\uff08Python\u7248\uff09 functional-programming-python.pdf PyToolz - Toolz provides a set of utility functions for iterators, functions, and dictionaries. example , Derive a Markov model from the human genome in Concurrence Programming Background tasks in Python 3.5 \u3010\u8bd1\u3011\u6df1\u5165\u7406\u89e3python3.4\u4e2dAsyncio\u5e93\u4e0eNode.js\u7684\u5f02\u6b65IO\u673a\u5236 Data science Comprehensive learning path \u2013 Data Science in Python INTRODUCTION TO PYTHON FOR DATA MINING Python: Getting Started with Data Analysis Practical Data Science in Python code_py - A collection of well-commented code snippits for data science data-science-ipython-notebooks DataPyR Learning Seattle's Work Habits from Bicycle Counts (Updated!) Eight Tools That Show What\u2019s on the Horizon for the Python Data Ecosystem IDE Things that surprised me in PyCharm | Python IDE\uff1aPyCharm\u4e2d\u7684\u90a3\u4e9b\u5b9e\u7528\u529f\u80fd Performance packages 1 theano numba - NumPy aware dynamic Python compiler using LLVM multiprocessing - offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Multiprocessing: Task parallelism for the masses , An introduction to parallel programming , Python\u591a\u8fdb\u7a0b\u6a21\u5757Multiprocessing\u4ecb\u7ecd , Communication Between Processes , Python\u591a\u8fdb\u7a0b\u7f16\u7a0b Python and Parallelism or Dask numexpr - Fast numerical array expression evaluator for Python, NumPy, PyTables, pandas, bcolz and more https://github.com/pydata/numexpr/wiki/Numexpr-Users-Guide packages 2 huey - a little multi-threaded task queue for python python-gearman Dask - provides multi-core execution on larger-than-memory datasets using blocked algorithms and task scheduling. Ruffus : a lightweight Python library for computational pipelines Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. pyflow - A lightweight parallel task engine http://Illumina.github.io/pyflow/ packages 3 How We Reduced CPU Usage by 80% through Python Profiling tools vprof - Visual Python profiler tutorials \u4f18\u5316 Python \u6027\u80fd\uff1aPyPy\u3001Numba \u4e0e Cython\uff0c\u8c01\u624d\u662f\u76ee\u524d\u6700\u4f18\u79c0\u7684 Python \u8fd0\u7b97\u89e3\u51b3\u65b9\u6848\uff1f How To Make Python Run As Fast As Juli A guide to analyzing Python performance | Python\u6027\u80fd\u5206\u6790\u6307\u5357 Python: Faster Way http://pythonfasterway.uni.me PerformanceTips 7 tips to Time Python scripts and control Memory CPU usage | 7 \u4e2a\u6d4b\u91cf Python \u811a\u672c\u548c\u63a7\u5236\u5185\u5b58\u4ee5\u53ca CPU \u4f7f\u7528\u7387\u7684\u6280\u5de7 Speeding Up Your Python Code | \u52a0\u901f\u4f60\u7684Python\u4ee3\u7801 Python \u4ee3\u7801\u6027\u80fd\u4f18\u5316\u6280\u5de7 Optimizing Python - a Case Study 6 Python Performance Tips (very new view for me) | \u63d0\u5347 Python \u7a0b\u5e8f\u6027\u80fd\u7684 6 \u4e2a\u6280\u5de7 Why Python is Slow: Looking Under the Hood RunSnakeRun - a small GUI utility that allows you to view (Python) cProfile or Profile profiler dumps in a sortable GUI view Why Python is Slow: Looking Under the Hood \u4f7f\u7528Python\u8fdb\u884c\u5e76\u53d1\u7f16\u7a0b Theano\u6559\u7a0b\uff1aPython\u7684\u5185\u5b58\u7ba1\u7406 A guide to analyzing Python performance Python\u591a\u6838\u7f16\u7a0bmpi4py\u5b9e\u8df5 Python \u6027\u80fd\u5206\u6790\u5927\u5168 Bigdata Using Spark DataFrames for large scale data science Spark Programming Guide | [\u7ffb\u8bd1]Spark\u7f16\u7a0b\u6307\u5357(Python\u7248) Using PyTables for Larger-Than-RAM Data Processing h5py Database CodernityDB - a more advanced key-value Native Python Database, with multiple key-values indexes in the same engine zerodb - ZeroDB is an end-to-end encrypted database. Data can be stored on untrusted database servers without ever exposing the encryption key. Clients can execute remote queries against the encrypted data without downloading all of it or suffering an excessive performance hit. mongo\u6570\u636e\u5e93\u57fa\u672c\u64cd\u4f5c-python\u7bc7 ZODB4 - ZODB makes it really fast and easy to build and distribute Persistent Python applications pickleDB - pickleDB is a lightweight and simple key-value store tinydb - TinyDB is a lightweight document oriented database optimized for your happiness :) https://tinydb.readthedocs.org orm peewee - Peewee is a simple and small ORM. It has few (but expressive) concepts, making it easy to learn and intuitive to use. File formats tablib - Python Module for Tabular Datasets in XLS, CSV, JSON, YAML, c. http://python-tablib.org Simple CSV Data Wrangling with Python \u7535\u5b50\u8868\u683c python-pdfkit - Wkhtmltopdf python wrapper to convert html to pdf python-docx-template Others Deploying Python Applications with Docker - A Suggestion Kits boltons - Like builtins, but boltons. Constructs/recipes/snippets that would be handy in the standard library. Nothing like Michael Bolton. https://boltons.readthedocs.org pattern - Web mining module for Python, with tools for scraping, natural language processing, machine learning, network analysis and visualization. Utils dateutil sh - a full-fledged subprocess interface for Python that allows you to call any program as if it were a function duct.py - A Python library for shelling out. plumbum - Plumbum: Shell Combinators http://plumbum.readthedocs.org argcomplete - https://github.com/kislyuk/argcomplete emoji - emoji terminal output for Python ( EMOJI CHEAT SHEET ) retrying - Retrying is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. dill - Dill extends python's 'pickle' module for serializing and de-serializing python objects to the majority of the built-in python types CLI tqdm - A fast, extensible progress bar for Python https://pypi.python.org/pypi/tqdm pythonpy - the swiss army knife of the command line colorama - Cross-platform colored terminal text. Gooey - Turn (almost) any Python command line program into a full GUI application with one line Data Science GraphLab Create - powerful and usable data science tools that enable you to go quickly from inspiration to production Statistical and visualization packages seaborn - Statistical data visualization using matplotlib Statsmodels - Statsmodels is a Python module that allows users to explore data, estimate statistical models, and perform statistical tests. prettyplotlib - Painlessly create beautiful matplotlib plots matplotlib - matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms ggplot - ggplot is a plotting system for Python based on R's ggplot2 and the Grammar of Graphics. It is built for making profressional looking, plots quickly with minimal code. pandas - Python Data Analysis Library NumPy - NumPy is the fundamental package for scientific computing with Python bokeh - Interactive Web Plotting for Python pygal - A python SVG Charts Creator trendvis - TrendVis is a plotting package that uses matplotlib to create information-dense, sparkline-like, quantitative visualizations of multiple disparate data sets in a common plot area against a common variable. MPLD3 - Bringing Matplotlib to the Browser tutorials an introduction to statistics scientific-python-lectures \u4e2d\u6587 Overview of Python Visualization Tools Simple Graphing with IPython and Pandas Generate HTML reports with D3 graphs using Python, Plotly, and Pandas Frequentism and Bayesianism: A Practical Introduction NumPy Tentative NumPy Tutorial Getting the Best Performance out of NumPy 100 numpy exercises theano - a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently Collection of SciPy and NumPy tutorials Pandas official tutorial: 10 Minutes to pandas , Cookbook 10-minute tour of pandas , another tutorial 14 Best Python Pandas Features Pandas cookbook sklearn-pandas - Pandas integration with sklearn Summarising, Aggregating, and Grouping data in Python Pandas Pandas Pivot Table Explained | Pandas\u900f\u89c6\u8868\uff08pivot_table\uff09\u8be6\u89e3 Common Excel Tasks Demonstrated in Pandas | \u7528Pandas\u5b8c\u6210Excel\u4e2d\u5e38\u89c1\u7684\u4efb\u52a1 matplotlib Matplotlib tutorial example matplotlib-gallery Implementation of typographic and design principles in matplotlib and iPython notebook matplotlib_for_papers matplotlib-venn - Area-weighted venn-diagrams for Python/matplotlib MPLD3 - Bringing Matplotlib to the Browser Creating publication-quality figures with matplotlib Plots Heatmap in matplotlib with pcolor Cortical networks correlation matrix , Correlation_Matrix_Example.py ROC Curves in Python and R How to Create NBA Shot Charts in Python Map folium - Python Data. Leaflet.js Maps GUI pyglet - a cross-platform windowing and multimedia library for Python. kivy - Open source UI framework written in Python, running on Windows, Linux, OS X, Android and iOS https://kivy.org Data structure and Algorithm dablooms - A Scalable, Counting, Bloom Filter. Java, Python, Go edition. inbloom - Cross language bloom filter implementation datasketch - MinHash, LSH, Weighted MinHash, b-bit MinHash, HyperLogLog, HyperLogLog++ addit - The Python Dict that's better than heroin. python-bloomfilter - Scalable Bloom Filter implemented in Python madoka-python - Memory-efficient Count-Min Sketch Counter (based on Madoka C++ library## ) graphlab.SGraph - A scalable graph data structure. userguide Algorithms - Data Structures and Algorithms in Python Fast Non-Standard Data Structures for Python Problem Solving with Algorithms and Data Structures using python pyrsistent - Persistent/Immutable/Functional data structures for Python graph-tool - Efficient network analysis NetworkX - High-productivity software for complex networks bidict - Efficient, Pythonic bidirectional map implementation and related functionality. https://bidict.readthedocs.org/ nlib - Python Library of Numerical Algorithms pomegranate - Fast, flexible and easy to use probabilistic modelling in Python. http://pomegranate.readthedocs.org/en/latest/ string python-Levenshtein -The Levenshtein Python C extension module contains functions for fast computation of Levenshtein distance and string similarity | similar package: editdistance , fuzzywuzzy - Fuzzy String Matching in Python http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/ Machine Learning General Advice for applying Machine Learning \u6700\u597d\u7684Python\u673a\u5668\u5b66\u4e60\u5e93 \u5927\u6570\u636e\u5206\u6790\u4e0e\u673a\u5668\u5b66\u4e60\u9886\u57dfPython\u5175\u5668\u8c31 , Python \u7f51\u9875\u722c\u866b \u6587\u672c\u5904\u7406 \u79d1\u5b66\u8ba1\u7b97 \u673a\u5668\u5b66\u4e60 \u6570\u636e\u6316\u6398\u5175\u5668\u8c31 Advice for applying Machine Learning Four steps to master machine learning with python (including free books resource Natural Language Processing with Python Hierarchical Clustering, Heatmaps, and Gridspec Linear SVM Classifier on Twitter User Recognition \u5728Python\u4e2d\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u9884\u6d4b\u6570\u636e Random Forests in Python , Random Forest Regression and Classification in R and Python , Powerful Guide to learn Random Forest (with codes in R Python) , another post: Random Forest in scikit-learn , \u673a\u5668\u5b66\u4e60\u53cd\u6b3a\u8bc8\u5b9e\u8df5\uff1aPython+scikit-learn+\u968f\u673a\u68ee\u6797 , Random forest interpretation with scikit-learn 6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python) k-Fold Cross Validation made simple packages mlxtend - A library of extension and helper modules for Python's data analysis and machine learning libraries. PyMC3 -a python module for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems. deap - Distributed Evolutionary Algorithms in Python http://deap.readthedocs.org/ Scikit-learn scikit-learn , Scikit-learn tutorials for the Scipy 2013 conference , I ntroduction to Machine Learning with Python and Scikit-Learn , scikit-learn-book , [\u7ed3\u5408Scikit-learn\u4ecb\u7ecd\u51e0\u79cd\u5e38\u7528\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5][8], Scikit-learn\u673a\u5668\u5b66\u4e60 , scipy_2015_sklearn_tutorial \u7ed3\u5408Scikit-learn\u4ecb\u7ecd\u51e0\u79cd\u5e38\u7528\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5 Tutorial on scikit-learn and IPython for parallel machine learning Theano Matrix factorization with Theano examples \u4e0d\u5230100\u884c\u4ee3\u7801\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u63a8\u8350\u7cfb\u7edf A Neural Network in 11 lines of Python (Part 1) Simple Genetic Algorithm In 15 Lines Of Python NLP Python library for processing Chinese text newspaper - News, full-text, and article metadata extraction python\u5229\u7528utf-8\u7f16\u7801\u5224\u65ad\u4e2d\u6587\u82f1\u6587\u5b57\u7b26(\u8f6c) python-pinyin Clustering annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk pysparnn - Approximate Nearest Neighbor Search for Sparse Data in Python! Optimization Spearmint - Spearmint Bayesian optimization codebase Deep learning neural-networks-and-deep-learning theano keras - Theano-based Deep Learning library (convnets, recurrent neural networks, and more). Runs on Theano and TensorFlow. http://keras.io/ DeepLearning tutorial\uff086\uff09\u6613\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6Keras\u7b80\u4ecb DeepLearning tutorial\uff087\uff09\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6Keras\u7684\u4f7f\u7528-\u8fdb\u9636 nolearn - Abstractions around neural net libraries, most notably Lasagne. http://pythonhosted.org/nolearn/\\ \u4eceTheano\u5230Lasagne\uff1a\u57fa\u4e8ePython\u7684\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\u548c\u5e93 Quick Coding Intro to Neural Networks \u5229\u7528python\u7684theano\u5e93\u5237kaggle mnist\u6392\u884c\u699c \u4f7f\u7528GPU\u548cTheano\u52a0\u901f\u6df1\u5ea6\u5b66\u4e60 \u6280\u672f\u5411\uff1a\u4e00\u6587\u8bfb\u61c2\u5377\u79ef\u795e\u7ecf\u7f51\u7edcCNN Deep Learning Libraries by Language PDNN: A Python Toolkit for Deep Learning keras-rl - Deep Reinforcement Learning for Keras. Image Scikit-Image Tutorials Pillow - The friendly PIL fork http://python-pillow.github.io/ OpenCV An Algorithm to Extract Looping GIFs From Videos | \u4ece\u89c6\u9891\u63d0\u53d6\u5faa\u73af\u64ad\u653e\u5f0fGIF\u52a8\u753b\u7684\u7b97\u6cd5 Basic motion detection and tracking with Python and OpenCV (part 1 , 2 )| \u7528 Python \u548c OpenCV \u68c0\u6d4b\u548c\u8ddf\u8e2a\u8fd0\u52a8\u5bf9\u8c61( \u4e0a \uff0c \u4e0b ) Beautiful Python: A Simple ASCII Art Generator from Images Python\u7f16\u7a0b\u4e2d\u4f7f\u7528Pillow\u6765\u5904\u7406\u56fe\u50cf\u7684\u57fa\u7840\u6559\u7a0b GEO geoplotlib - python toolbox for visualizing geographical data and making maps Web Flask the-flask-mega-tutorial | Flask\u5927\u578b\u6559\u7a0b\u9879\u76ee , Flask \u6587\u6863 , awesome-flask, Designing a RESTful API with Python and Flask , Handling User Authentication With Angular and Flask Build an API under 30 lines of code with Python and Flask python-and-flask-are-ridiculously-powerful Flask web\u5f00\u53d1\u7b14\u8bb0 pyxley Python helpers for building dashboards using Flask and React Comparison Django vs Flask vs Pyramid: Choosing a Python Web Framework Others Web Scraping With Scrapy and MongoDB , part2 pyquery , Beautiful Soup Let\u2019s Build A Web Server ( part2 ) ripozo - A tool for quickly creating REST/HATEOAS/Hypermedia APIs in python http://ripozo.readthedocs.org/ Pyxley: Python Powered Dashboards How to scrape a website that requires login with Python - Python, Ruby, and Golang: A Web Service Application Comparison grab - Web Scraping Framework http://grablib.org Fun Jarvis PyLatex pyautogui - a Python module for programmatically controlling the mouse and keyboard. Finance The Efficient Frontier: Markowitz portfolio optimization in Python More awesome-python \u975e\u5e38\u5e72\u8d27\u4e4bPython\u8d44\u6e90\u5927\u5168 Python\u6846\u67b6\u3001\u5e93\u548c\u8f6f\u4ef6\u8d44\u6e90\u5927\u5168(\u6574\u7406\u7bc7) Misc update all packages: pip upgrade all sudo pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 sudo pip install -U My Python Environment Workflow with Conda Open Sourcing a Python Project the Right Way , \u4e2d\u6587\u8bd1\u6587 fake-factory - Faker is a Python package that generates fake data for you.","title":"Python"},{"location":"python/#python","text":"Table of Contents Tutorials Comparison to R Coding Functional Programming Concurrence Programming Data science IDE Performance Bigdata Database File formats Others Kits Utils CLI Data Science Statistical and visualization NumPy Pandas matplotlib Plots Map GUI Data structure and Algorithm Machine Learning NLP Clustering Optimization Deep learning Image GEO Web Flask Comparison Others Fun Finance More Misc","title":"Python"},{"location":"python/#tutorials","text":"Python\u6559\u7a0b ( Python3\u7248\u672c ) by \u5ed6\u96ea\u5cf0, python\u8fdb\u9636 \u96f6\u57fa\u7840\u5b66python Intermediate Python Python Practice Book pymbook Full Stack Python Migrating to Python 3 with pleasure / \u5728Python 2.7\u5373\u5c06\u505c\u6b62\u652f\u6301\u65f6\uff0c\u6211\u4eec\u4e3a\u4f60\u51c6\u5907\u4e86\u4e00\u4efd3.x\u8fc1\u79fb\u6307\u5357 Writing Python 2-3 compatible code | \u7f16\u5199\u517c\u5bb9 Python 2 \u548c Python 3 \u7684\u4ee3\u7801 Python Cookbook more books good-to-great-python-reads/ Python \u5165\u95e8\u6307\u5357 Useful functions, tutorials, and other Python-related things Training Material | Addfor s.r.l. Notebook Gallery Python Notes Collections Automate the Boring Stuff with Python Practical Programming for Total Beginners More A COLLECTION OF PYTHON MUST READS 10 python blogs worth following Python for Data Science vs Python for Web Development Computational Statistics in Python","title":"Tutorials"},{"location":"python/#comparison-to-r","text":"Data munging in R with dplyr and in Python with Pandas: basic structures and going from one to the other","title":"Comparison to R"},{"location":"python/#coding","text":"python-patterns - A collection of design patterns/idioms in Python Code Style | Python\u4ee3\u7801\u98ce\u683c Python \u53d8\u91cf\u547d\u540d\u89c4\u8303 Python\u5b89\u5168\u7f16\u7801\u4e0e\u4ee3\u7801\u5ba1\u8ba1 Python\u547d\u540d\u7a7a\u95f4\u548c\u4f5c\u7528\u57df\u7aa5\u63a2 regular-expressions , Python\u6b63\u5219\u8868\u8fbe\u5f0f\u6307\u5357 , FuzzyFinder - in 10 lines of Python Python's Hidden Regular Expression Gems Python Tips and Traps Python best practices A random collection of useful Python snippets pyperform - An easy and convienent way to performance test python code. \u50cfPython\u4e13\u5bb6\u4e00\u6837\u7f16\u7a0b: \u9053\u5730\u7684Python , \u521d\u5b66\u8005\u5fc5\u77e5\u7684Python\u4e2d\u4f18\u96c5\u7684\u7528\u6cd5 The Most Diabolical Python Antipattern The Little Book of Python Anti-Patterns , The Most Diabolical Python Antipattern Potential Pythonic Pitfalls Python Patterns - An Optimization Anecdote Python\u4e2d\u7684\u53d8\u91cf\u3001\u5f15\u7528\u3001\u62f7\u8d1d\u548c\u4f5c\u7528\u57df \u6df1\u5165\u7406\u89e3Python\u4e2d\u7684\u751f\u6210\u5668 Understanding Python metaclasses , python-metaclasses-by-example , \u6df1\u523b\u7406\u89e3Python\u4e2d\u7684\u5143\u7c7b(metaclass) , \u4f7f\u7528\u5143\u7c7b Python: Tips, Tricks and Idioms , part2 PythonDecoratorLibrary , more about decorator Python\u4e2d\u7684\u7c7b\u548c\u5bf9\u8c61\uff08\u4e2d\u7ea7\uff09 Python\u81ea\u7701\uff08\u53cd\u5c04\uff09\u6307\u5357 Properties Python Multiple Inheritance: call super on all Python\u4e2d\u7684\u9ed8\u8ba4\u53c2\u6570 dict and slot \uff0c SAVING 9 GB OF RAM WITH PYTHON\u2019S SLOTS Python\u7684\u51e0\u4e2a\u7279\u522b\u7684__\u5f00\u5934\u7684\u65b9\u6cd5 [\u8bd1] \u4e0e Python \u65e0\u7f1d\u96c6\u6210\u2014\u2014\u57fa\u672c\u7279\u6b8a\u65b9\u6cd5 1 Using % and .format() for great good! Optimize Python with Closures A simple example of Python OOP development (with TDD) - Part 1 Python 3 OOP Notebooks 5 Simple Rules For Building Great Python Packages | \u6784\u5efa\u5065\u58ee Python \u5305\u7684 5 \u4e2a\u7b80\u5355\u89c4\u5219 Python \u4e2d\u7684\u8fdb\u7a0b\u3001\u7ebf\u7a0b\u3001\u534f\u7a0b\u3001\u540c\u6b65\u3001\u5f02\u6b65\u3001\u56de\u8c03 Inner Functions - What Are They Good For? __init__() \u65b9\u6cd5 1 , 2 , 3 , 4 Good logging practice in Python | \u6bcf\u4e2a Python \u7a0b\u5e8f\u5458\u90fd\u8981\u77e5\u9053\u7684\u65e5\u5fd7\u5b9e\u8df5 Why Doesn't Python Have Switch/Case? python\u7f16\u7a0b\u4e2d\u5e38\u7528\u768412\u79cd\u57fa\u7840\u77e5\u8bc6\u603b\u7ed3 Python Examples - Get python examples for popular modules. 30 Python Language Features and Tricks You May Not Know About | 30\u4e2a\u6709\u5173Python\u7684\u5c0f\u6280\u5de7 yapf - A formatter for Python files Python error check graph Python\u4e2deval\u5e26\u6765\u7684\u6f5c\u5728\u98ce\u9669 \u5173\u4e8ePython\u7684\u9762\u8bd5\u9898","title":"Coding"},{"location":"python/#functional-programming","text":"Functional Programming HOWTO A practical introduction to functional programming | \u51fd\u6570\u5f0f\u7f16\u7a0b\u5b9e\u6218\u6559\u7a0b\uff08Python\u7248\uff09 functional-programming-python.pdf PyToolz - Toolz provides a set of utility functions for iterators, functions, and dictionaries. example , Derive a Markov model from the human genome in","title":"Functional Programming"},{"location":"python/#concurrence-programming","text":"Background tasks in Python 3.5 \u3010\u8bd1\u3011\u6df1\u5165\u7406\u89e3python3.4\u4e2dAsyncio\u5e93\u4e0eNode.js\u7684\u5f02\u6b65IO\u673a\u5236","title":"Concurrence Programming"},{"location":"python/#data-science","text":"Comprehensive learning path \u2013 Data Science in Python INTRODUCTION TO PYTHON FOR DATA MINING Python: Getting Started with Data Analysis Practical Data Science in Python code_py - A collection of well-commented code snippits for data science data-science-ipython-notebooks DataPyR","title":"Data science"},{"location":"python/#learning-seattles-work-habits-from-bicycle-counts-updated","text":"Eight Tools That Show What\u2019s on the Horizon for the Python Data Ecosystem","title":"Learning Seattle's Work Habits from Bicycle Counts (Updated!)"},{"location":"python/#ide","text":"Things that surprised me in PyCharm | Python IDE\uff1aPyCharm\u4e2d\u7684\u90a3\u4e9b\u5b9e\u7528\u529f\u80fd","title":"IDE"},{"location":"python/#performance","text":"packages 1 theano numba - NumPy aware dynamic Python compiler using LLVM multiprocessing - offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Multiprocessing: Task parallelism for the masses , An introduction to parallel programming , Python\u591a\u8fdb\u7a0b\u6a21\u5757Multiprocessing\u4ecb\u7ecd , Communication Between Processes , Python\u591a\u8fdb\u7a0b\u7f16\u7a0b Python and Parallelism or Dask numexpr - Fast numerical array expression evaluator for Python, NumPy, PyTables, pandas, bcolz and more https://github.com/pydata/numexpr/wiki/Numexpr-Users-Guide packages 2 huey - a little multi-threaded task queue for python python-gearman Dask - provides multi-core execution on larger-than-memory datasets using blocked algorithms and task scheduling. Ruffus : a lightweight Python library for computational pipelines Luigi is a Python module that helps you build complex pipelines of batch jobs. It handles dependency resolution, workflow management, visualization etc. It also comes with Hadoop support built in. pyflow - A lightweight parallel task engine http://Illumina.github.io/pyflow/ packages 3 How We Reduced CPU Usage by 80% through Python Profiling tools vprof - Visual Python profiler tutorials \u4f18\u5316 Python \u6027\u80fd\uff1aPyPy\u3001Numba \u4e0e Cython\uff0c\u8c01\u624d\u662f\u76ee\u524d\u6700\u4f18\u79c0\u7684 Python \u8fd0\u7b97\u89e3\u51b3\u65b9\u6848\uff1f How To Make Python Run As Fast As Juli A guide to analyzing Python performance | Python\u6027\u80fd\u5206\u6790\u6307\u5357 Python: Faster Way http://pythonfasterway.uni.me PerformanceTips 7 tips to Time Python scripts and control Memory CPU usage | 7 \u4e2a\u6d4b\u91cf Python \u811a\u672c\u548c\u63a7\u5236\u5185\u5b58\u4ee5\u53ca CPU \u4f7f\u7528\u7387\u7684\u6280\u5de7 Speeding Up Your Python Code | \u52a0\u901f\u4f60\u7684Python\u4ee3\u7801 Python \u4ee3\u7801\u6027\u80fd\u4f18\u5316\u6280\u5de7 Optimizing Python - a Case Study 6 Python Performance Tips (very new view for me) | \u63d0\u5347 Python \u7a0b\u5e8f\u6027\u80fd\u7684 6 \u4e2a\u6280\u5de7 Why Python is Slow: Looking Under the Hood RunSnakeRun - a small GUI utility that allows you to view (Python) cProfile or Profile profiler dumps in a sortable GUI view Why Python is Slow: Looking Under the Hood \u4f7f\u7528Python\u8fdb\u884c\u5e76\u53d1\u7f16\u7a0b Theano\u6559\u7a0b\uff1aPython\u7684\u5185\u5b58\u7ba1\u7406 A guide to analyzing Python performance Python\u591a\u6838\u7f16\u7a0bmpi4py\u5b9e\u8df5 Python \u6027\u80fd\u5206\u6790\u5927\u5168","title":"Performance"},{"location":"python/#bigdata","text":"Using Spark DataFrames for large scale data science Spark Programming Guide | [\u7ffb\u8bd1]Spark\u7f16\u7a0b\u6307\u5357(Python\u7248) Using PyTables for Larger-Than-RAM Data Processing h5py","title":"Bigdata"},{"location":"python/#database","text":"CodernityDB - a more advanced key-value Native Python Database, with multiple key-values indexes in the same engine zerodb - ZeroDB is an end-to-end encrypted database. Data can be stored on untrusted database servers without ever exposing the encryption key. Clients can execute remote queries against the encrypted data without downloading all of it or suffering an excessive performance hit. mongo\u6570\u636e\u5e93\u57fa\u672c\u64cd\u4f5c-python\u7bc7 ZODB4 - ZODB makes it really fast and easy to build and distribute Persistent Python applications pickleDB - pickleDB is a lightweight and simple key-value store tinydb - TinyDB is a lightweight document oriented database optimized for your happiness :) https://tinydb.readthedocs.org orm peewee - Peewee is a simple and small ORM. It has few (but expressive) concepts, making it easy to learn and intuitive to use.","title":"Database"},{"location":"python/#file-formats","text":"tablib - Python Module for Tabular Datasets in XLS, CSV, JSON, YAML, c. http://python-tablib.org Simple CSV Data Wrangling with Python \u7535\u5b50\u8868\u683c python-pdfkit - Wkhtmltopdf python wrapper to convert html to pdf python-docx-template","title":"File formats"},{"location":"python/#others","text":"Deploying Python Applications with Docker - A Suggestion","title":"Others"},{"location":"python/#kits","text":"boltons - Like builtins, but boltons. Constructs/recipes/snippets that would be handy in the standard library. Nothing like Michael Bolton. https://boltons.readthedocs.org pattern - Web mining module for Python, with tools for scraping, natural language processing, machine learning, network analysis and visualization.","title":"Kits"},{"location":"python/#utils","text":"dateutil sh - a full-fledged subprocess interface for Python that allows you to call any program as if it were a function duct.py - A Python library for shelling out. plumbum - Plumbum: Shell Combinators http://plumbum.readthedocs.org argcomplete - https://github.com/kislyuk/argcomplete emoji - emoji terminal output for Python ( EMOJI CHEAT SHEET ) retrying - Retrying is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. dill - Dill extends python's 'pickle' module for serializing and de-serializing python objects to the majority of the built-in python types","title":"Utils"},{"location":"python/#cli","text":"tqdm - A fast, extensible progress bar for Python https://pypi.python.org/pypi/tqdm pythonpy - the swiss army knife of the command line colorama - Cross-platform colored terminal text. Gooey - Turn (almost) any Python command line program into a full GUI application with one line","title":"CLI"},{"location":"python/#data-science_1","text":"GraphLab Create - powerful and usable data science tools that enable you to go quickly from inspiration to production","title":"Data Science"},{"location":"python/#statistical-and-visualization","text":"packages seaborn - Statistical data visualization using matplotlib Statsmodels - Statsmodels is a Python module that allows users to explore data, estimate statistical models, and perform statistical tests. prettyplotlib - Painlessly create beautiful matplotlib plots matplotlib - matplotlib is a python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms ggplot - ggplot is a plotting system for Python based on R's ggplot2 and the Grammar of Graphics. It is built for making profressional looking, plots quickly with minimal code. pandas - Python Data Analysis Library NumPy - NumPy is the fundamental package for scientific computing with Python bokeh - Interactive Web Plotting for Python pygal - A python SVG Charts Creator trendvis - TrendVis is a plotting package that uses matplotlib to create information-dense, sparkline-like, quantitative visualizations of multiple disparate data sets in a common plot area against a common variable. MPLD3 - Bringing Matplotlib to the Browser tutorials an introduction to statistics scientific-python-lectures \u4e2d\u6587 Overview of Python Visualization Tools Simple Graphing with IPython and Pandas Generate HTML reports with D3 graphs using Python, Plotly, and Pandas Frequentism and Bayesianism: A Practical Introduction","title":"Statistical and\u00a0visualization"},{"location":"python/#numpy","text":"Tentative NumPy Tutorial Getting the Best Performance out of NumPy 100 numpy exercises theano - a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently Collection of SciPy and NumPy tutorials","title":"NumPy"},{"location":"python/#pandas","text":"official tutorial: 10 Minutes to pandas , Cookbook 10-minute tour of pandas , another tutorial 14 Best Python Pandas Features Pandas cookbook sklearn-pandas - Pandas integration with sklearn Summarising, Aggregating, and Grouping data in Python Pandas Pandas Pivot Table Explained | Pandas\u900f\u89c6\u8868\uff08pivot_table\uff09\u8be6\u89e3 Common Excel Tasks Demonstrated in Pandas | \u7528Pandas\u5b8c\u6210Excel\u4e2d\u5e38\u89c1\u7684\u4efb\u52a1","title":"Pandas"},{"location":"python/#matplotlib","text":"Matplotlib tutorial example matplotlib-gallery Implementation of typographic and design principles in matplotlib and iPython notebook matplotlib_for_papers matplotlib-venn - Area-weighted venn-diagrams for Python/matplotlib MPLD3 - Bringing Matplotlib to the Browser Creating publication-quality figures with matplotlib","title":"matplotlib"},{"location":"python/#plots","text":"Heatmap in matplotlib with pcolor Cortical networks correlation matrix , Correlation_Matrix_Example.py ROC Curves in Python and R How to Create NBA Shot Charts in Python","title":"Plots"},{"location":"python/#map","text":"folium - Python Data. Leaflet.js Maps","title":"Map"},{"location":"python/#gui","text":"pyglet - a cross-platform windowing and multimedia library for Python. kivy - Open source UI framework written in Python, running on Windows, Linux, OS X, Android and iOS https://kivy.org","title":"GUI"},{"location":"python/#data-structure-and-algorithm","text":"dablooms - A Scalable, Counting, Bloom Filter. Java, Python, Go edition. inbloom - Cross language bloom filter implementation datasketch - MinHash, LSH, Weighted MinHash, b-bit MinHash, HyperLogLog, HyperLogLog++ addit - The Python Dict that's better than heroin. python-bloomfilter - Scalable Bloom Filter implemented in Python madoka-python - Memory-efficient Count-Min Sketch Counter (based on Madoka C++ library## ) graphlab.SGraph - A scalable graph data structure. userguide Algorithms - Data Structures and Algorithms in Python Fast Non-Standard Data Structures for Python Problem Solving with Algorithms and Data Structures using python pyrsistent - Persistent/Immutable/Functional data structures for Python graph-tool - Efficient network analysis NetworkX - High-productivity software for complex networks bidict - Efficient, Pythonic bidirectional map implementation and related functionality. https://bidict.readthedocs.org/ nlib - Python Library of Numerical Algorithms pomegranate - Fast, flexible and easy to use probabilistic modelling in Python. http://pomegranate.readthedocs.org/en/latest/ string python-Levenshtein -The Levenshtein Python C extension module contains functions for fast computation of Levenshtein distance and string similarity | similar package: editdistance , fuzzywuzzy - Fuzzy String Matching in Python http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/","title":"Data structure and\u00a0Algorithm"},{"location":"python/#machine-learning","text":"General Advice for applying Machine Learning \u6700\u597d\u7684Python\u673a\u5668\u5b66\u4e60\u5e93 \u5927\u6570\u636e\u5206\u6790\u4e0e\u673a\u5668\u5b66\u4e60\u9886\u57dfPython\u5175\u5668\u8c31 , Python \u7f51\u9875\u722c\u866b \u6587\u672c\u5904\u7406 \u79d1\u5b66\u8ba1\u7b97 \u673a\u5668\u5b66\u4e60 \u6570\u636e\u6316\u6398\u5175\u5668\u8c31 Advice for applying Machine Learning Four steps to master machine learning with python (including free books resource Natural Language Processing with Python Hierarchical Clustering, Heatmaps, and Gridspec Linear SVM Classifier on Twitter User Recognition \u5728Python\u4e2d\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u9884\u6d4b\u6570\u636e Random Forests in Python , Random Forest Regression and Classification in R and Python , Powerful Guide to learn Random Forest (with codes in R Python) , another post: Random Forest in scikit-learn , \u673a\u5668\u5b66\u4e60\u53cd\u6b3a\u8bc8\u5b9e\u8df5\uff1aPython+scikit-learn+\u968f\u673a\u68ee\u6797 , Random forest interpretation with scikit-learn 6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python) k-Fold Cross Validation made simple packages mlxtend - A library of extension and helper modules for Python's data analysis and machine learning libraries. PyMC3 -a python module for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems. deap - Distributed Evolutionary Algorithms in Python http://deap.readthedocs.org/ Scikit-learn scikit-learn , Scikit-learn tutorials for the Scipy 2013 conference , I ntroduction to Machine Learning with Python and Scikit-Learn , scikit-learn-book , [\u7ed3\u5408Scikit-learn\u4ecb\u7ecd\u51e0\u79cd\u5e38\u7528\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5][8], Scikit-learn\u673a\u5668\u5b66\u4e60 , scipy_2015_sklearn_tutorial \u7ed3\u5408Scikit-learn\u4ecb\u7ecd\u51e0\u79cd\u5e38\u7528\u7684\u7279\u5f81\u9009\u62e9\u65b9\u6cd5 Tutorial on scikit-learn and IPython for parallel machine learning Theano Matrix factorization with Theano examples \u4e0d\u5230100\u884c\u4ee3\u7801\u5b9e\u73b0\u4e00\u4e2a\u7b80\u5355\u7684\u63a8\u8350\u7cfb\u7edf A Neural Network in 11 lines of Python (Part 1) Simple Genetic Algorithm In 15 Lines Of Python","title":"Machine Learning"},{"location":"python/#nlp","text":"Python library for processing Chinese text newspaper - News, full-text, and article metadata extraction python\u5229\u7528utf-8\u7f16\u7801\u5224\u65ad\u4e2d\u6587\u82f1\u6587\u5b57\u7b26(\u8f6c) python-pinyin","title":"NLP"},{"location":"python/#clustering","text":"annoy - Approximate Nearest Neighbors in C++/Python optimized for memory usage and loading/saving to disk pysparnn - Approximate Nearest Neighbor Search for Sparse Data in Python!","title":"Clustering"},{"location":"python/#optimization","text":"Spearmint - Spearmint Bayesian optimization codebase","title":"Optimization"},{"location":"python/#deep-learning","text":"neural-networks-and-deep-learning theano keras - Theano-based Deep Learning library (convnets, recurrent neural networks, and more). Runs on Theano and TensorFlow. http://keras.io/ DeepLearning tutorial\uff086\uff09\u6613\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6Keras\u7b80\u4ecb DeepLearning tutorial\uff087\uff09\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6Keras\u7684\u4f7f\u7528-\u8fdb\u9636 nolearn - Abstractions around neural net libraries, most notably Lasagne. http://pythonhosted.org/nolearn/\\ \u4eceTheano\u5230Lasagne\uff1a\u57fa\u4e8ePython\u7684\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\u548c\u5e93 Quick Coding Intro to Neural Networks \u5229\u7528python\u7684theano\u5e93\u5237kaggle mnist\u6392\u884c\u699c \u4f7f\u7528GPU\u548cTheano\u52a0\u901f\u6df1\u5ea6\u5b66\u4e60 \u6280\u672f\u5411\uff1a\u4e00\u6587\u8bfb\u61c2\u5377\u79ef\u795e\u7ecf\u7f51\u7edcCNN Deep Learning Libraries by Language PDNN: A Python Toolkit for Deep Learning keras-rl - Deep Reinforcement Learning for Keras.","title":"Deep learning"},{"location":"python/#image","text":"Scikit-Image Tutorials Pillow - The friendly PIL fork http://python-pillow.github.io/ OpenCV An Algorithm to Extract Looping GIFs From Videos | \u4ece\u89c6\u9891\u63d0\u53d6\u5faa\u73af\u64ad\u653e\u5f0fGIF\u52a8\u753b\u7684\u7b97\u6cd5 Basic motion detection and tracking with Python and OpenCV (part 1 , 2 )| \u7528 Python \u548c OpenCV \u68c0\u6d4b\u548c\u8ddf\u8e2a\u8fd0\u52a8\u5bf9\u8c61( \u4e0a \uff0c \u4e0b ) Beautiful Python: A Simple ASCII Art Generator from Images Python\u7f16\u7a0b\u4e2d\u4f7f\u7528Pillow\u6765\u5904\u7406\u56fe\u50cf\u7684\u57fa\u7840\u6559\u7a0b","title":"Image"},{"location":"python/#geo","text":"geoplotlib - python toolbox for visualizing geographical data and making maps","title":"GEO"},{"location":"python/#web","text":"","title":"Web"},{"location":"python/#flask","text":"the-flask-mega-tutorial | Flask\u5927\u578b\u6559\u7a0b\u9879\u76ee , Flask \u6587\u6863 , awesome-flask, Designing a RESTful API with Python and Flask , Handling User Authentication With Angular and Flask Build an API under 30 lines of code with Python and Flask python-and-flask-are-ridiculously-powerful Flask web\u5f00\u53d1\u7b14\u8bb0 pyxley Python helpers for building dashboards using Flask and React","title":"Flask"},{"location":"python/#comparison","text":"Django vs Flask vs Pyramid: Choosing a Python Web Framework","title":"Comparison"},{"location":"python/#others_1","text":"Web Scraping With Scrapy and MongoDB , part2 pyquery , Beautiful Soup Let\u2019s Build A Web Server ( part2 ) ripozo - A tool for quickly creating REST/HATEOAS/Hypermedia APIs in python http://ripozo.readthedocs.org/ Pyxley: Python Powered Dashboards How to scrape a website that requires login with Python - Python, Ruby, and Golang: A Web Service Application Comparison grab - Web Scraping Framework http://grablib.org","title":"Others"},{"location":"python/#fun","text":"Jarvis PyLatex pyautogui - a Python module for programmatically controlling the mouse and keyboard.","title":"Fun"},{"location":"python/#finance","text":"The Efficient Frontier: Markowitz portfolio optimization in Python","title":"Finance"},{"location":"python/#more","text":"awesome-python \u975e\u5e38\u5e72\u8d27\u4e4bPython\u8d44\u6e90\u5927\u5168 Python\u6846\u67b6\u3001\u5e93\u548c\u8f6f\u4ef6\u8d44\u6e90\u5927\u5168(\u6574\u7406\u7bc7)","title":"More"},{"location":"python/#misc","text":"update all packages: pip upgrade all sudo pip freeze --local | grep -v '^\\-e' | cut -d = -f 1 | xargs -n1 sudo pip install -U My Python Environment Workflow with Conda Open Sourcing a Python Project the Right Way , \u4e2d\u6587\u8bd1\u6587 fake-factory - Faker is a Python package that generates fake data for you.","title":"Misc"},{"location":"python_spider/","text":"Python Spider \u8d35\u6709\u6052\uff0c\u4f55\u5fc5\u4e09\u66f4\u8d77\u4e94\u66f4\u7761\uff1b\u6700\u65e0\u76ca\uff0c\u53ea\u6015\u4e00\u65e5\u66dd\u5341\u65e5\u5bd2\u3002 Python3\u722c\u866b\u5b9e\u6218\uff1a\u5b9e\u6218\u6e90\u7801+\u535a\u5ba2\u8bb2\u89e3 \u4e2a\u4eba\u7f51\u7ad9 CSDN\u535a\u5ba2 CSDN\u722c\u866b\u4e13\u680f \u5b66\u4e60\u4ea4\u6d41\u7fa4\u3010328127489\u3011 \u516c\u4f17\u53f7\uff1a JackCui-AI \u5206\u4eab\u6280\u672f\uff0c\u4e50\u4eab\u751f\u6d3b\uff1aJack Cui\u516c\u4f17\u53f7\u63a8\u9001\u201c\u7a0b\u5e8f\u5458\u6b22\u4e50\u9001\u201d\u7cfb\u5217\u8d44\u8baf\u7c7b\u6587\u7ae0\uff0c\u4ee5\u53ca\u6280\u672f\u7c7b\u6587\u7ae0\uff0c\u6b22\u8fce\u60a8\u7684\u5173\u6ce8\uff01 \u58f0\u660e \u4ee3\u7801\u3001\u6559\u7a0b \u4ec5\u9650\u4e8e\u5b66\u4e60\u4ea4\u6d41\uff0c\u8bf7\u52ff\u7528\u4e8e\u4efb\u4f55\u5546\u4e1a\u7528\u9014\uff01 \u6587\u7ae0\u9996\u53d1\u58f0\u660e \u6587\u7ae0\u5728\u81ea\u5df1\u7684\u4e2a\u4eba\u7f51\u7ad9\u9996\u53d1\uff0c\u5176\u4ed6\u5e73\u53f0\u6587\u7ae0\u5747\u5c5e\u8f6c\u53d1\uff0c\u5982\u60f3\u83b7\u5f97\u6700\u65b0\u66f4\u65b0\u8fdb\u5c55\uff0c\u6b22\u8fce\u5173\u6ce8\u6211\u7684\u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/ \u76ee\u5f55 \u722c\u866b\u5c0f\u5de5\u5177 \u6587\u4ef6\u4e0b\u8f7d\u5c0f\u52a9\u624b \u722c\u866b\u5b9e\u6218 \u7b14\u8da3\u770b\u5c0f\u8bf4\u4e0b\u8f7d \u767e\u5ea6\u6587\u5e93\u514d\u8d39\u6587\u7ae0\u4e0b\u8f7d\u52a9\u624b_rev1 \u767e\u5ea6\u6587\u5e93\u514d\u8d39\u6587\u7ae0\u4e0b\u8f7d\u52a9\u624b_rev2 \u300a\u5e05\u554a\u300b\u7f51\u5e05\u54e5\u56fe\u7247\u4e0b\u8f7d \u6784\u5efa\u4ee3\u7406IP\u6c60 \u300a\u706b\u5f71\u5fcd\u8005\u300b\u6f2b\u753b\u4e0b\u8f7d \u8d22\u52a1\u62a5\u8868\u4e0b\u8f7d\u5c0f\u52a9\u624b \u4e00\u5c0f\u65f6\u5165\u95e8\u7f51\u7edc\u722c\u866b \u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d GEETEST\u9a8c\u8bc1\u7801\u8bc6\u522b 12306\u62a2\u7968\u5c0f\u52a9\u624b \u767e\u4e07\u82f1\u96c4\u7b54\u9898\u8f85\u52a9\u7cfb\u7edf \u7f51\u6613\u4e91\u97f3\u4e50\u514d\u8d39\u97f3\u4e50\u6279\u91cf\u4e0b\u8f7d B\u7ad9\u514d\u8d39\u89c6\u9891\u548c\u5f39\u5e55\u6279\u91cf\u4e0b\u8f7d \u4eac\u4e1c\u5546\u54c1\u6652\u5355\u56fe\u4e0b\u8f7d \u6b63\u65b9\u6559\u52a1\u7ba1\u7406\u7cfb\u7edf\u4e2a\u4eba\u4fe1\u606f\u67e5\u8be2 \u5176\u5b83 \u722c\u866b\u5c0f\u5de5\u5177 downloader.py:\u6587\u4ef6\u4e0b\u8f7d\u5c0f\u52a9\u624b \u4e00\u4e2a\u53ef\u4ee5\u7528\u4e8e\u4e0b\u8f7d\u56fe\u7247\u3001\u89c6\u9891\u3001\u6587\u4ef6\u7684\u5c0f\u5de5\u5177\uff0c\u6709\u4e0b\u8f7d\u8fdb\u5ea6\u663e\u793a\u529f\u80fd\u3002\u7a0d\u52a0\u4fee\u6539\u5373\u53ef\u6dfb\u52a0\u5230\u81ea\u5df1\u7684\u722c\u866b\u4e2d\u3002 \u52a8\u6001\u793a\u610f\u56fe\uff1a \u722c\u866b\u5b9e\u6218 biqukan.py:\u300a\u7b14\u8da3\u770b\u300b\u76d7\u7248\u5c0f\u8bf4\u7f51\u7ad9\uff0c\u722c\u53d6\u5c0f\u8bf4\u5de5\u5177 \u7b2c\u4e09\u65b9\u4f9d\u8d56\u5e93\u5b89\u88c5\uff1a pip3 install beautifulsoup4 \u4f7f\u7528\u65b9\u6cd5\uff1a python biqukan.py baiduwenku.py: \u767e\u5ea6\u6587\u5e93word\u6587\u7ae0\u722c\u53d6 \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72331737 \u4ee3\u7801\u4e0d\u5b8c\u5584\uff0c\u6ca1\u6709\u8fdb\u884c\u6253\u5305\uff0c\u4e0d\u5177\u901a\u7528\u6027\uff0c\u7eaf\u5c5e\u5a31\u4e50\u3002 shuaia.py: \u722c\u53d6\u300a\u5e05\u554a\u300b\u7f51\uff0c\u5e05\u54e5\u56fe\u7247 \u300a\u5e05\u554a\u300b\u7f51URL\uff1ahttp://www.shuaia.net/index.html \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72597755 \u7b2c\u4e09\u65b9\u4f9d\u8d56\u5e93\u5b89\u88c5\uff1a pip3 install requests beautifulsoup4 daili.py: \u6784\u5efa\u4ee3\u7406IP\u6c60 \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72793480 carton: \u4f7f\u7528Scrapy\u722c\u53d6\u300a\u706b\u5f71\u5fcd\u8005\u300b\u6f2b\u753b \u4ee3\u7801\u53ef\u4ee5\u722c\u53d6\u6574\u4e2a\u300a\u706b\u5f71\u5fcd\u8005\u300b\u6f2b\u753b\u6240\u6709\u7ae0\u8282\u7684\u5185\u5bb9\uff0c\u4fdd\u5b58\u5230\u672c\u5730\u3002\u66f4\u6539\u5730\u5740\uff0c\u53ef\u4ee5\u722c\u53d6\u5176\u4ed6\u6f2b\u753b\u3002\u4fdd\u5b58\u5730\u5740\u53ef\u4ee5\u5728settings.py\u4e2d\u4fee\u6539\u3002 \u52a8\u6f2b\u7f51\u7ad9\uff1ahttp://comic.kukudm.com/ \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72858983 hero.py: \u300a\u738b\u8005\u8363\u8000\u300b\u63a8\u8350\u51fa\u88c5\u67e5\u8be2\u5c0f\u52a9\u624b \u7f51\u9875\u722c\u53d6\u5df2\u7ecf\u4f1a\u4e86\uff0c\u60f3\u8fc7\u722c\u53d6\u624b\u673aAPP\u91cc\u7684\u5185\u5bb9\u5417\uff1f \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/76850843 financical.py: \u8d22\u52a1\u62a5\u8868\u4e0b\u8f7d\u5c0f\u52a9\u624b \u722c\u53d6\u7684\u6570\u636e\u5b58\u5165\u6570\u636e\u5e93\u4f1a\u5417\uff1f\u300a\u8ddf\u80a1\u795e\u5df4\u83f2\u7279\u5b66\u4e60\u7092\u80a1\u4e4b\u8d22\u52a1\u62a5\u8868\u5165\u5e93(MySQL)\u300b\u4e5f\u8bb8\u80fd\u7ed9\u4f60\u4e00\u4e9b\u601d\u8def\u3002 \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/77801899 \u52a8\u6001\u793a\u610f\u56fe\uff1a one_hour_spider:\u4e00\u5c0f\u65f6\u5165\u95e8Python3\u7f51\u7edc\u722c\u866b\u3002 \u539f\u7406\u8bf4\u660e: \u77e5\u4e4e\uff1ahttps://zhuanlan.zhihu.com/p/29809609 CSDN\uff1ahttp://blog.csdn.net/c406495762/article/details/78123502 \u672c\u6b21\u5b9e\u6218\u5185\u5bb9\u6709\uff1a \u7f51\u7edc\u5c0f\u8bf4\u4e0b\u8f7d(\u9759\u6001\u7f51\u7ad9)-biqukan \u4f18\u7f8e\u58c1\u7eb8\u4e0b\u8f7d(\u52a8\u6001\u7f51\u7ad9)-unsplash \u89c6\u9891\u4e0b\u8f7d douyin.py:\u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d \u6296\u97f3App\u7684\u89c6\u9891\u4e0b\u8f7d\uff0c\u5c31\u662f\u666e\u901a\u7684App\u722c\u53d6\u3002 \u539f\u7406\u8bf4\u660e: \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/03/spider-5.html douyin_pro:\u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d\uff08\u5347\u7ea7\u7248\uff09 \u6296\u97f3App\u7684\u89c6\u9891\u4e0b\u8f7d\uff0c\u6dfb\u52a0\u89c6\u9891\u89e3\u6790\u7f51\u7ad9\uff0c\u652f\u6301\u65e0\u6c34\u5370\u89c6\u9891\u4e0b\u8f7d\uff0c\u4f7f\u7528\u7b2c\u4e09\u65b9\u5e73\u53f0\u89e3\u6790\u3002 \u539f\u7406\u8bf4\u660e: \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/03/spider-5.html douyin:\u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d\uff08\u5347\u7ea7\u72482\uff09 \u6296\u97f3App\u7684\u89c6\u9891\u4e0b\u8f7d\uff0c\u6dfb\u52a0\u89c6\u9891\u89e3\u6790\u7f51\u7ad9\uff0c\u652f\u6301\u65e0\u6c34\u5370\u89c6\u9891\u4e0b\u8f7d\uff0c\u901a\u8fc7url\u89e3\u6790\uff0c\u65e0\u9700\u7b2c\u4e09\u65b9\u5e73\u53f0\u3002 \u539f\u7406\u8bf4\u660e: \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/03/spider-5.html \u52a8\u6001\u793a\u610f\u56fe\uff1a geetest.py:GEETEST\u9a8c\u8bc1\u7801\u8bc6\u522b \u539f\u7406\u8bf4\u660e: \u65e0 12306.py:\u7528Python\u62a2\u706b\u8f66\u7968\u7b80\u5355\u4ee3\u7801 \u53ef\u4ee5\u81ea\u5df1\u6162\u6162\u4e30\u5bcc\uff0c\u86ee\u7b80\u5355\uff0c\u6709\u722c\u866b\u57fa\u7840\u5f88\u597d\u64cd\u4f5c\uff0c\u6ca1\u6709\u539f\u7406\u8bf4\u660e\u3002 baiwan:\u767e\u4e07\u82f1\u96c4\u8f85\u52a9\u7b54\u9898 \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/01/spider_3.html \u529f\u80fd\u4ecb\u7ecd\uff1a \u670d\u52a1\u5668\u7aef\uff0c\u4f7f\u7528Python\uff08baiwan.py\uff09\u901a\u8fc7\u6293\u5305\u83b7\u5f97\u7684\u63a5\u53e3\u83b7\u53d6\u7b54\u9898\u6570\u636e\uff0c\u89e3\u6790\u4e4b\u540e\u901a\u8fc7\u767e\u5ea6\u77e5\u9053\u641c\u7d22\u63a5\u53e3\u5339\u914d\u7b54\u6848\uff0c\u5c06\u6700\u7ec8\u5339\u914d\u7684\u7ed3\u679c\u5199\u5165\u6587\u4ef6\uff08file.txt)\u3002 \u624b\u673a\u6293\u5305\u4e0d\u4f1a\u7684\u670b\u53cb\uff0c\u53ef\u4ee5\u770b\u4e0b\u6211\u7684\u65e9\u671f \u624b\u673aAPP\u6293\u5305\u6559\u7a0b \u3002 Node.js\uff08app.js\uff09\u6bcf\u96941s\u8bfb\u53d6\u4e00\u6b21file.txt\u6587\u4ef6\uff0c\u5e76\u5c06\u8bfb\u53d6\u7ed3\u679c\u901a\u8fc7socket.io\u63a8\u9001\u7ed9\u5ba2\u6237\u7aef\uff08index.html\uff09\u3002 \u4eb2\u6d4b\u7b54\u9898\u5ef6\u65f6\u57283s\u5de6\u53f3\u3002 \u58f0\u660e\uff1a\u6ca1\u505a\u8fc7\u540e\u7aef\u548c\u524d\u7aef\uff0c\u82b1\u4e86\u4e00\u5929\u65f6\u95f4\uff0c\u73b0\u5b66\u73b0\u5356\u5f04\u597d\u7684\uff0cjavascript\u4e5f\u662f\u73b0\u770b\u73b0\u7528\uff0c\u767e\u5ea6\u7684\u7a0b\u5e8f\uff0c\u8c03\u8bd5\u8c03\u8bd5\u800c\u5df2\u3002\u53ef\u80fd\u6709\u5f88\u591a\u7528\u6cd5\u6bd4\u8f83low\u7684\u5730\u65b9\uff0c\u7528\u6cd5\u4e0d\u5bf9\uff0c\u8bf7\u52ff\u89c1\u602a\uff0c\u6709\u5927\u725b\u611f\u5174\u8da3\uff0c\u53ef\u4ee5\u81ea\u884c\u5b8c\u5584\u3002 Netease:\u6839\u636e\u6b4c\u5355\u4e0b\u8f7d\u7f51\u6613\u4e91\u97f3\u4e50 \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u529f\u80fd\u4ecb\u7ecd\uff1a \u6839\u636emusic_list.txt\u6587\u4ef6\u91cc\u7684\u6b4c\u5355\u7684\u4fe1\u606f\u4e0b\u8f7d\u7f51\u6613\u4e91\u97f3\u4e50\uff0c\u5c06\u81ea\u5df1\u559c\u6b22\u7684\u97f3\u4e50\u8fdb\u884c\u6279\u91cf\u4e0b\u8f7d\u3002 bilibili\uff1aB\u7ad9\u89c6\u9891\u548c\u5f39\u5e55\u6279\u91cf\u4e0b\u8f7d \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u4f7f\u7528\u8bf4\u660e\uff1a python bilibili.py -d \u732b -k \u732b -p 10 \u4e09\u4e2a\u53c2\u6570\uff1a -d \u4fdd\u5b58\u89c6\u9891\u7684\u6587\u4ef6\u5939\u540d -k B\u7ad9\u641c\u7d22\u7684\u5173\u952e\u5b57 -p \u4e0b\u8f7d\u641c\u7d22\u7ed3\u679c\u524d\u591a\u5c11\u9875 jingdong\uff1a\u4eac\u4e1c\u5546\u54c1\u6652\u5355\u56fe\u4e0b\u8f7d \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u4f7f\u7528\u8bf4\u660e\uff1a python jd.py -k \u8292\u679c \u4e09\u4e2a\u53c2\u6570\uff1a -d \u4fdd\u5b58\u56fe\u7247\u7684\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3afd.py\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939 -k \u641c\u7d22\u5173\u952e\u8bcd -n \u4e0b\u8f7d\u5546\u54c1\u7684\u6652\u5355\u56fe\u4e2a\u6570\uff0c\u5373n\u4e2a\u5546\u5e97\u7684\u6652\u5355\u56fe zhengfang_system_spider\uff1a\u5bf9\u6b63\u65b9\u6559\u52a1\u7ba1\u7406\u7cfb\u7edf\u4e2a\u4eba\u8bfe\u8868\uff0c\u4e2a\u4eba\u5b66\u751f\u6210\u7ee9\uff0c\u7ee9\u70b9\u7b49\u7b80\u5355\u722c\u53d6 \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u4f7f\u7528\u8bf4\u660e\uff1a cd zhengfang_system_spider pip install -r requirements.txt python spider.py \u5176\u5b83 \u6b22\u8fce Pull requests\uff0c\u611f\u8c22\u8d21\u732e\u3002","title":"Python_spider"},{"location":"python_spider/#python-spider","text":"\u8d35\u6709\u6052\uff0c\u4f55\u5fc5\u4e09\u66f4\u8d77\u4e94\u66f4\u7761\uff1b\u6700\u65e0\u76ca\uff0c\u53ea\u6015\u4e00\u65e5\u66dd\u5341\u65e5\u5bd2\u3002 Python3\u722c\u866b\u5b9e\u6218\uff1a\u5b9e\u6218\u6e90\u7801+\u535a\u5ba2\u8bb2\u89e3 \u4e2a\u4eba\u7f51\u7ad9 CSDN\u535a\u5ba2 CSDN\u722c\u866b\u4e13\u680f \u5b66\u4e60\u4ea4\u6d41\u7fa4\u3010328127489\u3011 \u516c\u4f17\u53f7\uff1a JackCui-AI \u5206\u4eab\u6280\u672f\uff0c\u4e50\u4eab\u751f\u6d3b\uff1aJack Cui\u516c\u4f17\u53f7\u63a8\u9001\u201c\u7a0b\u5e8f\u5458\u6b22\u4e50\u9001\u201d\u7cfb\u5217\u8d44\u8baf\u7c7b\u6587\u7ae0\uff0c\u4ee5\u53ca\u6280\u672f\u7c7b\u6587\u7ae0\uff0c\u6b22\u8fce\u60a8\u7684\u5173\u6ce8\uff01","title":"Python Spider"},{"location":"python_spider/#_1","text":"\u4ee3\u7801\u3001\u6559\u7a0b \u4ec5\u9650\u4e8e\u5b66\u4e60\u4ea4\u6d41\uff0c\u8bf7\u52ff\u7528\u4e8e\u4efb\u4f55\u5546\u4e1a\u7528\u9014\uff01","title":"\u58f0\u660e"},{"location":"python_spider/#_2","text":"\u6587\u7ae0\u5728\u81ea\u5df1\u7684\u4e2a\u4eba\u7f51\u7ad9\u9996\u53d1\uff0c\u5176\u4ed6\u5e73\u53f0\u6587\u7ae0\u5747\u5c5e\u8f6c\u53d1\uff0c\u5982\u60f3\u83b7\u5f97\u6700\u65b0\u66f4\u65b0\u8fdb\u5c55\uff0c\u6b22\u8fce\u5173\u6ce8\u6211\u7684\u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/","title":"\u6587\u7ae0\u9996\u53d1\u58f0\u660e"},{"location":"python_spider/#_3","text":"\u722c\u866b\u5c0f\u5de5\u5177 \u6587\u4ef6\u4e0b\u8f7d\u5c0f\u52a9\u624b \u722c\u866b\u5b9e\u6218 \u7b14\u8da3\u770b\u5c0f\u8bf4\u4e0b\u8f7d \u767e\u5ea6\u6587\u5e93\u514d\u8d39\u6587\u7ae0\u4e0b\u8f7d\u52a9\u624b_rev1 \u767e\u5ea6\u6587\u5e93\u514d\u8d39\u6587\u7ae0\u4e0b\u8f7d\u52a9\u624b_rev2 \u300a\u5e05\u554a\u300b\u7f51\u5e05\u54e5\u56fe\u7247\u4e0b\u8f7d \u6784\u5efa\u4ee3\u7406IP\u6c60 \u300a\u706b\u5f71\u5fcd\u8005\u300b\u6f2b\u753b\u4e0b\u8f7d \u8d22\u52a1\u62a5\u8868\u4e0b\u8f7d\u5c0f\u52a9\u624b \u4e00\u5c0f\u65f6\u5165\u95e8\u7f51\u7edc\u722c\u866b \u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d GEETEST\u9a8c\u8bc1\u7801\u8bc6\u522b 12306\u62a2\u7968\u5c0f\u52a9\u624b \u767e\u4e07\u82f1\u96c4\u7b54\u9898\u8f85\u52a9\u7cfb\u7edf \u7f51\u6613\u4e91\u97f3\u4e50\u514d\u8d39\u97f3\u4e50\u6279\u91cf\u4e0b\u8f7d B\u7ad9\u514d\u8d39\u89c6\u9891\u548c\u5f39\u5e55\u6279\u91cf\u4e0b\u8f7d \u4eac\u4e1c\u5546\u54c1\u6652\u5355\u56fe\u4e0b\u8f7d \u6b63\u65b9\u6559\u52a1\u7ba1\u7406\u7cfb\u7edf\u4e2a\u4eba\u4fe1\u606f\u67e5\u8be2 \u5176\u5b83","title":"\u76ee\u5f55"},{"location":"python_spider/#_4","text":"downloader.py:\u6587\u4ef6\u4e0b\u8f7d\u5c0f\u52a9\u624b \u4e00\u4e2a\u53ef\u4ee5\u7528\u4e8e\u4e0b\u8f7d\u56fe\u7247\u3001\u89c6\u9891\u3001\u6587\u4ef6\u7684\u5c0f\u5de5\u5177\uff0c\u6709\u4e0b\u8f7d\u8fdb\u5ea6\u663e\u793a\u529f\u80fd\u3002\u7a0d\u52a0\u4fee\u6539\u5373\u53ef\u6dfb\u52a0\u5230\u81ea\u5df1\u7684\u722c\u866b\u4e2d\u3002 \u52a8\u6001\u793a\u610f\u56fe\uff1a","title":"\u722c\u866b\u5c0f\u5de5\u5177"},{"location":"python_spider/#_5","text":"biqukan.py:\u300a\u7b14\u8da3\u770b\u300b\u76d7\u7248\u5c0f\u8bf4\u7f51\u7ad9\uff0c\u722c\u53d6\u5c0f\u8bf4\u5de5\u5177 \u7b2c\u4e09\u65b9\u4f9d\u8d56\u5e93\u5b89\u88c5\uff1a pip3 install beautifulsoup4 \u4f7f\u7528\u65b9\u6cd5\uff1a python biqukan.py baiduwenku.py: \u767e\u5ea6\u6587\u5e93word\u6587\u7ae0\u722c\u53d6 \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72331737 \u4ee3\u7801\u4e0d\u5b8c\u5584\uff0c\u6ca1\u6709\u8fdb\u884c\u6253\u5305\uff0c\u4e0d\u5177\u901a\u7528\u6027\uff0c\u7eaf\u5c5e\u5a31\u4e50\u3002 shuaia.py: \u722c\u53d6\u300a\u5e05\u554a\u300b\u7f51\uff0c\u5e05\u54e5\u56fe\u7247 \u300a\u5e05\u554a\u300b\u7f51URL\uff1ahttp://www.shuaia.net/index.html \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72597755 \u7b2c\u4e09\u65b9\u4f9d\u8d56\u5e93\u5b89\u88c5\uff1a pip3 install requests beautifulsoup4 daili.py: \u6784\u5efa\u4ee3\u7406IP\u6c60 \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72793480 carton: \u4f7f\u7528Scrapy\u722c\u53d6\u300a\u706b\u5f71\u5fcd\u8005\u300b\u6f2b\u753b \u4ee3\u7801\u53ef\u4ee5\u722c\u53d6\u6574\u4e2a\u300a\u706b\u5f71\u5fcd\u8005\u300b\u6f2b\u753b\u6240\u6709\u7ae0\u8282\u7684\u5185\u5bb9\uff0c\u4fdd\u5b58\u5230\u672c\u5730\u3002\u66f4\u6539\u5730\u5740\uff0c\u53ef\u4ee5\u722c\u53d6\u5176\u4ed6\u6f2b\u753b\u3002\u4fdd\u5b58\u5730\u5740\u53ef\u4ee5\u5728settings.py\u4e2d\u4fee\u6539\u3002 \u52a8\u6f2b\u7f51\u7ad9\uff1ahttp://comic.kukudm.com/ \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/72858983 hero.py: \u300a\u738b\u8005\u8363\u8000\u300b\u63a8\u8350\u51fa\u88c5\u67e5\u8be2\u5c0f\u52a9\u624b \u7f51\u9875\u722c\u53d6\u5df2\u7ecf\u4f1a\u4e86\uff0c\u60f3\u8fc7\u722c\u53d6\u624b\u673aAPP\u91cc\u7684\u5185\u5bb9\u5417\uff1f \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/76850843 financical.py: \u8d22\u52a1\u62a5\u8868\u4e0b\u8f7d\u5c0f\u52a9\u624b \u722c\u53d6\u7684\u6570\u636e\u5b58\u5165\u6570\u636e\u5e93\u4f1a\u5417\uff1f\u300a\u8ddf\u80a1\u795e\u5df4\u83f2\u7279\u5b66\u4e60\u7092\u80a1\u4e4b\u8d22\u52a1\u62a5\u8868\u5165\u5e93(MySQL)\u300b\u4e5f\u8bb8\u80fd\u7ed9\u4f60\u4e00\u4e9b\u601d\u8def\u3002 \u539f\u7406\u8bf4\u660e\uff1ahttp://blog.csdn.net/c406495762/article/details/77801899 \u52a8\u6001\u793a\u610f\u56fe\uff1a one_hour_spider:\u4e00\u5c0f\u65f6\u5165\u95e8Python3\u7f51\u7edc\u722c\u866b\u3002 \u539f\u7406\u8bf4\u660e: \u77e5\u4e4e\uff1ahttps://zhuanlan.zhihu.com/p/29809609 CSDN\uff1ahttp://blog.csdn.net/c406495762/article/details/78123502 \u672c\u6b21\u5b9e\u6218\u5185\u5bb9\u6709\uff1a \u7f51\u7edc\u5c0f\u8bf4\u4e0b\u8f7d(\u9759\u6001\u7f51\u7ad9)-biqukan \u4f18\u7f8e\u58c1\u7eb8\u4e0b\u8f7d(\u52a8\u6001\u7f51\u7ad9)-unsplash \u89c6\u9891\u4e0b\u8f7d douyin.py:\u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d \u6296\u97f3App\u7684\u89c6\u9891\u4e0b\u8f7d\uff0c\u5c31\u662f\u666e\u901a\u7684App\u722c\u53d6\u3002 \u539f\u7406\u8bf4\u660e: \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/03/spider-5.html douyin_pro:\u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d\uff08\u5347\u7ea7\u7248\uff09 \u6296\u97f3App\u7684\u89c6\u9891\u4e0b\u8f7d\uff0c\u6dfb\u52a0\u89c6\u9891\u89e3\u6790\u7f51\u7ad9\uff0c\u652f\u6301\u65e0\u6c34\u5370\u89c6\u9891\u4e0b\u8f7d\uff0c\u4f7f\u7528\u7b2c\u4e09\u65b9\u5e73\u53f0\u89e3\u6790\u3002 \u539f\u7406\u8bf4\u660e: \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/03/spider-5.html douyin:\u6296\u97f3App\u89c6\u9891\u4e0b\u8f7d\uff08\u5347\u7ea7\u72482\uff09 \u6296\u97f3App\u7684\u89c6\u9891\u4e0b\u8f7d\uff0c\u6dfb\u52a0\u89c6\u9891\u89e3\u6790\u7f51\u7ad9\uff0c\u652f\u6301\u65e0\u6c34\u5370\u89c6\u9891\u4e0b\u8f7d\uff0c\u901a\u8fc7url\u89e3\u6790\uff0c\u65e0\u9700\u7b2c\u4e09\u65b9\u5e73\u53f0\u3002 \u539f\u7406\u8bf4\u660e: \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/03/spider-5.html \u52a8\u6001\u793a\u610f\u56fe\uff1a geetest.py:GEETEST\u9a8c\u8bc1\u7801\u8bc6\u522b \u539f\u7406\u8bf4\u660e: \u65e0 12306.py:\u7528Python\u62a2\u706b\u8f66\u7968\u7b80\u5355\u4ee3\u7801 \u53ef\u4ee5\u81ea\u5df1\u6162\u6162\u4e30\u5bcc\uff0c\u86ee\u7b80\u5355\uff0c\u6709\u722c\u866b\u57fa\u7840\u5f88\u597d\u64cd\u4f5c\uff0c\u6ca1\u6709\u539f\u7406\u8bf4\u660e\u3002 baiwan:\u767e\u4e07\u82f1\u96c4\u8f85\u52a9\u7b54\u9898 \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u4e2a\u4eba\u7f51\u7ad9\uff1ahttp://cuijiahua.com/blog/2018/01/spider_3.html \u529f\u80fd\u4ecb\u7ecd\uff1a \u670d\u52a1\u5668\u7aef\uff0c\u4f7f\u7528Python\uff08baiwan.py\uff09\u901a\u8fc7\u6293\u5305\u83b7\u5f97\u7684\u63a5\u53e3\u83b7\u53d6\u7b54\u9898\u6570\u636e\uff0c\u89e3\u6790\u4e4b\u540e\u901a\u8fc7\u767e\u5ea6\u77e5\u9053\u641c\u7d22\u63a5\u53e3\u5339\u914d\u7b54\u6848\uff0c\u5c06\u6700\u7ec8\u5339\u914d\u7684\u7ed3\u679c\u5199\u5165\u6587\u4ef6\uff08file.txt)\u3002 \u624b\u673a\u6293\u5305\u4e0d\u4f1a\u7684\u670b\u53cb\uff0c\u53ef\u4ee5\u770b\u4e0b\u6211\u7684\u65e9\u671f \u624b\u673aAPP\u6293\u5305\u6559\u7a0b \u3002 Node.js\uff08app.js\uff09\u6bcf\u96941s\u8bfb\u53d6\u4e00\u6b21file.txt\u6587\u4ef6\uff0c\u5e76\u5c06\u8bfb\u53d6\u7ed3\u679c\u901a\u8fc7socket.io\u63a8\u9001\u7ed9\u5ba2\u6237\u7aef\uff08index.html\uff09\u3002 \u4eb2\u6d4b\u7b54\u9898\u5ef6\u65f6\u57283s\u5de6\u53f3\u3002 \u58f0\u660e\uff1a\u6ca1\u505a\u8fc7\u540e\u7aef\u548c\u524d\u7aef\uff0c\u82b1\u4e86\u4e00\u5929\u65f6\u95f4\uff0c\u73b0\u5b66\u73b0\u5356\u5f04\u597d\u7684\uff0cjavascript\u4e5f\u662f\u73b0\u770b\u73b0\u7528\uff0c\u767e\u5ea6\u7684\u7a0b\u5e8f\uff0c\u8c03\u8bd5\u8c03\u8bd5\u800c\u5df2\u3002\u53ef\u80fd\u6709\u5f88\u591a\u7528\u6cd5\u6bd4\u8f83low\u7684\u5730\u65b9\uff0c\u7528\u6cd5\u4e0d\u5bf9\uff0c\u8bf7\u52ff\u89c1\u602a\uff0c\u6709\u5927\u725b\u611f\u5174\u8da3\uff0c\u53ef\u4ee5\u81ea\u884c\u5b8c\u5584\u3002 Netease:\u6839\u636e\u6b4c\u5355\u4e0b\u8f7d\u7f51\u6613\u4e91\u97f3\u4e50 \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u529f\u80fd\u4ecb\u7ecd\uff1a \u6839\u636emusic_list.txt\u6587\u4ef6\u91cc\u7684\u6b4c\u5355\u7684\u4fe1\u606f\u4e0b\u8f7d\u7f51\u6613\u4e91\u97f3\u4e50\uff0c\u5c06\u81ea\u5df1\u559c\u6b22\u7684\u97f3\u4e50\u8fdb\u884c\u6279\u91cf\u4e0b\u8f7d\u3002 bilibili\uff1aB\u7ad9\u89c6\u9891\u548c\u5f39\u5e55\u6279\u91cf\u4e0b\u8f7d \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u4f7f\u7528\u8bf4\u660e\uff1a python bilibili.py -d \u732b -k \u732b -p 10 \u4e09\u4e2a\u53c2\u6570\uff1a -d \u4fdd\u5b58\u89c6\u9891\u7684\u6587\u4ef6\u5939\u540d -k B\u7ad9\u641c\u7d22\u7684\u5173\u952e\u5b57 -p \u4e0b\u8f7d\u641c\u7d22\u7ed3\u679c\u524d\u591a\u5c11\u9875 jingdong\uff1a\u4eac\u4e1c\u5546\u54c1\u6652\u5355\u56fe\u4e0b\u8f7d \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u4f7f\u7528\u8bf4\u660e\uff1a python jd.py -k \u8292\u679c \u4e09\u4e2a\u53c2\u6570\uff1a -d \u4fdd\u5b58\u56fe\u7247\u7684\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3afd.py\u6587\u4ef6\u6240\u5728\u6587\u4ef6\u5939 -k \u641c\u7d22\u5173\u952e\u8bcd -n \u4e0b\u8f7d\u5546\u54c1\u7684\u6652\u5355\u56fe\u4e2a\u6570\uff0c\u5373n\u4e2a\u5546\u5e97\u7684\u6652\u5355\u56fe zhengfang_system_spider\uff1a\u5bf9\u6b63\u65b9\u6559\u52a1\u7ba1\u7406\u7cfb\u7edf\u4e2a\u4eba\u8bfe\u8868\uff0c\u4e2a\u4eba\u5b66\u751f\u6210\u7ee9\uff0c\u7ee9\u70b9\u7b49\u7b80\u5355\u722c\u53d6 \u6548\u679c\u56fe\uff1a \u539f\u7406\u8bf4\u660e\uff1a \u6682\u65e0 \u4f7f\u7528\u8bf4\u660e\uff1a cd zhengfang_system_spider pip install -r requirements.txt python spider.py","title":"\u722c\u866b\u5b9e\u6218"},{"location":"python_spider/#_6","text":"\u6b22\u8fce Pull requests\uff0c\u611f\u8c22\u8d21\u732e\u3002","title":"\u5176\u5b83"},{"location":"r/","text":"R Table of Contents Base Tutorial Packages Text Performance Data wrangling CLI ML Visualization Color Heatmap ggplot2 Statistics Cluster File formats Misc Base update all R packages install.packages(\"rvcheck\") update_all() Tutorial R for data science Cheatsheets by rstudio https://github.com/rstudio/cheatsheets Hands-On Data Science with R DataPyR http://www.sthda.com/english/ http://www.r-bloggers.com/search/ggplot2 Mastering Software Development in R Packages 10 R packages I wish I knew about earlier Great R packages for data import, wrangling visualization R package development - the Leek group way! Text stringr - A fresh approach to string manipulation in R http://stringr.tidyverse.org Text Processing in R pystr - Python String Methods in R. http://cran.r-project.org/web/packages/pystr R\u8bed\u8a00\u57fa\u7840\u77e5\u8bc6-\u5b57\u7b26\u4e32\u7684\u5904\u7406 Performance docs http://rstatistics.net/strategies-to-speed-up-r-code/ packages Data wrangling data structure tibble - A tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. tools faster-data-manipulation-7-packages data-wrangling-cheatsheet.pdf tidyr tidyr is a reframing of reshape2 designed to accompany the tidy data framework, and to work hand-in-hand with magrittr and dplyr to build a solid pipeline for data analysis. dplyr-and-plyr , When I use plyr/dplyr \uff0c[Using dplyr, and a comparison with plyr.]http://scicomp2014.edc.uri.edu/posts/2014-04-14-Smith.html dplyr: How to do data manipulation with R Data Wrangling Part 1: Basic to Advanced Ways to Select Columns Data Wrangling Part 2: Transforming your columns into the right shape Programming with dplyr normalising-data-within-groups , script broom - Convert Statistical Analysis Objects into Tidy Data Frames widyr - Widen, process, and re-tidy a dataset CLI argparse ML docs useR-machine-learning-tutorial packages caret - (Classification And Regression Training) R package that contains misc functions for training and plotting classification and regression models ROCR - visualizing classifier performance in R, with only 3 commands xgboost - Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, for Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink and DataFlow How to use XGBoost algorithm in R in easy steps Visualization Large number of images: http://rgm3.lab.nig.ac.jp/RGM/R_image_list Static and dynamic network visualization with R http://rcharts.io/gallery/ corrplot ordination plots igraph , Network Analysis and Visualization with R and igraph Dendrogram: hclust factoextra - Visualization of the outputs of a multivariate analysis http://www.sthda.com Sankey from Scratch using rCharts, d3.js, and igraph largevis - This is an implementation of the largeVis algorithm described in (https://arxiv.org/abs/1602.00370). It also incorporates code for a very fast algorithm for estimating k-nearest neighbors. Vennerable - Vennerable provides Venn diagrams in R. It displays Venn and Euler diagrams for up to 9 different sets and using a variety of geometries. It allows the display of area-weighted Venn diagrams and allows fine graphical control over the result. alluvial - Alluvial diagrams (you can also use ggalluvial UpSetR - An R implementation of the UpSet set visualization technique published by Lex, Gehlenborg, et al.. book Fundamentals of Data Visualization misc Mathematical Annotation in R circos like rCircos : R package for circular plots. [last update: 2013] OmicCircos : R package for circular plots for omics data. vignette [last update: 2016-05] circlize - circular layout in R http://jokergoo.github.io/circlize Shiny Building Web Data Products with R Shiny Color R color cheatsheet RColorBrewer Heatmap ComplexHeatmap pheatmap ggplot2 Extended packages. More: RStartHere , ggally ggplot2 extensions - gallery ggthemes - Some extra themes, geoms, and scales for ggplot2. hrbrthemes - Opinionated, typographic-centric ggplot2 themes and theme components ggsci - Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2 cowplot - provide a publication-ready theme for ggplot2. similar package: ggmatrix , ggpairs , ggscatmat ggfortify - Define fortify and autoplot functions to allow ggplot2 to handle some popular R packages. http://cos.name/2015/11/ggfortify-visualization-in-one-line-of-code/ ggrepel - Repel overlapping text labels away from each other. ggalt - Extra Coordinate Systems, Geoms and Statistical Transformations for 'ggplot2' https://cran.rstudio.com/web/packages/ggalt/ggalt.pdf ggforce - aime primarily at ad hoc data visualization in order to investigate the data at hand, and less at utilities for composing custom plots a la D3.js. sina plot : geom_sina : enhanced jitter strip char gganimate - Create easy animations with ggplot2 ggord - a take on ordination plots using ggplot2 gbiplot - A biplot based on ggplot2 ggnet - Simple network plots with ggplot2 in R. Similar packages: ggnetworkmap , ggraph - Grammar of Graph Graphics ggvis - Interactive grammar of graphics for R ggcorr : correlation matrixes with ggplot2 ggtree : an R package for visualization and annotation of phylogenetic trees with their covariates and other associated data. ggsurv - survival curves with ggplot2 ggExtra Add marginal histograms to ggplot2, and more ggplot2 enhancements http://daattali.com/shiny/ggExtra-ggMarginal-demo/ ggradar - radar charts with ggplot2 ggRandomForests : Visually Exploring Random Forests ggmcmc : Tools for Analyzing MCMC Simulations from Bayesian Inference ggalluvial - alluvial diagrams in ggplot2 ggpubr - 'ggplot2' Based Publication Ready Plots gggenes - Draw gene arrow maps in ggplot2 ggfittext - ggplot2 geoms to fit text into boxes ggbeeswarm - Provides methods for beeswarm plots in ggplot2 gghighlight - Highlight points and lines in ggplot2 waffle - Make waffle (square pie) charts in R introduction ggridges - Geoms to make ridgeline plots with ggplot2 [https://github.com/wilkox/ggwrap] - \u2018ggwrap\u2019 wraps a \u2018ggplot2\u2019 plot over multiple rows, to make plots with long x axes easier to read. Tutorial \u4f7f\u7528ggplot2\u753b\u56fe - YGC ggplot2-cheatsheet Getting started with ggplot2 Beautiful plotting in R: A ggplot2 cheatsheet Recreating a famous visualisation http://stackoverflow.com/questions/14379737/how-can-i-make-xlab-and-ylab-visible-when-using-theme-wsj-ggthemes How to format plots for publication using ggplot2 (with some help from Inkscape) Remove grid and background from plot (ggplot2) Colors ggplot2 - Easy way to mix multiple graphs on the same page 2D plot with histograms for each dimension R Recipe: Aligning Axes in ggplot2 making-faceted-heatmaps-with-ggplot2/ R\u8bed\u8a00\uff1aggplot2\u7cbe\u7ec6\u5316\u7ed8\u56fe\u2014\u2014\u4ee5\u5b9e\u7528\u5546\u4e1a\u5316\u56fe\u8868\u7ed8\u56fe\u4e3a\u4f8b Statistics book Summary and Analysis of Extension Program Evaluation in R misc Quick Multivariate data analysis (PCA, CA, MCA) and visualization The R Stats Package ( Adjust P-values for Multiple Comparisons , \u591a\u91cd\u68c0\u9a8c\u4e2d\u7684FDR\u9519\u8bef\u63a7\u5236\u65b9\u6cd5\u4e0ep-value\u7684\u6821\u6b63\u53caBonferroni\u6821\u6b63 ) How can I do post-hoc pairwise comparisons in R? Kruskal\u2013Wallis Test Multiple Comparison Procedures Cluster A gentle introduction to cluster analysis using R File formats rio - A Swiss-army knife for data I/O reads xlsx/xls library(xslx) data - read.xlsx( datafile.xlsx , 1) library(gdata) # Read first sheet data - read.xls( datafile.xls ) Misc How to summarize data by group in R? Error in eval(expr, envir, enclos) : object X' not found solution: ggplot(..., **environment = environment()**) how to apply a function to every row of a matrix (or a data frame) in R How to apply function over each matrix element How do I pass variables to a custom function in ddply? R-Transposing a data frame","title":"R"},{"location":"r/#r","text":"Table of Contents Base Tutorial Packages Text Performance Data wrangling CLI ML Visualization Color Heatmap ggplot2 Statistics Cluster File formats Misc","title":"R"},{"location":"r/#base","text":"update all R packages install.packages(\"rvcheck\") update_all()","title":"Base"},{"location":"r/#tutorial","text":"R for data science Cheatsheets by rstudio https://github.com/rstudio/cheatsheets Hands-On Data Science with R DataPyR http://www.sthda.com/english/ http://www.r-bloggers.com/search/ggplot2 Mastering Software Development in R","title":"Tutorial"},{"location":"r/#packages","text":"10 R packages I wish I knew about earlier Great R packages for data import, wrangling visualization R package development - the Leek group way!","title":"Packages"},{"location":"r/#text","text":"stringr - A fresh approach to string manipulation in R http://stringr.tidyverse.org Text Processing in R pystr - Python String Methods in R. http://cran.r-project.org/web/packages/pystr R\u8bed\u8a00\u57fa\u7840\u77e5\u8bc6-\u5b57\u7b26\u4e32\u7684\u5904\u7406","title":"Text"},{"location":"r/#performance","text":"docs http://rstatistics.net/strategies-to-speed-up-r-code/ packages","title":"Performance"},{"location":"r/#data-wrangling","text":"data structure tibble - A tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. tools faster-data-manipulation-7-packages data-wrangling-cheatsheet.pdf tidyr tidyr is a reframing of reshape2 designed to accompany the tidy data framework, and to work hand-in-hand with magrittr and dplyr to build a solid pipeline for data analysis. dplyr-and-plyr , When I use plyr/dplyr \uff0c[Using dplyr, and a comparison with plyr.]http://scicomp2014.edc.uri.edu/posts/2014-04-14-Smith.html dplyr: How to do data manipulation with R Data Wrangling Part 1: Basic to Advanced Ways to Select Columns Data Wrangling Part 2: Transforming your columns into the right shape Programming with dplyr normalising-data-within-groups , script broom - Convert Statistical Analysis Objects into Tidy Data Frames widyr - Widen, process, and re-tidy a dataset","title":"Data\u00a0wrangling"},{"location":"r/#cli","text":"argparse","title":"CLI"},{"location":"r/#ml","text":"docs useR-machine-learning-tutorial packages caret - (Classification And Regression Training) R package that contains misc functions for training and plotting classification and regression models ROCR - visualizing classifier performance in R, with only 3 commands xgboost - Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or GBM) Library, for Python, R, Java, Scala, C++ and more. Runs on single machine, Hadoop, Spark, Flink and DataFlow How to use XGBoost algorithm in R in easy steps","title":"ML"},{"location":"r/#visualization","text":"Large number of images: http://rgm3.lab.nig.ac.jp/RGM/R_image_list Static and dynamic network visualization with R http://rcharts.io/gallery/ corrplot ordination plots igraph , Network Analysis and Visualization with R and igraph Dendrogram: hclust factoextra - Visualization of the outputs of a multivariate analysis http://www.sthda.com Sankey from Scratch using rCharts, d3.js, and igraph largevis - This is an implementation of the largeVis algorithm described in (https://arxiv.org/abs/1602.00370). It also incorporates code for a very fast algorithm for estimating k-nearest neighbors. Vennerable - Vennerable provides Venn diagrams in R. It displays Venn and Euler diagrams for up to 9 different sets and using a variety of geometries. It allows the display of area-weighted Venn diagrams and allows fine graphical control over the result. alluvial - Alluvial diagrams (you can also use ggalluvial UpSetR - An R implementation of the UpSet set visualization technique published by Lex, Gehlenborg, et al.. book Fundamentals of Data Visualization misc Mathematical Annotation in R circos like rCircos : R package for circular plots. [last update: 2013] OmicCircos : R package for circular plots for omics data. vignette [last update: 2016-05] circlize - circular layout in R http://jokergoo.github.io/circlize Shiny Building Web Data Products with R Shiny","title":"Visualization"},{"location":"r/#color","text":"R color cheatsheet RColorBrewer","title":"Color"},{"location":"r/#heatmap","text":"ComplexHeatmap pheatmap","title":"Heatmap"},{"location":"r/#ggplot2","text":"Extended packages. More: RStartHere , ggally ggplot2 extensions - gallery ggthemes - Some extra themes, geoms, and scales for ggplot2. hrbrthemes - Opinionated, typographic-centric ggplot2 themes and theme components ggsci - Scientific Journal and Sci-Fi Themed Color Palettes for ggplot2 cowplot - provide a publication-ready theme for ggplot2. similar package: ggmatrix , ggpairs , ggscatmat ggfortify - Define fortify and autoplot functions to allow ggplot2 to handle some popular R packages. http://cos.name/2015/11/ggfortify-visualization-in-one-line-of-code/ ggrepel - Repel overlapping text labels away from each other. ggalt - Extra Coordinate Systems, Geoms and Statistical Transformations for 'ggplot2' https://cran.rstudio.com/web/packages/ggalt/ggalt.pdf ggforce - aime primarily at ad hoc data visualization in order to investigate the data at hand, and less at utilities for composing custom plots a la D3.js. sina plot : geom_sina : enhanced jitter strip char gganimate - Create easy animations with ggplot2 ggord - a take on ordination plots using ggplot2 gbiplot - A biplot based on ggplot2 ggnet - Simple network plots with ggplot2 in R. Similar packages: ggnetworkmap , ggraph - Grammar of Graph Graphics ggvis - Interactive grammar of graphics for R ggcorr : correlation matrixes with ggplot2 ggtree : an R package for visualization and annotation of phylogenetic trees with their covariates and other associated data. ggsurv - survival curves with ggplot2 ggExtra Add marginal histograms to ggplot2, and more ggplot2 enhancements http://daattali.com/shiny/ggExtra-ggMarginal-demo/ ggradar - radar charts with ggplot2 ggRandomForests : Visually Exploring Random Forests ggmcmc : Tools for Analyzing MCMC Simulations from Bayesian Inference ggalluvial - alluvial diagrams in ggplot2 ggpubr - 'ggplot2' Based Publication Ready Plots gggenes - Draw gene arrow maps in ggplot2 ggfittext - ggplot2 geoms to fit text into boxes ggbeeswarm - Provides methods for beeswarm plots in ggplot2 gghighlight - Highlight points and lines in ggplot2 waffle - Make waffle (square pie) charts in R introduction ggridges - Geoms to make ridgeline plots with ggplot2 [https://github.com/wilkox/ggwrap] - \u2018ggwrap\u2019 wraps a \u2018ggplot2\u2019 plot over multiple rows, to make plots with long x axes easier to read. Tutorial \u4f7f\u7528ggplot2\u753b\u56fe - YGC ggplot2-cheatsheet Getting started with ggplot2 Beautiful plotting in R: A ggplot2 cheatsheet Recreating a famous visualisation http://stackoverflow.com/questions/14379737/how-can-i-make-xlab-and-ylab-visible-when-using-theme-wsj-ggthemes How to format plots for publication using ggplot2 (with some help from Inkscape) Remove grid and background from plot (ggplot2) Colors ggplot2 - Easy way to mix multiple graphs on the same page 2D plot with histograms for each dimension R Recipe: Aligning Axes in ggplot2 making-faceted-heatmaps-with-ggplot2/ R\u8bed\u8a00\uff1aggplot2\u7cbe\u7ec6\u5316\u7ed8\u56fe\u2014\u2014\u4ee5\u5b9e\u7528\u5546\u4e1a\u5316\u56fe\u8868\u7ed8\u56fe\u4e3a\u4f8b","title":"ggplot2"},{"location":"r/#statistics","text":"book Summary and Analysis of Extension Program Evaluation in R misc Quick Multivariate data analysis (PCA, CA, MCA) and visualization The R Stats Package ( Adjust P-values for Multiple Comparisons , \u591a\u91cd\u68c0\u9a8c\u4e2d\u7684FDR\u9519\u8bef\u63a7\u5236\u65b9\u6cd5\u4e0ep-value\u7684\u6821\u6b63\u53caBonferroni\u6821\u6b63 ) How can I do post-hoc pairwise comparisons in R? Kruskal\u2013Wallis Test Multiple Comparison Procedures","title":"Statistics"},{"location":"r/#cluster","text":"A gentle introduction to cluster analysis using R","title":"Cluster"},{"location":"r/#file-formats","text":"rio - A Swiss-army knife for data I/O reads xlsx/xls library(xslx) data - read.xlsx( datafile.xlsx , 1) library(gdata) # Read first sheet data - read.xls( datafile.xls )","title":"File formats"},{"location":"r/#misc","text":"How to summarize data by group in R? Error in eval(expr, envir, enclos) : object X' not found solution: ggplot(..., **environment = environment()**) how to apply a function to every row of a matrix (or a data frame) in R How to apply function over each matrix element How do I pass variables to a custom function in ddply? R-Transposing a data frame","title":"Misc"},{"location":"r4ds/","text":"r4ds \u5b98\u65b9\u94fe\u63a5 r\u7edf\u8ba1 statistic for r","title":"r4ds"},{"location":"r4ds/#r4ds","text":"","title":"r4ds\u5b98\u65b9\u94fe\u63a5"},{"location":"r4ds/#rstatistic-for-r","text":"","title":"r\u7edf\u8ba1statistic for r"},{"location":"sci/","text":"Table of Contents Scientific Research Scientific Research How To Make A Scientific Research Poster How to stand out in academic scientific research (particularly post-docs) My notes for a panel presentation on reviewing papers and grants \uff08\u53ef\u4ee5\u4e86\u89e3Reviewer\u7684\u60f3\u6cd5\uff09 Writing reviews of academic papers 11 steps to structuring a science paper editors will take seriously","title":"Sci"},{"location":"sci/#scientific-research","text":"How To Make A Scientific Research Poster How to stand out in academic scientific research (particularly post-docs) My notes for a panel presentation on reviewing papers and grants \uff08\u53ef\u4ee5\u4e86\u89e3Reviewer\u7684\u60f3\u6cd5\uff09 Writing reviews of academic papers 11 steps to structuring a science paper editors will take seriously","title":"Scientific Research"},{"location":"statistic_python/","text":"statistical-learning-method \u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u7b14\u8bb0-\u57fa\u4e8ePython\u7b97\u6cd5\u5b9e\u73b0 \u7b2c\u4e00\u7ae0 \u6700\u5c0f\u4e8c\u4e58\u6cd5 \u7b2c\u4e8c\u7ae0 \u611f\u77e5\u673a \u7b2c\u4e09\u7ae0 k\u8fd1\u90bb\u6cd5 \u7b2c\u56db\u7ae0 \u6734\u7d20\u8d1d\u53f6\u65af \u7b2c\u4e94\u7ae0 \u51b3\u7b56\u6811 \u7b2c\u516d\u7ae0 \u903b\u8f91\u65af\u8c1b\u56de\u5f52 \u7b2c\u4e03\u7ae0 \u652f\u6301\u5411\u91cf\u673a \u7b2c\u516b\u7ae0 AdaBoost \u7b2c\u4e5d\u7ae0 EM\u7b97\u6cd5","title":"Statistic_python"},{"location":"statistic_python/#statistical-learning-method","text":"\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u7b14\u8bb0-\u57fa\u4e8ePython\u7b97\u6cd5\u5b9e\u73b0 \u7b2c\u4e00\u7ae0 \u6700\u5c0f\u4e8c\u4e58\u6cd5 \u7b2c\u4e8c\u7ae0 \u611f\u77e5\u673a \u7b2c\u4e09\u7ae0 k\u8fd1\u90bb\u6cd5 \u7b2c\u56db\u7ae0 \u6734\u7d20\u8d1d\u53f6\u65af \u7b2c\u4e94\u7ae0 \u51b3\u7b56\u6811 \u7b2c\u516d\u7ae0 \u903b\u8f91\u65af\u8c1b\u56de\u5f52 \u7b2c\u4e03\u7ae0 \u652f\u6301\u5411\u91cf\u673a \u7b2c\u516b\u7ae0 AdaBoost \u7b2c\u4e5d\u7ae0 EM\u7b97\u6cd5","title":"statistical-learning-method"},{"location":"think/","text":"think about it 1.how to do thing? it's difficult to do job like this. 2.scond things to see ok,that's intresting.","title":"Think"},{"location":"think/#think-about-it","text":"","title":"think about it"},{"location":"think/#1how-to-do-thing","text":"it's difficult to do job like this.","title":"1.how to do thing?"},{"location":"think/#2scond-things-to-see","text":"ok,that's intresting.","title":"2.scond things to see"},{"location":"utilities/","text":"Utilities Table of Contents data MISC Fun ascii PC others data rsync\u7684\u6587\u4ef6\u540c\u6b65\uff0c\u590d\u5236\uff0c\u955c\u50cf\uff0c\u589e\u91cf\u5907\u4efd v1.2 gut-sync - Realtime bidirectional folder synchronization magic-wormhole - get things from one computer to another, safely syncthing - Open Source Continuous File Synchronization http://forum.syncthing.net/ restic - restic backup program pget - Parallel file download client MISC mkcast - A tool for creating GIF screencasts of a terminal, with key presses overlaid. maybe - :open_file_folder: :rabbit2: :tophat: See what a program does before deciding whether you really want it to happen. Fun no-more-secrets - A tool set to recreate the famous \"decrypting text\" effect as seen in the 1992 movie Sneakers. ascii Graph-Easy - Convert or render graphs (as ASCII, HTML, SVG or via Graphviz) http://p3rl.org/Graph::Easy ASCII ART websequencediagrams PC battery - cross-platform, normalized battery information library others word_cloud - A little word cloud generator in Python","title":"Uti"},{"location":"utilities/#utilities","text":"Table of Contents data MISC Fun ascii PC others","title":"Utilities"},{"location":"utilities/#data","text":"rsync\u7684\u6587\u4ef6\u540c\u6b65\uff0c\u590d\u5236\uff0c\u955c\u50cf\uff0c\u589e\u91cf\u5907\u4efd v1.2 gut-sync - Realtime bidirectional folder synchronization magic-wormhole - get things from one computer to another, safely syncthing - Open Source Continuous File Synchronization http://forum.syncthing.net/ restic - restic backup program pget - Parallel file download client","title":"data"},{"location":"utilities/#misc","text":"mkcast - A tool for creating GIF screencasts of a terminal, with key presses overlaid. maybe - :open_file_folder: :rabbit2: :tophat: See what a program does before deciding whether you really want it to happen.","title":"MISC"},{"location":"utilities/#fun","text":"no-more-secrets - A tool set to recreate the famous \"decrypting text\" effect as seen in the 1992 movie Sneakers.","title":"Fun"},{"location":"utilities/#ascii","text":"Graph-Easy - Convert or render graphs (as ASCII, HTML, SVG or via Graphviz) http://p3rl.org/Graph::Easy ASCII ART websequencediagrams","title":"ascii"},{"location":"utilities/#pc","text":"battery - cross-platform, normalized battery information library","title":"PC"},{"location":"utilities/#others","text":"word_cloud - A little word cloud generator in Python","title":"others"},{"location":"web/","text":"Web Static web server hugo - A Fast and Flexible Static Site Generator built with love by spf13 in GoLang http://gohugo.io MkDocs - Project documentation with Markdown. gohttpserver - The best HTTP Static File Server, write with golang+vue Doc Let 360 broswer use webkit: meta name=\"renderer\" content=\"webkit\" meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge, chrome=1\"","title":"Web"},{"location":"web/#web","text":"","title":"Web"},{"location":"web/#static-web-server","text":"hugo - A Fast and Flexible Static Site Generator built with love by spf13 in GoLang http://gohugo.io MkDocs - Project documentation with Markdown. gohttpserver - The best HTTP Static File Server, write with golang+vue","title":"Static web server"},{"location":"web/#doc","text":"Let 360 broswer use webkit: meta name=\"renderer\" content=\"webkit\" meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge, chrome=1\"","title":"Doc"},{"location":"LeastSquaresMethod/","text":"Least Squares Method","title":"Least Squares Method"},{"location":"LeastSquaresMethod/#least-squares-method","text":"","title":"Least Squares Method"},{"location":"Perceptron/","text":"Perceptron \u611f\u77e5\u673a","title":"Perceptron \u611f\u77e5\u673a"},{"location":"Perceptron/#perceptron","text":"","title":"Perceptron \u611f\u77e5\u673a"},{"location":"dm_r/","text":"\u6570\u636e\u79d1\u5b66 \u672c\u7cfb\u5217\u7531\u300a \u6570\u636e\u79d1\u5b66\u4e2d\u7684R\u8bed\u8a00 \u300b\u4e00\u4e66\u4f5c\u8005 \u8096\u51ef\u8001\u5e08 \u6240\u8457\uff0c\u73b0\u5df2\u5f00\u6e90\u3002 \u76ee\u524d\u5171\u5341\u7bc7\uff1a \u6570\u636e\u6316\u6398\u5bfc\u8bba\u548c\u4fe1\u8d37\u6a21\u578b \u56de\u5f52\u6a21\u578b\u548c\u623f\u4ef7\u9884\u6d4b \u611f\u77e5\u673a\u548c\u903b\u8f91\u56de\u5f52 \u51b3\u7b56\u6811\u548c\u96c6\u6210\u5b66\u4e60 \u7279\u5f81\u5de5\u7a0b \u53c2\u6570\u8c03\u4f18 \u65e0\u76d1\u7763\u5b66\u4e60 \u6587\u672c\u6316\u6398 \u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60 \u6b22\u8fcefork and make contributions\uff0c \u4ed3\u5e93\u5730\u5740 \u3002","title":"Home"},{"location":"dm_r/#_1","text":"\u672c\u7cfb\u5217\u7531\u300a \u6570\u636e\u79d1\u5b66\u4e2d\u7684R\u8bed\u8a00 \u300b\u4e00\u4e66\u4f5c\u8005 \u8096\u51ef\u8001\u5e08 \u6240\u8457\uff0c\u73b0\u5df2\u5f00\u6e90\u3002 \u76ee\u524d\u5171\u5341\u7bc7\uff1a \u6570\u636e\u6316\u6398\u5bfc\u8bba\u548c\u4fe1\u8d37\u6a21\u578b \u56de\u5f52\u6a21\u578b\u548c\u623f\u4ef7\u9884\u6d4b \u611f\u77e5\u673a\u548c\u903b\u8f91\u56de\u5f52 \u51b3\u7b56\u6811\u548c\u96c6\u6210\u5b66\u4e60 \u7279\u5f81\u5de5\u7a0b \u53c2\u6570\u8c03\u4f18 \u65e0\u76d1\u7763\u5b66\u4e60 \u6587\u672c\u6316\u6398 \u795e\u7ecf\u7f51\u7edc \u6df1\u5ea6\u5b66\u4e60 \u6b22\u8fcefork and make contributions\uff0c \u4ed3\u5e93\u5730\u5740 \u3002","title":"\u6570\u636e\u79d1\u5b66"},{"location":"w0-introduction/0w/","text":"\u6570\u636e\u6316\u6398\u6982\u89c8 \u4e00\u3001\u5f15\u5b50\uff1a\u5173\u4e8e\u6570\u636e\u5229\u7528\u7684\u6545\u4e8b \u6797\u5f6a\u53d1\u73b0\u654c\u519b\u6307\u6325\u90e8 \u5728\u8fd9\u4e2a\u6545\u4e8b\u4e2d\uff0c\u6307\u6325\u5b98\u505a\u4e86\u8fd9\u6837\u51e0\u4ef6\u6709\u5173\u8054\u7684\u4e8b\u60c5\u3002\u4e00\u5e76\u79f0\u4e4b\u4e3a\u201d\u6570\u636e\u95ed\u73af\u201c\u3002 - \u4e0d\u95f4\u65ad\u7684\u6536\u96c6\u6218\u573a\u6570\u636e\uff1b - \u57fa\u4e8e\u67d0\u4e2a\u6307\u6807\u62bd\u8c61\u51fa\u6218\u573a\u5b9e\u9645\uff1b - \u5bf9\u6307\u6807\u8fdb\u884c\u5206\u6790\u5efa\u6a21\uff0c\u53d1\u73b0\u4e00\u4e2a\u673a\u4f1a\uff1b - \u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u8fdb\u884c\u6218\u573a\u51b3\u65ad\uff0c\u83b7\u53d6\u6700\u5927\u5229\u76ca\u3002 \u5c06\u6307\u6325\u5b98\u6362\u6210CEO\uff0c\u5c31\u662f\u5546\u4e1a\u6570\u636e\u95ed\u73af\uff0c\u57fa\u672c\u4e0a\u6240\u6709\u7684\u5546\u4e1a\u6570\u636e\u5e94\u7528\uff0c\u79bb\u4e0d\u5f00\u8fd9\u4e2a\u5957\u8def\u3002 \u4f60\u89c9\u5f97\u6307\u6325\u5b98\u5efa\u7acb\u4e86\u4ec0\u4e48\u6a21\u578b\uff1f\u5148\u60f3\u60f3\uff0c\u522b\u6025\u7740\u770b \u7b54\u6848 \u4ec0\u4e48\u662f\u6570\u636e\u95ed\u73af \u5ea6\u91cf\u5546\u4e1a\u884c\u52a8\uff1a\u4f8b\u5982\u7528\u4e00\u4e2a\u6307\u6807\u6d4b\u91cf\u7528\u6237\u54cd\u5e94\u7387 \u8bc6\u522b\u5546\u4e1a\u673a\u4f1a\uff1a\u89c4\u5212\u5149\u68cd\u8282\u65b0\u4ea7\u54c1\u6216\u65b0\u6d3b\u52a8\uff0c\u7406\u89e3\u5ba2\u6237\u6570\u636e\u7684\u6ce2\u52a8\uff0c\u8bc4\u4ef7\u8425\u9500\u6d3b\u52a8\u7684\u7ed3\u679c \u5c06\u6570\u636e\u8f6c\u4e3a\u77e5\u8bc6\uff1a\u901a\u8fc7\u6570\u636e\u6316\u6398\u5b9e\u65bd \u57fa\u4e8e\u77e5\u8bc6\u884c\u52a8\uff1a\u901a\u5e38\u7ed3\u5408\u73b0\u6709\u7684\u4e1a\u52a1\u6d41\u7a0b\uff0c\u5ba2\u6237\u51fa\u73b0\u65f6\u63a8\u9001\u4fe1\u606f\uff0c\u4e0d\u540c\u5ba2\u6237\u7ed9\u4e88\u4e0d\u540c\u7684\u8d44\u6e90 \u6570\u636e\u5229\u7528\u7684\u56db\u5c42\u5883\u754c \u6570\u636e\uff1a \u6570\u636e\u5e95\u5c42\uff0c\u539f\u59cb\u6570\u636e\u7684\u6c6a\u6d0b\u5927\u6d77\u3002\u5f62\u6001\uff1a\u6570\u636e\u5e93\u3002\u529f\u80fd\uff1a\u76f4\u63a5\u53d6\u6570 \u4fe1\u606f\uff1a \u57fa\u4e8e\u6570\u636e\u63d0\u70bc\u5f97\u5230\u7684\u6307\u6807\uff0c\u65b0\u5ba2\u6709\u591a\u5c11\uff1f\u8001\u5ba2\u6709\u591a\u5c11\uff1f\u8001\u5ba2\u90fd\u6709\u4ec0\u4e48\u7279\u5f81\uff0c\u65b0\u5ba2\u90fd\u6709\u4ec0\u4e48\u7279\u5f81\uff1f\u5f62\u6001\uff1a\u6c47\u603b\u62a5\u8868\u3002\u529f\u80fd\uff1a\u6307\u6807\u63d0\u4f9b\uff0c\u56de\u7b54\u8fc7\u53bb\u5df2\u7ecf\u53d1\u751f\u4e86\u4ec0\u4e48\u7684\u95ee\u9898\uff0c\u4e1a\u52a1\u4eba\u5458\u8fd0\u7528\u5f97\u5f53\u4e5f\u53ef\u4ee5\u89e3\u51b3\u5f88\u591a\u95ee\u9898\u3002 \u77e5\u8bc6\uff1a \u57fa\u4e8e\u4fe1\u606f\u5efa\u7acb\u5404\u6307\u6807\u4e4b\u95f4\u7684\u5173\u7cfb\u6a21\u578b\u3002\u4ec0\u4e48\u60c5\u51b5\u4e0b\u65b0\u5ba2\u4f1a\u8f6c\u5316\u4e3a\u8001\u5ba2\uff1f\u6a21\u578b\u7ed3\u679c\u3002\u5f62\u6001\uff1a\u6a21\u578b\uff0c\u529f\u80fd\uff1a\u56de\u7b54\u4e3a\u4ec0\u4e48\u7684\u95ee\u9898\uff0c\u89e3\u91ca\u5173\u7cfb\u548c\u56e0\u679c\uff0c\u9884\u6d4b\u672a\u6765 \u667a\u6167\uff1a \u5c06\u77e5\u8bc6\u878d\u5165\u51b3\u7b56\u6d41\u7a0b\uff0c\u5c06\u6a21\u578b\u5d4c\u5165\u4ea7\u54c1\u3002\u6211\u4eec\u8981\u600e\u4e48\u505a\uff0c\u624d\u4f1a\u8ba9\u65b0\u5ba2\u8f6c\u5316\u4e3a\u8001\u5ba2\u3002\u5f62\u6001\uff1a\u6570\u636e\u4ea7\u54c1\u3002\u529f\u80fd\uff1a\u63a7\u5236\u672a\u6765 \u6570\u636e\u4ea7\u54c1 \u6570\u636e\u4ea7\u54c1\u5c31\u662f\u7ed9\u51b3\u7b56\u8005\u63d0\u4f9b\u884c\u52a8\u4fe1\u606f\u7684\u8f7d\u4f53\uff0c\u4f8b\u5982 Amazon\u7684\u5546\u54c1\u63a8\u8350 \u5929\u6c14\u9884\u62a5 Stock Market Predictions Production Process Improvements Health Diagnosis Flu Trend Predictions \u6709\u4e9b\u770b\u8d77\u6765\u4e5f\u80fd\u63d0\u4f9b\u51b3\u7b56\u8005\u884c\u52a8\u4fe1\u606f\uff0c\u5982\u9ec4\u5386\uff0c\u661f\u76f8\uff0c\u4f46\u5b83\u4eec\u4e0d\u662f\u57fa\u4e8e\u6570\u636e\u7684\u6d1e\u5bdf\u3002 \u4e8c\u3001\u4ec0\u4e48\u662f\u6570\u636e\u6316\u6398 \u6570\u636e\u6316\u6398\u4e5f\u79f0\u4e3a\u77e5\u8bc6\u53d1\u73b0\u3002\u662f\u4e00\u4e2a\u53bb\u7c97\u5b58\u7cbe\u3001\u53bb\u4f2a\u5b58\u771f\u7684\u8fc7\u7a0b\u3002\u662f\u4ece\u5927\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u3001\u5f52\u7eb3\u6709\u7528\u77e5\u8bc6\u7684\u8fc7\u7a0b\u548c\u65b9\u6cd5\u3002\u5c06\u5176\u7528\u4e8e\u51b3\u7b56\uff0c\u53ef\u4ee5\u63d0\u9ad8\u4eba\u7c7b\u7684\u798f\u5229\u3002 \u5f00\u666e\u52d2\u4e09\u5927\u5b9a\u5f8b \u5f00\u666e\u52d2\u7684\u8001\u5e08\u7b2c\u8c37\u6536\u96c6\u4e86\u5927\u91cf\u5929\u6587\u89c2\u6d4b\u6570\u636e\uff0c\u4f46\u5374\u662f\u5f00\u666e\u52d2\u901a\u8fc7\u7814\u7a76\u6570\u636e\u627e\u5230\u80cc\u540e\u7684\u89c4\u5f8b \u51e0\u4e2a\u76f8\u5173\u6982\u5ff5 \u673a\u5668\u5b66\u4e60 \u7edf\u8ba1\u7406\u8bba \u6570\u636e\u79d1\u5b66 \u6a21\u5f0f\u8bc6\u522b \u9700\u8981\u7b97\u6cd5\u5f00\u53d1 \u4e0d\u9700\u8981\u7b97\u6cd5\u5f00\u53d1 \u9700\u8981\u6570\u636e\u5f00\u53d1 \u6570\u636e\u79d1\u5b66\u5bb6 \u6570\u636e\u6316\u6398\u5de5\u7a0b\u5e08 \u4e0d\u9700\u8981\u6570\u636e\u5f00\u53d1 \u7b97\u6cd5\u5de5\u7a0b\u5e08 kaggle\u73a9\u5bb6 \u4e09\u3001\u6570\u636e\u6316\u6398\u548c\u6211\u4eec\u7684\u5173\u7cfb \u4e3a\u4f55\u9700\u8981\u6570\u636e\u6316\u6398\uff1a \u5982\u679c\u6ca1\u6709\u6570\u636e\uff0c\u53ef\u4ee5\u7528\u4ec0\u4e48\u51b3\u7b56\uff1f\uff08\u76f4\u89c9\uff0c\u7ecf\u9a8c\u5f52\u7eb3\uff0c\u903b\u8f91\u63a8\u7406\uff0c\u7b97\u547d\uff09 \u9700\u8981\u6570\u636e\uff0c\u56e0\u4e3a\u6570\u636e\u5c31\u662f\u73b0\u5b9e\u4e16\u754c\u7684\u5386\u53f2\u75d5\u8ff9\uff0c\u9700\u8981\u901a\u8fc7\u5404\u79cd\u75d5\u8ff9\u6765\u63a8\u65ad\u672a\u6765\uff0c\u6570\u636e\u5c31\u662f\u5386\u53f2\uff0c\u6316\u6398\u5c31\u662f\u5f52\u7eb3\u3002 \u6570\u636e\u592a\u591a\uff0c\u4eba\u8111\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u3002\u8bb0\u5f55\u7684\u6570\u636e\u8d8a\u6765\u8d8a\u591a\uff0c\u5f62\u5f0f\u548c\u6765\u6e90\u90fd\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u81ea\u7136\u4ea7\u751f\u7684\u6570\u636e\uff0c\u4eba\u7c7b\u793e\u4f1a\u4ea7\u751f\u7684\u6570\u636e\uff08\u793e\u4ea4\u7f51\u7edc\uff0c\u6587\u672c\uff0c\u56fe\u50cf\uff0c\u8bed\u97f3\uff0c\u89c6\u9891......\uff09 \u6240\u4ee5\u9700\u8981\u6316\u6398\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u5e2e\u52a9\u3002 \u6570\u636e\u6316\u6398\u4e3a\u4ec0\u4e48\u706b\uff1f \u5f53\u524d\u7684\u5404\u9879\u524d\u63d0\u6761\u4ef6\u5df2\u7ecf\u5177\u5907\u3002 \u786c\u4ef6\u4ef7\u683c\u7684\u4e0b\u964d\uff0c\u4f7f\u6570\u636e\u7684\u5b58\u50a8\u548c\u8fd0\u7b97\u6210\u672c\u66f4\u4f4e\u3002 \u4e2a\u4eba\u548c\u521b\u4e1a\u516c\u53f8\u5f97\u4ee5\u8fdb\u5165\u6570\u636e\u9886\u57df\u3002 \u5f00\u6e90\u8f6f\u4ef6\u5de5\u5177\u548c\u516c\u5f00\u8bfe\u5206\u4eab\u4f7f\u8de8\u754c\u66f4\u4e3a\u5bb9\u6613\u3002 \u4e0d\u540c\u5b66\u79d1\u7684\u58c1\u5792\u88ab\u6253\u7834\uff0c\u53ef\u4ee5\u8f83\u4e3a\u5bb9\u6613\u7684\u83b7\u5f97\u5e76\u5b66\u4e60\u5176\u5b83\u5b66\u79d1\u7684\u77e5\u8bc6\u548c\u5de5\u5177\uff0c\u6210\u4e3a\u4e13\u4e1a\u4f59\u4eba\u58eb\u3002 \u6570\u636e\u6316\u6398\u548c\u8c01\u6253\u4ea4\u9053 \u4ea7\u54c1\uff1a\u4fa7\u91cd\u4e8e\u5e95\u5c42\u6570\u636e\u6846\u67b6\u642d\u5efa\uff0c\u6570\u636e\u62a5\u8868\u5f00\u53d1\uff0c\u6570\u636e\u4ea7\u54c1\u5f00\u53d1\uff0c\u4f8b\u5982\u6dd8\u5b9d\u7684\u6570\u636e\u9b54\u65b9\u7684\u5f00\u53d1\u5de5\u4f5c\uff0c\u8fd9\u7c7b\u5de5\u4f5c\u9700\u8981\u5f88\u5f3a\u7684\u8f6f\u4ef6\u5f00\u53d1\u80cc\u666f\u3002 \u6a21\u578b\uff1a\u4fa7\u91cd\u4e8e\u5bf9\u6570\u636e\u7684\u7814\u7a76\uff0c\u7528\u7edf\u8ba1\u7406\u8bba\u6216\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u6790\u5efa\u6a21\uff0c\u4f8b\u5982\u5e7f\u544a\u7684\u70b9\u51fb\u7387\u5206\u6790\u5efa\u6a21\uff0c\u8fd9\u7c7b\u5de5\u4f5c\u9700\u8981\u4e30\u5bcc\u7684\u7edf\u8ba1\u7406\u8bba\u548c\u6a21\u578b\u7b97\u6cd5\u77e5\u8bc6\u3002 \u7f8e\u5b66\uff1a\u4fa7\u91cd\u4e8e\u5bf9\u6570\u636e\u7684\u521b\u4f5c\uff0c\u7528WEB\u6280\u672f\u8fdb\u884c\u6570\u636e\u53ef\u89c6\u5316\u6216\u8005\u5236\u4f5c\u4fe1\u606f\u56fe\uff0c\u4f8b\u5982\u536b\u62a5\u7684\u6570\u636e\u7f51\u7ad9\uff0c\u9700\u8981\u5f88\u5f3a\u7684\u53ef\u89c6\u5316\u80fd\u529b\u548c\u524d\u7aef\u6280\u672f\u3002 \u4ef7\u503c\uff1a\u4fa7\u91cd\u4e8e\u6570\u636e\u4e2d\u5305\u542b\u7684\u5546\u4e1a\u4ef7\u503c\u7814\u7a76\uff0c\u5f3a\u8c03\u5bf9\u4e13\u4e1a\u9886\u57df\u7684\u4e1a\u52a1\u7406\u89e3\u548c\u4ea4\u6d41\u6c9f\u901a\uff0c\u4f8b\u5982\u54a8\u8be2\u516c\u53f8\u53d1\u5e03\u7684\u5546\u4e1a\u5206\u6790\u62a5\u544a\uff0c\u9700\u8981\u5e7f\u6cdb\u7684\u4e1a\u52a1\u77e5\u8bc6\u548c\u5546\u4e1a\u654f\u611f\u5ea6\u3002 \u5e94\u7528\u9886\u57df\u6709\u54ea\u4e9b\uff1a \u5546\u4e1a\u96f6\u552e \u533b\u7597 \u91d1\u878d \u592a\u7a7a\u63a2\u7d22 \u6587\u5b57\u8bed\u97f3\u8bc6\u522b \u4e0b\u68cb\u3002\u3002\u3002 \u56db\u3001\u4ec0\u4e48\u662f\u6570\u636e\u6316\u6398\u6a21\u578b\uff1f \u4e00\u4e2a\u5173\u4e8e\u6a21\u578b\u7684\u6d45\u663e\u4f8b\u5b50\uff1a\u5982\u4f55\u5224\u65ad\u4e00\u4e2a\u672a\u5207\u5f00\u7684\u897f\u74dc\u751c\u4e0d\u751c\uff1f \u53ef\u80fd\u7684\u65b9\u6848\uff1a \u51c6\u5907N\u4e2a\u897f\u74dc \u8bbe\u8ba1M\u4e2a\u53d8\u91cf\u6216\u6307\u6807\uff08\u7279\u5f81\u5de5\u7a0b\uff09\uff0c\u91cd\u91cf\u3001\u82b1\u7eb9\u3001\u830e\u53f6\u7684\u65b0\u9c9c\u7a0b\u5ea6\u3001\u5356\u5bb6\u7684\u4f4d\u7f6e...\uff0c\u5207\u5f00\u524d\u8bb0\u5f55\u8fd9\u4e9b\u6307\u6807\uff0c\u8bb0\u4e3aX\u3002 \u5207\u5f00\u540e\u8ba9n\u4e2a\u4eba\u54c1\u5c1d\u6253\u5206(0\u8868\u793a\u4e0d\u751c\uff0c1\u8868\u793a\u751c)\uff0c\u8bb0\u4e3aY\u3002 \u4f7f\u7528\u5176\u4e2d\u4e00\u90e8\u5206\u6570\u636e\uff0c\u7ed3\u5408\u5206\u7c7b\u7b97\u6cd5\u5bf9X\u548cY\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002 \u7528\u5269\u4e0b\u53e6\u4e00\u90e8\u5206\u6570\u636e\uff0c\u68c0\u67e5\u6a21\u578b\u7684\u6548\u679c\u3002 \u628a\u6a21\u578b\u7684\u903b\u8f91\u5199\u6210\u4e00\u4e2aAPP\u653e\u5230\u5e94\u7528\u5e02\u573a\u4e0a\uff0c\u6301\u7eed\u6536\u83b7\u6570\u636e\uff0c\u6539\u8fdb\u6a21\u578b\u3002 \u4e00\u4e2a\u590d\u6742\u7684\u4f8b\u5b50\uff0c\u5982\u4f55\u5224\u65ad\u4e00\u4e2a\u8001\u4eba\u672a\u6765\u662f\u5426\u4f1a\u5f97\u75f4\u5446\u75c7\uff08\u601d\u8003\u601d\u8003\uff09 \u5546\u4e1a\u4e2d\u4f7f\u7528\u6a21\u578b\u7684\u4f8b\u5b50\uff1a \u94f6\u884c\u7684\u4fe1\u7528\u5361\u53d1\u653e\uff1a \u94f6\u884c\u5728\u4fe1\u7528\u5361\u53d1\u653e\u7684\u65f6\u5019\u4f1a\u8fdb\u884c\u5ba1\u6838\u3002\u5ba1\u6838\u67d0\u4e2a\u4eba\u7684\u8d44\u683c\u662f\u5426\u7b26\u5408\u6761\u4ef6\u4ee5\u6388\u4fe1\u3002\u5728\u4f20\u7edf\u7684\u5ba1\u6838\u5de5\u4f5c\uff0c\u8fd9\u79cd\u4e8b\u662f\u4eba\u5de5\u6765\u505a\u7684\uff0c\u7533\u8bf7\u4eba\u586b\u4e00\u5f20\u8868\uff0c\u5199\u4e0a\u4e2a\u4eba\u7684\u5e74\u9f84\u3001\u804c\u4e1a\u3001\u6536\u5165\u7b49\u4fe1\u606f\uff08X\u53d8\u91cf\uff09\u3002\u4ea4\u7ed9\u6709\u7ecf\u9a8c\u7684\u94f6\u884c\u98ce\u63a7\u5e08\uff0c\u4ed6\u4eec\u6765\u8fdb\u884c\u8bc4\u4ef7\uff0c\u662f\u53d1\u4fe1\u7528\u5361\uff0c\u8fd8\u662f\u4e0d\u53d1\u4fe1\u7528\u5361\u3002 \u4f46\u5728\u4e92\u8054\u7f51\u65f6\u4ee3\uff0c\u8fd9\u79cd\u4eba\u5de5\u5ba1\u6838\u5c31\u592a\u6162\u4e86\u3002\u4e92\u8054\u7f51\u91d1\u878d\u7684\u5d1b\u8d77\u5c31\u662f\u6700\u660e\u663e\u7684\u8d8b\u52bf\u3002\u5b83\u5c06\u5168\u7f51\u4e2d\u5173\u4e8e\u4e2a\u4eba\u7684\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u6536\u96c6\u6574\u5408\uff0c\u5176\u4e2d\u6709\u5fc5\u7136\u6709\u4e00\u90e8\u5206\u4eba\u5df2\u7ecf\u5728\u91d1\u878d\u673a\u6784\u6709\u8fc7\u501f\u8d37\u884c\u4e3a\uff0c\u8003\u5bdf\u8fd9\u79cd\u884c\u4e3a\u662f\u5426\u6709\u8fdd\u7ea6\uff0c\u5c06\u5176\u4f5c\u4e3aY\u3002\u5c06\u5176\u5b83\u7684\u884c\u4e3a\u6570\u636e\u4f5c\u4e3aX\u3002\u8fd9\u6837\u5c31\u6784\u6210\u4e86\u4e00\u4e2a\u53ef\u4ee5\u5582\u5230\u5206\u7c7b\u7b97\u6cd5\u4e2d\u7684\u6570\u636e\u96c6\u3002\u7136\u540e\u8fd9\u4e2a\u6a21\u578b\u5c31\u53ef\u4ee5\u7528\u5728\u672a\u6765\u7684\u7533\u8bf7\u4eba\u8eab\u4e0a\uff0c\u5f62\u6210\u5ba1\u6838\u81ea\u52a8\u5316\u7cfb\u7edf\u3002 gmail\u5783\u573e\u90ae\u4ef6\u81ea\u52a8\u5206\u7c7b\uff1a \u5982\u679c\u4f60\u70b9\u5f00\u81ea\u5df1\u7684gmail\u90ae\u7bb1\uff0c\u4ed4\u7ec6\u89c2\u5bdf\u4f1a\u53d1\u73b0\u4e00\u4e2a\u5783\u573e\u90ae\u4ef6\u7684\u6807\u7b7e\uff0c\u5b83\u5e73\u65f6\u9ed8\u9ed8\u7684\u4e3a\u4f60\u6321\u4e0b\u5927\u91cf\u7684\u5783\u573e\u4fe1\u606f\uff0c\u800c\u53c8\u4e0d\u53bb\u5e72\u6270\u4f60\uff0c\u5b9e\u5728\u662f\u6570\u636e\u4ea7\u54c1\u7684\u5178\u8303\u3002\u90a3\u4e48\u5982\u679c\u4f60\u6765\u505a\u8fd9\u79cd\u5783\u573e\u90ae\u4ef6\u7684\u81ea\u52a8\u5206\u7c7b\uff0c\u8981\u600e\u4e48\u505a\u7684\u5462\uff1f \u5982\u679c\u6211\u4eec\u8003\u8651\u7b80\u5355\u4e9b\uff0c\u629b\u5f00\u4e00\u5c01\u7535\u90ae\u4e2d\u7684\u5176\u5b83\u4fe1\u606f\uff08\u53d1\u4ef6\u4eba\uff0cIP...\uff09\uff0c\u800c\u53ea\u53d6\u6587\u672c\u4fe1\u606f\u7684\u8bdd\uff0c\u8fd9\u4e2a\u95ee\u9898\u5c31\u8f6c\u4e3a\u4e00\u4e2a\u6587\u672c\u5206\u7c7b\u7684\u95ee\u9898\u3002 \u6587\u672c\u7684\u5206\u6790\u96be\u70b9\u5728\u4e8e\uff1a\u6587\u672c\u4e0d\u662f\u7ed9\u8ba1\u7b97\u673a\u9605\u8bfb\u7684\uff0c\u5b83\u6709\u590d\u6742\u7684\u8bed\u8a00\u7ed3\u6784\uff08\u8bed\u6cd5\u3001\u8bed\u4e49\u3001\u8bed\u7528\uff09\uff0c\u4f46\u8bed\u8a00\u4e2d\u4f9d\u7136\u5b58\u5728\u7edf\u8ba1\u89c4\u5f8b\uff08\u7edf\u8ba1\u8bed\u8a00\u6a21\u578b\uff09\u3002 \u4e00\u4e2a\u7b80\u5355\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff1a\u5224\u65ad\u4e00\u5c01\u90ae\u4ef6\u662f\u5426\u5783\u573e\u90ae\u4ef6 - \u6536\u96c6N\u4e2a\u90ae\u4ef6 - \u4ece\u90ae\u4ef6\u4e2d\u63d0\u53d6\u6307\u6807\uff08\u5206\u8bcd\uff0c\u7a7a\u95f4\u5411\u91cf\u6a21\u578b\uff09\uff0c\u6784\u6210\u6587\u6863-\u8bcd\u9879\u77e9\u9635 - \u4eba\u5de5\u6807\u6ce8\u8fd9\u4e9b\u90ae\u4ef6\u662f\u5426\u5783\u573e\u90ae\u4ef6 - \u7528\u4e00\u90e8\u5206\u6570\u636e\uff0c\u7ed3\u5408\u7b97\u6cd5\u5bf9X\u548cY\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002 - \u7528\u5269\u4e0b\u7684\u6570\u636e\uff0c\u68c0\u67e5\u6a21\u578b\u7684\u6548\u679c\u3002 \u4e94\u3001\u6570\u636e\u6316\u6398\u65e5\u5e38\u5de5\u4f5c\u6709\u54ea\u4e9b \u9879\u76ee\u8ba8\u8bba\u548c\u89c4\u5212\uff0c\u5c31\u662f\u5f00\u4f1a\u3002\u8fd9\u65b9\u9762\u5de5\u4f5c\u76ee\u7684\u4e3b\u8981\u662f\u660e\u786e\u4e1a\u52a1\u95ee\u9898\u3002\u662f\u4e0d\u662f\u53ef\u4ee5\u505a\uff1f\u5927\u6982\u53ef\u4ee5\u600e\u4e48\u505a\uff1f\u786e\u5b9a\u4e86\u4e1a\u52a1\u95ee\u9898\u4e4b\u540e\uff0c\u9700\u8981\u5c06\u8fd9\u4e2a\u4e1a\u52a1\u95ee\u9898\u7ffb\u8bd1\u6210\u4e00\u4e2a\u6570\u636e\u95ee\u9898\u3002 \u9879\u76ee\u51c6\u5907\uff0c\u51c6\u5907\u5f00\u5de5\u5e72\u6d3b\u4e86\u3002\u8fd9\u65b9\u9762\u662f\u6700\u4e3a\u7e41\u7410\u4e5f\u6700\u5bb9\u6613\u51fa\u9519\u7684\u5730\u65b9\u3002\u9700\u8981\u548c\u6570\u636e\u4ed3\u5e93\u7684\u540c\u5b66\u914d\u5408\u53d6\u5f97\u5fc5\u8981\u7684\u6570\u636e\uff0c\u63a2\u7d22\u7406\u89e3\u6570\u636e\u7684\u4e1a\u52a1\u610f\u4e49\uff0c\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u6839\u636e\u9879\u76ee\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u6574\u7406\u8f6c\u6362\uff0c\u505a\u5927\u91cf\u7684\u7279\u5f81\u5de5\u7a0b\u7684\u5de5\u4f5c\u3002 \u9879\u76ee\u5b9e\u65bd\uff0c\u5373\u6570\u636e\u5efa\u6a21\uff0c\u5f00\u59cb\u62f7\u6253\u6570\u636e\u4e86\u3002\u9009\u62e9\u5c1d\u8bd5\u4e0d\u540c\u7684\u6a21\u578b\u7b97\u6cd5\uff0c\u4ece\u6570\u636e\u4e2d\u5f97\u5230\u9700\u8981\u7684\u7ed3\u679c\uff0c\u7136\u540e\u4ece\u4e0d\u540c\u65b9\u9762\u8bc4\u4ef7\u6548\u679c\u600e\u4e48\u6837\u3002 \u9879\u76ee\u7ed3\u675f\uff0c\u4ea4\u4ed8\u7ed3\u679c\u3002\u786e\u5b9a\u6a21\u578b\u5982\u4f55\u90e8\u7f72\uff0c\u5e76\u5b9e\u65bd\u90e8\u7f72\u5de5\u4f5c\u3002\u8fd9\u79cd\u90e8\u7f72\u5c31\u662f\u6a21\u578b\u7684\u5e94\u7528\uff0c\u591a\u6570\u60c5\u51b5\u4e0b\u662f\u5c06\u7ed3\u679c\u56de\u5199\u5230\u6570\u636e\u5e93\u4e2d\u3002\u540c\u65f6\u7ed3\u679c\u4ea4\u4ed8\u7ed9\u9700\u6c42\u65b9\uff0c\u5199\u6700\u7ec8\u7684\u9879\u76ee\u62a5\u544a\uff0c\u5f52\u6863\u6240\u6709\u6587\u4ef6\u3002 \u9605\u8bfb\u6587\u732e\uff0c\u65b9\u6cd5\u7814\u7a76\u3002\u5728\u6bd4\u8f83\u7a7a\u95f2\u7684\u65f6\u95f4\uff0c\u6216\u8005\u9047\u5230\u96be\u9898\u7684\u65f6\u5019\uff0c\u90fd\u9700\u8981\u53bb\u627e\u5de8\u4eba\u7684\u80a9\u8180\u4f9d\u9760\u4e00\u4e0b\u3002 \u4e00\u4e2a\u5178\u578b\u7684\u6b65\u9aa4\u6d41\u7a0b \u5546\u4e1a\u7406\u89e3\uff1a\u7406\u89e3\u4e1a\u52a1\u76ee\u6807\u548c\u9700\u6c42\uff0c\u5e76\u8f6c\u5316\u4e3a\u6570\u636e\u6316\u6398\u53ef\u7406\u89e3\u7684\u95ee\u9898\u5b9a\u4e49\u3002\u5efa\u6a21\u5e08\u4f1a\u53c2\u52a0\u4e1a\u52a1\u7ec4\u7684\u4f1a\u8bae\uff0c\u4e3b\u8981\u662f\u4e86\u89e3\u6536\u96c6\u4e1a\u52a1\u9700\u6c42\u3002 \u6570\u636e\u7406\u89e3\uff1a\u7b5b\u9009\u76ee\u6807\u6570\u636e\uff0c\u68c0\u9a8c\u6570\u636e\u8d28\u91cf\uff0c\u63a2\u7d22\u6570\u636e\u7279\u5f81\uff0c\u8bc4\u4f30\u53ef\u7528\u6570\u636e\u3002\u5efa\u6a21\u5e08\u4f1a\u5c06\u4e00\u4e9b\u521d\u6b65\u7ed3\u679c\u5448\u73b0\u7ed9\u4e1a\u52a1\u7ec4\uff0c\u5f97\u5230\u8fdb\u4e00\u6b65\u53cd\u9988\u3002 \u6570\u636e\u51c6\u5907\uff1a\u901a\u8fc7\u6e05\u6d17\uff0c\u96c6\u6210\uff0c\u53d8\u6362\uff0c\u5f52\u7ea6\u7b49\u5904\u7406\u65b9\u6cd5\u6784\u9020\u6700\u7ec8\u6570\u636e\u96c6\u5408\u3002\u5efa\u6a21\u5e08\u5f00\u59cb\u75af\u72c2\u7684\u5199SQL\u7c7b\u7684\u811a\u672c\u53bb\u6d17\u6570\u636e\u3002 \u6a21\u578b\u5efa\u7acb\uff1a\u9009\u62e9\u548c\u5e94\u7528\u5404\u79cd\u673a\u5668\u5b66\u4e60\u6216\u7edf\u8ba1\u65b9\u6cd5\u3001\u6784\u5efa\u6a21\u578b\u5e76\u8c03\u6821\u5404\u79cd\u53c2\u6570\u3002\u5efa\u6a21\u5e08\u8fdb\u5165\u70bc\u4e39\u9636\u6bb5\uff0c\u671f\u5f85\u80fd\u6709\u597d\u7684\u7ed3\u679c\u3002 \u6a21\u578b\u8bc4\u4ef7\uff1a\u7ed3\u5408\u6700\u521d\u7684\u5546\u4e1a\u76ee\u6807\u8bc4\u4ef7\u5e76\u89e3\u91ca\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u53ef\u80fd\u7684\u5546\u4e1a\u6548\u679c\u3002\u5efa\u6a21\u5e08\u5c06\u6a21\u578b\u7ed3\u679c\u548c\u4e1a\u52a1\u56e2\u961f\u8fdb\u884c\u6c9f\u901a\u3002 \u6a21\u578b\u90e8\u7f72\uff1a\u6309\u7528\u6237\u4e60\u60ef\u65b9\u5f0f\u5b9e\u65bd\u5e76\u53d1\u5e03\u6a21\u578b\uff0c\u63d0\u4f9b\u5206\u6790\u7ed3\u8bba\uff0c\u5e76\u6301\u7eed\u8ddf\u8e2a\u3002\u5efa\u6a21\u5e08\u5c06\u6a21\u578b\u4e0a\u7ebf\uff0c\u76d1\u6d4b\u6027\u80fd\u3002 \u516d\u3001\u6570\u636e\u6316\u6398\u7684\u4efb\u52a1\u6a21\u5f0f \u5206\u7c7b Classification [Predictive] \u805a\u7c7b Clustering [Descriptive] \u5173\u8054\u89c4\u5219 Association Rule Discovery [Descriptive] \u5e8f\u5217\u6316\u6398 Sequential Pattern Discovery [Descriptive] \u56de\u5f52 Regression [Predictive] \u5f02\u5e38\u68c0\u6d4b Deviation Detection [Predictive] \u5206\u7c7b\u65b9\u6cd5\u7684\u5e94\u7528 \u7cbe\u51c6\u5316\u8425\u9500 \u95ee\u9898: \u51c6\u5907\u53d1\u552eiphone\u65b0\u54c1\u4e86\uff0c\u54ea\u4e9b\u7528\u6237\u53ef\u80fd\u4f1a\u4e70\uff1f \u65b9\u6cd5: \u627e\u5230\u76f8\u4f3c\u4ea7\u54c1\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\uff0c\u89c2\u5bdf\u662f\u5426\u8d2d\u4e70\u4f5c\u4e3a\u76ee\u6807\u53d8\u91cf \u6536\u96c6\u8fd9\u4e9b\u7528\u6237\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4f5c\u4e3a\u6a21\u578b\u7684\u89e3\u91ca\u53d8\u91cf \u805a\u7c7b\u65b9\u6cd5\u7684\u5e94\u7528 \u7528\u6237\u5206\u7fa4: \u95ee\u9898: \u5982\u4f55\u5c06\u7528\u6237\u5206\u6210\u82e5\u5e72\u7ec4\uff0c\u7136\u540e\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7ec4\u8fdb\u884c\u8425\u9500\u6d3b\u52a8 \u65b9\u6cd5: \u6536\u96c6\u7528\u6237\u7684\u57fa\u672c\u793e\u4f1a\u7279\u5f81\u548c\u884c\u4e3a\u7279\u5f81 \u6839\u636e\u7528\u6237\u7684\u76f8\u4f3c\u7a0b\u5ea6\u8fdb\u884c\u805a\u7c7b\u5206\u7ec4 \u6839\u636e\u540c\u7ec4\u7684\u7528\u6237\u8d2d\u4e70\u884c\u4e3a\u5224\u65ad\u5206\u7fa4\u7684\u6548\u679c \u5173\u8054\u89c4\u5219\u5e94\u7528 \u76f8\u4f3c\u5546\u54c1\u63a8\u8350 \u5173\u8054\u89c4\u5219\u662f\u4e00\u79cd\u89c4\u5f8b {\u9762\u5305\u5e72, \u2026 } -- {\u85af\u7247} \u4e70\u4e86\u9762\u5305\u5e72\u7684\u7528\u6237\u5f80\u5f80\u4e5f\u4f1a\u53bb\u4e70\u85af\u7247\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u5173\u8054\u89c4\u5219\uff0c\u5c06\u82e5\u5e72\u5546\u54c1\u4e00\u8d77\u653e\u5728\u8d27\u67b6\u4e0a\u6346\u7ed1\u9500\u552e \u4e0d\u540c\u7684\u5173\u8054\u5546\u54c1\uff0c\u6697\u793a\u4e86\u4e0d\u540c\u7684\u6d88\u8d39\u573a\u666f \u5f02\u5e38\u68c0\u6d4b\u5e94\u7528 \u4ece\u6b63\u5e38\u884c\u4e3a\u4e2d\u53d1\u73b0\u4e0d\u6b63\u5e38\u7684\u884c\u4e3a\u6a21\u5f0f \u4fe1\u7528\u5361\u6b3a\u8bc8 \u7f51\u7edc\u5165\u4fb5 \u4e03\u3001\u9700\u8981\u638c\u63e1\u54ea\u4e9b\u6280\u80fd \u6709\u5f62\u7684\u6280\u80fd: \u7406\u8bba\uff1a\u6c14\u5b97\u3002 \u4f8b\u5982\u7edf\u8ba1\u7406\u8bba\u3001\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002\u4e2a\u4eba\u4f53\u4f1a\u7cbe\u901a\u7406\u8bba\u540e\u518d\u505a\u6570\u636e\u5de5\u4f5c\u5c31\u5982\u6c64\u6cfc\u96ea\u3002\u6211\u4e5f\u627f\u8ba4\u5b66\u4e60\u7406\u8bba\u662f\u8270\u96be\u7684\uff0c\u4f46\u662f\u4e00\u5b9a\u8981\u5728\u5e74\u8f7b\u7684\u65f6\u5019\u8bfb\u6700\u96be\u7684\u4e66\u3002\u300a\u6570\u5b66\u4e4b\u7f8e\u300b\u4e2d\u8c08\u5230\uff0c\u6280\u672f\u5206\u4e3a\u672f\u548c\u9053\u4e24\u79cd\uff0c\u5177\u4f53\u7684\u505a\u4e8b\u65b9\u6cd5\u662f\u672f\uff0c\u505a\u4e8b\u7684\u539f\u7406\u548c\u539f\u5219\u662f\u9053\uff0c\u53ea\u8ffd\u6c42\u672f\u7684\u4eba\u5de5\u4f5c\u5f88\u8f9b\u82e6\uff0c\u53ea\u6709\u638c\u63e1\u4e86\u9053\u624d\u80fd\u6c38\u8fdc\u6e38\u5203\u6709\u4f59\u3002 \u5de5\u5177\uff1a\u5251\u5b97\u3002\u7406\u8bba\u4e0d\u7528\u5728\u4ea7\u54c1\u4e0a\u5c31\u662f\u738b\u8bed\u7109\u7684\u5b66\u9662\u6d3e\u3002\u4ece\u7406\u8bba\u5230\u4ea7\u54c1\uff0c\u9700\u8981\u638c\u63e1\u5404\u79cd\u5de5\u5177\u3002\u8fd9\u7c7b\u5de5\u5177\u7528\u5f97\u719f\u4e86\u80fd\u4e8b\u534a\u529f\u500d\uff0c\u4f8b\u5982R\u3001python\u3001SQL\u3001hadoop\u8fd9\u7c7b\u3002\u5b66\u4e60\u5de5\u5177\u548c\u5b66\u4e60\u8bed\u8a00\u4e00\u6837\uff0c\u90fd\u8981\u591a\u8bfb\u591a\u5199\uff0c\u6a21\u4eff\u63e3\u6469\uff0c\u5c31\u53ef\u4ee5\u8fd0\u7528\u81ea\u5982\u3002\u4e0d\u8fc7\u8ff7\u4fe1\u5de5\u5177\u662f\u6ca1\u6709\u610f\u4e49\u7684\uff0c\u6ca1\u6709\u6700\u597d\u7684\u5de5\u5177\uff0c\u53ea\u6709\u6700\u5408\u9002\u7684\u5de5\u5177\u3002\u5982\u679c\u4f60\u662f\u72ec\u5b64\u6c42\u8d25\uff0c\u53ef\u4ee5\u73a9\u7384\u94c1\u5251\uff0c\u5982\u679c\u4f60\u662f\u4e1c\u65b9\u4e0d\u8d25\uff0c\u53ef\u4ee5\u73a9\u7ee3\u82b1\u9488\u3002 \u7ecf\u9a8c\uff1a\u5b9e\u6218\u3002\u6709\u5185\u529b\u6709\u5251\u6cd5\uff0c\u5c31\u9700\u8981\u4e0b\u5c71\u4e86\u3002\u5bf9\u6218\u6700\u5f3a\u608d\u7684\u5bf9\u624b\uff0c\u624d\u80fd\u8ba9\u4f60\u7684\u5185\u529b\u5251\u6cd5\u878d\u4e3a\u4e00\u4f53\u3002\u505a\u9879\u76ee\uff0c\u5728\u5de5\u4f5c\u89e3\u51b3\u96be\u9898\uff0c\u624d\u662f\u957f\u8fdb\u6700\u5feb\u7684\u3002 \u65e0\u5f62\u7684\u6c14\u8d28: \u597d\u5947\uff0c\u597d\u5947\u5fc3\u548c\u5174\u8da3\u662f\u4ece\u6570\u636e\u4e2d\u5f97\u5230\u6d1e\u5bdf\u7684\u9a71\u52a8\u529b\u3002\u6709\u597d\u5947\u5fc3\u7684\u4eba\u624d\u4f1a\u5bf9\u6570\u636e\u6709\u6301\u7eed\u7684\u70ed\u60c5\u3002 \u521b\u9020\uff0c\u5175\u65e0\u5e38\u52bf\uff0c\u6570\u636e\u7684\u5de5\u4f5c\u90fd\u662f\u5343\u5dee\u4e07\u522b\u7684\uff0c\u867d\u7136\u53ef\u4ee5\u4f9d\u9760\u4e00\u4e9b\u8001\u7684\u7ecf\u9a8c\u505a\u4e9b\u7167\u732b\u753b\u864e\u7684\u4e8b\u3002\u4f46\u6700\u597d\u8fd8\u662f\u9700\u8981\u6839\u636e\u4e0d\u540c\u7684\u9879\u76ee\u60c5\u51b5\u6765\u505a\u51fa\u5224\u65ad\u3002\u72ec\u7acb\u601d\u8003\u548c\u521b\u9020\u8ba9\u4f60\u8d70\u5f97\u66f4\u8fdc\u3002 \u6c42\u8d25\uff0c\u521b\u9020\u3001\u524d\u6cbf\u3001\u63a2\u7d22\u6027\u7684\u5de5\u4f5c\uff0c\u4e00\u5b9a\u4f1a\u6709\u5931\u8d25\uff0c\u5feb\u901f\u5931\u8d25\uff0c\u5feb\u901f\u5b66\u4e60\uff0c\u4e0d\u65ad\u4fee\u6b63\uff0c\u80fd\u591f\u8d25\u4e2d\u6c42\u80dc\u3002 \u5982\u4f55\u57f9\u517b\u6570\u636e\u6316\u6398\u7684\u6280\u80fd \u4e24\u4e2a\u5b57\uff1a\u81ea\u5b66\u3002 \u201c\u77e5\u8bc6\u4e0e\u8010\u5fc3\uff0c\u662f\u51fb\u8d25\u5f3a\u8005\u7684\u552f\u4e00\u65b9\u6cd5\u3002\u201d \u901a\u8fc7\u9605\u8bfb\u6765\u5b66\u4e60\u3002\u5305\u62ec\u4e86\u9605\u8bfb\u7ecf\u5178\u7684\u7406\u8bba\u6559\u6750\u3001\u4ee3\u7801\u3001\u8bba\u6587\u3001\u4e0a\u516c\u5f00\u8bfe\u3002 \u901a\u8fc7\u725b\u4eba\u6765\u5b66\u4e60\u3002\u5305\u62ec\u540c\u884c\u7684\u805a\u4f1a\u3001\u8ba8\u8bba\u3001\u5927\u725b\u7684\u535a\u5ba2\u3001\u5fae\u535a\u3001twitter\u3001RSS\u3002 \u901a\u8fc7\u7ec3\u4e60\u6765\u5b66\u4e60\u3002\u5305\u62ec\u4ee3\u7801\u7ec3\u4e60\u9898\u3001\u53c2\u52a0kaggle\u6bd4\u8d5b\u3001\u89e3\u51b3\u5b9e\u9645\u5de5\u4f5c\u4e2d\u7684\u96be\u9898\u3002 \u901a\u8fc7\u5206\u4eab\u6765\u5b66\u4e60\u3002\u5305\u62ec\u81ea\u5df1\u5199\u7b14\u8bb0\u3001\u5199\u535a\u5ba2\u3001\u548c\u540c\u4e8b\u5206\u4eab\u4ea4\u6d41\u3001\u57f9\u8bad\u65b0\u4eba\u3002 \u516b\u3001\u7ecf\u9a8c\u4e4b\u8c08 \u5efa\u6a21\u4e2d\u7684\u5751 \u5efa\u6a21\u8fc7\u7a0b\u7684\u95ee\u9898 \u7f3a\u4e4f\u4e1a\u52a1\u95ee\u9898\u7684\u6c9f\u901a\u548c\u7406\u89e3 \u53ea\u5173\u6ce8\u8bad\u7ec3\u6570\u636e\u6216\u53ea\u8fc7\u4e8e\u76f8\u4fe1\u6570\u636e \u53ea\u4f9d\u8d56\u4e8e\u4e00\u79cd\u6280\u672f \u9519\u8bef\u7684\u53d8\u91cf\u8f93\u5165 \u6316\u6398\u7ed3\u679c\u4e0d\u771f\u5b9e \u6a21\u578b\u7ed3\u679c\u4e0d\u4ee3\u8868\u4efb\u4f55\u89c4\u5f8b \u6a21\u578b\u8bad\u7ec3\u96c6\u53ef\u80fd\u4e0d\u53cd\u6620\u771f\u6b63\u7684\u603b\u4f53 \u6570\u636e\u7684\u8be6\u7ec6\u7a0b\u5ea6\u6709\u8bef \u6316\u6398\u7ed3\u679c\u6ca1\u6709\u7528 \u6316\u6398\u7ed3\u679c\u4f17\u6240\u5468\u77e5 \u6316\u6398\u7ed3\u679c\u4e0d\u53ef\u7528\u4e8e\u51b3\u7b56 \u4e00\u70b9\u5fc3\u5f97 \u63d0\u95ee\u9898\u6bd4\u56de\u7b54\u95ee\u9898\u66f4\u91cd\u8981 \u4e00\u4e2a\u5177\u4f53\u7684\u4e1a\u52a1\u75db\u70b9\u662f\u6570\u636e\u6316\u6398\u7684\u8d77\u70b9\uff0c\u7cbe\u5fc3\u8ba1\u5212\u6d41\u7a0b\u6b65\u9aa4\uff0c\u4e1a\u52a1\u77e5\u8bc6\u8d2f\u7a7f\u6316\u6398\u5efa\u6a21\u7684\u6bcf\u4e2a\u9636\u6bb5\u3002 \u5bf9\u6570\u636e\u6301\u8c28\u614e\u7684\u6001\u5ea6 \u6570\u636e\u5f88\u53ef\u80fd\u51fa\u9519\uff0c\u6570\u636e\u6574\u7406\u5360\u636e\u5927\u90e8\u5206\u7684\u5de5\u4f5c\u65f6\u95f4\u3002 \u6570\u636e\u672c\u8eab\u4ec5\u80fd\u7528\u4e8e\u63cf\u8ff0\u5386\u53f2 \u4e0d\u80fd\u5c55\u73b0\u56e0\u679c\uff0c\u4e5f\u4e0d\u80fd\u9884\u77e5\u672a\u6765\u3002 \u6570\u636e\u4ef7\u503c\u4f53\u73b0\u5728\u843d\u5730\u5e94\u7528 \u6570\u636e\u6316\u6398\u4ef7\u503c\u5e76\u4e0d\u53d6\u51b3\u4e8e\u6a21\u578b\u7684\u51c6\u786e\u6216\u7a33\u5b9a\uff0c\u53d6\u51b3\u4e8e\u80cc\u540e\u7684\u51b3\u7b56\u7ec4\u7ec7\u3002 \u4e0d\u540c\u7684\u6307\u6807\u548c\u6a21\u578b\u90fd\u6709\u5176\u9002\u7528\u8303\u56f4 \u968f\u65f6\u95f4\u73af\u5883\u53d8\u5316\uff0c\u6240\u6709\u7684\u6a21\u5f0f\u90fd\u4f1a\u6539\u53d8\uff0c\u4e0d\u65ad\u5c1d\u8bd5\uff0c\u4e0d\u65ad\u4fee\u6b63\u3002 \u5de5\u4f5c\u4e2d\u7684\u6587\u6863\u5316\u548c\u81ea\u52a8\u5316 \u4e00\u4e2a\u7efc\u5408\u6848\u4f8b\u7684\u5efa\u6a21\u6b65\u9aa4 \u95ee\u9898\u5b9a\u4e49 \u6570\u636e\u63a2\u7d22 \u7279\u5f81\u5de5\u7a0b \u5efa\u6a21\u548c\u8bc4\u4f30 \u95ee\u9898\u5b9a\u4e49 \u6211\u4eec\u4f7f\u7528kaggle\u4e0a\u4e00\u4e2a\u7ecf\u5178\u7684\u95ee\u9898\u505a\u4e3a\u6848\u4f8b\u793a\u8303\uff0c\u5373\u5224\u65ad\u4e00\u4e2a\u8d37\u6b3e\u8005\u5728\u540e\u7eed\u4e24\u5e74\u5185\u662f\u5426\u4f1a\u8fdd\u7ea6\u7684\u6982\u7387\u3002 \u9700\u8981\u601d\u8003\u7684\u5173\u952e\u95ee\u9898 \u635f\u5931\u662f\u5982\u4f55\u53d1\u751f\u7684? \u8fdd\u7ea6\u7684\u4eba\u6709\u54ea\u4e9b\u7279\u70b9? \u8fdd\u7ea6\u7684\u5360\u6bd4\u6709\u591a\u5c11\uff1f \u5982\u4f55\u80fd\u6539\u5584\u6211\u4eec\u7684\u635f\u5931\uff1f \u6570\u636e\u4e0b\u8f7d http://www.kaggle.com/c/GiveMeSomeCredit \u53d8\u91cf\u7684\u610f\u4e49 SeriousDlqin2yrs \u7528\u6237\u5728\u540e\u7eed\u4e24\u5e74\u5185\u51fa\u73b090\u5929\u4ee5\u4e0a\u7684\u8fd8\u6b3e\u903e\u671f\uff0c\u8fd9\u662f\u76ee\u6807\u53d8\u91cf\uff0c\u4ee5\u4e0b\u90fd\u662f\u89e3\u91ca\u53d8\u91cf RevolvingUtilizationOfUnsecuredLines \u4fe1\u7528\u5361\u501f\u6b3e\u5360\u6bd4 age \u501f\u6b3e\u4eba\u5e74\u9f84 NumberOfTime30-59DaysPastDueNotWorse \u8fc7\u53bb\u6709\u903e\u671f30-59\u5929\u8fd8\u6b3e\u7684\u6b21\u6570 DebtRatio \u6708\u5ea6\u751f\u6d3b\u6210\u672c\u5360\u6708\u6536\u5165\u7684\u6bd4\u7387 MonthlyIncome \u6708\u6536\u5165 NumberOfOpenCreditLinesAndLoans \u5305\u62ec\u8f66\u8d37\u623f\u8d37\u5728\u5185\u7684\u8d37\u6b3e\u7b14\u6570 NumberOfTimes90DaysLate \u8fc7\u53bb\u6709\u903e\u671f90\u5929\u4ee5\u4e0a\u8fd8\u6b3e\u7684\u6b21\u6570 NumberRealEstateLoansOrLines \u623f\u8d37\u7684\u4fe1\u8d37\u6b21\u6570 NumberOfTime60-89DaysPastDueNotWorse \u8fc7\u53bb\u6709\u903e\u671f60-89\u5929\u8fd8\u6b3e\u7684\u6b21\u6570 NumberOfDependents \u5bb6\u5ead\u4e2d\u9700\u8981\u629a\u517b\u8005\u7684\u4eba\u6570 import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline df = pd.read_csv( data/credit-training.csv ) #\u8bfb\u53d6\u6570\u636e df.head() SeriousDlqin2yrs RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents 0 1 0.766127 45 2 0.802982 9120.0 13 0 6 0 2.0 1 0 0.957151 40 0 0.121876 2600.0 4 0 0 0 1.0 2 0 0.658180 38 1 0.085113 3042.0 2 1 0 0 0.0 3 0 0.233810 30 0 0.036050 3300.0 5 0 0 0 0.0 4 0 0.907239 49 1 0.024926 63588.0 7 0 1 0 0.0 df.shape # \u4e00\u517115\u4e07\u6761\u8bb0\u5f55 (150000, 11) \u6570\u636e\u63a2\u7d22 df.info() # \u6709\u7f3a\u5931\u503c class 'pandas.core.frame.DataFrame' RangeIndex: 150000 entries, 0 to 149999 Data columns (total 11 columns): SeriousDlqin2yrs 150000 non-null int64 RevolvingUtilizationOfUnsecuredLines 150000 non-null float64 age 150000 non-null int64 NumberOfTime30-59DaysPastDueNotWorse 150000 non-null int64 DebtRatio 150000 non-null float64 MonthlyIncome 120269 non-null float64 NumberOfOpenCreditLinesAndLoans 150000 non-null int64 NumberOfTimes90DaysLate 150000 non-null int64 NumberRealEstateLoansOrLines 150000 non-null int64 NumberOfTime60-89DaysPastDueNotWorse 150000 non-null int64 NumberOfDependents 146076 non-null float64 dtypes: float64(4), int64(7) memory usage: 12.6 MB df.describe() # \u63cf\u8ff0\u6027\u7edf\u8ba1 SeriousDlqin2yrs RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents count 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 1.202690e+05 150000.000000 150000.000000 150000.000000 150000.000000 146076.000000 mean 0.066840 6.048438 52.295207 0.421033 353.005076 6.670221e+03 8.452760 0.265973 1.018240 0.240387 0.757222 std 0.249746 249.755371 14.771866 4.192781 2037.818523 1.438467e+04 5.145951 4.169304 1.129771 4.155179 1.115086 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.029867 41.000000 0.000000 0.175074 3.400000e+03 5.000000 0.000000 0.000000 0.000000 0.000000 50% 0.000000 0.154181 52.000000 0.000000 0.366508 5.400000e+03 8.000000 0.000000 1.000000 0.000000 0.000000 75% 0.000000 0.559046 63.000000 0.000000 0.868254 8.249000e+03 11.000000 0.000000 2.000000 0.000000 1.000000 max 1.000000 50708.000000 109.000000 98.000000 329664.000000 3.008750e+06 58.000000 98.000000 54.000000 98.000000 20.000000 df.SeriousDlqin2yrs.value_counts() 0 139974 1 10026 Name: SeriousDlqin2yrs, dtype: int64 df.SeriousDlqin2yrs.mean() # \u5e73\u5747\u4e25\u91cd\u62d6\u6b20 0.06684 df.NumberOfDependents.unique() # \u53d7\u629a\u517b\u8005, \u67e5\u770b\u6709\u591a\u5c11\u53d6\u503c array([ 2., 1., 0., nan, 3., 4., 5., 6., 8., 7., 20., 10., 9., 13.]) df.NumberOfDependents.value_counts() # \u89c2\u5bdf\u9891\u6570\u8868\uff0c\u4e00\u534a\u90fd\u6ca1\u6709, \u6709\u7684\u8bdd\u96c6\u4e2d1-3\u4e2a, \u518d\u5f80\u4e0a\u8f83\u5c11 0.0 86902 1.0 26316 2.0 19522 3.0 9483 4.0 2862 5.0 746 6.0 158 7.0 51 8.0 24 10.0 5 9.0 5 20.0 1 13.0 1 Name: NumberOfDependents, dtype: int64 df.groupby( SeriousDlqin2yrs ).mean() RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents SeriousDlqin2yrs 0 6.168855 52.751375 0.280109 357.151168 6747.837774 8.493620 0.135225 1.020368 0.126666 0.743417 1 4.367282 45.926591 2.388490 295.121066 5630.826493 7.882306 2.091362 0.988530 1.828047 0.948208 pd.value_counts(df.NumberOfDependents).plot(kind='bar'); pd.crosstab(df.NumberOfTimes90DaysLate, df.SeriousDlqin2yrs) # \u8ba1\u7b97\u4ea4\u53c9\u9891\u6570\u8868 SeriousDlqin2yrs 0 1 NumberOfTimes90DaysLate 0 135108 6554 1 3478 1765 2 779 776 3 282 385 4 96 195 5 48 83 6 32 48 7 7 31 8 6 15 9 5 14 10 3 5 11 2 3 12 1 1 13 2 2 14 1 1 15 2 0 17 0 1 96 1 4 98 121 143 pd.crosstab(df.age, df.NumberOfDependents) # \u5e74\u9f84\u4e0e\u53d7\u629a\u517b\u8005\u7684\u5173\u7cfb # \u5c81\u6570\u8d8a\u5927\u629a\u517b\u8005\u4f1a\u591a NumberOfDependents 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 13.0 20.0 age 0 0 0 1 0 0 0 0 0 0 0 0 0 0 21 148 3 1 0 0 0 0 0 0 0 0 0 0 22 385 7 2 2 0 0 0 0 0 0 0 0 0 23 550 33 13 3 0 0 0 0 0 0 0 0 0 24 689 48 19 3 1 0 0 0 0 0 0 0 0 25 774 91 31 7 5 1 0 0 0 0 0 0 0 26 946 128 56 14 4 0 0 0 0 0 0 0 0 27 1001 192 53 32 4 0 0 0 0 0 0 0 0 28 1142 210 114 45 8 1 0 0 0 0 0 0 0 29 1195 254 145 53 14 1 1 0 0 0 0 0 0 30 1337 288 178 72 14 7 1 1 0 0 0 0 0 31 1314 344 245 91 18 4 0 0 0 0 0 0 0 32 1207 380 275 106 53 3 0 0 0 0 0 0 0 33 1254 449 315 140 50 10 2 0 0 0 0 0 0 34 1152 389 360 145 65 11 1 0 0 0 0 0 0 35 1136 418 398 184 58 15 3 1 1 0 0 0 0 36 1139 445 486 202 50 15 5 1 0 0 0 0 0 37 1088 488 541 274 77 18 2 0 0 1 0 0 0 38 1113 484 604 283 84 26 3 0 1 1 0 0 0 39 1202 544 691 383 111 27 4 1 1 0 1 0 0 40 1185 570 758 391 104 43 5 1 1 0 0 0 1 41 1193 517 791 445 113 30 4 0 1 0 0 0 0 42 1159 526 755 427 142 38 7 2 0 0 0 0 0 43 1205 556 789 431 156 33 9 4 1 0 1 0 0 44 1209 574 854 461 133 32 7 2 1 0 1 0 0 45 1363 608 828 457 165 35 11 3 0 0 0 0 0 46 1373 695 876 498 165 35 8 8 2 0 0 0 0 47 1439 672 863 491 148 46 9 0 2 0 1 0 0 48 1509 771 791 459 157 37 11 3 1 2 0 0 0 49 1632 717 790 439 143 46 8 6 1 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 74 1164 184 23 4 2 0 0 1 1 0 0 0 0 75 981 167 17 3 0 0 0 0 0 0 0 0 0 76 933 165 16 0 0 0 0 0 0 0 0 0 0 77 890 129 11 2 0 0 0 0 0 0 0 0 0 78 837 137 13 2 1 0 0 0 0 0 0 0 0 79 770 131 10 4 0 0 0 0 0 0 0 0 0 80 704 109 7 2 0 0 0 0 0 0 0 0 0 81 629 76 6 1 0 0 0 0 0 0 0 0 0 82 527 69 1 0 1 0 0 0 0 0 0 0 0 83 417 44 2 0 0 0 0 0 0 0 0 0 0 84 406 32 1 0 0 0 0 0 0 0 0 0 0 85 403 30 2 0 0 0 0 0 0 0 0 0 0 86 318 46 2 0 1 0 0 0 0 0 0 0 0 87 279 29 1 1 0 0 0 0 0 0 0 0 0 88 244 20 1 0 0 0 0 0 0 0 0 0 0 89 230 11 1 0 0 0 0 0 0 0 0 0 0 90 148 19 0 0 0 0 0 0 0 0 0 0 0 91 119 10 0 0 0 0 0 0 0 0 0 0 0 92 75 7 0 0 0 0 0 0 0 0 0 0 0 93 67 3 0 0 0 0 0 0 0 0 0 0 0 94 33 2 0 0 0 0 0 0 0 0 0 0 0 95 33 4 0 0 0 0 0 0 0 0 0 0 0 96 12 2 0 0 0 0 0 0 0 0 0 0 0 97 11 1 0 0 0 0 0 0 0 0 0 0 0 98 5 0 0 0 0 0 0 0 0 0 0 0 0 99 5 0 0 0 0 0 0 0 0 0 0 0 0 101 3 0 0 0 0 0 0 0 0 0 0 0 0 102 2 1 0 0 0 0 0 0 0 0 0 0 0 103 3 0 0 0 0 0 0 0 0 0 0 0 0 107 1 0 0 0 0 0 0 0 0 0 0 0 0 84 rows \u00d7 13 columns \u6e05\u6d17\u6570\u636e import re # \u5c06\u540d\u5b57\u90fd\u6539\u4e3a snake_case def camel_to_snake(column_name): converts a string that is camelCase into snake_case s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', column_name) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower() camel_to_snake( javaLovesCamelCase ) 'java_loves_camel_case' df.columns = [camel_to_snake(col) for col in df.columns] df.columns.tolist() ['serious_dlqin2yrs', 'revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents'] from sklearn.neighbors import KNeighborsRegressor income_imputer = KNeighborsRegressor(n_neighbors=1) # \u6570\u636e\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u6709\u7f3a\u5931\u7684\u548c\u65e0\u7f3a\u5931\u7684\uff0c\u7528\u65e0\u7f3a\u5931\u7684\u6570\u636e\u5efa\u7acb\u6a21\u578b\u6765\u5224\u65ad\u7f3a\u5931\u6570\u636e\u7684\u53ef\u80fd\u53d6\u503c train_w_monthly_income = df[df.monthly_income.isnull()==False] train_w_null_monthly_income = df[df.monthly_income.isnull()==True] cols = ['number_real_estate_loans_or_lines', 'number_of_open_credit_lines_and_loans'] income_imputer.fit(train_w_monthly_income[cols], train_w_monthly_income.monthly_income) # \u7528\u623f\u4ea7\u8d37\u6b3e\u6b21\u6570\u4ee5\u53ca\u672a\u7ed3\u675f\u8d37\u6b3e\u6b21\u6570\u6765\u8bad\u7ec3\u6708\u6536\u5165 KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2, weights='uniform') new_values = income_imputer.predict(train_w_null_monthly_income[cols]) # \u518d\u7528\u6a21\u578b\u9884\u6d4b\u7f3a\u5931\u503c\u4e2d\u7684\u6708\u6536\u5165 train_w_null_monthly_income.ix[:,'monthly_income']=new_values # imputation df_imputed = train_w_monthly_income.append(train_w_null_monthly_income) df_imputed.shape # \u586b\u5145\u7f3a\u5931\u503c done (150000, 11) df_imputed.ix[df_imputed.number_of_dependents.isnull(),'number_of_dependents'] = -1 df_imputed.info() class 'pandas.core.frame.DataFrame' Int64Index: 150000 entries, 0 to 149997 Data columns (total 11 columns): serious_dlqin2yrs 150000 non-null int64 revolving_utilization_of_unsecured_lines 150000 non-null float64 age 150000 non-null int64 number_of_time30-59_days_past_due_not_worse 150000 non-null int64 debt_ratio 150000 non-null float64 monthly_income 150000 non-null float64 number_of_open_credit_lines_and_loans 150000 non-null int64 number_of_times90_days_late 150000 non-null int64 number_real_estate_loans_or_lines 150000 non-null int64 number_of_time60-89_days_past_due_not_worse 150000 non-null int64 number_of_dependents 150000 non-null float64 dtypes: float64(4), int64(7) memory usage: 13.7 MB \u7279\u5f81\u5de5\u7a0b df_imputed.monthly_income.hist(); def cap_values(x, cap): if x cap: return cap else: return x # \u8bbe\u5b9a\u4e0a\u9650 df_imputed.monthly_income = df_imputed.monthly_income.apply(lambda x: cap_values(x, 15000)) # \u53d8\u91cf\u79bb\u6563\u5316\uff0c\u5206\u4e3a15\u4e2abin df_imputed['income_bins'] = pd.cut(df_imputed.monthly_income, bins=15, labels=False) pd.value_counts(df_imputed.income_bins) 3 23168 4 19944 5 15583 6 14475 2 14038 7 10766 8 8609 14 7775 9 7672 1 7504 10 6298 0 4994 11 4454 12 2547 13 2173 Name: income_bins, dtype: int64 df_imputed[[ income_bins , serious_dlqin2yrs ]].groupby( income_bins ).mean() # \u6bcf\u4e2a\u6708\u6536\u5165\u5206\u7c7b\u4e2d\u7edf\u8ba1default\u5e73\u5747\u9891\u6570 serious_dlqin2yrs income_bins 0 0.051862 1 0.104211 2 0.093674 3 0.084168 4 0.073305 5 0.066033 6 0.059067 7 0.054338 8 0.050877 9 0.048488 10 0.039695 11 0.037270 12 0.040832 13 0.042338 14 0.047203 # \u753b\u51fa\u56fe\u6765, \u53ef\u4ee5\u660e\u663e\u53d1\u73b0\u57281-2\u4e2a bin\u4e2d\u7684 default \u6700\u591a cols = [ income_bins , serious_dlqin2yrs ] df_imputed[cols].groupby( income_bins ).mean().plot(); # \u4ee5\u5e74\u9f84\u6765\u770b\u662f20-30\u5c81\u6700\u9ad8, \u53e6\u5916\u5c31\u662f100\u5c81\u524d\u540e, \u53ef\u80fd\u662f\u53bb\u4e16\u4e86? cols = ['age', 'serious_dlqin2yrs'] age_means = df_imputed[cols].groupby( age ).mean() age_means.plot(); mybins = [0] + range(20, 80, 5) + [120] df_imputed['age_bin'] = pd.cut(df_imputed.age, bins=mybins) pd.value_counts(df_imputed['age_bin']) (45, 50] 18829 (50, 55] 17861 (55, 60] 16945 (60, 65] 16461 (40, 45] 16208 (35, 40] 13611 (65, 70] 10963 (30, 35] 10728 (75, 120] 10129 (25, 30] 7730 (70, 75] 7507 (20, 25] 3027 (0, 20] 0 dtype: int64 from sklearn.preprocessing import StandardScaler # Standardize features by removing the mean and scaling to unit variance # \u5c06\u6708\u6536\u5165\u6807\u51c6\u5316 df_imputed['monthly_income_scaled'] = StandardScaler().fit_transform(df_imputed.monthly_income.reshape(-1,1)) \u5efa\u6a21\u548c\u8bc4\u4f30 df_imputed.columns Index([u'serious_dlqin2yrs', u'revolving_utilization_of_unsecured_lines', u'age', u'number_of_time30-59_days_past_due_not_worse', u'debt_ratio', u'monthly_income', u'number_of_open_credit_lines_and_loans', u'number_of_times90_days_late', u'number_real_estate_loans_or_lines', u'number_of_time60-89_days_past_due_not_worse', u'number_of_dependents', u'income_bins', u'age_bin', u'monthly_income_scaled'], dtype='object') # \u7279\u5f81 features = ['revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents', 'income_bins', 'age_bin', 'monthly_income_scaled'] X = pd.get_dummies(df_imputed[features], columns = ['income_bins', 'age_bin']) # dummy var print X.columns.tolist() print X.shape ['revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents', 'monthly_income_scaled', 'income_bins_0', 'income_bins_1', 'income_bins_2', 'income_bins_3', 'income_bins_4', 'income_bins_5', 'income_bins_6', 'income_bins_7', 'income_bins_8', 'income_bins_9', 'income_bins_10', 'income_bins_11', 'income_bins_12', 'income_bins_13', 'income_bins_14', 'age_bin_(0, 20]', 'age_bin_(20, 25]', 'age_bin_(25, 30]', 'age_bin_(30, 35]', 'age_bin_(35, 40]', 'age_bin_(40, 45]', 'age_bin_(45, 50]', 'age_bin_(50, 55]', 'age_bin_(55, 60]', 'age_bin_(60, 65]', 'age_bin_(65, 70]', 'age_bin_(70, 75]', 'age_bin_(75, 120]'] (150000, 39) y = df_imputed.serious_dlqin2yrs # target from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(X, y ,train_size=0.7,random_state=1) # 70% \u662f train data print train_X.shape print test_X.shape (105000, 39) (45000, 39) from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier clf = LogisticRegression() # logit \u903b\u8f91\u56de\u5f52 clf.fit(train_X,train_y) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) clf.predict_proba(test_X) # \u9884\u6d4b\u6982\u7387 array([[ 0.92834155, 0.07165845], [ 0.96492419, 0.03507581], [ 0.95753197, 0.04246803], ..., [ 0.96021551, 0.03978449], [ 0.87166534, 0.12833466], [ 0.9777555 , 0.0222445 ]]) from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix # \u8bc4\u4ef7\u65b9\u6cd5 preds = clf.predict(test_X) confusion_matrix(test_y, preds) array([[41880, 88], [ 2908, 124]]) print classification_report(test_y, preds, labels=[0, 1]) precision recall f1-score support 0 0.94 1.00 0.97 41968 1 0.58 0.04 0.08 3032 avg / total 0.91 0.93 0.91 45000 pre = clf.predict_proba(test_X) roc_auc_score(test_y,pre[:,1]) 0.7078206950866951 fpr, tpr, thresholds = roc_curve(test_y,pre[:,1]) plt.plot(fpr,tpr,); # \u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668 clf = RandomForestClassifier() clf.fit(train_X,train_y) preds = clf.predict(test_X) print classification_report(test_y, preds, labels=[0, 1]) precision recall f1-score support 0 0.94 0.99 0.97 41968 1 0.51 0.15 0.24 3032 avg / total 0.91 0.93 0.92 45000 pre = clf.predict_proba(test_X) roc_auc_score(test_y,pre[:,1]) 0.7867644886115015 \u7ec3\u4e60\uff1a \u4f60\u6765\u6539\u8fdb\u8fd9\u4e2a\u6316\u6398\u7684\u7ed3\u679c\uff0c\u4f7fauc\u53ef\u4ee5\u6bd40.78\u66f4\u5927 df_imputed.to_csv('df_imputed',index = False)","title":"0w"},{"location":"w0-introduction/0w/#_1","text":"","title":"\u6570\u636e\u6316\u6398\u6982\u89c8"},{"location":"w0-introduction/0w/#_2","text":"\u6797\u5f6a\u53d1\u73b0\u654c\u519b\u6307\u6325\u90e8 \u5728\u8fd9\u4e2a\u6545\u4e8b\u4e2d\uff0c\u6307\u6325\u5b98\u505a\u4e86\u8fd9\u6837\u51e0\u4ef6\u6709\u5173\u8054\u7684\u4e8b\u60c5\u3002\u4e00\u5e76\u79f0\u4e4b\u4e3a\u201d\u6570\u636e\u95ed\u73af\u201c\u3002 - \u4e0d\u95f4\u65ad\u7684\u6536\u96c6\u6218\u573a\u6570\u636e\uff1b - \u57fa\u4e8e\u67d0\u4e2a\u6307\u6807\u62bd\u8c61\u51fa\u6218\u573a\u5b9e\u9645\uff1b - \u5bf9\u6307\u6807\u8fdb\u884c\u5206\u6790\u5efa\u6a21\uff0c\u53d1\u73b0\u4e00\u4e2a\u673a\u4f1a\uff1b - \u57fa\u4e8e\u5206\u6790\u7ed3\u679c\u8fdb\u884c\u6218\u573a\u51b3\u65ad\uff0c\u83b7\u53d6\u6700\u5927\u5229\u76ca\u3002 \u5c06\u6307\u6325\u5b98\u6362\u6210CEO\uff0c\u5c31\u662f\u5546\u4e1a\u6570\u636e\u95ed\u73af\uff0c\u57fa\u672c\u4e0a\u6240\u6709\u7684\u5546\u4e1a\u6570\u636e\u5e94\u7528\uff0c\u79bb\u4e0d\u5f00\u8fd9\u4e2a\u5957\u8def\u3002 \u4f60\u89c9\u5f97\u6307\u6325\u5b98\u5efa\u7acb\u4e86\u4ec0\u4e48\u6a21\u578b\uff1f\u5148\u60f3\u60f3\uff0c\u522b\u6025\u7740\u770b \u7b54\u6848","title":"\u4e00\u3001\u5f15\u5b50\uff1a\u5173\u4e8e\u6570\u636e\u5229\u7528\u7684\u6545\u4e8b"},{"location":"w0-introduction/0w/#_3","text":"\u5ea6\u91cf\u5546\u4e1a\u884c\u52a8\uff1a\u4f8b\u5982\u7528\u4e00\u4e2a\u6307\u6807\u6d4b\u91cf\u7528\u6237\u54cd\u5e94\u7387 \u8bc6\u522b\u5546\u4e1a\u673a\u4f1a\uff1a\u89c4\u5212\u5149\u68cd\u8282\u65b0\u4ea7\u54c1\u6216\u65b0\u6d3b\u52a8\uff0c\u7406\u89e3\u5ba2\u6237\u6570\u636e\u7684\u6ce2\u52a8\uff0c\u8bc4\u4ef7\u8425\u9500\u6d3b\u52a8\u7684\u7ed3\u679c \u5c06\u6570\u636e\u8f6c\u4e3a\u77e5\u8bc6\uff1a\u901a\u8fc7\u6570\u636e\u6316\u6398\u5b9e\u65bd \u57fa\u4e8e\u77e5\u8bc6\u884c\u52a8\uff1a\u901a\u5e38\u7ed3\u5408\u73b0\u6709\u7684\u4e1a\u52a1\u6d41\u7a0b\uff0c\u5ba2\u6237\u51fa\u73b0\u65f6\u63a8\u9001\u4fe1\u606f\uff0c\u4e0d\u540c\u5ba2\u6237\u7ed9\u4e88\u4e0d\u540c\u7684\u8d44\u6e90","title":"\u4ec0\u4e48\u662f\u6570\u636e\u95ed\u73af"},{"location":"w0-introduction/0w/#_4","text":"\u6570\u636e\uff1a \u6570\u636e\u5e95\u5c42\uff0c\u539f\u59cb\u6570\u636e\u7684\u6c6a\u6d0b\u5927\u6d77\u3002\u5f62\u6001\uff1a\u6570\u636e\u5e93\u3002\u529f\u80fd\uff1a\u76f4\u63a5\u53d6\u6570 \u4fe1\u606f\uff1a \u57fa\u4e8e\u6570\u636e\u63d0\u70bc\u5f97\u5230\u7684\u6307\u6807\uff0c\u65b0\u5ba2\u6709\u591a\u5c11\uff1f\u8001\u5ba2\u6709\u591a\u5c11\uff1f\u8001\u5ba2\u90fd\u6709\u4ec0\u4e48\u7279\u5f81\uff0c\u65b0\u5ba2\u90fd\u6709\u4ec0\u4e48\u7279\u5f81\uff1f\u5f62\u6001\uff1a\u6c47\u603b\u62a5\u8868\u3002\u529f\u80fd\uff1a\u6307\u6807\u63d0\u4f9b\uff0c\u56de\u7b54\u8fc7\u53bb\u5df2\u7ecf\u53d1\u751f\u4e86\u4ec0\u4e48\u7684\u95ee\u9898\uff0c\u4e1a\u52a1\u4eba\u5458\u8fd0\u7528\u5f97\u5f53\u4e5f\u53ef\u4ee5\u89e3\u51b3\u5f88\u591a\u95ee\u9898\u3002 \u77e5\u8bc6\uff1a \u57fa\u4e8e\u4fe1\u606f\u5efa\u7acb\u5404\u6307\u6807\u4e4b\u95f4\u7684\u5173\u7cfb\u6a21\u578b\u3002\u4ec0\u4e48\u60c5\u51b5\u4e0b\u65b0\u5ba2\u4f1a\u8f6c\u5316\u4e3a\u8001\u5ba2\uff1f\u6a21\u578b\u7ed3\u679c\u3002\u5f62\u6001\uff1a\u6a21\u578b\uff0c\u529f\u80fd\uff1a\u56de\u7b54\u4e3a\u4ec0\u4e48\u7684\u95ee\u9898\uff0c\u89e3\u91ca\u5173\u7cfb\u548c\u56e0\u679c\uff0c\u9884\u6d4b\u672a\u6765 \u667a\u6167\uff1a \u5c06\u77e5\u8bc6\u878d\u5165\u51b3\u7b56\u6d41\u7a0b\uff0c\u5c06\u6a21\u578b\u5d4c\u5165\u4ea7\u54c1\u3002\u6211\u4eec\u8981\u600e\u4e48\u505a\uff0c\u624d\u4f1a\u8ba9\u65b0\u5ba2\u8f6c\u5316\u4e3a\u8001\u5ba2\u3002\u5f62\u6001\uff1a\u6570\u636e\u4ea7\u54c1\u3002\u529f\u80fd\uff1a\u63a7\u5236\u672a\u6765","title":"\u6570\u636e\u5229\u7528\u7684\u56db\u5c42\u5883\u754c"},{"location":"w0-introduction/0w/#_5","text":"\u6570\u636e\u4ea7\u54c1\u5c31\u662f\u7ed9\u51b3\u7b56\u8005\u63d0\u4f9b\u884c\u52a8\u4fe1\u606f\u7684\u8f7d\u4f53\uff0c\u4f8b\u5982 Amazon\u7684\u5546\u54c1\u63a8\u8350 \u5929\u6c14\u9884\u62a5 Stock Market Predictions Production Process Improvements Health Diagnosis Flu Trend Predictions \u6709\u4e9b\u770b\u8d77\u6765\u4e5f\u80fd\u63d0\u4f9b\u51b3\u7b56\u8005\u884c\u52a8\u4fe1\u606f\uff0c\u5982\u9ec4\u5386\uff0c\u661f\u76f8\uff0c\u4f46\u5b83\u4eec\u4e0d\u662f\u57fa\u4e8e\u6570\u636e\u7684\u6d1e\u5bdf\u3002","title":"\u6570\u636e\u4ea7\u54c1"},{"location":"w0-introduction/0w/#_6","text":"\u6570\u636e\u6316\u6398\u4e5f\u79f0\u4e3a\u77e5\u8bc6\u53d1\u73b0\u3002\u662f\u4e00\u4e2a\u53bb\u7c97\u5b58\u7cbe\u3001\u53bb\u4f2a\u5b58\u771f\u7684\u8fc7\u7a0b\u3002\u662f\u4ece\u5927\u91cf\u6570\u636e\u4e2d\u63d0\u53d6\u3001\u5f52\u7eb3\u6709\u7528\u77e5\u8bc6\u7684\u8fc7\u7a0b\u548c\u65b9\u6cd5\u3002\u5c06\u5176\u7528\u4e8e\u51b3\u7b56\uff0c\u53ef\u4ee5\u63d0\u9ad8\u4eba\u7c7b\u7684\u798f\u5229\u3002 \u5f00\u666e\u52d2\u4e09\u5927\u5b9a\u5f8b \u5f00\u666e\u52d2\u7684\u8001\u5e08\u7b2c\u8c37\u6536\u96c6\u4e86\u5927\u91cf\u5929\u6587\u89c2\u6d4b\u6570\u636e\uff0c\u4f46\u5374\u662f\u5f00\u666e\u52d2\u901a\u8fc7\u7814\u7a76\u6570\u636e\u627e\u5230\u80cc\u540e\u7684\u89c4\u5f8b \u51e0\u4e2a\u76f8\u5173\u6982\u5ff5 \u673a\u5668\u5b66\u4e60 \u7edf\u8ba1\u7406\u8bba \u6570\u636e\u79d1\u5b66 \u6a21\u5f0f\u8bc6\u522b \u9700\u8981\u7b97\u6cd5\u5f00\u53d1 \u4e0d\u9700\u8981\u7b97\u6cd5\u5f00\u53d1 \u9700\u8981\u6570\u636e\u5f00\u53d1 \u6570\u636e\u79d1\u5b66\u5bb6 \u6570\u636e\u6316\u6398\u5de5\u7a0b\u5e08 \u4e0d\u9700\u8981\u6570\u636e\u5f00\u53d1 \u7b97\u6cd5\u5de5\u7a0b\u5e08 kaggle\u73a9\u5bb6","title":"\u4e8c\u3001\u4ec0\u4e48\u662f\u6570\u636e\u6316\u6398"},{"location":"w0-introduction/0w/#_7","text":"","title":"\u4e09\u3001\u6570\u636e\u6316\u6398\u548c\u6211\u4eec\u7684\u5173\u7cfb"},{"location":"w0-introduction/0w/#_8","text":"\u5982\u679c\u6ca1\u6709\u6570\u636e\uff0c\u53ef\u4ee5\u7528\u4ec0\u4e48\u51b3\u7b56\uff1f\uff08\u76f4\u89c9\uff0c\u7ecf\u9a8c\u5f52\u7eb3\uff0c\u903b\u8f91\u63a8\u7406\uff0c\u7b97\u547d\uff09 \u9700\u8981\u6570\u636e\uff0c\u56e0\u4e3a\u6570\u636e\u5c31\u662f\u73b0\u5b9e\u4e16\u754c\u7684\u5386\u53f2\u75d5\u8ff9\uff0c\u9700\u8981\u901a\u8fc7\u5404\u79cd\u75d5\u8ff9\u6765\u63a8\u65ad\u672a\u6765\uff0c\u6570\u636e\u5c31\u662f\u5386\u53f2\uff0c\u6316\u6398\u5c31\u662f\u5f52\u7eb3\u3002 \u6570\u636e\u592a\u591a\uff0c\u4eba\u8111\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u3002\u8bb0\u5f55\u7684\u6570\u636e\u8d8a\u6765\u8d8a\u591a\uff0c\u5f62\u5f0f\u548c\u6765\u6e90\u90fd\u8d8a\u6765\u8d8a\u590d\u6742\u3002\u81ea\u7136\u4ea7\u751f\u7684\u6570\u636e\uff0c\u4eba\u7c7b\u793e\u4f1a\u4ea7\u751f\u7684\u6570\u636e\uff08\u793e\u4ea4\u7f51\u7edc\uff0c\u6587\u672c\uff0c\u56fe\u50cf\uff0c\u8bed\u97f3\uff0c\u89c6\u9891......\uff09 \u6240\u4ee5\u9700\u8981\u6316\u6398\u5de5\u5177\u548c\u65b9\u6cd5\u7684\u5e2e\u52a9\u3002","title":"\u4e3a\u4f55\u9700\u8981\u6570\u636e\u6316\u6398\uff1a"},{"location":"w0-introduction/0w/#_9","text":"\u5f53\u524d\u7684\u5404\u9879\u524d\u63d0\u6761\u4ef6\u5df2\u7ecf\u5177\u5907\u3002 \u786c\u4ef6\u4ef7\u683c\u7684\u4e0b\u964d\uff0c\u4f7f\u6570\u636e\u7684\u5b58\u50a8\u548c\u8fd0\u7b97\u6210\u672c\u66f4\u4f4e\u3002 \u4e2a\u4eba\u548c\u521b\u4e1a\u516c\u53f8\u5f97\u4ee5\u8fdb\u5165\u6570\u636e\u9886\u57df\u3002 \u5f00\u6e90\u8f6f\u4ef6\u5de5\u5177\u548c\u516c\u5f00\u8bfe\u5206\u4eab\u4f7f\u8de8\u754c\u66f4\u4e3a\u5bb9\u6613\u3002 \u4e0d\u540c\u5b66\u79d1\u7684\u58c1\u5792\u88ab\u6253\u7834\uff0c\u53ef\u4ee5\u8f83\u4e3a\u5bb9\u6613\u7684\u83b7\u5f97\u5e76\u5b66\u4e60\u5176\u5b83\u5b66\u79d1\u7684\u77e5\u8bc6\u548c\u5de5\u5177\uff0c\u6210\u4e3a\u4e13\u4e1a\u4f59\u4eba\u58eb\u3002","title":"\u6570\u636e\u6316\u6398\u4e3a\u4ec0\u4e48\u706b\uff1f"},{"location":"w0-introduction/0w/#_10","text":"\u4ea7\u54c1\uff1a\u4fa7\u91cd\u4e8e\u5e95\u5c42\u6570\u636e\u6846\u67b6\u642d\u5efa\uff0c\u6570\u636e\u62a5\u8868\u5f00\u53d1\uff0c\u6570\u636e\u4ea7\u54c1\u5f00\u53d1\uff0c\u4f8b\u5982\u6dd8\u5b9d\u7684\u6570\u636e\u9b54\u65b9\u7684\u5f00\u53d1\u5de5\u4f5c\uff0c\u8fd9\u7c7b\u5de5\u4f5c\u9700\u8981\u5f88\u5f3a\u7684\u8f6f\u4ef6\u5f00\u53d1\u80cc\u666f\u3002 \u6a21\u578b\uff1a\u4fa7\u91cd\u4e8e\u5bf9\u6570\u636e\u7684\u7814\u7a76\uff0c\u7528\u7edf\u8ba1\u7406\u8bba\u6216\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5bf9\u6570\u636e\u8fdb\u884c\u5206\u6790\u5efa\u6a21\uff0c\u4f8b\u5982\u5e7f\u544a\u7684\u70b9\u51fb\u7387\u5206\u6790\u5efa\u6a21\uff0c\u8fd9\u7c7b\u5de5\u4f5c\u9700\u8981\u4e30\u5bcc\u7684\u7edf\u8ba1\u7406\u8bba\u548c\u6a21\u578b\u7b97\u6cd5\u77e5\u8bc6\u3002 \u7f8e\u5b66\uff1a\u4fa7\u91cd\u4e8e\u5bf9\u6570\u636e\u7684\u521b\u4f5c\uff0c\u7528WEB\u6280\u672f\u8fdb\u884c\u6570\u636e\u53ef\u89c6\u5316\u6216\u8005\u5236\u4f5c\u4fe1\u606f\u56fe\uff0c\u4f8b\u5982\u536b\u62a5\u7684\u6570\u636e\u7f51\u7ad9\uff0c\u9700\u8981\u5f88\u5f3a\u7684\u53ef\u89c6\u5316\u80fd\u529b\u548c\u524d\u7aef\u6280\u672f\u3002 \u4ef7\u503c\uff1a\u4fa7\u91cd\u4e8e\u6570\u636e\u4e2d\u5305\u542b\u7684\u5546\u4e1a\u4ef7\u503c\u7814\u7a76\uff0c\u5f3a\u8c03\u5bf9\u4e13\u4e1a\u9886\u57df\u7684\u4e1a\u52a1\u7406\u89e3\u548c\u4ea4\u6d41\u6c9f\u901a\uff0c\u4f8b\u5982\u54a8\u8be2\u516c\u53f8\u53d1\u5e03\u7684\u5546\u4e1a\u5206\u6790\u62a5\u544a\uff0c\u9700\u8981\u5e7f\u6cdb\u7684\u4e1a\u52a1\u77e5\u8bc6\u548c\u5546\u4e1a\u654f\u611f\u5ea6\u3002","title":"\u6570\u636e\u6316\u6398\u548c\u8c01\u6253\u4ea4\u9053"},{"location":"w0-introduction/0w/#_11","text":"\u5546\u4e1a\u96f6\u552e \u533b\u7597 \u91d1\u878d \u592a\u7a7a\u63a2\u7d22 \u6587\u5b57\u8bed\u97f3\u8bc6\u522b \u4e0b\u68cb\u3002\u3002\u3002","title":"\u5e94\u7528\u9886\u57df\u6709\u54ea\u4e9b\uff1a"},{"location":"w0-introduction/0w/#_12","text":"\u4e00\u4e2a\u5173\u4e8e\u6a21\u578b\u7684\u6d45\u663e\u4f8b\u5b50\uff1a\u5982\u4f55\u5224\u65ad\u4e00\u4e2a\u672a\u5207\u5f00\u7684\u897f\u74dc\u751c\u4e0d\u751c\uff1f \u53ef\u80fd\u7684\u65b9\u6848\uff1a \u51c6\u5907N\u4e2a\u897f\u74dc \u8bbe\u8ba1M\u4e2a\u53d8\u91cf\u6216\u6307\u6807\uff08\u7279\u5f81\u5de5\u7a0b\uff09\uff0c\u91cd\u91cf\u3001\u82b1\u7eb9\u3001\u830e\u53f6\u7684\u65b0\u9c9c\u7a0b\u5ea6\u3001\u5356\u5bb6\u7684\u4f4d\u7f6e...\uff0c\u5207\u5f00\u524d\u8bb0\u5f55\u8fd9\u4e9b\u6307\u6807\uff0c\u8bb0\u4e3aX\u3002 \u5207\u5f00\u540e\u8ba9n\u4e2a\u4eba\u54c1\u5c1d\u6253\u5206(0\u8868\u793a\u4e0d\u751c\uff0c1\u8868\u793a\u751c)\uff0c\u8bb0\u4e3aY\u3002 \u4f7f\u7528\u5176\u4e2d\u4e00\u90e8\u5206\u6570\u636e\uff0c\u7ed3\u5408\u5206\u7c7b\u7b97\u6cd5\u5bf9X\u548cY\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002 \u7528\u5269\u4e0b\u53e6\u4e00\u90e8\u5206\u6570\u636e\uff0c\u68c0\u67e5\u6a21\u578b\u7684\u6548\u679c\u3002 \u628a\u6a21\u578b\u7684\u903b\u8f91\u5199\u6210\u4e00\u4e2aAPP\u653e\u5230\u5e94\u7528\u5e02\u573a\u4e0a\uff0c\u6301\u7eed\u6536\u83b7\u6570\u636e\uff0c\u6539\u8fdb\u6a21\u578b\u3002 \u4e00\u4e2a\u590d\u6742\u7684\u4f8b\u5b50\uff0c\u5982\u4f55\u5224\u65ad\u4e00\u4e2a\u8001\u4eba\u672a\u6765\u662f\u5426\u4f1a\u5f97\u75f4\u5446\u75c7\uff08\u601d\u8003\u601d\u8003\uff09","title":"\u56db\u3001\u4ec0\u4e48\u662f\u6570\u636e\u6316\u6398\u6a21\u578b\uff1f"},{"location":"w0-introduction/0w/#_13","text":"\u94f6\u884c\u7684\u4fe1\u7528\u5361\u53d1\u653e\uff1a \u94f6\u884c\u5728\u4fe1\u7528\u5361\u53d1\u653e\u7684\u65f6\u5019\u4f1a\u8fdb\u884c\u5ba1\u6838\u3002\u5ba1\u6838\u67d0\u4e2a\u4eba\u7684\u8d44\u683c\u662f\u5426\u7b26\u5408\u6761\u4ef6\u4ee5\u6388\u4fe1\u3002\u5728\u4f20\u7edf\u7684\u5ba1\u6838\u5de5\u4f5c\uff0c\u8fd9\u79cd\u4e8b\u662f\u4eba\u5de5\u6765\u505a\u7684\uff0c\u7533\u8bf7\u4eba\u586b\u4e00\u5f20\u8868\uff0c\u5199\u4e0a\u4e2a\u4eba\u7684\u5e74\u9f84\u3001\u804c\u4e1a\u3001\u6536\u5165\u7b49\u4fe1\u606f\uff08X\u53d8\u91cf\uff09\u3002\u4ea4\u7ed9\u6709\u7ecf\u9a8c\u7684\u94f6\u884c\u98ce\u63a7\u5e08\uff0c\u4ed6\u4eec\u6765\u8fdb\u884c\u8bc4\u4ef7\uff0c\u662f\u53d1\u4fe1\u7528\u5361\uff0c\u8fd8\u662f\u4e0d\u53d1\u4fe1\u7528\u5361\u3002 \u4f46\u5728\u4e92\u8054\u7f51\u65f6\u4ee3\uff0c\u8fd9\u79cd\u4eba\u5de5\u5ba1\u6838\u5c31\u592a\u6162\u4e86\u3002\u4e92\u8054\u7f51\u91d1\u878d\u7684\u5d1b\u8d77\u5c31\u662f\u6700\u660e\u663e\u7684\u8d8b\u52bf\u3002\u5b83\u5c06\u5168\u7f51\u4e2d\u5173\u4e8e\u4e2a\u4eba\u7684\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u6536\u96c6\u6574\u5408\uff0c\u5176\u4e2d\u6709\u5fc5\u7136\u6709\u4e00\u90e8\u5206\u4eba\u5df2\u7ecf\u5728\u91d1\u878d\u673a\u6784\u6709\u8fc7\u501f\u8d37\u884c\u4e3a\uff0c\u8003\u5bdf\u8fd9\u79cd\u884c\u4e3a\u662f\u5426\u6709\u8fdd\u7ea6\uff0c\u5c06\u5176\u4f5c\u4e3aY\u3002\u5c06\u5176\u5b83\u7684\u884c\u4e3a\u6570\u636e\u4f5c\u4e3aX\u3002\u8fd9\u6837\u5c31\u6784\u6210\u4e86\u4e00\u4e2a\u53ef\u4ee5\u5582\u5230\u5206\u7c7b\u7b97\u6cd5\u4e2d\u7684\u6570\u636e\u96c6\u3002\u7136\u540e\u8fd9\u4e2a\u6a21\u578b\u5c31\u53ef\u4ee5\u7528\u5728\u672a\u6765\u7684\u7533\u8bf7\u4eba\u8eab\u4e0a\uff0c\u5f62\u6210\u5ba1\u6838\u81ea\u52a8\u5316\u7cfb\u7edf\u3002 gmail\u5783\u573e\u90ae\u4ef6\u81ea\u52a8\u5206\u7c7b\uff1a \u5982\u679c\u4f60\u70b9\u5f00\u81ea\u5df1\u7684gmail\u90ae\u7bb1\uff0c\u4ed4\u7ec6\u89c2\u5bdf\u4f1a\u53d1\u73b0\u4e00\u4e2a\u5783\u573e\u90ae\u4ef6\u7684\u6807\u7b7e\uff0c\u5b83\u5e73\u65f6\u9ed8\u9ed8\u7684\u4e3a\u4f60\u6321\u4e0b\u5927\u91cf\u7684\u5783\u573e\u4fe1\u606f\uff0c\u800c\u53c8\u4e0d\u53bb\u5e72\u6270\u4f60\uff0c\u5b9e\u5728\u662f\u6570\u636e\u4ea7\u54c1\u7684\u5178\u8303\u3002\u90a3\u4e48\u5982\u679c\u4f60\u6765\u505a\u8fd9\u79cd\u5783\u573e\u90ae\u4ef6\u7684\u81ea\u52a8\u5206\u7c7b\uff0c\u8981\u600e\u4e48\u505a\u7684\u5462\uff1f \u5982\u679c\u6211\u4eec\u8003\u8651\u7b80\u5355\u4e9b\uff0c\u629b\u5f00\u4e00\u5c01\u7535\u90ae\u4e2d\u7684\u5176\u5b83\u4fe1\u606f\uff08\u53d1\u4ef6\u4eba\uff0cIP...\uff09\uff0c\u800c\u53ea\u53d6\u6587\u672c\u4fe1\u606f\u7684\u8bdd\uff0c\u8fd9\u4e2a\u95ee\u9898\u5c31\u8f6c\u4e3a\u4e00\u4e2a\u6587\u672c\u5206\u7c7b\u7684\u95ee\u9898\u3002 \u6587\u672c\u7684\u5206\u6790\u96be\u70b9\u5728\u4e8e\uff1a\u6587\u672c\u4e0d\u662f\u7ed9\u8ba1\u7b97\u673a\u9605\u8bfb\u7684\uff0c\u5b83\u6709\u590d\u6742\u7684\u8bed\u8a00\u7ed3\u6784\uff08\u8bed\u6cd5\u3001\u8bed\u4e49\u3001\u8bed\u7528\uff09\uff0c\u4f46\u8bed\u8a00\u4e2d\u4f9d\u7136\u5b58\u5728\u7edf\u8ba1\u89c4\u5f8b\uff08\u7edf\u8ba1\u8bed\u8a00\u6a21\u578b\uff09\u3002 \u4e00\u4e2a\u7b80\u5355\u7684\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff1a\u5224\u65ad\u4e00\u5c01\u90ae\u4ef6\u662f\u5426\u5783\u573e\u90ae\u4ef6 - \u6536\u96c6N\u4e2a\u90ae\u4ef6 - \u4ece\u90ae\u4ef6\u4e2d\u63d0\u53d6\u6307\u6807\uff08\u5206\u8bcd\uff0c\u7a7a\u95f4\u5411\u91cf\u6a21\u578b\uff09\uff0c\u6784\u6210\u6587\u6863-\u8bcd\u9879\u77e9\u9635 - \u4eba\u5de5\u6807\u6ce8\u8fd9\u4e9b\u90ae\u4ef6\u662f\u5426\u5783\u573e\u90ae\u4ef6 - \u7528\u4e00\u90e8\u5206\u6570\u636e\uff0c\u7ed3\u5408\u7b97\u6cd5\u5bf9X\u548cY\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002 - \u7528\u5269\u4e0b\u7684\u6570\u636e\uff0c\u68c0\u67e5\u6a21\u578b\u7684\u6548\u679c\u3002","title":"\u5546\u4e1a\u4e2d\u4f7f\u7528\u6a21\u578b\u7684\u4f8b\u5b50\uff1a"},{"location":"w0-introduction/0w/#_14","text":"\u9879\u76ee\u8ba8\u8bba\u548c\u89c4\u5212\uff0c\u5c31\u662f\u5f00\u4f1a\u3002\u8fd9\u65b9\u9762\u5de5\u4f5c\u76ee\u7684\u4e3b\u8981\u662f\u660e\u786e\u4e1a\u52a1\u95ee\u9898\u3002\u662f\u4e0d\u662f\u53ef\u4ee5\u505a\uff1f\u5927\u6982\u53ef\u4ee5\u600e\u4e48\u505a\uff1f\u786e\u5b9a\u4e86\u4e1a\u52a1\u95ee\u9898\u4e4b\u540e\uff0c\u9700\u8981\u5c06\u8fd9\u4e2a\u4e1a\u52a1\u95ee\u9898\u7ffb\u8bd1\u6210\u4e00\u4e2a\u6570\u636e\u95ee\u9898\u3002 \u9879\u76ee\u51c6\u5907\uff0c\u51c6\u5907\u5f00\u5de5\u5e72\u6d3b\u4e86\u3002\u8fd9\u65b9\u9762\u662f\u6700\u4e3a\u7e41\u7410\u4e5f\u6700\u5bb9\u6613\u51fa\u9519\u7684\u5730\u65b9\u3002\u9700\u8981\u548c\u6570\u636e\u4ed3\u5e93\u7684\u540c\u5b66\u914d\u5408\u53d6\u5f97\u5fc5\u8981\u7684\u6570\u636e\uff0c\u63a2\u7d22\u7406\u89e3\u6570\u636e\u7684\u4e1a\u52a1\u610f\u4e49\uff0c\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u6839\u636e\u9879\u76ee\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u6574\u7406\u8f6c\u6362\uff0c\u505a\u5927\u91cf\u7684\u7279\u5f81\u5de5\u7a0b\u7684\u5de5\u4f5c\u3002 \u9879\u76ee\u5b9e\u65bd\uff0c\u5373\u6570\u636e\u5efa\u6a21\uff0c\u5f00\u59cb\u62f7\u6253\u6570\u636e\u4e86\u3002\u9009\u62e9\u5c1d\u8bd5\u4e0d\u540c\u7684\u6a21\u578b\u7b97\u6cd5\uff0c\u4ece\u6570\u636e\u4e2d\u5f97\u5230\u9700\u8981\u7684\u7ed3\u679c\uff0c\u7136\u540e\u4ece\u4e0d\u540c\u65b9\u9762\u8bc4\u4ef7\u6548\u679c\u600e\u4e48\u6837\u3002 \u9879\u76ee\u7ed3\u675f\uff0c\u4ea4\u4ed8\u7ed3\u679c\u3002\u786e\u5b9a\u6a21\u578b\u5982\u4f55\u90e8\u7f72\uff0c\u5e76\u5b9e\u65bd\u90e8\u7f72\u5de5\u4f5c\u3002\u8fd9\u79cd\u90e8\u7f72\u5c31\u662f\u6a21\u578b\u7684\u5e94\u7528\uff0c\u591a\u6570\u60c5\u51b5\u4e0b\u662f\u5c06\u7ed3\u679c\u56de\u5199\u5230\u6570\u636e\u5e93\u4e2d\u3002\u540c\u65f6\u7ed3\u679c\u4ea4\u4ed8\u7ed9\u9700\u6c42\u65b9\uff0c\u5199\u6700\u7ec8\u7684\u9879\u76ee\u62a5\u544a\uff0c\u5f52\u6863\u6240\u6709\u6587\u4ef6\u3002 \u9605\u8bfb\u6587\u732e\uff0c\u65b9\u6cd5\u7814\u7a76\u3002\u5728\u6bd4\u8f83\u7a7a\u95f2\u7684\u65f6\u95f4\uff0c\u6216\u8005\u9047\u5230\u96be\u9898\u7684\u65f6\u5019\uff0c\u90fd\u9700\u8981\u53bb\u627e\u5de8\u4eba\u7684\u80a9\u8180\u4f9d\u9760\u4e00\u4e0b\u3002","title":"\u4e94\u3001\u6570\u636e\u6316\u6398\u65e5\u5e38\u5de5\u4f5c\u6709\u54ea\u4e9b"},{"location":"w0-introduction/0w/#_15","text":"\u5546\u4e1a\u7406\u89e3\uff1a\u7406\u89e3\u4e1a\u52a1\u76ee\u6807\u548c\u9700\u6c42\uff0c\u5e76\u8f6c\u5316\u4e3a\u6570\u636e\u6316\u6398\u53ef\u7406\u89e3\u7684\u95ee\u9898\u5b9a\u4e49\u3002\u5efa\u6a21\u5e08\u4f1a\u53c2\u52a0\u4e1a\u52a1\u7ec4\u7684\u4f1a\u8bae\uff0c\u4e3b\u8981\u662f\u4e86\u89e3\u6536\u96c6\u4e1a\u52a1\u9700\u6c42\u3002 \u6570\u636e\u7406\u89e3\uff1a\u7b5b\u9009\u76ee\u6807\u6570\u636e\uff0c\u68c0\u9a8c\u6570\u636e\u8d28\u91cf\uff0c\u63a2\u7d22\u6570\u636e\u7279\u5f81\uff0c\u8bc4\u4f30\u53ef\u7528\u6570\u636e\u3002\u5efa\u6a21\u5e08\u4f1a\u5c06\u4e00\u4e9b\u521d\u6b65\u7ed3\u679c\u5448\u73b0\u7ed9\u4e1a\u52a1\u7ec4\uff0c\u5f97\u5230\u8fdb\u4e00\u6b65\u53cd\u9988\u3002 \u6570\u636e\u51c6\u5907\uff1a\u901a\u8fc7\u6e05\u6d17\uff0c\u96c6\u6210\uff0c\u53d8\u6362\uff0c\u5f52\u7ea6\u7b49\u5904\u7406\u65b9\u6cd5\u6784\u9020\u6700\u7ec8\u6570\u636e\u96c6\u5408\u3002\u5efa\u6a21\u5e08\u5f00\u59cb\u75af\u72c2\u7684\u5199SQL\u7c7b\u7684\u811a\u672c\u53bb\u6d17\u6570\u636e\u3002 \u6a21\u578b\u5efa\u7acb\uff1a\u9009\u62e9\u548c\u5e94\u7528\u5404\u79cd\u673a\u5668\u5b66\u4e60\u6216\u7edf\u8ba1\u65b9\u6cd5\u3001\u6784\u5efa\u6a21\u578b\u5e76\u8c03\u6821\u5404\u79cd\u53c2\u6570\u3002\u5efa\u6a21\u5e08\u8fdb\u5165\u70bc\u4e39\u9636\u6bb5\uff0c\u671f\u5f85\u80fd\u6709\u597d\u7684\u7ed3\u679c\u3002 \u6a21\u578b\u8bc4\u4ef7\uff1a\u7ed3\u5408\u6700\u521d\u7684\u5546\u4e1a\u76ee\u6807\u8bc4\u4ef7\u5e76\u89e3\u91ca\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u53ef\u80fd\u7684\u5546\u4e1a\u6548\u679c\u3002\u5efa\u6a21\u5e08\u5c06\u6a21\u578b\u7ed3\u679c\u548c\u4e1a\u52a1\u56e2\u961f\u8fdb\u884c\u6c9f\u901a\u3002 \u6a21\u578b\u90e8\u7f72\uff1a\u6309\u7528\u6237\u4e60\u60ef\u65b9\u5f0f\u5b9e\u65bd\u5e76\u53d1\u5e03\u6a21\u578b\uff0c\u63d0\u4f9b\u5206\u6790\u7ed3\u8bba\uff0c\u5e76\u6301\u7eed\u8ddf\u8e2a\u3002\u5efa\u6a21\u5e08\u5c06\u6a21\u578b\u4e0a\u7ebf\uff0c\u76d1\u6d4b\u6027\u80fd\u3002","title":"\u4e00\u4e2a\u5178\u578b\u7684\u6b65\u9aa4\u6d41\u7a0b"},{"location":"w0-introduction/0w/#_16","text":"\u5206\u7c7b Classification [Predictive] \u805a\u7c7b Clustering [Descriptive] \u5173\u8054\u89c4\u5219 Association Rule Discovery [Descriptive] \u5e8f\u5217\u6316\u6398 Sequential Pattern Discovery [Descriptive] \u56de\u5f52 Regression [Predictive] \u5f02\u5e38\u68c0\u6d4b Deviation Detection [Predictive]","title":"\u516d\u3001\u6570\u636e\u6316\u6398\u7684\u4efb\u52a1\u6a21\u5f0f"},{"location":"w0-introduction/0w/#_17","text":"","title":"\u5206\u7c7b\u65b9\u6cd5\u7684\u5e94\u7528"},{"location":"w0-introduction/0w/#_18","text":"\u95ee\u9898: \u51c6\u5907\u53d1\u552eiphone\u65b0\u54c1\u4e86\uff0c\u54ea\u4e9b\u7528\u6237\u53ef\u80fd\u4f1a\u4e70\uff1f \u65b9\u6cd5: \u627e\u5230\u76f8\u4f3c\u4ea7\u54c1\u7684\u7528\u6237\u884c\u4e3a\u6570\u636e\uff0c\u89c2\u5bdf\u662f\u5426\u8d2d\u4e70\u4f5c\u4e3a\u76ee\u6807\u53d8\u91cf \u6536\u96c6\u8fd9\u4e9b\u7528\u6237\u7684\u884c\u4e3a\u7279\u5f81\uff0c\u4f5c\u4e3a\u6a21\u578b\u7684\u89e3\u91ca\u53d8\u91cf","title":"\u7cbe\u51c6\u5316\u8425\u9500"},{"location":"w0-introduction/0w/#_19","text":"","title":"\u805a\u7c7b\u65b9\u6cd5\u7684\u5e94\u7528"},{"location":"w0-introduction/0w/#_20","text":"\u95ee\u9898: \u5982\u4f55\u5c06\u7528\u6237\u5206\u6210\u82e5\u5e72\u7ec4\uff0c\u7136\u540e\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7ec4\u8fdb\u884c\u8425\u9500\u6d3b\u52a8 \u65b9\u6cd5: \u6536\u96c6\u7528\u6237\u7684\u57fa\u672c\u793e\u4f1a\u7279\u5f81\u548c\u884c\u4e3a\u7279\u5f81 \u6839\u636e\u7528\u6237\u7684\u76f8\u4f3c\u7a0b\u5ea6\u8fdb\u884c\u805a\u7c7b\u5206\u7ec4 \u6839\u636e\u540c\u7ec4\u7684\u7528\u6237\u8d2d\u4e70\u884c\u4e3a\u5224\u65ad\u5206\u7fa4\u7684\u6548\u679c","title":"\u7528\u6237\u5206\u7fa4:"},{"location":"w0-introduction/0w/#_21","text":"","title":"\u5173\u8054\u89c4\u5219\u5e94\u7528"},{"location":"w0-introduction/0w/#_22","text":"\u5173\u8054\u89c4\u5219\u662f\u4e00\u79cd\u89c4\u5f8b {\u9762\u5305\u5e72, \u2026 } -- {\u85af\u7247} \u4e70\u4e86\u9762\u5305\u5e72\u7684\u7528\u6237\u5f80\u5f80\u4e5f\u4f1a\u53bb\u4e70\u85af\u7247\uff0c\u53ef\u4ee5\u5229\u7528\u8fd9\u79cd\u5173\u8054\u89c4\u5219\uff0c\u5c06\u82e5\u5e72\u5546\u54c1\u4e00\u8d77\u653e\u5728\u8d27\u67b6\u4e0a\u6346\u7ed1\u9500\u552e \u4e0d\u540c\u7684\u5173\u8054\u5546\u54c1\uff0c\u6697\u793a\u4e86\u4e0d\u540c\u7684\u6d88\u8d39\u573a\u666f","title":"\u76f8\u4f3c\u5546\u54c1\u63a8\u8350"},{"location":"w0-introduction/0w/#_23","text":"\u4ece\u6b63\u5e38\u884c\u4e3a\u4e2d\u53d1\u73b0\u4e0d\u6b63\u5e38\u7684\u884c\u4e3a\u6a21\u5f0f \u4fe1\u7528\u5361\u6b3a\u8bc8 \u7f51\u7edc\u5165\u4fb5","title":"\u5f02\u5e38\u68c0\u6d4b\u5e94\u7528"},{"location":"w0-introduction/0w/#_24","text":"\u6709\u5f62\u7684\u6280\u80fd: \u7406\u8bba\uff1a\u6c14\u5b97\u3002 \u4f8b\u5982\u7edf\u8ba1\u7406\u8bba\u3001\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u3002\u4e2a\u4eba\u4f53\u4f1a\u7cbe\u901a\u7406\u8bba\u540e\u518d\u505a\u6570\u636e\u5de5\u4f5c\u5c31\u5982\u6c64\u6cfc\u96ea\u3002\u6211\u4e5f\u627f\u8ba4\u5b66\u4e60\u7406\u8bba\u662f\u8270\u96be\u7684\uff0c\u4f46\u662f\u4e00\u5b9a\u8981\u5728\u5e74\u8f7b\u7684\u65f6\u5019\u8bfb\u6700\u96be\u7684\u4e66\u3002\u300a\u6570\u5b66\u4e4b\u7f8e\u300b\u4e2d\u8c08\u5230\uff0c\u6280\u672f\u5206\u4e3a\u672f\u548c\u9053\u4e24\u79cd\uff0c\u5177\u4f53\u7684\u505a\u4e8b\u65b9\u6cd5\u662f\u672f\uff0c\u505a\u4e8b\u7684\u539f\u7406\u548c\u539f\u5219\u662f\u9053\uff0c\u53ea\u8ffd\u6c42\u672f\u7684\u4eba\u5de5\u4f5c\u5f88\u8f9b\u82e6\uff0c\u53ea\u6709\u638c\u63e1\u4e86\u9053\u624d\u80fd\u6c38\u8fdc\u6e38\u5203\u6709\u4f59\u3002 \u5de5\u5177\uff1a\u5251\u5b97\u3002\u7406\u8bba\u4e0d\u7528\u5728\u4ea7\u54c1\u4e0a\u5c31\u662f\u738b\u8bed\u7109\u7684\u5b66\u9662\u6d3e\u3002\u4ece\u7406\u8bba\u5230\u4ea7\u54c1\uff0c\u9700\u8981\u638c\u63e1\u5404\u79cd\u5de5\u5177\u3002\u8fd9\u7c7b\u5de5\u5177\u7528\u5f97\u719f\u4e86\u80fd\u4e8b\u534a\u529f\u500d\uff0c\u4f8b\u5982R\u3001python\u3001SQL\u3001hadoop\u8fd9\u7c7b\u3002\u5b66\u4e60\u5de5\u5177\u548c\u5b66\u4e60\u8bed\u8a00\u4e00\u6837\uff0c\u90fd\u8981\u591a\u8bfb\u591a\u5199\uff0c\u6a21\u4eff\u63e3\u6469\uff0c\u5c31\u53ef\u4ee5\u8fd0\u7528\u81ea\u5982\u3002\u4e0d\u8fc7\u8ff7\u4fe1\u5de5\u5177\u662f\u6ca1\u6709\u610f\u4e49\u7684\uff0c\u6ca1\u6709\u6700\u597d\u7684\u5de5\u5177\uff0c\u53ea\u6709\u6700\u5408\u9002\u7684\u5de5\u5177\u3002\u5982\u679c\u4f60\u662f\u72ec\u5b64\u6c42\u8d25\uff0c\u53ef\u4ee5\u73a9\u7384\u94c1\u5251\uff0c\u5982\u679c\u4f60\u662f\u4e1c\u65b9\u4e0d\u8d25\uff0c\u53ef\u4ee5\u73a9\u7ee3\u82b1\u9488\u3002 \u7ecf\u9a8c\uff1a\u5b9e\u6218\u3002\u6709\u5185\u529b\u6709\u5251\u6cd5\uff0c\u5c31\u9700\u8981\u4e0b\u5c71\u4e86\u3002\u5bf9\u6218\u6700\u5f3a\u608d\u7684\u5bf9\u624b\uff0c\u624d\u80fd\u8ba9\u4f60\u7684\u5185\u529b\u5251\u6cd5\u878d\u4e3a\u4e00\u4f53\u3002\u505a\u9879\u76ee\uff0c\u5728\u5de5\u4f5c\u89e3\u51b3\u96be\u9898\uff0c\u624d\u662f\u957f\u8fdb\u6700\u5feb\u7684\u3002 \u65e0\u5f62\u7684\u6c14\u8d28: \u597d\u5947\uff0c\u597d\u5947\u5fc3\u548c\u5174\u8da3\u662f\u4ece\u6570\u636e\u4e2d\u5f97\u5230\u6d1e\u5bdf\u7684\u9a71\u52a8\u529b\u3002\u6709\u597d\u5947\u5fc3\u7684\u4eba\u624d\u4f1a\u5bf9\u6570\u636e\u6709\u6301\u7eed\u7684\u70ed\u60c5\u3002 \u521b\u9020\uff0c\u5175\u65e0\u5e38\u52bf\uff0c\u6570\u636e\u7684\u5de5\u4f5c\u90fd\u662f\u5343\u5dee\u4e07\u522b\u7684\uff0c\u867d\u7136\u53ef\u4ee5\u4f9d\u9760\u4e00\u4e9b\u8001\u7684\u7ecf\u9a8c\u505a\u4e9b\u7167\u732b\u753b\u864e\u7684\u4e8b\u3002\u4f46\u6700\u597d\u8fd8\u662f\u9700\u8981\u6839\u636e\u4e0d\u540c\u7684\u9879\u76ee\u60c5\u51b5\u6765\u505a\u51fa\u5224\u65ad\u3002\u72ec\u7acb\u601d\u8003\u548c\u521b\u9020\u8ba9\u4f60\u8d70\u5f97\u66f4\u8fdc\u3002 \u6c42\u8d25\uff0c\u521b\u9020\u3001\u524d\u6cbf\u3001\u63a2\u7d22\u6027\u7684\u5de5\u4f5c\uff0c\u4e00\u5b9a\u4f1a\u6709\u5931\u8d25\uff0c\u5feb\u901f\u5931\u8d25\uff0c\u5feb\u901f\u5b66\u4e60\uff0c\u4e0d\u65ad\u4fee\u6b63\uff0c\u80fd\u591f\u8d25\u4e2d\u6c42\u80dc\u3002","title":"\u4e03\u3001\u9700\u8981\u638c\u63e1\u54ea\u4e9b\u6280\u80fd"},{"location":"w0-introduction/0w/#_25","text":"\u4e24\u4e2a\u5b57\uff1a\u81ea\u5b66\u3002 \u201c\u77e5\u8bc6\u4e0e\u8010\u5fc3\uff0c\u662f\u51fb\u8d25\u5f3a\u8005\u7684\u552f\u4e00\u65b9\u6cd5\u3002\u201d \u901a\u8fc7\u9605\u8bfb\u6765\u5b66\u4e60\u3002\u5305\u62ec\u4e86\u9605\u8bfb\u7ecf\u5178\u7684\u7406\u8bba\u6559\u6750\u3001\u4ee3\u7801\u3001\u8bba\u6587\u3001\u4e0a\u516c\u5f00\u8bfe\u3002 \u901a\u8fc7\u725b\u4eba\u6765\u5b66\u4e60\u3002\u5305\u62ec\u540c\u884c\u7684\u805a\u4f1a\u3001\u8ba8\u8bba\u3001\u5927\u725b\u7684\u535a\u5ba2\u3001\u5fae\u535a\u3001twitter\u3001RSS\u3002 \u901a\u8fc7\u7ec3\u4e60\u6765\u5b66\u4e60\u3002\u5305\u62ec\u4ee3\u7801\u7ec3\u4e60\u9898\u3001\u53c2\u52a0kaggle\u6bd4\u8d5b\u3001\u89e3\u51b3\u5b9e\u9645\u5de5\u4f5c\u4e2d\u7684\u96be\u9898\u3002 \u901a\u8fc7\u5206\u4eab\u6765\u5b66\u4e60\u3002\u5305\u62ec\u81ea\u5df1\u5199\u7b14\u8bb0\u3001\u5199\u535a\u5ba2\u3001\u548c\u540c\u4e8b\u5206\u4eab\u4ea4\u6d41\u3001\u57f9\u8bad\u65b0\u4eba\u3002","title":"\u5982\u4f55\u57f9\u517b\u6570\u636e\u6316\u6398\u7684\u6280\u80fd"},{"location":"w0-introduction/0w/#_26","text":"","title":"\u516b\u3001\u7ecf\u9a8c\u4e4b\u8c08"},{"location":"w0-introduction/0w/#_27","text":"\u5efa\u6a21\u8fc7\u7a0b\u7684\u95ee\u9898 \u7f3a\u4e4f\u4e1a\u52a1\u95ee\u9898\u7684\u6c9f\u901a\u548c\u7406\u89e3 \u53ea\u5173\u6ce8\u8bad\u7ec3\u6570\u636e\u6216\u53ea\u8fc7\u4e8e\u76f8\u4fe1\u6570\u636e \u53ea\u4f9d\u8d56\u4e8e\u4e00\u79cd\u6280\u672f \u9519\u8bef\u7684\u53d8\u91cf\u8f93\u5165 \u6316\u6398\u7ed3\u679c\u4e0d\u771f\u5b9e \u6a21\u578b\u7ed3\u679c\u4e0d\u4ee3\u8868\u4efb\u4f55\u89c4\u5f8b \u6a21\u578b\u8bad\u7ec3\u96c6\u53ef\u80fd\u4e0d\u53cd\u6620\u771f\u6b63\u7684\u603b\u4f53 \u6570\u636e\u7684\u8be6\u7ec6\u7a0b\u5ea6\u6709\u8bef \u6316\u6398\u7ed3\u679c\u6ca1\u6709\u7528 \u6316\u6398\u7ed3\u679c\u4f17\u6240\u5468\u77e5 \u6316\u6398\u7ed3\u679c\u4e0d\u53ef\u7528\u4e8e\u51b3\u7b56","title":"\u5efa\u6a21\u4e2d\u7684\u5751"},{"location":"w0-introduction/0w/#_28","text":"\u63d0\u95ee\u9898\u6bd4\u56de\u7b54\u95ee\u9898\u66f4\u91cd\u8981 \u4e00\u4e2a\u5177\u4f53\u7684\u4e1a\u52a1\u75db\u70b9\u662f\u6570\u636e\u6316\u6398\u7684\u8d77\u70b9\uff0c\u7cbe\u5fc3\u8ba1\u5212\u6d41\u7a0b\u6b65\u9aa4\uff0c\u4e1a\u52a1\u77e5\u8bc6\u8d2f\u7a7f\u6316\u6398\u5efa\u6a21\u7684\u6bcf\u4e2a\u9636\u6bb5\u3002 \u5bf9\u6570\u636e\u6301\u8c28\u614e\u7684\u6001\u5ea6 \u6570\u636e\u5f88\u53ef\u80fd\u51fa\u9519\uff0c\u6570\u636e\u6574\u7406\u5360\u636e\u5927\u90e8\u5206\u7684\u5de5\u4f5c\u65f6\u95f4\u3002 \u6570\u636e\u672c\u8eab\u4ec5\u80fd\u7528\u4e8e\u63cf\u8ff0\u5386\u53f2 \u4e0d\u80fd\u5c55\u73b0\u56e0\u679c\uff0c\u4e5f\u4e0d\u80fd\u9884\u77e5\u672a\u6765\u3002 \u6570\u636e\u4ef7\u503c\u4f53\u73b0\u5728\u843d\u5730\u5e94\u7528 \u6570\u636e\u6316\u6398\u4ef7\u503c\u5e76\u4e0d\u53d6\u51b3\u4e8e\u6a21\u578b\u7684\u51c6\u786e\u6216\u7a33\u5b9a\uff0c\u53d6\u51b3\u4e8e\u80cc\u540e\u7684\u51b3\u7b56\u7ec4\u7ec7\u3002 \u4e0d\u540c\u7684\u6307\u6807\u548c\u6a21\u578b\u90fd\u6709\u5176\u9002\u7528\u8303\u56f4 \u968f\u65f6\u95f4\u73af\u5883\u53d8\u5316\uff0c\u6240\u6709\u7684\u6a21\u5f0f\u90fd\u4f1a\u6539\u53d8\uff0c\u4e0d\u65ad\u5c1d\u8bd5\uff0c\u4e0d\u65ad\u4fee\u6b63\u3002 \u5de5\u4f5c\u4e2d\u7684\u6587\u6863\u5316\u548c\u81ea\u52a8\u5316","title":"\u4e00\u70b9\u5fc3\u5f97"},{"location":"w0-introduction/0w/#_29","text":"\u95ee\u9898\u5b9a\u4e49 \u6570\u636e\u63a2\u7d22 \u7279\u5f81\u5de5\u7a0b \u5efa\u6a21\u548c\u8bc4\u4f30","title":"\u4e00\u4e2a\u7efc\u5408\u6848\u4f8b\u7684\u5efa\u6a21\u6b65\u9aa4"},{"location":"w0-introduction/0w/#_30","text":"\u6211\u4eec\u4f7f\u7528kaggle\u4e0a\u4e00\u4e2a\u7ecf\u5178\u7684\u95ee\u9898\u505a\u4e3a\u6848\u4f8b\u793a\u8303\uff0c\u5373\u5224\u65ad\u4e00\u4e2a\u8d37\u6b3e\u8005\u5728\u540e\u7eed\u4e24\u5e74\u5185\u662f\u5426\u4f1a\u8fdd\u7ea6\u7684\u6982\u7387\u3002","title":"\u95ee\u9898\u5b9a\u4e49"},{"location":"w0-introduction/0w/#_31","text":"\u635f\u5931\u662f\u5982\u4f55\u53d1\u751f\u7684? \u8fdd\u7ea6\u7684\u4eba\u6709\u54ea\u4e9b\u7279\u70b9? \u8fdd\u7ea6\u7684\u5360\u6bd4\u6709\u591a\u5c11\uff1f \u5982\u4f55\u80fd\u6539\u5584\u6211\u4eec\u7684\u635f\u5931\uff1f","title":"\u9700\u8981\u601d\u8003\u7684\u5173\u952e\u95ee\u9898"},{"location":"w0-introduction/0w/#_32","text":"http://www.kaggle.com/c/GiveMeSomeCredit","title":"\u6570\u636e\u4e0b\u8f7d"},{"location":"w0-introduction/0w/#_33","text":"SeriousDlqin2yrs \u7528\u6237\u5728\u540e\u7eed\u4e24\u5e74\u5185\u51fa\u73b090\u5929\u4ee5\u4e0a\u7684\u8fd8\u6b3e\u903e\u671f\uff0c\u8fd9\u662f\u76ee\u6807\u53d8\u91cf\uff0c\u4ee5\u4e0b\u90fd\u662f\u89e3\u91ca\u53d8\u91cf RevolvingUtilizationOfUnsecuredLines \u4fe1\u7528\u5361\u501f\u6b3e\u5360\u6bd4 age \u501f\u6b3e\u4eba\u5e74\u9f84 NumberOfTime30-59DaysPastDueNotWorse \u8fc7\u53bb\u6709\u903e\u671f30-59\u5929\u8fd8\u6b3e\u7684\u6b21\u6570 DebtRatio \u6708\u5ea6\u751f\u6d3b\u6210\u672c\u5360\u6708\u6536\u5165\u7684\u6bd4\u7387 MonthlyIncome \u6708\u6536\u5165 NumberOfOpenCreditLinesAndLoans \u5305\u62ec\u8f66\u8d37\u623f\u8d37\u5728\u5185\u7684\u8d37\u6b3e\u7b14\u6570 NumberOfTimes90DaysLate \u8fc7\u53bb\u6709\u903e\u671f90\u5929\u4ee5\u4e0a\u8fd8\u6b3e\u7684\u6b21\u6570 NumberRealEstateLoansOrLines \u623f\u8d37\u7684\u4fe1\u8d37\u6b21\u6570 NumberOfTime60-89DaysPastDueNotWorse \u8fc7\u53bb\u6709\u903e\u671f60-89\u5929\u8fd8\u6b3e\u7684\u6b21\u6570 NumberOfDependents \u5bb6\u5ead\u4e2d\u9700\u8981\u629a\u517b\u8005\u7684\u4eba\u6570 import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline df = pd.read_csv( data/credit-training.csv ) #\u8bfb\u53d6\u6570\u636e df.head() SeriousDlqin2yrs RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents 0 1 0.766127 45 2 0.802982 9120.0 13 0 6 0 2.0 1 0 0.957151 40 0 0.121876 2600.0 4 0 0 0 1.0 2 0 0.658180 38 1 0.085113 3042.0 2 1 0 0 0.0 3 0 0.233810 30 0 0.036050 3300.0 5 0 0 0 0.0 4 0 0.907239 49 1 0.024926 63588.0 7 0 1 0 0.0 df.shape # \u4e00\u517115\u4e07\u6761\u8bb0\u5f55 (150000, 11)","title":"\u53d8\u91cf\u7684\u610f\u4e49"},{"location":"w0-introduction/0w/#_34","text":"df.info() # \u6709\u7f3a\u5931\u503c class 'pandas.core.frame.DataFrame' RangeIndex: 150000 entries, 0 to 149999 Data columns (total 11 columns): SeriousDlqin2yrs 150000 non-null int64 RevolvingUtilizationOfUnsecuredLines 150000 non-null float64 age 150000 non-null int64 NumberOfTime30-59DaysPastDueNotWorse 150000 non-null int64 DebtRatio 150000 non-null float64 MonthlyIncome 120269 non-null float64 NumberOfOpenCreditLinesAndLoans 150000 non-null int64 NumberOfTimes90DaysLate 150000 non-null int64 NumberRealEstateLoansOrLines 150000 non-null int64 NumberOfTime60-89DaysPastDueNotWorse 150000 non-null int64 NumberOfDependents 146076 non-null float64 dtypes: float64(4), int64(7) memory usage: 12.6 MB df.describe() # \u63cf\u8ff0\u6027\u7edf\u8ba1 SeriousDlqin2yrs RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents count 150000.000000 150000.000000 150000.000000 150000.000000 150000.000000 1.202690e+05 150000.000000 150000.000000 150000.000000 150000.000000 146076.000000 mean 0.066840 6.048438 52.295207 0.421033 353.005076 6.670221e+03 8.452760 0.265973 1.018240 0.240387 0.757222 std 0.249746 249.755371 14.771866 4.192781 2037.818523 1.438467e+04 5.145951 4.169304 1.129771 4.155179 1.115086 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000e+00 0.000000 0.000000 0.000000 0.000000 0.000000 25% 0.000000 0.029867 41.000000 0.000000 0.175074 3.400000e+03 5.000000 0.000000 0.000000 0.000000 0.000000 50% 0.000000 0.154181 52.000000 0.000000 0.366508 5.400000e+03 8.000000 0.000000 1.000000 0.000000 0.000000 75% 0.000000 0.559046 63.000000 0.000000 0.868254 8.249000e+03 11.000000 0.000000 2.000000 0.000000 1.000000 max 1.000000 50708.000000 109.000000 98.000000 329664.000000 3.008750e+06 58.000000 98.000000 54.000000 98.000000 20.000000 df.SeriousDlqin2yrs.value_counts() 0 139974 1 10026 Name: SeriousDlqin2yrs, dtype: int64 df.SeriousDlqin2yrs.mean() # \u5e73\u5747\u4e25\u91cd\u62d6\u6b20 0.06684 df.NumberOfDependents.unique() # \u53d7\u629a\u517b\u8005, \u67e5\u770b\u6709\u591a\u5c11\u53d6\u503c array([ 2., 1., 0., nan, 3., 4., 5., 6., 8., 7., 20., 10., 9., 13.]) df.NumberOfDependents.value_counts() # \u89c2\u5bdf\u9891\u6570\u8868\uff0c\u4e00\u534a\u90fd\u6ca1\u6709, \u6709\u7684\u8bdd\u96c6\u4e2d1-3\u4e2a, \u518d\u5f80\u4e0a\u8f83\u5c11 0.0 86902 1.0 26316 2.0 19522 3.0 9483 4.0 2862 5.0 746 6.0 158 7.0 51 8.0 24 10.0 5 9.0 5 20.0 1 13.0 1 Name: NumberOfDependents, dtype: int64 df.groupby( SeriousDlqin2yrs ).mean() RevolvingUtilizationOfUnsecuredLines age NumberOfTime30-59DaysPastDueNotWorse DebtRatio MonthlyIncome NumberOfOpenCreditLinesAndLoans NumberOfTimes90DaysLate NumberRealEstateLoansOrLines NumberOfTime60-89DaysPastDueNotWorse NumberOfDependents SeriousDlqin2yrs 0 6.168855 52.751375 0.280109 357.151168 6747.837774 8.493620 0.135225 1.020368 0.126666 0.743417 1 4.367282 45.926591 2.388490 295.121066 5630.826493 7.882306 2.091362 0.988530 1.828047 0.948208 pd.value_counts(df.NumberOfDependents).plot(kind='bar'); pd.crosstab(df.NumberOfTimes90DaysLate, df.SeriousDlqin2yrs) # \u8ba1\u7b97\u4ea4\u53c9\u9891\u6570\u8868 SeriousDlqin2yrs 0 1 NumberOfTimes90DaysLate 0 135108 6554 1 3478 1765 2 779 776 3 282 385 4 96 195 5 48 83 6 32 48 7 7 31 8 6 15 9 5 14 10 3 5 11 2 3 12 1 1 13 2 2 14 1 1 15 2 0 17 0 1 96 1 4 98 121 143 pd.crosstab(df.age, df.NumberOfDependents) # \u5e74\u9f84\u4e0e\u53d7\u629a\u517b\u8005\u7684\u5173\u7cfb # \u5c81\u6570\u8d8a\u5927\u629a\u517b\u8005\u4f1a\u591a NumberOfDependents 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 10.0 13.0 20.0 age 0 0 0 1 0 0 0 0 0 0 0 0 0 0 21 148 3 1 0 0 0 0 0 0 0 0 0 0 22 385 7 2 2 0 0 0 0 0 0 0 0 0 23 550 33 13 3 0 0 0 0 0 0 0 0 0 24 689 48 19 3 1 0 0 0 0 0 0 0 0 25 774 91 31 7 5 1 0 0 0 0 0 0 0 26 946 128 56 14 4 0 0 0 0 0 0 0 0 27 1001 192 53 32 4 0 0 0 0 0 0 0 0 28 1142 210 114 45 8 1 0 0 0 0 0 0 0 29 1195 254 145 53 14 1 1 0 0 0 0 0 0 30 1337 288 178 72 14 7 1 1 0 0 0 0 0 31 1314 344 245 91 18 4 0 0 0 0 0 0 0 32 1207 380 275 106 53 3 0 0 0 0 0 0 0 33 1254 449 315 140 50 10 2 0 0 0 0 0 0 34 1152 389 360 145 65 11 1 0 0 0 0 0 0 35 1136 418 398 184 58 15 3 1 1 0 0 0 0 36 1139 445 486 202 50 15 5 1 0 0 0 0 0 37 1088 488 541 274 77 18 2 0 0 1 0 0 0 38 1113 484 604 283 84 26 3 0 1 1 0 0 0 39 1202 544 691 383 111 27 4 1 1 0 1 0 0 40 1185 570 758 391 104 43 5 1 1 0 0 0 1 41 1193 517 791 445 113 30 4 0 1 0 0 0 0 42 1159 526 755 427 142 38 7 2 0 0 0 0 0 43 1205 556 789 431 156 33 9 4 1 0 1 0 0 44 1209 574 854 461 133 32 7 2 1 0 1 0 0 45 1363 608 828 457 165 35 11 3 0 0 0 0 0 46 1373 695 876 498 165 35 8 8 2 0 0 0 0 47 1439 672 863 491 148 46 9 0 2 0 1 0 0 48 1509 771 791 459 157 37 11 3 1 2 0 0 0 49 1632 717 790 439 143 46 8 6 1 0 0 0 0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... 74 1164 184 23 4 2 0 0 1 1 0 0 0 0 75 981 167 17 3 0 0 0 0 0 0 0 0 0 76 933 165 16 0 0 0 0 0 0 0 0 0 0 77 890 129 11 2 0 0 0 0 0 0 0 0 0 78 837 137 13 2 1 0 0 0 0 0 0 0 0 79 770 131 10 4 0 0 0 0 0 0 0 0 0 80 704 109 7 2 0 0 0 0 0 0 0 0 0 81 629 76 6 1 0 0 0 0 0 0 0 0 0 82 527 69 1 0 1 0 0 0 0 0 0 0 0 83 417 44 2 0 0 0 0 0 0 0 0 0 0 84 406 32 1 0 0 0 0 0 0 0 0 0 0 85 403 30 2 0 0 0 0 0 0 0 0 0 0 86 318 46 2 0 1 0 0 0 0 0 0 0 0 87 279 29 1 1 0 0 0 0 0 0 0 0 0 88 244 20 1 0 0 0 0 0 0 0 0 0 0 89 230 11 1 0 0 0 0 0 0 0 0 0 0 90 148 19 0 0 0 0 0 0 0 0 0 0 0 91 119 10 0 0 0 0 0 0 0 0 0 0 0 92 75 7 0 0 0 0 0 0 0 0 0 0 0 93 67 3 0 0 0 0 0 0 0 0 0 0 0 94 33 2 0 0 0 0 0 0 0 0 0 0 0 95 33 4 0 0 0 0 0 0 0 0 0 0 0 96 12 2 0 0 0 0 0 0 0 0 0 0 0 97 11 1 0 0 0 0 0 0 0 0 0 0 0 98 5 0 0 0 0 0 0 0 0 0 0 0 0 99 5 0 0 0 0 0 0 0 0 0 0 0 0 101 3 0 0 0 0 0 0 0 0 0 0 0 0 102 2 1 0 0 0 0 0 0 0 0 0 0 0 103 3 0 0 0 0 0 0 0 0 0 0 0 0 107 1 0 0 0 0 0 0 0 0 0 0 0 0 84 rows \u00d7 13 columns","title":"\u6570\u636e\u63a2\u7d22"},{"location":"w0-introduction/0w/#_35","text":"import re # \u5c06\u540d\u5b57\u90fd\u6539\u4e3a snake_case def camel_to_snake(column_name): converts a string that is camelCase into snake_case s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', column_name) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower() camel_to_snake( javaLovesCamelCase ) 'java_loves_camel_case' df.columns = [camel_to_snake(col) for col in df.columns] df.columns.tolist() ['serious_dlqin2yrs', 'revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents'] from sklearn.neighbors import KNeighborsRegressor income_imputer = KNeighborsRegressor(n_neighbors=1) # \u6570\u636e\u5206\u4e3a\u4e24\u90e8\u5206\uff0c\u6709\u7f3a\u5931\u7684\u548c\u65e0\u7f3a\u5931\u7684\uff0c\u7528\u65e0\u7f3a\u5931\u7684\u6570\u636e\u5efa\u7acb\u6a21\u578b\u6765\u5224\u65ad\u7f3a\u5931\u6570\u636e\u7684\u53ef\u80fd\u53d6\u503c train_w_monthly_income = df[df.monthly_income.isnull()==False] train_w_null_monthly_income = df[df.monthly_income.isnull()==True] cols = ['number_real_estate_loans_or_lines', 'number_of_open_credit_lines_and_loans'] income_imputer.fit(train_w_monthly_income[cols], train_w_monthly_income.monthly_income) # \u7528\u623f\u4ea7\u8d37\u6b3e\u6b21\u6570\u4ee5\u53ca\u672a\u7ed3\u675f\u8d37\u6b3e\u6b21\u6570\u6765\u8bad\u7ec3\u6708\u6536\u5165 KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2, weights='uniform') new_values = income_imputer.predict(train_w_null_monthly_income[cols]) # \u518d\u7528\u6a21\u578b\u9884\u6d4b\u7f3a\u5931\u503c\u4e2d\u7684\u6708\u6536\u5165 train_w_null_monthly_income.ix[:,'monthly_income']=new_values # imputation df_imputed = train_w_monthly_income.append(train_w_null_monthly_income) df_imputed.shape # \u586b\u5145\u7f3a\u5931\u503c done (150000, 11) df_imputed.ix[df_imputed.number_of_dependents.isnull(),'number_of_dependents'] = -1 df_imputed.info() class 'pandas.core.frame.DataFrame' Int64Index: 150000 entries, 0 to 149997 Data columns (total 11 columns): serious_dlqin2yrs 150000 non-null int64 revolving_utilization_of_unsecured_lines 150000 non-null float64 age 150000 non-null int64 number_of_time30-59_days_past_due_not_worse 150000 non-null int64 debt_ratio 150000 non-null float64 monthly_income 150000 non-null float64 number_of_open_credit_lines_and_loans 150000 non-null int64 number_of_times90_days_late 150000 non-null int64 number_real_estate_loans_or_lines 150000 non-null int64 number_of_time60-89_days_past_due_not_worse 150000 non-null int64 number_of_dependents 150000 non-null float64 dtypes: float64(4), int64(7) memory usage: 13.7 MB","title":"\u6e05\u6d17\u6570\u636e"},{"location":"w0-introduction/0w/#_36","text":"df_imputed.monthly_income.hist(); def cap_values(x, cap): if x cap: return cap else: return x # \u8bbe\u5b9a\u4e0a\u9650 df_imputed.monthly_income = df_imputed.monthly_income.apply(lambda x: cap_values(x, 15000)) # \u53d8\u91cf\u79bb\u6563\u5316\uff0c\u5206\u4e3a15\u4e2abin df_imputed['income_bins'] = pd.cut(df_imputed.monthly_income, bins=15, labels=False) pd.value_counts(df_imputed.income_bins) 3 23168 4 19944 5 15583 6 14475 2 14038 7 10766 8 8609 14 7775 9 7672 1 7504 10 6298 0 4994 11 4454 12 2547 13 2173 Name: income_bins, dtype: int64 df_imputed[[ income_bins , serious_dlqin2yrs ]].groupby( income_bins ).mean() # \u6bcf\u4e2a\u6708\u6536\u5165\u5206\u7c7b\u4e2d\u7edf\u8ba1default\u5e73\u5747\u9891\u6570 serious_dlqin2yrs income_bins 0 0.051862 1 0.104211 2 0.093674 3 0.084168 4 0.073305 5 0.066033 6 0.059067 7 0.054338 8 0.050877 9 0.048488 10 0.039695 11 0.037270 12 0.040832 13 0.042338 14 0.047203 # \u753b\u51fa\u56fe\u6765, \u53ef\u4ee5\u660e\u663e\u53d1\u73b0\u57281-2\u4e2a bin\u4e2d\u7684 default \u6700\u591a cols = [ income_bins , serious_dlqin2yrs ] df_imputed[cols].groupby( income_bins ).mean().plot(); # \u4ee5\u5e74\u9f84\u6765\u770b\u662f20-30\u5c81\u6700\u9ad8, \u53e6\u5916\u5c31\u662f100\u5c81\u524d\u540e, \u53ef\u80fd\u662f\u53bb\u4e16\u4e86? cols = ['age', 'serious_dlqin2yrs'] age_means = df_imputed[cols].groupby( age ).mean() age_means.plot(); mybins = [0] + range(20, 80, 5) + [120] df_imputed['age_bin'] = pd.cut(df_imputed.age, bins=mybins) pd.value_counts(df_imputed['age_bin']) (45, 50] 18829 (50, 55] 17861 (55, 60] 16945 (60, 65] 16461 (40, 45] 16208 (35, 40] 13611 (65, 70] 10963 (30, 35] 10728 (75, 120] 10129 (25, 30] 7730 (70, 75] 7507 (20, 25] 3027 (0, 20] 0 dtype: int64 from sklearn.preprocessing import StandardScaler # Standardize features by removing the mean and scaling to unit variance # \u5c06\u6708\u6536\u5165\u6807\u51c6\u5316 df_imputed['monthly_income_scaled'] = StandardScaler().fit_transform(df_imputed.monthly_income.reshape(-1,1))","title":"\u7279\u5f81\u5de5\u7a0b"},{"location":"w0-introduction/0w/#_37","text":"df_imputed.columns Index([u'serious_dlqin2yrs', u'revolving_utilization_of_unsecured_lines', u'age', u'number_of_time30-59_days_past_due_not_worse', u'debt_ratio', u'monthly_income', u'number_of_open_credit_lines_and_loans', u'number_of_times90_days_late', u'number_real_estate_loans_or_lines', u'number_of_time60-89_days_past_due_not_worse', u'number_of_dependents', u'income_bins', u'age_bin', u'monthly_income_scaled'], dtype='object') # \u7279\u5f81 features = ['revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents', 'income_bins', 'age_bin', 'monthly_income_scaled'] X = pd.get_dummies(df_imputed[features], columns = ['income_bins', 'age_bin']) # dummy var print X.columns.tolist() print X.shape ['revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents', 'monthly_income_scaled', 'income_bins_0', 'income_bins_1', 'income_bins_2', 'income_bins_3', 'income_bins_4', 'income_bins_5', 'income_bins_6', 'income_bins_7', 'income_bins_8', 'income_bins_9', 'income_bins_10', 'income_bins_11', 'income_bins_12', 'income_bins_13', 'income_bins_14', 'age_bin_(0, 20]', 'age_bin_(20, 25]', 'age_bin_(25, 30]', 'age_bin_(30, 35]', 'age_bin_(35, 40]', 'age_bin_(40, 45]', 'age_bin_(45, 50]', 'age_bin_(50, 55]', 'age_bin_(55, 60]', 'age_bin_(60, 65]', 'age_bin_(65, 70]', 'age_bin_(70, 75]', 'age_bin_(75, 120]'] (150000, 39) y = df_imputed.serious_dlqin2yrs # target from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(X, y ,train_size=0.7,random_state=1) # 70% \u662f train data print train_X.shape print test_X.shape (105000, 39) (45000, 39) from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier clf = LogisticRegression() # logit \u903b\u8f91\u56de\u5f52 clf.fit(train_X,train_y) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) clf.predict_proba(test_X) # \u9884\u6d4b\u6982\u7387 array([[ 0.92834155, 0.07165845], [ 0.96492419, 0.03507581], [ 0.95753197, 0.04246803], ..., [ 0.96021551, 0.03978449], [ 0.87166534, 0.12833466], [ 0.9777555 , 0.0222445 ]]) from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix # \u8bc4\u4ef7\u65b9\u6cd5 preds = clf.predict(test_X) confusion_matrix(test_y, preds) array([[41880, 88], [ 2908, 124]]) print classification_report(test_y, preds, labels=[0, 1]) precision recall f1-score support 0 0.94 1.00 0.97 41968 1 0.58 0.04 0.08 3032 avg / total 0.91 0.93 0.91 45000 pre = clf.predict_proba(test_X) roc_auc_score(test_y,pre[:,1]) 0.7078206950866951 fpr, tpr, thresholds = roc_curve(test_y,pre[:,1]) plt.plot(fpr,tpr,); # \u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668 clf = RandomForestClassifier() clf.fit(train_X,train_y) preds = clf.predict(test_X) print classification_report(test_y, preds, labels=[0, 1]) precision recall f1-score support 0 0.94 0.99 0.97 41968 1 0.51 0.15 0.24 3032 avg / total 0.91 0.93 0.92 45000 pre = clf.predict_proba(test_X) roc_auc_score(test_y,pre[:,1]) 0.7867644886115015","title":"\u5efa\u6a21\u548c\u8bc4\u4f30"},{"location":"w0-introduction/0w/#auc078","text":"df_imputed.to_csv('df_imputed',index = False)","title":"\u7ec3\u4e60\uff1a \u4f60\u6765\u6539\u8fdb\u8fd9\u4e2a\u6316\u6398\u7684\u7ed3\u679c\uff0c\u4f7fauc\u53ef\u4ee5\u6bd40.78\u66f4\u5927"},{"location":"w1-regression/1w/","text":"Sections Exploring and visualizing the Housing dataset Implementing a simple regression model - Ordinary least squares Solving regression parameters with gradient descent Estimating coefficient of a regression model via scikit-learn Fitting a robust regression model using RANSAC Evaluating the performance of linear regression models Turning a linear regression model into a curve - Polynomial regression Modeling nonlinear relationships in the Housing dataset Exploring and visualizing the Housing dataset [ back to top ] \u6ce2\u58eb\u987f\u623f\u4ef7\u6570\u636e Source: https://archive.ics.uci.edu/ml/datasets/Housing Attributes: 1. CRIM per capita crime rate by town \u6bcf\u4e2a\u57ce\u9547\u4eba\u5747\u72af\u7f6a\u7387 2. ZN proportion of residential land zoned for lots over 25,000 sq.ft. \u8d85\u8fc725000\u5e73\u65b9\u5c3a\u7528\u5730\u5212\u4e3a\u5c45\u4f4f\u7528\u5730\u7684\u767e\u5206\u6bd4 3. INDUS proportion of non-retail business acres per town \u975e\u96f6\u552e\u5546\u7528\u5730\u767e\u5206\u6bd4 4. CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \u662f\u5426\u88ab\u6cb3\u9053\u5305\u56f4 5. NOX nitric oxides concentration (parts per 10 million) \u6c2e\u6c27\u5316\u7269\u6d53\u5ea6 6. RM average number of rooms per dwelling \u4f4f\u5b85\u5e73\u5747\u623f\u95f4\u6570\u76ee 7. AGE proportion of owner-occupied units built prior to 1940 1940\u5e74\u524d\u5efa\u6210\u81ea\u7528\u5355\u4f4d\u6bd4\u4f8b 8. DIS weighted distances to five Boston employment centres 5\u4e2a\u6ce2\u58eb\u987f\u5c31\u4e1a\u670d\u52a1\u4e2d\u5fc3\u7684\u52a0\u6743\u8ddd\u79bb 9. RAD index of accessibility to radial highways \u65e0\u969c\u788d\u5f84\u5411\u9ad8\u901f\u516c\u8def\u6307\u6570 10. TAX full-value property-tax rate per $10,000 \u6bcf\u4e07\u5143\u7269\u4e1a\u7a0e\u7387 11. PTRATIO pupil-teacher ratio by town \u5c0f\u5b66\u5e08\u751f\u6bd4\u4f8b 12. B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \u9ed1\u4eba\u6bd4\u4f8b\u6307\u6570 13. LSTAT % lower status of the population \u4f4e\u5c42\u4eba\u53e3\u6bd4\u4f8b 14. MEDV Median value of owner-occupied homes in $1000's \u4e1a\u4e3b\u81ea\u4f4f\u623f\u5c4b\u4e2d\u503c \uff08\u8981\u9884\u6d4b\u7684\u53d8\u91cf\uff09 # \u8bfb\u53d6\u6570\u636e import pandas as pd df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', header=None, sep='\\s+') # \u5206\u9694\u7b26\u4e3a\u7a7a\u683c df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] df.head() CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 \u6570\u636e\u5206\u6790\u7684\u7b2c\u4e00\u6b65\u662f\u8fdb\u884c \u63a2\u7d22\u6027\u6570\u636e\u5206\u6790 (Exploratory Data Analysis, EDA) \uff0c\u7406\u89e3\u53d8\u91cf\u7684\u5206\u5e03\u4e0e\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns.set(style='whitegrid', context='notebook') # \u8bbe\u5b9a\u6837\u5f0f\uff0c\u8fd8\u539f\u53ef\u7528 sns.reset_orig # MEDV \u662f\u76ee\u6807\u53d8\u91cf\uff0c\u4e3a\u4e86\u65b9\u4fbf\u6f14\u793a\uff0c\u53ea\u6311 4 \u4e2a\u9884\u6d4b\u53d8\u91cf cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV'] # scatterplot matrix, \u5bf9\u89d2\u7ebf\u4e0a\u662f\u53d8\u91cf\u5206\u5e03\u7684\u76f4\u65b9\u56fe\uff0c\u975e\u5bf9\u89d2\u7ebf\u4e0a\u662f\u4e24\u4e2a\u53d8\u91cf\u7684\u6563\u70b9\u56fe sns.pairplot(df[cols], size=2.5) plt.tight_layout() # \u7528\u4e0b\u9762\u8fd9\u884c\u4ee3\u7801\u53ef\u4ee5\u5b58\u50a8\u56fe\u7247\u5230\u786c\u76d8\u4e2d # plt.savefig('./figures/scatter.png', dpi=300) \u4ece\u56fe\u4e2d\u770b\u51fa + RM \u548c MEDV \u4f3c\u4e4e\u662f\u6709\u7ebf\u6027\u5173\u7cfb\u7684 + MEDV \u7c7b\u4f3c normal distribution # correlation map import numpy as np cm = np.corrcoef(df[cols].values.T) # \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570 sns.set(font_scale=1.5) # \u753b\u76f8\u5173\u7cfb\u6570\u77e9\u9635\u7684\u70ed\u70b9\u56fe hm = sns.heatmap(cm, annot=True, square=True, fmt='.2f', annot_kws={'size': 15}, yticklabels=cols, xticklabels=cols) plt.tight_layout() # plt.savefig('./figures/corr_mat.png', dpi=300) \u5bf9\u4e0e MEDV correlation \u9ad8\u7684\u53d8\u91cf\u611f\u5174\u8da3, LSTAT \u6700\u9ad8(-0.74), \u5176\u6b21\u662f RM (0.7) \u4f46\u4ece\u4e4b\u524d\u7684\u56fe\u770b\u51fa MEDV \u4e0e LSTAT \u5448\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u800c\u4e0e RM \u66f4\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u6240\u4ee5\u4e0b\u9762\u9009\u7528 RM \u6765\u6f14\u793a\u7b80\u5355\u7ebf\u6027\u56de\u5f52 sns.reset_orig() Implementing a simple regression model - Ordinary least squares Solving regression parameters with gradient descent \u68af\u5ea6\u4e0b\u964d\u6cd5 \u68af\u5ea6\u4e0b\u964d\u6cd5\u662f\u4e00\u4e2a\u6700\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u5e38\u4e5f\u79f0\u4e3a\u6700\u901f\u4e0b\u964d\u6cd5\u3002\u6700\u901f\u4e0b\u964d\u6cd5\u662f\u6c42\u89e3\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6700\u7b80\u5355\u548c\u6700\u53e4\u8001\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u867d\u7136\u73b0\u5728\u5df2\u7ecf\u4e0d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u662f\u8bb8\u591a\u6709\u6548\u7b97\u6cd5\u90fd\u662f\u4ee5\u5b83\u4e3a\u57fa\u7840\u8fdb\u884c\u6539\u8fdb\u548c\u4fee\u6b63\u800c\u5f97\u5230\u7684\u3002\u6700\u901f\u4e0b\u964d\u6cd5\u662f\u7528\u8d1f\u68af\u5ea6\u65b9\u5411\u4e3a\u641c\u7d22\u65b9\u5411\u7684\uff0c\u6700\u901f\u4e0b\u964d\u6cd5\u8d8a\u63a5\u8fd1\u76ee\u6807\u503c\uff0c\u6b65\u957f\u8d8a\u5c0f\uff0c\u524d\u8fdb\u8d8a\u6162\u3002 Wiki\u4e0a\u7684\u89e3\u91ca\u4e3a\u5982\u679c\u76ee\u6807\u51fd\u6570$$F(x)$$\u5728\u70b9$$a$$\u5904\u53ef\u5fae\u4e14\u6709\u5b9a\u4e49\uff0c\u90a3\u4e48\u51fd\u6570$$F(x)$$\u5728\u70b9$$a$$\u6cbf\u7740\u68af\u5ea6\u76f8\u53cd\u7684\u65b9\u5411$$-\\nabla F(a)$$\u4e0b\u964d\u6700\u5feb\u3002\u5176\u4e2d,$$\\nabla$$\u4e3a\u68af\u5ea6\u7b97\u5b50\uff0c$$\\nabla = (\\frac{\\partial}{\\partial x_1}, \\frac{\\partial}{\\partial x_2}, \\ldots, \\frac{\\partial}{\\partial x_n})^T$$ \u4ec0\u4e48\u662f\u68af\u5ea6\u4e0b\u964d\u6cd5\uff1f \u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u4e00\u79cd\u6c42\u89e3\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u65b9\u5f0f\uff0c\u5b83\u662f\u6700\u4f18\u5316\u4e2d\u6bd4\u8f83\u53e4\u8001\u7684\u4e00\u79cd\u65b9\u6cd5 \u68af\u5ea6\u4e0b\u964d\uff0c\u8bbe\u5b9a\u8d77\u59cb\u70b9\u8d1f\u68af\u5ea6\u65b9\u5411 (\u5373\u6570\u503c\u51cf\u5c0f\u7684\u65b9\u5411) \u4e3a\u641c\u7d22\u65b9\u5411\uff0c\u5bfb\u627e\u6700\u5c0f\u503c\u3002\u68af\u5ea6\u4e0b\u964d\u6cd5\u8d8a\u63a5\u8fd1\u76ee\u6807\u503c\uff0c\u6b65\u957f\u8d8a\u5c0f\uff0c\u524d\u8fdb\u8d8a\u6162. \u635f\u5931\u51fd\u6570 $$ J(w) = \\frac{1}{2} \\sum^n_{i=1} (y^{(i)} - \\hat y^{(i)})^2$$ \u68af\u5ea6 $$ \\frac {\\partial J}{\\partial w_j}=-\\sum^n_{i=1} (y^{(i)}-\\hat y^{(i)})x_j^{(i)}$$ \u66f4\u65b0\u89c4\u5219 $$w:=w-\\eta\\frac{\\partial J}{\\partial w}$$ class LinearRegressionGD(object): def __init__(self, eta=0.001, n_iter=20): self.eta = eta # learning rate \u5b66\u4e60\u901f\u7387 self.n_iter = n_iter # \u8fed\u4ee3\u6b21\u6570 def fit(self, X, y): # \u8bad\u7ec3\u51fd\u6570 # self.w_ = np.zeros(1, 1 + X.shape[1]) self.coef_ = np.zeros(shape=(1, X.shape[1])) # \u4ee3\u8868\u88ab\u8bad\u7ec3\u7684\u7cfb\u6570\uff0c\u521d\u59cb\u5316\u4e3a 0 self.intercept_ = np.zeros(1) self.cost_ = [] # \u7528\u4e8e\u4fdd\u5b58\u635f\u5931\u7684\u7a7alist for i in range(self.n_iter): output = self.net_input(X) # \u8ba1\u7b97\u9884\u6d4b\u7684Y errors = y - output self.coef_ += self.eta * np.dot(errors.T, X) # \u6839\u636e\u66f4\u65b0\u89c4\u5219\u66f4\u65b0\u7cfb\u6570\uff0c\u601d\u8003\u4e00\u4e0b\u4e3a\u4ec0\u4e48\u4e0d\u662f\u51cf\u53f7\uff1f self.intercept_ += self.eta * errors.sum() # \u66f4\u65b0 bias\uff0c\u76f8\u5f53\u4e8ex\u53d6\u5e38\u65701 cost = (errors**2).sum() / 2.0 # \u8ba1\u7b97\u635f\u5931 self.cost_.append(cost) # \u8bb0\u5f55\u635f\u5931\u51fd\u6570\u7684\u503c return self def net_input(self, X): # \u7ed9\u5b9a\u7cfb\u6570\u548cX\u8ba1\u7b97\u9884\u6d4b\u7684Y return np.dot(X, self.coef_.T) + self.intercept_ def predict(self, X): return self.net_input(X) # RM \u4f5c\u4e3a explanatory variable X = df[['RM']].values y = df[['MEDV']].values # standardize from sklearn.preprocessing import StandardScaler sc_x = StandardScaler() sc_y = StandardScaler() X_std = sc_x.fit_transform(X) y_std = sc_y.fit_transform(y) lr = LinearRegressionGD() lr.fit(X_std, y_std); # \u5582\u5165\u6570\u636e\u8fdb\u884c\u8bad\u7ec3 # cost function plt.plot(range(1, lr.n_iter+1), lr.cost_) plt.ylabel('SSE') plt.xlabel('Epoch') plt.tight_layout() \u53d1\u73b0\u5728 epoch 5\u4e4b\u540e cost \u57fa\u672c\u5c31\u4e0d\u80fd\u518d\u51cf\u5c0f\u4e86 # \u5b9a\u4e49\u4e00\u4e2a\u7ed8\u56fe\u51fd\u6570\u7528\u4e8e\u5c55\u793a def lin_regplot(X, y, model): plt.scatter(X, y, c='lightblue') plt.plot(X, model.predict(X), color='red', linewidth=2) return None # \u753b\u51fa\u9884\u6d4b lin_regplot(X_std, y_std, lr) plt.xlabel('Average number of rooms [RM] (standardized)') plt.ylabel('Price in $1000\\'s [MEDV] (standardized)') plt.tight_layout() plt.show() print('Slope: %.3f' % lr.coef_[0]) print('Intercept: %.3f' % lr.intercept_) # \u76f4\u7ebf\u7684\u659c\u7387\u53ca\u622a\u8ddd Slope: 0.695 Intercept: -0.000 # \u9884\u6d4b RM=5 \u65f6\uff0c\u623f\u4ef7\u4e3a\u591a\u5c11 num_rooms_std = sc_x.transform([[5.0]]) price_std = lr.predict(num_rooms_std) print( Price in $1000's: %.3f % sc_y.inverse_transform(price_std)) Price in $1000's: 10.840 Estimating coefficient of a regression model via scikit-learn [ back to top ] from sklearn.linear_model import LinearRegression slr = LinearRegression() slr.fit(X_std, y_std) print('Slope: %.3f' % slr.coef_[0]) print('Intercept: %.3f' % slr.intercept_) Slope: 0.695 Intercept: -0.000 lin_regplot(X_std, y_std, slr) plt.xlabel('Average number of rooms [RM] (standardized)') plt.ylabel('Price in $1000\\'s [MEDV] (standardized)') plt.tight_layout() # \u5982\u679c\u4e0d\u6807\u51c6\u5316\uff0c\u76f4\u63a5\u7528\u539f\u59cb\u6570\u636e\u8fdb\u884c\u56de\u5f52 slr.fit(X, y) lin_regplot(X, y, slr) plt.xlabel('Average number of rooms [RM]') plt.ylabel('Price in $1000\\'s [MEDV]') plt.tight_layout() \u7ed3\u679c\u4e0e\u4f7f\u7528 gradient descent \u7684\u7ed3\u679c\u63a5\u8fd1\uff0c\u601d\u8003\u4e00\u4e0b\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u4f7f\u7528\u6807\u51c6\u5316\uff1f Fitting a robust regression model using RANSAC [ back to top ] \u7ebf\u6027\u56de\u5f52\u5bf9 outlier \u6bd4\u8f83\u654f\u611f, \u800c\u5bf9\u662f\u5426\u5220\u9664 outlier \u662f\u9700\u8981\u81ea\u5df1\u8fdb\u884c\u5224\u65ad\u7684. \u53e6\u4e00\u79cd\u65b9\u6cd5\u5c31\u662f RANdom SAmple Consensus (RANSAC) \u5927\u81f4\u7b97\u6cd5\u5982\u4e0b: 1. Select a random number of samples to be inliers and fit the model. 2. Test all other data points against the fitted model and add those points that fall within a user-given tolerance to the inliers. 3. Refit the model using all inliers. 4. Estimate the error of the fitted model versus the inliers. 5. Terminate the algorithm if the performance meets a certain user-defined threshold or if a fixed number of iterations has been reached; go back to step 1 otherwise. # \u4f7f\u7528 sklearn \u4e2d\u5df2\u6709\u51fd\u6570 from sklearn.linear_model import RANSACRegressor ransac = RANSACRegressor(LinearRegression(), max_trials=100, # max iteration min_samples=50, # min number of randomly chosen samples residual_metric=lambda dy: np.sum(np.abs(dy), axis=1), # absolute vertical distances to measure residual_threshold=5.0, # allow sample as inlier within 5 distance units random_state=0) ransac.fit(X, y) # \u5206\u51fa inlier \u548c outlier inlier_mask = ransac.inlier_mask_ outlier_mask = np.logical_not(inlier_mask) line_X = np.arange(3, 10, 1) line_y_ransac = ransac.predict(line_X[:, np.newaxis]) plt.scatter(X[inlier_mask], y[inlier_mask], c='blue', marker='o', label='Inliers') plt.scatter(X[outlier_mask], y[outlier_mask], c='lightgreen', marker='s', label='Outliers') plt.plot(line_X, line_y_ransac, color='red') plt.xlabel('Average number of rooms [RM]') plt.ylabel('Price in $1000\\'s [MEDV]') plt.legend(loc='upper left') plt.tight_layout() print('Slope: %.3f' % ransac.estimator_.coef_[0]) print('Intercept: %.3f' % ransac.estimator_.intercept_) Slope: 9.621 Intercept: -37.137 RANSAC \u51cf\u5c11\u4e86 outlier \u7684\u5f71\u54cd, \u4f46\u5bf9\u4e8e\u672a\u77e5\u6570\u636e\u7684\u9884\u6d4b\u80fd\u529b\u662f\u5426\u6709\u5f71\u54cd\u672a\u77e5. \u5bf9\u6bd4 RANSAC \u56de\u5f52\u548c OLS \u56de\u5f52 from sklearn import datasets n_samples = 1000 n_outliers = 50 X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1, n_informative=1, noise=10, coef=True, random_state=0) # Add outlier data np.random.seed(0) X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1)) y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers) # Fit line using all data model = LinearRegression() model.fit(X, y) # Robustly fit linear model with RANSAC algorithm model_ransac = RANSACRegressor(LinearRegression()) model_ransac.fit(X, y) inlier_mask = model_ransac.inlier_mask_ outlier_mask = np.logical_not(inlier_mask) # Predict data of estimated models line_X = np.arange(-5, 5) line_y = model.predict(line_X[:, np.newaxis]) line_y_ransac = model_ransac.predict(line_X[:, np.newaxis]) # Compare estimated coefficients print( Estimated coefficients (true, normal, RANSAC): ) print(coef, model.coef_, model_ransac.estimator_.coef_) plt.plot(X[inlier_mask], y[inlier_mask], '.g', label='Inliers') plt.plot(X[outlier_mask], y[outlier_mask], '.r', label='Outliers') plt.plot(line_X, line_y, '-k', label='Linear regressor') plt.plot(line_X, line_y_ransac, '-b', label='RANSAC regressor') plt.legend(loc='lower right'); Estimated coefficients (true, normal, RANSAC): (array(82.1903908407869), array([ 54.17236387]), array([ 82.08533159])) Evaluating the performance of linear regression models It is crucial to test the model on data that it hasn't seen during training to obtain an unbiased estimate of its performance. [ back to top ] from sklearn.cross_validation import train_test_split X = df.iloc[:, :-1].values y = df['MEDV'].values X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) # 70% \u7528\u4e8e train, 30%\u7528\u4e8e test slr = LinearRegression() slr.fit(X_train, y_train) y_train_pred = slr.predict(X_train) y_test_pred = slr.predict(X_test) # residual plot, \u7ecf\u5e38\u88ab\u7528\u6765\u68c0\u67e5\u56de\u5f52\u6a21\u578b plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training data') plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Test data') plt.xlabel('Predicted values') plt.ylabel('Residuals') plt.legend(loc='upper left') plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red') plt.xlim([-10, 50]) plt.tight_layout() # plt.savefig('./figures/slr_residuals.png', dpi=300) \u5982\u679c\u9884\u6d4b\u90fd\u662f\u6b63\u786e\u7684, \u90a3\u4e48 residual \u5c31\u662f0. \u8fd9\u662f\u7406\u60f3\u60c5\u51b5, \u5b9e\u9645\u4e2d, \u6211\u4eec\u5e0c\u671b error \u662f\u968f\u673a\u5206\u5e03\u7684. \u4ece\u4e0a\u56fe\u770b, \u6709\u90e8\u5206 error \u662f\u79bb\u7ea2\u8272\u7ebf\u8f83\u8fdc\u7684, \u53ef\u80fd\u662f outlier \u5f15\u8d77\u8f83\u5927\u7684\u504f\u5dee # \u53e6\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u662f Mean Squred Error, MSE, \u5c31\u662f SSE \u7684\u5e73\u5747\u503c # R-squre \u4e5f\u662f\u91cd\u8981\u7684 measurement, \u5b83\u4ee3\u8868\u7740\u6709\u591a\u5c11\u767e\u5206\u6bd4\u7684\u6570\u636e\u88ab\u6a21\u578b\u89e3\u91ca. \u8d8a\u9ad8\u4ee3\u8868\u6a21\u578b\u62df\u5408\u8d8a\u597d from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error print('MSE train: %.3f, test: %.3f' % ( mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred))) print('R^2 train: %.3f, test: %.3f' % ( r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred))) MSE train: 19.958, test: 27.196 R^2 train: 0.765, test: 0.673 $$ MSE = \\frac 1 n \\sum^n_{i=1} (y^{(i)} - \\hat y^{(i)})$$ $$ R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{MSE}{Var(y)}$$ Turning a linear regression model into a curve - Polynomial regression [ back to top ] import numpy as np from X = np.array([258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0])[:, np.newaxis] y = np.array([236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8]) # \u6dfb\u52a0\u4e8c\u6b21\u9879\u548c\u622a\u8ddd\u9879 from sklearn.preprocessing import PolynomialFeatures lr = LinearRegression() pr = LinearRegression() quadratic = PolynomialFeatures(degree=2) X_quad = quadratic.fit_transform(X) print(X.shape) print(X_quad.shape) (10, 1) (10, 3) X_quad array([[ 1.00000000e+00, 2.58000000e+02, 6.65640000e+04], [ 1.00000000e+00, 2.70000000e+02, 7.29000000e+04], [ 1.00000000e+00, 2.94000000e+02, 8.64360000e+04], [ 1.00000000e+00, 3.20000000e+02, 1.02400000e+05], [ 1.00000000e+00, 3.42000000e+02, 1.16964000e+05], [ 1.00000000e+00, 3.68000000e+02, 1.35424000e+05], [ 1.00000000e+00, 3.96000000e+02, 1.56816000e+05], [ 1.00000000e+00, 4.46000000e+02, 1.98916000e+05], [ 1.00000000e+00, 4.80000000e+02, 2.30400000e+05], [ 1.00000000e+00, 5.86000000e+02, 3.43396000e+05]]) # fit linear features lr.fit(X, y) X_fit = np.arange(250,600,10)[:, np.newaxis] y_lin_fit = lr.predict(X_fit) # fit quadratic features pr.fit(X_quad, y) y_quad_fit = pr.predict(quadratic.fit_transform(X_fit)) # plot results plt.scatter(X, y, label='training points') plt.plot(X_fit, y_lin_fit, label='linear fit', linestyle='--') plt.plot(X_fit, y_quad_fit, label='quadratic fit') plt.legend(loc='upper left') plt.tight_layout() \u56fe\u4e0a\u53ef\u4ee5\u53d1\u73b0 quadratic fit\u6bd4 linear \u62df\u5408\u6548\u679c\u66f4\u597d y_lin_pred = lr.predict(X) y_quad_pred = pr.predict(X_quad) print('Training MSE linear: %.3f, quadratic: %.3f' % ( mean_squared_error(y, y_lin_pred), mean_squared_error(y, y_quad_pred))) print('Training R^2 linear: %.3f, quadratic: %.3f' % ( r2_score(y, y_lin_pred), r2_score(y, y_quad_pred))) Training MSE linear: 569.780, quadratic: 61.330 Training R^2 linear: 0.832, quadratic: 0.982 MSE \u4e0b\u964d\u523061, R^2 \u4e0a\u5347\u523098%, \u8bf4\u660e\u5728\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0a quadratic fit \u6548\u679c\u66f4\u597d Modeling nonlinear relationships in the Housing dataset \u6211\u4eec\u4f1a\u5c06house prices \u4e0e LSTAT \u7684 quadratic \u53ca cubic polynomials fit, \u5e76\u4e0e linear fit \u5bf9\u6bd4 back to top X = df[['LSTAT']].values y = df['MEDV'].values regr = LinearRegression() # create quadratic features quadratic = PolynomialFeatures(degree=2) cubic = PolynomialFeatures(degree=3) X_quad = quadratic.fit_transform(X) X_cubic = cubic.fit_transform(X) # fit features X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis] regr = regr.fit(X, y) y_lin_fit = regr.predict(X_fit) linear_r2 = r2_score(y, regr.predict(X)) regr = regr.fit(X_quad, y) y_quad_fit = regr.predict(quadratic.fit_transform(X_fit)) quadratic_r2 = r2_score(y, regr.predict(X_quad)) regr = regr.fit(X_cubic, y) y_cubic_fit = regr.predict(cubic.fit_transform(X_fit)) cubic_r2 = r2_score(y, regr.predict(X_cubic)) # plot results plt.scatter(X, y, label='training points', color='lightgray') plt.plot(X_fit, y_lin_fit, label='linear (d=1), $R^2=%.2f$' % linear_r2, color='blue', lw=2, linestyle=':') plt.plot(X_fit, y_quad_fit, label='quadratic (d=2), $R^2=%.2f$' % quadratic_r2, color='red', lw=2, linestyle='-') plt.plot(X_fit, y_cubic_fit, label='cubic (d=3), $R^2=%.2f$' % cubic_r2, color='green', lw=2, linestyle='--') plt.xlabel('% lower status of the population [LSTAT]') plt.ylabel('Price in $1000\\'s [MEDV]') plt.legend(loc='upper right') plt.tight_layout() # plt.savefig('./figures/polyhouse_example.png', dpi=300) Transforming the dataset by log: \u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u505a\uff1f\u662f\u56e0\u4e3a\u6709\u753b\u56fe\u63a2\u7d22\u7684\u542f\u793a\uff1f X = df[['LSTAT']].values y = df['MEDV'].values # transform features X_log = np.log(X) y_sqrt = np.sqrt(y) # fit features X_fit = np.arange(X_log.min()-1, X_log.max()+1, 1)[:, np.newaxis] regr = regr.fit(X_log, y_sqrt) y_lin_fit = regr.predict(X_fit) linear_r2 = r2_score(y_sqrt, regr.predict(X_log)) # plot results plt.scatter(X_log, y_sqrt, label='training points', color='lightgray') plt.plot(X_fit, y_lin_fit, label='linear (d=1), $R^2=%.2f$' % linear_r2, color='blue', lw=2) plt.xlabel('log(% lower status of the population [LSTAT])') plt.ylabel('$\\sqrt{Price \\; in \\; \\$1000\\'s [MEDV]}$') plt.legend(loc='lower left') plt.tight_layout() # plt.savefig('./figures/transform_example.png', dpi=300) plt.show() \u7ecf\u8fc7 log \u53d8\u6362\u540e\uff0c\u7ebf\u6027\u62df\u5408\u6548\u679c\u5df2\u7ecf\u4e0d\u9519, \u6bd4\u5355\u7eaf polynomial fit \u66f4\u597d \u7ec3\u4e60\uff1a\u7528\u623f\u4ef7\u6570\u636e\u7684\u5176\u5b83\u81ea\u53d8\u91cf\u4e00\u8d77\u505a\u4e00\u4e2a\u591a\u5143\u6a21\u578b\u770b\u770bR2\u6709\u6ca1\u6709\u6539\u5584","title":"1w"},{"location":"w1-regression/1w/#sections","text":"Exploring and visualizing the Housing dataset Implementing a simple regression model - Ordinary least squares Solving regression parameters with gradient descent Estimating coefficient of a regression model via scikit-learn Fitting a robust regression model using RANSAC Evaluating the performance of linear regression models Turning a linear regression model into a curve - Polynomial regression Modeling nonlinear relationships in the Housing dataset","title":"Sections"},{"location":"w1-regression/1w/#exploring-and-visualizing-the-housing-dataset","text":"[ back to top ] \u6ce2\u58eb\u987f\u623f\u4ef7\u6570\u636e Source: https://archive.ics.uci.edu/ml/datasets/Housing Attributes: 1. CRIM per capita crime rate by town \u6bcf\u4e2a\u57ce\u9547\u4eba\u5747\u72af\u7f6a\u7387 2. ZN proportion of residential land zoned for lots over 25,000 sq.ft. \u8d85\u8fc725000\u5e73\u65b9\u5c3a\u7528\u5730\u5212\u4e3a\u5c45\u4f4f\u7528\u5730\u7684\u767e\u5206\u6bd4 3. INDUS proportion of non-retail business acres per town \u975e\u96f6\u552e\u5546\u7528\u5730\u767e\u5206\u6bd4 4. CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \u662f\u5426\u88ab\u6cb3\u9053\u5305\u56f4 5. NOX nitric oxides concentration (parts per 10 million) \u6c2e\u6c27\u5316\u7269\u6d53\u5ea6 6. RM average number of rooms per dwelling \u4f4f\u5b85\u5e73\u5747\u623f\u95f4\u6570\u76ee 7. AGE proportion of owner-occupied units built prior to 1940 1940\u5e74\u524d\u5efa\u6210\u81ea\u7528\u5355\u4f4d\u6bd4\u4f8b 8. DIS weighted distances to five Boston employment centres 5\u4e2a\u6ce2\u58eb\u987f\u5c31\u4e1a\u670d\u52a1\u4e2d\u5fc3\u7684\u52a0\u6743\u8ddd\u79bb 9. RAD index of accessibility to radial highways \u65e0\u969c\u788d\u5f84\u5411\u9ad8\u901f\u516c\u8def\u6307\u6570 10. TAX full-value property-tax rate per $10,000 \u6bcf\u4e07\u5143\u7269\u4e1a\u7a0e\u7387 11. PTRATIO pupil-teacher ratio by town \u5c0f\u5b66\u5e08\u751f\u6bd4\u4f8b 12. B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \u9ed1\u4eba\u6bd4\u4f8b\u6307\u6570 13. LSTAT % lower status of the population \u4f4e\u5c42\u4eba\u53e3\u6bd4\u4f8b 14. MEDV Median value of owner-occupied homes in $1000's \u4e1a\u4e3b\u81ea\u4f4f\u623f\u5c4b\u4e2d\u503c \uff08\u8981\u9884\u6d4b\u7684\u53d8\u91cf\uff09 # \u8bfb\u53d6\u6570\u636e import pandas as pd df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data', header=None, sep='\\s+') # \u5206\u9694\u7b26\u4e3a\u7a7a\u683c df.columns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] df.head() CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT MEDV 0 0.00632 18.0 2.31 0 0.538 6.575 65.2 4.0900 1 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0 0.469 6.421 78.9 4.9671 2 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0 0.469 7.185 61.1 4.9671 2 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0 0.458 6.998 45.8 6.0622 3 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0 0.458 7.147 54.2 6.0622 3 222.0 18.7 396.90 5.33 36.2 \u6570\u636e\u5206\u6790\u7684\u7b2c\u4e00\u6b65\u662f\u8fdb\u884c \u63a2\u7d22\u6027\u6570\u636e\u5206\u6790 (Exploratory Data Analysis, EDA) \uff0c\u7406\u89e3\u53d8\u91cf\u7684\u5206\u5e03\u4e0e\u53d8\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002 %matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns.set(style='whitegrid', context='notebook') # \u8bbe\u5b9a\u6837\u5f0f\uff0c\u8fd8\u539f\u53ef\u7528 sns.reset_orig # MEDV \u662f\u76ee\u6807\u53d8\u91cf\uff0c\u4e3a\u4e86\u65b9\u4fbf\u6f14\u793a\uff0c\u53ea\u6311 4 \u4e2a\u9884\u6d4b\u53d8\u91cf cols = ['LSTAT', 'INDUS', 'NOX', 'RM', 'MEDV'] # scatterplot matrix, \u5bf9\u89d2\u7ebf\u4e0a\u662f\u53d8\u91cf\u5206\u5e03\u7684\u76f4\u65b9\u56fe\uff0c\u975e\u5bf9\u89d2\u7ebf\u4e0a\u662f\u4e24\u4e2a\u53d8\u91cf\u7684\u6563\u70b9\u56fe sns.pairplot(df[cols], size=2.5) plt.tight_layout() # \u7528\u4e0b\u9762\u8fd9\u884c\u4ee3\u7801\u53ef\u4ee5\u5b58\u50a8\u56fe\u7247\u5230\u786c\u76d8\u4e2d # plt.savefig('./figures/scatter.png', dpi=300) \u4ece\u56fe\u4e2d\u770b\u51fa + RM \u548c MEDV \u4f3c\u4e4e\u662f\u6709\u7ebf\u6027\u5173\u7cfb\u7684 + MEDV \u7c7b\u4f3c normal distribution # correlation map import numpy as np cm = np.corrcoef(df[cols].values.T) # \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570 sns.set(font_scale=1.5) # \u753b\u76f8\u5173\u7cfb\u6570\u77e9\u9635\u7684\u70ed\u70b9\u56fe hm = sns.heatmap(cm, annot=True, square=True, fmt='.2f', annot_kws={'size': 15}, yticklabels=cols, xticklabels=cols) plt.tight_layout() # plt.savefig('./figures/corr_mat.png', dpi=300) \u5bf9\u4e0e MEDV correlation \u9ad8\u7684\u53d8\u91cf\u611f\u5174\u8da3, LSTAT \u6700\u9ad8(-0.74), \u5176\u6b21\u662f RM (0.7) \u4f46\u4ece\u4e4b\u524d\u7684\u56fe\u770b\u51fa MEDV \u4e0e LSTAT \u5448\u975e\u7ebf\u6027\u5173\u7cfb\uff0c\u800c\u4e0e RM \u66f4\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u6240\u4ee5\u4e0b\u9762\u9009\u7528 RM \u6765\u6f14\u793a\u7b80\u5355\u7ebf\u6027\u56de\u5f52 sns.reset_orig()","title":"Exploring and visualizing the Housing dataset"},{"location":"w1-regression/1w/#implementing-a-simple-regression-model-ordinary-least-squares","text":"","title":"Implementing a simple regression model - Ordinary least squares"},{"location":"w1-regression/1w/#solving-regression-parameters-with-gradient-descent","text":"","title":"Solving regression parameters with gradient descent"},{"location":"w1-regression/1w/#_1","text":"\u68af\u5ea6\u4e0b\u964d\u6cd5\u662f\u4e00\u4e2a\u6700\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u5e38\u4e5f\u79f0\u4e3a\u6700\u901f\u4e0b\u964d\u6cd5\u3002\u6700\u901f\u4e0b\u964d\u6cd5\u662f\u6c42\u89e3\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u6700\u7b80\u5355\u548c\u6700\u53e4\u8001\u7684\u65b9\u6cd5\u4e4b\u4e00\uff0c\u867d\u7136\u73b0\u5728\u5df2\u7ecf\u4e0d\u5177\u6709\u5b9e\u7528\u6027\uff0c\u4f46\u662f\u8bb8\u591a\u6709\u6548\u7b97\u6cd5\u90fd\u662f\u4ee5\u5b83\u4e3a\u57fa\u7840\u8fdb\u884c\u6539\u8fdb\u548c\u4fee\u6b63\u800c\u5f97\u5230\u7684\u3002\u6700\u901f\u4e0b\u964d\u6cd5\u662f\u7528\u8d1f\u68af\u5ea6\u65b9\u5411\u4e3a\u641c\u7d22\u65b9\u5411\u7684\uff0c\u6700\u901f\u4e0b\u964d\u6cd5\u8d8a\u63a5\u8fd1\u76ee\u6807\u503c\uff0c\u6b65\u957f\u8d8a\u5c0f\uff0c\u524d\u8fdb\u8d8a\u6162\u3002 Wiki\u4e0a\u7684\u89e3\u91ca\u4e3a\u5982\u679c\u76ee\u6807\u51fd\u6570$$F(x)$$\u5728\u70b9$$a$$\u5904\u53ef\u5fae\u4e14\u6709\u5b9a\u4e49\uff0c\u90a3\u4e48\u51fd\u6570$$F(x)$$\u5728\u70b9$$a$$\u6cbf\u7740\u68af\u5ea6\u76f8\u53cd\u7684\u65b9\u5411$$-\\nabla F(a)$$\u4e0b\u964d\u6700\u5feb\u3002\u5176\u4e2d,$$\\nabla$$\u4e3a\u68af\u5ea6\u7b97\u5b50\uff0c$$\\nabla = (\\frac{\\partial}{\\partial x_1}, \\frac{\\partial}{\\partial x_2}, \\ldots, \\frac{\\partial}{\\partial x_n})^T$$","title":"\u68af\u5ea6\u4e0b\u964d\u6cd5"},{"location":"w1-regression/1w/#_2","text":"\u68af\u5ea6\u4e0b\u964d\u6cd5\uff0c\u53ef\u4f5c\u4e3a\u4e00\u79cd\u6c42\u89e3\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u65b9\u5f0f\uff0c\u5b83\u662f\u6700\u4f18\u5316\u4e2d\u6bd4\u8f83\u53e4\u8001\u7684\u4e00\u79cd\u65b9\u6cd5 \u68af\u5ea6\u4e0b\u964d\uff0c\u8bbe\u5b9a\u8d77\u59cb\u70b9\u8d1f\u68af\u5ea6\u65b9\u5411 (\u5373\u6570\u503c\u51cf\u5c0f\u7684\u65b9\u5411) \u4e3a\u641c\u7d22\u65b9\u5411\uff0c\u5bfb\u627e\u6700\u5c0f\u503c\u3002\u68af\u5ea6\u4e0b\u964d\u6cd5\u8d8a\u63a5\u8fd1\u76ee\u6807\u503c\uff0c\u6b65\u957f\u8d8a\u5c0f\uff0c\u524d\u8fdb\u8d8a\u6162. \u635f\u5931\u51fd\u6570 $$ J(w) = \\frac{1}{2} \\sum^n_{i=1} (y^{(i)} - \\hat y^{(i)})^2$$ \u68af\u5ea6 $$ \\frac {\\partial J}{\\partial w_j}=-\\sum^n_{i=1} (y^{(i)}-\\hat y^{(i)})x_j^{(i)}$$ \u66f4\u65b0\u89c4\u5219 $$w:=w-\\eta\\frac{\\partial J}{\\partial w}$$ class LinearRegressionGD(object): def __init__(self, eta=0.001, n_iter=20): self.eta = eta # learning rate \u5b66\u4e60\u901f\u7387 self.n_iter = n_iter # \u8fed\u4ee3\u6b21\u6570 def fit(self, X, y): # \u8bad\u7ec3\u51fd\u6570 # self.w_ = np.zeros(1, 1 + X.shape[1]) self.coef_ = np.zeros(shape=(1, X.shape[1])) # \u4ee3\u8868\u88ab\u8bad\u7ec3\u7684\u7cfb\u6570\uff0c\u521d\u59cb\u5316\u4e3a 0 self.intercept_ = np.zeros(1) self.cost_ = [] # \u7528\u4e8e\u4fdd\u5b58\u635f\u5931\u7684\u7a7alist for i in range(self.n_iter): output = self.net_input(X) # \u8ba1\u7b97\u9884\u6d4b\u7684Y errors = y - output self.coef_ += self.eta * np.dot(errors.T, X) # \u6839\u636e\u66f4\u65b0\u89c4\u5219\u66f4\u65b0\u7cfb\u6570\uff0c\u601d\u8003\u4e00\u4e0b\u4e3a\u4ec0\u4e48\u4e0d\u662f\u51cf\u53f7\uff1f self.intercept_ += self.eta * errors.sum() # \u66f4\u65b0 bias\uff0c\u76f8\u5f53\u4e8ex\u53d6\u5e38\u65701 cost = (errors**2).sum() / 2.0 # \u8ba1\u7b97\u635f\u5931 self.cost_.append(cost) # \u8bb0\u5f55\u635f\u5931\u51fd\u6570\u7684\u503c return self def net_input(self, X): # \u7ed9\u5b9a\u7cfb\u6570\u548cX\u8ba1\u7b97\u9884\u6d4b\u7684Y return np.dot(X, self.coef_.T) + self.intercept_ def predict(self, X): return self.net_input(X) # RM \u4f5c\u4e3a explanatory variable X = df[['RM']].values y = df[['MEDV']].values # standardize from sklearn.preprocessing import StandardScaler sc_x = StandardScaler() sc_y = StandardScaler() X_std = sc_x.fit_transform(X) y_std = sc_y.fit_transform(y) lr = LinearRegressionGD() lr.fit(X_std, y_std); # \u5582\u5165\u6570\u636e\u8fdb\u884c\u8bad\u7ec3 # cost function plt.plot(range(1, lr.n_iter+1), lr.cost_) plt.ylabel('SSE') plt.xlabel('Epoch') plt.tight_layout() \u53d1\u73b0\u5728 epoch 5\u4e4b\u540e cost \u57fa\u672c\u5c31\u4e0d\u80fd\u518d\u51cf\u5c0f\u4e86 # \u5b9a\u4e49\u4e00\u4e2a\u7ed8\u56fe\u51fd\u6570\u7528\u4e8e\u5c55\u793a def lin_regplot(X, y, model): plt.scatter(X, y, c='lightblue') plt.plot(X, model.predict(X), color='red', linewidth=2) return None # \u753b\u51fa\u9884\u6d4b lin_regplot(X_std, y_std, lr) plt.xlabel('Average number of rooms [RM] (standardized)') plt.ylabel('Price in $1000\\'s [MEDV] (standardized)') plt.tight_layout() plt.show() print('Slope: %.3f' % lr.coef_[0]) print('Intercept: %.3f' % lr.intercept_) # \u76f4\u7ebf\u7684\u659c\u7387\u53ca\u622a\u8ddd Slope: 0.695 Intercept: -0.000 # \u9884\u6d4b RM=5 \u65f6\uff0c\u623f\u4ef7\u4e3a\u591a\u5c11 num_rooms_std = sc_x.transform([[5.0]]) price_std = lr.predict(num_rooms_std) print( Price in $1000's: %.3f % sc_y.inverse_transform(price_std)) Price in $1000's: 10.840","title":"\u4ec0\u4e48\u662f\u68af\u5ea6\u4e0b\u964d\u6cd5\uff1f"},{"location":"w1-regression/1w/#estimating-coefficient-of-a-regression-model-via-scikit-learn","text":"[ back to top ] from sklearn.linear_model import LinearRegression slr = LinearRegression() slr.fit(X_std, y_std) print('Slope: %.3f' % slr.coef_[0]) print('Intercept: %.3f' % slr.intercept_) Slope: 0.695 Intercept: -0.000 lin_regplot(X_std, y_std, slr) plt.xlabel('Average number of rooms [RM] (standardized)') plt.ylabel('Price in $1000\\'s [MEDV] (standardized)') plt.tight_layout() # \u5982\u679c\u4e0d\u6807\u51c6\u5316\uff0c\u76f4\u63a5\u7528\u539f\u59cb\u6570\u636e\u8fdb\u884c\u56de\u5f52 slr.fit(X, y) lin_regplot(X, y, slr) plt.xlabel('Average number of rooms [RM]') plt.ylabel('Price in $1000\\'s [MEDV]') plt.tight_layout() \u7ed3\u679c\u4e0e\u4f7f\u7528 gradient descent \u7684\u7ed3\u679c\u63a5\u8fd1\uff0c\u601d\u8003\u4e00\u4e0b\u4ec0\u4e48\u65f6\u5019\u9700\u8981\u4f7f\u7528\u6807\u51c6\u5316\uff1f","title":"Estimating coefficient of a regression model via scikit-learn"},{"location":"w1-regression/1w/#fitting-a-robust-regression-model-using-ransac","text":"[ back to top ] \u7ebf\u6027\u56de\u5f52\u5bf9 outlier \u6bd4\u8f83\u654f\u611f, \u800c\u5bf9\u662f\u5426\u5220\u9664 outlier \u662f\u9700\u8981\u81ea\u5df1\u8fdb\u884c\u5224\u65ad\u7684. \u53e6\u4e00\u79cd\u65b9\u6cd5\u5c31\u662f RANdom SAmple Consensus (RANSAC) \u5927\u81f4\u7b97\u6cd5\u5982\u4e0b: 1. Select a random number of samples to be inliers and fit the model. 2. Test all other data points against the fitted model and add those points that fall within a user-given tolerance to the inliers. 3. Refit the model using all inliers. 4. Estimate the error of the fitted model versus the inliers. 5. Terminate the algorithm if the performance meets a certain user-defined threshold or if a fixed number of iterations has been reached; go back to step 1 otherwise. # \u4f7f\u7528 sklearn \u4e2d\u5df2\u6709\u51fd\u6570 from sklearn.linear_model import RANSACRegressor ransac = RANSACRegressor(LinearRegression(), max_trials=100, # max iteration min_samples=50, # min number of randomly chosen samples residual_metric=lambda dy: np.sum(np.abs(dy), axis=1), # absolute vertical distances to measure residual_threshold=5.0, # allow sample as inlier within 5 distance units random_state=0) ransac.fit(X, y) # \u5206\u51fa inlier \u548c outlier inlier_mask = ransac.inlier_mask_ outlier_mask = np.logical_not(inlier_mask) line_X = np.arange(3, 10, 1) line_y_ransac = ransac.predict(line_X[:, np.newaxis]) plt.scatter(X[inlier_mask], y[inlier_mask], c='blue', marker='o', label='Inliers') plt.scatter(X[outlier_mask], y[outlier_mask], c='lightgreen', marker='s', label='Outliers') plt.plot(line_X, line_y_ransac, color='red') plt.xlabel('Average number of rooms [RM]') plt.ylabel('Price in $1000\\'s [MEDV]') plt.legend(loc='upper left') plt.tight_layout() print('Slope: %.3f' % ransac.estimator_.coef_[0]) print('Intercept: %.3f' % ransac.estimator_.intercept_) Slope: 9.621 Intercept: -37.137 RANSAC \u51cf\u5c11\u4e86 outlier \u7684\u5f71\u54cd, \u4f46\u5bf9\u4e8e\u672a\u77e5\u6570\u636e\u7684\u9884\u6d4b\u80fd\u529b\u662f\u5426\u6709\u5f71\u54cd\u672a\u77e5. \u5bf9\u6bd4 RANSAC \u56de\u5f52\u548c OLS \u56de\u5f52 from sklearn import datasets n_samples = 1000 n_outliers = 50 X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1, n_informative=1, noise=10, coef=True, random_state=0) # Add outlier data np.random.seed(0) X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1)) y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers) # Fit line using all data model = LinearRegression() model.fit(X, y) # Robustly fit linear model with RANSAC algorithm model_ransac = RANSACRegressor(LinearRegression()) model_ransac.fit(X, y) inlier_mask = model_ransac.inlier_mask_ outlier_mask = np.logical_not(inlier_mask) # Predict data of estimated models line_X = np.arange(-5, 5) line_y = model.predict(line_X[:, np.newaxis]) line_y_ransac = model_ransac.predict(line_X[:, np.newaxis]) # Compare estimated coefficients print( Estimated coefficients (true, normal, RANSAC): ) print(coef, model.coef_, model_ransac.estimator_.coef_) plt.plot(X[inlier_mask], y[inlier_mask], '.g', label='Inliers') plt.plot(X[outlier_mask], y[outlier_mask], '.r', label='Outliers') plt.plot(line_X, line_y, '-k', label='Linear regressor') plt.plot(line_X, line_y_ransac, '-b', label='RANSAC regressor') plt.legend(loc='lower right'); Estimated coefficients (true, normal, RANSAC): (array(82.1903908407869), array([ 54.17236387]), array([ 82.08533159]))","title":"Fitting a robust regression model using RANSAC"},{"location":"w1-regression/1w/#evaluating-the-performance-of-linear-regression-models","text":"It is crucial to test the model on data that it hasn't seen during training to obtain an unbiased estimate of its performance. [ back to top ] from sklearn.cross_validation import train_test_split X = df.iloc[:, :-1].values y = df['MEDV'].values X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) # 70% \u7528\u4e8e train, 30%\u7528\u4e8e test slr = LinearRegression() slr.fit(X_train, y_train) y_train_pred = slr.predict(X_train) y_test_pred = slr.predict(X_test) # residual plot, \u7ecf\u5e38\u88ab\u7528\u6765\u68c0\u67e5\u56de\u5f52\u6a21\u578b plt.scatter(y_train_pred, y_train_pred - y_train, c='blue', marker='o', label='Training data') plt.scatter(y_test_pred, y_test_pred - y_test, c='lightgreen', marker='s', label='Test data') plt.xlabel('Predicted values') plt.ylabel('Residuals') plt.legend(loc='upper left') plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red') plt.xlim([-10, 50]) plt.tight_layout() # plt.savefig('./figures/slr_residuals.png', dpi=300) \u5982\u679c\u9884\u6d4b\u90fd\u662f\u6b63\u786e\u7684, \u90a3\u4e48 residual \u5c31\u662f0. \u8fd9\u662f\u7406\u60f3\u60c5\u51b5, \u5b9e\u9645\u4e2d, \u6211\u4eec\u5e0c\u671b error \u662f\u968f\u673a\u5206\u5e03\u7684. \u4ece\u4e0a\u56fe\u770b, \u6709\u90e8\u5206 error \u662f\u79bb\u7ea2\u8272\u7ebf\u8f83\u8fdc\u7684, \u53ef\u80fd\u662f outlier \u5f15\u8d77\u8f83\u5927\u7684\u504f\u5dee # \u53e6\u4e00\u79cd\u8bc4\u4f30\u65b9\u6cd5\u662f Mean Squred Error, MSE, \u5c31\u662f SSE \u7684\u5e73\u5747\u503c # R-squre \u4e5f\u662f\u91cd\u8981\u7684 measurement, \u5b83\u4ee3\u8868\u7740\u6709\u591a\u5c11\u767e\u5206\u6bd4\u7684\u6570\u636e\u88ab\u6a21\u578b\u89e3\u91ca. \u8d8a\u9ad8\u4ee3\u8868\u6a21\u578b\u62df\u5408\u8d8a\u597d from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error print('MSE train: %.3f, test: %.3f' % ( mean_squared_error(y_train, y_train_pred), mean_squared_error(y_test, y_test_pred))) print('R^2 train: %.3f, test: %.3f' % ( r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred))) MSE train: 19.958, test: 27.196 R^2 train: 0.765, test: 0.673 $$ MSE = \\frac 1 n \\sum^n_{i=1} (y^{(i)} - \\hat y^{(i)})$$ $$ R^2 = 1 - \\frac{SSE}{SST} = 1 - \\frac{MSE}{Var(y)}$$","title":"Evaluating the performance of linear regression models"},{"location":"w1-regression/1w/#turning-a-linear-regression-model-into-a-curve-polynomial-regression","text":"[ back to top ] import numpy as np from X = np.array([258.0, 270.0, 294.0, 320.0, 342.0, 368.0, 396.0, 446.0, 480.0, 586.0])[:, np.newaxis] y = np.array([236.4, 234.4, 252.8, 298.6, 314.2, 342.2, 360.8, 368.0, 391.2, 390.8]) # \u6dfb\u52a0\u4e8c\u6b21\u9879\u548c\u622a\u8ddd\u9879 from sklearn.preprocessing import PolynomialFeatures lr = LinearRegression() pr = LinearRegression() quadratic = PolynomialFeatures(degree=2) X_quad = quadratic.fit_transform(X) print(X.shape) print(X_quad.shape) (10, 1) (10, 3) X_quad array([[ 1.00000000e+00, 2.58000000e+02, 6.65640000e+04], [ 1.00000000e+00, 2.70000000e+02, 7.29000000e+04], [ 1.00000000e+00, 2.94000000e+02, 8.64360000e+04], [ 1.00000000e+00, 3.20000000e+02, 1.02400000e+05], [ 1.00000000e+00, 3.42000000e+02, 1.16964000e+05], [ 1.00000000e+00, 3.68000000e+02, 1.35424000e+05], [ 1.00000000e+00, 3.96000000e+02, 1.56816000e+05], [ 1.00000000e+00, 4.46000000e+02, 1.98916000e+05], [ 1.00000000e+00, 4.80000000e+02, 2.30400000e+05], [ 1.00000000e+00, 5.86000000e+02, 3.43396000e+05]]) # fit linear features lr.fit(X, y) X_fit = np.arange(250,600,10)[:, np.newaxis] y_lin_fit = lr.predict(X_fit) # fit quadratic features pr.fit(X_quad, y) y_quad_fit = pr.predict(quadratic.fit_transform(X_fit)) # plot results plt.scatter(X, y, label='training points') plt.plot(X_fit, y_lin_fit, label='linear fit', linestyle='--') plt.plot(X_fit, y_quad_fit, label='quadratic fit') plt.legend(loc='upper left') plt.tight_layout() \u56fe\u4e0a\u53ef\u4ee5\u53d1\u73b0 quadratic fit\u6bd4 linear \u62df\u5408\u6548\u679c\u66f4\u597d y_lin_pred = lr.predict(X) y_quad_pred = pr.predict(X_quad) print('Training MSE linear: %.3f, quadratic: %.3f' % ( mean_squared_error(y, y_lin_pred), mean_squared_error(y, y_quad_pred))) print('Training R^2 linear: %.3f, quadratic: %.3f' % ( r2_score(y, y_lin_pred), r2_score(y, y_quad_pred))) Training MSE linear: 569.780, quadratic: 61.330 Training R^2 linear: 0.832, quadratic: 0.982 MSE \u4e0b\u964d\u523061, R^2 \u4e0a\u5347\u523098%, \u8bf4\u660e\u5728\u8fd9\u4e2a\u6570\u636e\u96c6\u4e0a quadratic fit \u6548\u679c\u66f4\u597d","title":"Turning a linear regression model into a curve - Polynomial regression"},{"location":"w1-regression/1w/#modeling-nonlinear-relationships-in-the-housing-dataset","text":"\u6211\u4eec\u4f1a\u5c06house prices \u4e0e LSTAT \u7684 quadratic \u53ca cubic polynomials fit, \u5e76\u4e0e linear fit \u5bf9\u6bd4 back to top X = df[['LSTAT']].values y = df['MEDV'].values regr = LinearRegression() # create quadratic features quadratic = PolynomialFeatures(degree=2) cubic = PolynomialFeatures(degree=3) X_quad = quadratic.fit_transform(X) X_cubic = cubic.fit_transform(X) # fit features X_fit = np.arange(X.min(), X.max(), 1)[:, np.newaxis] regr = regr.fit(X, y) y_lin_fit = regr.predict(X_fit) linear_r2 = r2_score(y, regr.predict(X)) regr = regr.fit(X_quad, y) y_quad_fit = regr.predict(quadratic.fit_transform(X_fit)) quadratic_r2 = r2_score(y, regr.predict(X_quad)) regr = regr.fit(X_cubic, y) y_cubic_fit = regr.predict(cubic.fit_transform(X_fit)) cubic_r2 = r2_score(y, regr.predict(X_cubic)) # plot results plt.scatter(X, y, label='training points', color='lightgray') plt.plot(X_fit, y_lin_fit, label='linear (d=1), $R^2=%.2f$' % linear_r2, color='blue', lw=2, linestyle=':') plt.plot(X_fit, y_quad_fit, label='quadratic (d=2), $R^2=%.2f$' % quadratic_r2, color='red', lw=2, linestyle='-') plt.plot(X_fit, y_cubic_fit, label='cubic (d=3), $R^2=%.2f$' % cubic_r2, color='green', lw=2, linestyle='--') plt.xlabel('% lower status of the population [LSTAT]') plt.ylabel('Price in $1000\\'s [MEDV]') plt.legend(loc='upper right') plt.tight_layout() # plt.savefig('./figures/polyhouse_example.png', dpi=300) Transforming the dataset by log: \u4e3a\u4ec0\u4e48\u8981\u8fd9\u6837\u505a\uff1f\u662f\u56e0\u4e3a\u6709\u753b\u56fe\u63a2\u7d22\u7684\u542f\u793a\uff1f X = df[['LSTAT']].values y = df['MEDV'].values # transform features X_log = np.log(X) y_sqrt = np.sqrt(y) # fit features X_fit = np.arange(X_log.min()-1, X_log.max()+1, 1)[:, np.newaxis] regr = regr.fit(X_log, y_sqrt) y_lin_fit = regr.predict(X_fit) linear_r2 = r2_score(y_sqrt, regr.predict(X_log)) # plot results plt.scatter(X_log, y_sqrt, label='training points', color='lightgray') plt.plot(X_fit, y_lin_fit, label='linear (d=1), $R^2=%.2f$' % linear_r2, color='blue', lw=2) plt.xlabel('log(% lower status of the population [LSTAT])') plt.ylabel('$\\sqrt{Price \\; in \\; \\$1000\\'s [MEDV]}$') plt.legend(loc='lower left') plt.tight_layout() # plt.savefig('./figures/transform_example.png', dpi=300) plt.show() \u7ecf\u8fc7 log \u53d8\u6362\u540e\uff0c\u7ebf\u6027\u62df\u5408\u6548\u679c\u5df2\u7ecf\u4e0d\u9519, \u6bd4\u5355\u7eaf polynomial fit \u66f4\u597d","title":"Modeling nonlinear relationships in the Housing dataset"},{"location":"w1-regression/1w/#r2","text":"","title":"\u7ec3\u4e60\uff1a\u7528\u623f\u4ef7\u6570\u636e\u7684\u5176\u5b83\u81ea\u53d8\u91cf\u4e00\u8d77\u505a\u4e00\u4e2a\u591a\u5143\u6a21\u578b\u770b\u770bR2\u6709\u6ca1\u6709\u6539\u5584"},{"location":"w2-preceptron-and-logistic-regression/2w/","text":"Sections Implementing a perceptron learning algorithm in Python Training a perceptron model on the Iris dataset Adaptive linear neurons and the convergence of learning Implementing an adaptive linear neuron in Python Implementing logistic regression in Python Classification with scikit-learn Loading and preprocessing the data Other Available Data Training a perceptron via scikit-learn Modeling class probabilities via logistic regression Maximum margin classification with support vector machines Solving non-linear problems using a kernel SVM K-nearest neighbors - a lazy learning algorithm Scoring metrics for classification Classification metrics in Scikit-learn Reading a confusion matrix Precision, recall and F-measures ROC and AUC Hinge loss Log loss \u4ec0\u4e48\u662f\u611f\u77e5\u673a\u5206\u7c7b \u6700\u7b80\u5355\u5f62\u5f0f\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u662f\u4e00\u79cd\u4e8c\u5143\u7ebf\u6027\u5206\u7c7b\u5668, \u628a\u77e9\u9635\u4e0a\u7684\u8f93\u5165 $$\\displaystyle x$$ \uff08\u5b9e\u6570\u503c\u5411\u91cf\uff09\u6620\u5c04\u5230\u8f93\u51fa\u503c $$\\displaystyle f(x)$$ \u4e0a\uff08\u4e00\u4e2a\u4e8c\u5143\u7684\u503c\uff09\u3002 $$\\displaystyle f(x)={\\begin{cases}+1 {\\text{if }}w\\cdot x+b 0\\-1 {\\text{else}}\\end{cases}}$$ \u5b66\u4e60\u7b97\u6cd5 \u6211\u4eec\u9996\u5148\u5b9a\u4e49\u4e00\u4e9b\u53d8\u91cf\uff1a - $$\\displaystyle x(j)$$ \u8868\u793an\u7ef4\u8f93\u5165\u5411\u91cf\u4e2d\u7684\u7b2cj\u9879 - $$\\displaystyle w(j)$$ \u8868\u793a\u6743\u91cd\u5411\u91cf\u7684\u7b2cj\u9879 - $$\\displaystyle f(x)$$ \u8868\u793a\u795e\u7ecf\u5143\u63a5\u53d7\u8f93\u5165 $$\\displaystyle x$$ \u4ea7\u751f\u7684\u8f93\u51fa - $$\\displaystyle \\alpha $$ \u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u7b26\u5408 $$\\displaystyle 0 \\alpha \\leq 1$$ \uff08\u63a5\u53d7\u7387\uff09 - \u66f4\u8fdb\u4e00\u6b65\uff0c\u4e3a\u4e86\u7b80\u4fbf\u6211\u4eec\u5047\u5b9a\u504f\u7f6e\u91cf $$\\displaystyle b$$ \u7b49\u4e8e0\u3002\u56e0\u4e3a\u4e00\u4e2a\u989d\u5916\u7684\u7ef4\u5ea6 $$\\displaystyle n+1$$ \u7ef4\uff0c\u53ef\u4ee5\u7528 $$\\displaystyle x(n+1)=1$$ \u7684\u5f62\u5f0f\u52a0\u5230\u8f93\u5165\u5411\u91cf\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u7528 $$\\displaystyle w(n+1)$$ \u4ee3\u66ff\u504f\u7f6e\u91cf\u3002 \u611f\u77e5\u5668\u7684\u5b66\u4e60\u901a\u8fc7\u5bf9 \u6240\u6709\u8bad\u7ec3\u5b9e\u4f8b \u8fdb\u884c \u591a\u6b21\u7684\u8fed\u4ee3\u8fdb\u884c\u66f4\u65b0 \u7684\u65b9\u5f0f\u6765\u5efa\u6a21\u3002 \u4ee4 $$\\displaystyle D_{m}={(x_{1},y_{1}),\\dots ,(x_{m},y_{m})}$$ \u8868\u793a\u4e00\u4e2a\u6709 $$\\displaystyle m$$ \u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u7684\u8bad\u7ec3\u96c6\u3002 \u6bcf\u6b21\u8fed\u4ee3\u6743\u91cd\u5411\u91cf\u4ee5\u5982\u4e0b\u65b9\u5f0f\u66f4\u65b0\uff1a \u5bf9\u4e8e\u6bcf\u4e2a $$\\displaystyle D_{m}={(x_{1},y_{1}),\\dots ,(x_{m},y_{m})}$$ \u4e2d\u7684\u6bcf\u4e2a $$\\displaystyle (x,y)$$ \u5bf9\uff0c $$\\displaystyle w(j):=w(j)+{\\alpha (y-f(x))}{x(j)}\\quad (j=1,\\ldots ,n)$$ \u6ce8\u610f\u8fd9\u610f\u5473\u7740\uff0c\u4ec5\u5f53\u9488\u5bf9\u7ed9\u5b9a\u8bad\u7ec3\u5b9e\u4f8b $$\\displaystyle (x,y)$$ \u4ea7\u751f\u7684\u8f93\u51fa\u503c $$\\displaystyle f(x)$$ \u4e0e\u9884\u671f\u7684\u8f93\u51fa\u503c $$\\displaystyle y$$ \u4e0d\u540c\u65f6\uff0c\u6743\u91cd\u5411\u91cf\u624d\u4f1a\u53d1\u751f\u6539\u53d8\u3002 \u5982\u679c\u5b58\u5728\u4e00\u4e2a\u6b63\u7684\u5e38\u6570 $$\\displaystyle \\gamma $$ \u548c\u6743\u91cd\u5411\u91cf $$\\displaystyle w$$ \uff0c\u5bf9\u6240\u6709\u7684 $$\\displaystyle i$$ \u6ee1\u8db3 $$\\displaystyle y_{i}\\cdot \\left(\\langle w,x_{i}\\rangle +b\\right) \\gamma $$ \uff0c\u8bad\u7ec3\u96c6 $$\\displaystyle D_{m}$$ \u5c31\u88ab\u53eb\u505a\u7ebf\u6027\u5206\u9694\u3002 \u7136\u800c\uff0c\u5982\u679c\u8bad\u7ec3\u96c6\u4e0d\u662f\u7ebf\u6027\u5206\u9694\u7684\uff0c\u90a3\u4e48\u8fd9\u4e2a\u7b97\u6cd5\u5219\u4e0d\u80fd\u786e\u4fdd\u4f1a\u6536\u655b\u3002 Implementing a perceptron learning algorithm in Python [ back to top ] import numpy as np class Perceptron(object): Perceptron classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. def __init__(self, eta=0.01, n_iter=10): self.eta = eta self.n_iter = n_iter # the number of epochs def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) # weights, \u521d\u59cb\u503c0 self.errors_ = [] # \u5bf9\u6bcf\u4e2a sample \u5faa\u73af\u66f4\u65b0 for _ in range(self.n_iter): errors = 0 for xi, target in zip(X, y): update = self.eta * (target - self.predict(xi)) #learning rate*error self.w_[1:] += update * xi self.w_[0] += update errors += int(update != 0.0) self.errors_.append(errors) # \u9519\u8bef\u7684\u5206\u7c7b\u7ed3\u679c return self def net_input(self, X): Calculate net input w*x return np.dot(X, self.w_[1:]) + self.w_[0] def predict(self, X): Return class label after unit step return np.where(self.net_input(X) = 0.0, 1, -1) Training a perceptron model on the Iris dataset \u8fd9\u91cc\u53ea\u8003\u8651\u4e24\u79cd\u82b1 Setosa \u548c Versicolor , \u4ee5\u53ca\u4e24\u79cd\u7279\u5f81 sepal length \u548c petal length. \u4f46\u662f Perceptron Model \u53ef\u4ee5\u89e3\u51b3\u591a\u7c7b\u522b\u5206\u7c7b\u95ee\u9898, \u53c2\u8003 one-vs-all [ back to top ] Reading-in the Iris data import pandas as pd df = pd.read_csv('data/iris.csv', header=None) df.tail() 0 1 2 3 4 146 6.7 3 5.2 2.3 virginica 147 6.3 2.5 5 1.9 virginica 148 6.5 3 5.2 2 virginica 149 6.2 3.4 5.4 2.3 virginica 150 5.9 3 5.1 1.8 virginica Plotting the Iris data # \u5c06\u4e24\u4e2a\u5206\u7c7b\u5148\u53ef\u89c6\u5316 %matplotlib inline import matplotlib.pyplot as plt import numpy as np # select setosa and versicolor # \u4e24\u79cd\u5404\u9009\u62e950\u4e2a, \u628a\u7c7b\u522b\u6539\u4e3a -1 \u548c 1, \u65b9\u4fbf\u753b\u56fe y = df.iloc[0:100, 4].values y = np.where(y == 'Iris-setosa', -1, 1) # extract sepal length and petal length X = df.iloc[0:100, [0, 2]].values # plot data plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa') plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor') plt.xlabel('petal length [cm]') plt.ylabel('sepal length [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./iris_1.png', dpi=300) Training the perceptron model ppn = Perceptron(eta=0.1, n_iter=10) ppn.fit(X, y) ppn.errors_ [2, 2, 3, 2, 1, 0, 0, 0, 0, 0] # error \u753b\u56fe, \u68c0\u67e5\u662f\u5426 error \u8d8b\u8fd1\u4e8e0 \u5728\u591a\u6b21 loop \u66f4\u65b0\u540e plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o') plt.xlabel('Epochs') plt.ylabel('Number of misclassifications') plt.tight_layout() # plt.savefig('./perceptron_1.png', dpi=300) \u7ed3\u679c error \u7684\u786e\u6700\u540e\u4e3a 0, \u8bc1\u660e\u662f convergent \u7684, \u4e14\u5206\u7c7b\u6548\u679c\u5e94\u8be5\u8bf4\u662f\u975e\u5e38\u51c6\u786e\u4e86 A function for plotting decision regions \u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u753b\u51b3\u7b56\u8fb9\u754c from matplotlib.colors import ListedColormap # Colormap object generated from a list of colors. def plot_decision_regions(X, y, classifier, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface \u786e\u5b9a\u6a2a\u7eb5\u8f74\u8fb9\u754c x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 # \u6700\u5c0f-1, \u6700\u5927+1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 # create a pair of grid arrays # flatten the grid arrays then predict xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) # maps the different decision regions to different colors plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot class samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) plot_decision_regions(X, y, classifier=ppn) plt.xlabel('sepal length [cm]') plt.ylabel('petal length [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./perceptron_2.png', dpi=300) \u867d\u7136 Perceptron Model \u5728\u4e0a\u9762 Iris \u4f8b\u5b50\u91cc\u8868\u73b0\u5f97\u5f88\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u95ee\u9898\u4e0a\u5374\u4e0d\u4e00\u5b9a\u8868\u73b0\u5f97\u597d\u3002 Frank Rosenblatt \u4ece\u6570\u5b66\u4e0a\u8bc1\u660e\u4e86\uff0c\u5728\u7ebf\u6027\u53ef\u5206\u7684\u6570\u636e\u91cc\uff0cPerceptron \u7684\u5b66\u4e60\u89c4\u5219\u4f1a converge\uff0c\u4f46\u5728\u7ebf\u6027\u4e0d\u53ef\u5206\u7684\u60c5\u51b5\u4e0b\uff0c\u5374\u65e0\u6cd5 converge Adaptive linear neurons and the convergence of learning (Adaline) [ back to top ] Implementing an adaptive linear neuron in Python [ back to top ] ADAptive LInear NEuron classifier \u4e5f\u662f\u4e00\u4e2a\u5355\u5c42\u795e\u7ecf\u7f51\u7edc. \u5b83\u7684\u91cd\u70b9\u5c31\u662f\u5b9a\u4e49\u53ca\u6700\u4f18\u5316 cost function, \u5bf9\u4e8e\u7406\u89e3\u66f4\u9ad8\u5c42\u6b21\u66f4\u96be\u7684\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u6a21\u578b\u662f\u975e\u5e38\u597d\u7684\u5165\u95e8. \u5b83\u4e0e Perceptron \u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\u66f4\u65b0 weights \u65f6\u662f\u7528\u7684 linear activation function, \u800c\u4e0d\u662funit step function. Adaline \u4e2d\u8fd9\u4e2a linear activation function \u8f93\u51fa\u7b49\u4e8e\u8f93\u5165, $$ \u03c6(w^T x)=w^T x$$. \u7136\u540e activation \u540e\u4f1a\u6709\u4e00\u4e2a quantizer \u7528\u6765\u5b66\u4e60\u66f4\u65b0 weights \u5b9a\u4e49 cost function \u4e3a SSE: Sum of Squared Errors $$ \\displaystyle J(w)= \\frac 1 2 \\sum_i(y^{(i)} \u2212\\phi(z^{(i)}))^2$$ \u8fd9\u4e2a function \u662f\u53ef\u5bfc\u7684, \u5e76\u4e14\u662f convex \u7684, \u53ef\u4ee5\u8fdb\u884c\u6700\u4f18\u5316, \u4f7f\u7528 gradient descent \u7b97\u6cd5. class AdalineGD(object): ADAptive LInear NEuron classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. def __init__(self, eta=0.01, n_iter=50): self.eta = eta self.n_iter = n_iter def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] # gradient descent for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() cost = (errors**2).sum() / 2.0 self.cost_.append(cost) # cost list, to check algorithm convergence return self def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): Compute linear activation return self.net_input(X) def predict(self, X): Return class label after unit step return np.where(self.activation(X) = 0.0, 1, -1) # \u6d4b\u8bd5\u4e24\u79cd learning rate, 0.01 \u548c 0.0001 fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4)) ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y) ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o') ax[0].set_xlabel('Epochs') ax[0].set_ylabel('log(Sum-squared-error)') ax[0].set_title('Adaline - Learning rate 0.01') ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y) ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o') ax[1].set_xlabel('Epochs') ax[1].set_ylabel('Sum-squared-error') ax[1].set_title('Adaline - Learning rate 0.0001') plt.tight_layout() # plt.savefig('./adaline_1.png', dpi=300) \u5de6\u56fe\u663e\u793a learning rate \u592a\u5927, error \u6ca1\u6709\u53d8\u5c0f, \u53cd\u800c\u53d8\u5927\u4e86. \u53f3\u56fe\u663e\u793a learning rate \u592a\u5c0f, error \u53d8\u5316\u901f\u5ea6\u592a\u5c0f Standardizing features and re-training adaline $$ x^\\prime_j = \\frac {x_j - \\mu_j} {\\sigma_j} $$ # standardize features X_std = np.copy(X) X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std() X_std[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std() ada = AdalineGD(n_iter=15, eta=0.01) ada.fit(X_std, y) plot_decision_regions(X_std, y, classifier=ada) plt.title('Adaline - Gradient Descent') plt.xlabel('sepal length [standardized]') plt.ylabel('petal length [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./adaline_2.png', dpi=300) plt.show() plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o') plt.xlabel('Epochs') plt.ylabel('Sum-squared-error') plt.tight_layout() # plt.savefig('./adaline_3.png', dpi=300) \u5206\u7c7b\u6548\u679c\u4e0d\u9519, error \u6700\u7ec8\u63a5\u8fd1\u4e8e0 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136\u6211\u4eec\u7684\u5206\u7c7b\u5168\u90e8\u6b63\u786e\uff0c\u4f46 error \u4e5f\u4e0d\u7b49\u4e8e0 ada.w_ #weights array([ 1.36557432e-16, -1.26256159e-01, 1.10479201e+00]) Large scale machine learning and stochastic gradient descent Stochastic gradient descent \u968f\u673a\u68af\u5ea6\u4e0b\u964d \u6bd4\u4e00\u822c\u7684\u68af\u5ea6\u4e0b\u964d\u66f4\u6709\u4f18\u52bf, \u56e0\u4e3a\u6bcf\u4e00\u6b65\u8ba1\u7b97\u7684 cost \u66f4\u5c0f, \u6bcf\u4e00\u6b65\u66f4\u65b0\u90fd\u662f\u968f\u673a\u53d6\u5176\u4e2d\u4e00\u5c0f\u6b65\u66f4\u65b0\u5c31\u53ef. batch gradient descent \u4e00\u6b21\u66f4\u65b0\u9700\u8981\u8ba1\u7b97\u4e00\u904d\u6574\u4e2a\u6570\u636e\u96c6 $$ \\Delta w = \\eta \\sum_i (y^{(i)} - \\phi(z^{(i)}))x^{(i)} $$ stochastic gradient descent \u4e00\u6b21\u66f4\u65b0\u53ea\u9700\u8ba1\u7b97\u4e00\u4e2a\u6570\u636e\u70b9 $$ \\Delta w = \\eta(y^{(i)} - \\phi(z^{(i)}))x^{(i)} $$ class AdalineSGD(object): ADAptive LInear NEuron classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. shuffle : bool (default: True) Shuffles training data every epoch if True to prevent cycles. random_state : int (default: None) Set random state for shuffling and initializing the weights. def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None): self.eta = eta self.n_iter = n_iter self.w_initialized = False self.shuffle = shuffle if random_state: # allow the specication of a random seed for consistency np.random.seed(random_state) def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self._initialize_weights(X.shape[1]) self.cost_ = [] for i in range(self.n_iter): if self.shuffle: X, y = self._shuffle(X, y) cost = [] for xi, target in zip(X, y): cost.append(self._update_weights(xi, target)) avg_cost = sum(cost) / len(y) self.cost_.append(avg_cost) return self # \u5728\u6bcf\u4e2a epoch \u524d\u662f\u5426 shuffle data def _shuffle(self, X, y): Shuffle training data r = np.random.permutation(len(y)) return X[r], y[r] def _initialize_weights(self, m): Initialize weights to zeros self.w_ = np.zeros(1 + m) self.w_initialized = True #stochastic gradient descent def _update_weights(self, xi, target): Apply Adaline learning rule to update the weights output = self.net_input(xi) error = (target - output) self.w_[1:] += self.eta * xi.dot(error) # \u4ec5\u4e00\u4e2a error \u76f8\u4e58 self.w_[0] += self.eta * error # \u4ec5\u4ec5\u662f\u4e00\u4e2a error, \u800c\u975e sum cost = 0.5 * error**2 return cost def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): Compute linear activation return self.net_input(X) def predict(self, X): Return class label after unit step return np.where(self.activation(X) = 0.0, 1, -1) # plot result ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1) ada.fit(X_std, y) plot_decision_regions(X_std, y, classifier=ada) plt.title('Adaline - Stochastic Gradient Descent') plt.xlabel('sepal length [standardized]') plt.ylabel('petal length [standardized]') plt.legend(loc='upper left') plt.tight_layout() #plt.savefig('./adaline_4.png', dpi=300) plt.show() plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o') plt.xlabel('Epochs') plt.ylabel('Average Cost') plt.tight_layout() # plt.savefig('./adaline_5.png', dpi=300) \u5bf9\u6bd4\u53ef\u4ee5\u53d1\u73b0\uff0cStochastic gradient descent \u7684\u5b66\u4e60\u901f\u7387\u6bd4 Batch gradient descnent \u7684\u9ad8\u5f88\u591a Implementing logistic regression in Python [ back to top ] logistic regression: powerful algorithm for linear and binary classication problems odds ratio: the odds in favor of a particular event.$$\\displaystyle \\frac{p}{(1-p)}$$, if p stands for the probability of the positive event, we want to predict.$$\\displaystyle logit(p) = log\\frac{p}{1-p}$$ and $$\\displaystyle p(y=1|x)$$ is conditional probability that a particular sample belongs to class 1 given its features x. Therefore, the logit is $$\\displaystyle logit(p(y=1|x)) = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{i=0}^nw_ix_i$$ we want to know the probability, which is inverse of logit function, we call it logistic funcition, or sigmoid function . $$\\displaystyle \\phi (z) = \\frac{1}{1+e^{-z}}$$ output of sigmoid function is as the probability of particular sample belonging to class 1 Plot sigmoid function: %matplotlib inline import matplotlib.pyplot as plt import numpy as np def sigmoid(z): return 1.0 / (1.0 + np.exp(-z)) z = np.arange(-7, 7, 0.1) phi_z = sigmoid(z) plt.plot(z, phi_z) plt.axvline(0.0, color='k') plt.ylim(-0.1, 1.1) plt.xlabel('z') plt.ylabel('$\\phi (z)$') # y axis ticks and gridline plt.yticks([0.0, 0.5, 1.0]) ax = plt.gca() ax.yaxis.grid(True) plt.tight_layout() # plt.savefig('./figures/sigmoid.png', dpi=300) when $$\\phi(z)$$ approached 1 if z\u2192$$\\infty$$, goes to 1 if z\u2192$$-\\infty$$ Plot cost function: use log-likelihood function to redefine cost function then $$\\displaystyle J(\\phi(z),y;w)={\\begin{cases}-log(\\phi(z)) {\\text{if }}y=1\\-log(1-\\phi(z)) {\\text{if }}y=0\\end{cases}}$$ def cost_1(z): return - np.log(sigmoid(z)) def cost_0(z): return - np.log(1 - sigmoid(z)) z = np.arange(-10, 10, 0.1) phi_z = sigmoid(z) c1 = [cost_1(x) for x in z] plt.plot(phi_z, c1, label='J(w) if y=1') c0 = [cost_0(x) for x in z] plt.plot(phi_z, c0, linestyle='--', label='J(w) if y=0') plt.ylim(0.0, 5.1) plt.xlim([0, 1]) plt.xlabel('$\\phi$(z)') plt.ylabel('J(w)') plt.legend(loc='best') plt.tight_layout() # plt.savefig('./figures/log_cost.png', dpi=300) # this illustrates the cost for the classification of a single-sample instance for diff values of phi(z) cost \u8d8b\u8fd1\u4e8e0 \u5982\u679c\u6b63\u786e\u9884\u6d4b class. Implement in Python The following implementation is similar to the Adaline implementation except that we replace the sum of squared errors cost function with the logistic cost function $$J(\\mathbf{w}) = \\sum_{i=1}^{m} - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg).$$ class LogisticRegression(object): LogisticRegression classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. cost_ : list Cost in every epoch. def __init__(self, eta=0.01, n_iter=50): self.eta = eta self.n_iter = n_iter def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): y_val = self.activation(X) errors = (y - y_val) neg_grad = X.T.dot(errors) self.w_[1:] += self.eta * neg_grad self.w_[0] += self.eta * errors.sum() self.cost_.append(self._logit_cost(y, self.activation(X))) return self def _logit_cost(self, y, y_val): logit = -y.dot(np.log(y_val)) - ((1 - y).dot(np.log(1 - y_val))) return logit def _sigmoid(self, z): return 1.0 / (1.0 + np.exp(-z)) def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): Activate the logistic neuron z = self.net_input(X) return self._sigmoid(z) def predict_proba(self, X): Predict class probabilities for X. Parameters ---------- X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns ---------- Class 1 probability : float return activation(X) def predict(self, X): Predict class labels for X. Parameters ---------- X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns ---------- class : int Predicted class label. # equivalent to np.where(self.activation(X) = 0.5, 1, 0) return np.where(self.net_input(X) = 0.0, 1, 0) y[y == -1] = 0 # \u5c06\u8d1f\u6027\u6807\u7b7e\u7f16\u7801\u4e3a 0 y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) lr = LogisticRegression(n_iter=500, eta=0.02).fit(X_std, y) plt.plot(range(1, len(lr.cost_) + 1), np.log10(lr.cost_)) plt.xlabel('Epochs') plt.ylabel('Cost') plt.title('Logistic Regression - Learning rate 0.02') plt.tight_layout() plot_decision_regions(X_std, y, classifier=lr) plt.title('Logistic Regression - Gradient Descent') plt.xlabel('sepal length [standardized]') plt.ylabel('petal length [standardized]') plt.legend(loc='upper left') plt.tight_layout() Classification with scikit-learn [ back to top ] Loading and preprocessing the data [ back to top ] Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica. from sklearn import datasets import numpy as np iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target print('Class labels:', np.unique(y)) print(iris.target_names) ('Class labels:', array([0, 1, 2])) ['setosa' 'versicolor' 'virginica'] Splitting data into 70% training and 30% test data: from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) Standardizing the features: from sklearn.preprocessing import StandardScaler sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) # standardize by mean std X_test_std = sc.transform(X_test) Other Available Data [ back to top ] Scikit-learn makes available a host of datasets for testing learning algorithms . They come in three flavors: Packaged Data: these small datasets are packaged with the scikit-learn installation, and can be downloaded using the tools in sklearn.datasets.load_* Downloadable Data: these larger datasets are available for download, and scikit-learn includes tools which streamline this process. These tools can be found in sklearn.datasets.fetch_* Generated Data: there are several datasets which are generated from models based on a random seed. These are available in the sklearn.datasets.make_* You can explore the available dataset loaders, fetchers, and generators using IPython's tab-completion functionality. After importing the datasets submodule from sklearn , type datasets.load_ TAB or datasets.fetch_ TAB or datasets.make_ TAB to see a list of available functions. The data downloaded using the fetch_ scripts are stored locally, within a subdirectory of your home directory. You can use the following to determine where it is: from sklearn.datasets import get_data_home get_data_home() Training a perceptron via scikit-learn [ back to top ] from sklearn.linear_model import Perceptron # sklearn \u4e2d\u6709\u5c01\u88c5\u597d\u7684 Perceptron \u51fd\u6570 ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0) ppn.fit(X_train_std, y_train) Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True, n_iter=40, n_jobs=1, penalty=None, random_state=0, shuffle=True, verbose=0, warm_start=False) y_test.shape (45,) y_pred = ppn.predict(X_test_std) # predict print('Misclassified samples: %d' % (y_test != y_pred).sum()) # \u9519\u8bef\u4e2a\u6570 Misclassified samples: 4 from sklearn.metrics import accuracy_score print('Accuracy: %.2f' % accuracy_score(y_test, y_pred)) # 91% \u7684\u51c6\u786e\u7387 Accuracy: 0.91 from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt %matplotlib inline # \u91cd\u65b0\u5b9a\u4e49\u753b\u51b3\u7b56\u8fb9\u754c\u51fd\u6570, \u4f7f\u5f97\u80fd\u533a\u5206\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot all samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) # highlight test samples if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidth=1, marker='o', s=55, label='test set') Training a perceptron model using the standardized training data: X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X=X_combined_std, y=y_combined, classifier=ppn, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/iris_perceptron_scikit.png', dpi=300) # \u8fd9\u6b21\u662f3\u4e2a\u5206\u7c7b\u4e00\u8d77 Peceptron \u6a21\u578b\u5bf9\u4e8e\u5e76\u4e0d\u662f\u5b8c\u5168\u7ebf\u6027\u9694\u79bb\u7684 dataset \u4e0d\u80fd converge, \u6240\u4ee5\u5b9e\u9645\u5e94\u7528\u4e2d\u5e76\u4e0d\u591a\u7528. Modeling class probabilities via logistic regression [ back to top ] # use Logistic Regression from sklearn.linear_model import LogisticRegression # C parameter \u662f\u4ec0\u4e48\u5462? lr = LogisticRegression(C=1000.0, random_state=0) lr.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/logistic_regression.png', dpi=300) lr.predict_proba(X_test_std[0,:].reshape(1,-1)) # predict probability array([[ 2.05743774e-11, 6.31620264e-02, 9.36837974e-01]]) Regularization path \u89e3\u51b3 overfitting: \u6a21\u578b\u62df\u5408\u7684\u8fc7\u597d, \u4ee5\u81f4\u4e8e\u6ca1\u6709\u4e00\u822c\u6027, \u9884\u6d4b\u65b0\u7684\u6837\u672c\u7684\u7ed3\u679c\u5c31\u4f1a\u5f88\u5dee \u4e00\u822c\u8fc7\u62df\u5408\u7684\u6a21\u578b\u4f1a\u6709 high variance \u6700\u5e38\u7528\u7684\u89e3\u51b3\u65b9\u6cd5\u5c31\u53eb\u505a L2 regulatization$$\\displaystyle \\frac{\\lambda}{2} \\lVert w \\rVert^2 = \\frac{\\lambda}{2}\\sum_{j=1}^m w_j^2$$ \u5176\u4e2d$$\\lambda$$ \u5c31\u662f regularization parameter, \u53ef\u4ee5\u7528\u6765\u63a7\u5236\u62df\u5408\u8bad\u7ec3\u6570\u636e\u7684\u597d\u574f, \u800c $$C = \\frac{1}{\\lambda}$$ \u5c31\u662f\u524d\u9762\u63d0\u5230\u8fc7\u7684 parameter weights, params = [], [] for c in np.arange(-5, 5): lr = LogisticRegression(C=10**c, random_state=0) lr.fit(X_train_std, y_train) weights.append(lr.coef_[1]) params.append(10**c) weights = np.array(weights) plt.plot(params, weights[:, 0], label='petal length') plt.plot(params, weights[:, 1], linestyle='--', label='petal width') plt.ylabel('weight coefficient') plt.xlabel('C') plt.legend(loc='upper left') plt.xscale('log') # plt.savefig('./figures/regression_path.png', dpi=300) # C \u51cf\u5c0f\u7684\u8bdd, \u5c31\u662f\u589e\u52a0 regularization Logistic regression with regularization class LogitGD(object): Logistic Regression classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. def __init__(self, eta=0.01, lamb = 0.01, n_iter=50): self.eta = eta self.n_iter = n_iter self.lamb = lamb def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) - self.lamb* self.w_[1:] self.w_[0] += self.eta * errors.sum() cost = (errors**2).sum() / 2.0 + self.lamb* np.sum(self.w_[1:]**2) self.cost_.append(cost) return self def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def sigmoid(z): return 1.0 / (1.0 + np.exp(-z)) def activation(self, X): Compute linear activation return sigmoid(self.net_input(X)) def predict(self, X): Return class label after unit step return np.where(self.activation(X) = 0.5, 1, -1) \u5176\u5b83\u7684\u5206\u7c7b\u5668\u7b80\u4ecb Maximum margin classification with support vector machines \u76ee\u7684\u662f maximize the margin , margin \u662f\u5206\u79bb\u51b3\u7b56\u8fb9\u754c\u4e0e\u79bb\u4e4b\u6700\u8fd1\u7684\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u8ddd\u79bb. [ back to top ] # train SVC from sklearn.svm import SVC svm = SVC(kernel='linear', C=1.0, random_state=0) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_linear.png', dpi=300) Solving non-linear problems using a kernel SVM SVM \u53ef\u4ee5\u89e3\u51b3\u975e\u7ebf\u6027\u95ee\u9898 [ back to top ] # create a simple dataset np.random.seed(0) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] 0, X_xor[:, 1] 0) # 100\u4e2a with label 1, 100 withlabel 0 y_xor = np.where(y_xor, 1, -1) plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='b', marker='x', label='1') plt.scatter(X_xor[y_xor==-1, 0], X_xor[y_xor==-1, 1], c='r', marker='s', label='-1') plt.xlim([-3, 3]) plt.ylim([-3, 3]) plt.legend(loc='best') plt.tight_layout() # plt.savefig('./figures/xor.png', dpi=300) # \u4f7f\u7528\u666e\u901a\u7684 linear logistic Regression \u4e0d\u80fd\u5f88\u597d\u5c06\u6837\u672c\u5206\u4e3a+ve \u548c-ve rbf \u662f\u6307 radial basis function kernel \u6216\u8005 Gaussian kernel $$\\displaystyle k(x^{(i)}, x^{(j)}) = exp (- \\frac{\\lVert x^{(i)}-x^{(j)} \\rVert^2}{2 \\sigma^2})$$ simplified to $$\\displaystyle exp(-\\gamma \\lVert x^{(i)}-x^{(j)} \\rVert^2)$$ with $$\\gamma = \\frac{1}{2\\sigma^2}$$ # \u4f7f\u7528 svm kernel \u65b9\u6cd5, \u6295\u5c04\u5230\u9ad8\u7eac\u5ea6\u4e2d, \u4f7f\u4e4b\u6210\u4e3a\u7ebf\u6027\u53ef\u5206\u79bb\u7684 svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_rbf_xor.png', dpi=300) \u5176\u4e2d$$\\gamma$$ parameter \u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a cut-off parameter for Gaussian sphere \u5f53$$\\gamma$$ \u589e\u52a0, \u4e5f\u5c31\u589e\u52a0\u4e86\u8bad\u7ec3\u6837\u672c\u7684\u5f71\u54cd, \u4e5f\u5c31\u4f1a\u4f7f\u51b3\u7b56\u8fb9\u754c\u53d8\u5f97\u6a21\u7cca from sklearn.svm import SVC ## gamma \u8f83\u5c0f svm = SVC(kernel='rbf', random_state=0, gamma=0.2, C=1.0) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_rbf_iris_1.png', dpi=300) # gamma \u5f88\u5927, \u8fb9\u754c tight svm = SVC(kernel='rbf', random_state=0, gamma=100.0, C=1.0) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_rbf_iris_2.png', dpi=300) K-nearest neighbors - a lazy learning algorithm it doesn't learn a discriminative function from the training data but memorizes the training dataset instead. Choose the number of k and a distance metric. Find the k nearest neighbors of the sample that we want to classify. Assign the class label by majority vote. \u8fd9\u79cd\u65b9\u6cd5\u597d\u5904\u5728\u4e8e\u65b0\u6570\u636e\u8fdb\u6765, \u5206\u7c7b\u5668\u53ef\u4ee5\u9a6c\u4e0a\u5b66\u4e60\u5e76\u9002\u5e94, \u4f46\u662f\u8ba1\u7b97\u6210\u672c\u4e5f\u662f\u7ebf\u6027\u589e\u957f, \u5b58\u50a8\u4e5f\u662f\u95ee\u9898. [ back to top ] from sklearn.neighbors import KNeighborsClassifier # \u5bfb\u627e5\u4e2a\u90bb\u5c45 knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski') knn.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=knn, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/k_nearest_neighbors.png', dpi=300) \u5982\u4f55\u9009\u62e9 k \u662f\u4e00\u4e2a\u91cd\u70b9, \u5e76\u4e14\u9700\u8981\u6807\u51c6\u5316\u6570\u636e. \u4f8b\u5b50\u4e2d\u7528\u5230\u7684'minkowski' distance \u662f\u666e\u901a\u7684 Euclidean \u548c Manhattan distance \u7684\u6269\u5c55. $$\\displaystyle d(x^{(i)}, x^{(j)}) = \\sqrt[p]{\\sum_k \\left|x_k^{(i)}x_k^{(j)}\\right|^p} $$ Scoring metrics for classification [ back to top ] Classification metrics in Scikit-learn [ back to top ] The sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the sample_weight parameter. Some of these are restricted to the binary classification case: matthews_corrcoef (y_true, y_pred) Compute the Matthews correlation coefficient (MCC) for binary classes precision_recall_curve (y_true, probas_pred) Compute precision-recall pairs for different probability thresholds roc_curve (y_true, y_score[, pos_label, ...]) Compute Receiver operating characteristic (ROC) Others also work in the multiclass case: confusion_matrix (y_true, y_pred[, labels]) Compute confusion matrix to evaluate the accuracy of a classification hinge_loss (y_true, pred_decision[, labels, ...]) Average hinge loss (non-regularized) Some also work in the multilabel case: accuracy_score (y_true, y_pred[, normalize, ...]) Accuracy classification score. classification_report (y_true, y_pred[, ...]) Build a text report showing the main classification metrics f1_score (y_true, y_pred[, labels, ...]) Compute the F1 score, also known as balanced F-score or F-measure fbeta_score (y_true, y_pred, beta[, labels, ...]) Compute the F-beta score hamming_loss (y_true, y_pred[, classes]) Compute the average Hamming loss. jaccard_similarity_score (y_true, y_pred[, ...]) Jaccard similarity coefficient score log_loss (y_true, y_pred[, eps, normalize, ...]) Log loss, aka logistic loss or cross-entropy loss. precision_recall_fscore_support (y_true, y_pred) Compute precision, recall, F-measure and support for each class precision_score (y_true, y_pred[, labels, ...]) Compute the precision recall_score (y_true, y_pred[, labels, ...]) Compute the recall zero_one_loss (y_true, y_pred[, normalize, ...]) Zero-one classification loss. And some work with binary and multilabel (but not multiclass) problems: average_precision_score (y_true, y_score[, ...]) Compute average precision (AP) from prediction scores roc_auc_score (y_true, y_score[, average, ...]) Compute Area Under the Curve (AUC) from prediction scores # \u6784\u5efa\u6570\u636e from sklearn import datasets from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC X, y = datasets.make_classification(n_classes=2, random_state=0) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) # standardize by mean std X_test_std = sc.transform(X_test) model = SVC(probability=True, random_state=0) model.fit(X_train_std, y_train); default score for classification in sklearn is accuracy (\u6807\u7b7e\u9884\u6d4b\u6b63\u786e\u7684\u6bd4\u4f8b) $$ accuracy(y, \\hat y) = \\frac 1 n \\sum^{n - 1}_{i=0} 1 (\\hat y_i = y_i)$$ where $$1(x)$$ is the indicator function model.score(X_test_std, y_test) 0.83333333333333337 from sklearn.metrics import accuracy_score y_pred = model.predict(X_test_std) accuracy_score(y_test, y_pred) 0.83333333333333337 Reading a confusion matrix [ back to top ] For multi-class problems, it is often interesting to know which of the classes are hard to predict, and which are easy, or which classes get confused. One way to get more information about misclassifications is the confusion_matrix , which shows for each true class, how frequent a given predicted outcome is. from sklearn.metrics import confusion_matrix y_test_pred = model.predict(X_test_std) confmat = confusion_matrix(y_test, y_test_pred) print(confmat) [[15 3] [ 2 10]] plt.matshow(confusion_matrix(y_test, y_test_pred), cmap=plt.cm.Blues) plt.colorbar() plt.xlabel( Predicted label ) plt.ylabel( True label ); Precision, recall and F-measures [ back to top ] Precision is how many of the predictions for a class are actually that class. Recall is how many of the true positives were recovered: f1-score is the geometric average of precision and recall: With TP, FP, TN, FN, FPR, TPR standing for \"true positive\", \"false positive\", \"true negative\" and \"false negative\", \"false positive rate\", \"true positive rate\" repectively: \\begin{align} PRE = \\frac{TP}{TP+FP} \\ REC = TPR = \\frac{TP}{FN+TP} \\ F1 = 2 \\frac{PRE \\times REC}{PRE+REC} \\ F_\\beta = (1+\\beta^2)\\frac{PRE \\times REC}{\\beta^2 PRE+REC} \\ FPR = \\frac{FP}{FP+TN} \\ TPR = \\frac{TP}{FN+TP} \\end{align} from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_test_pred)) print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_test_pred)) print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_test_pred)) print('F_beta2: %.3f' % fbeta_score(y_true=y_test, y_pred=y_test_pred, beta=2)) Precision: 0.769 Recall: 0.833 F1: 0.800 F_beta2: 0.820 Another useful function is the classification_report which provides precision, recall, fscore and support for all classes. from sklearn.metrics import classification_report print(classification_report(y_test, y_test_pred)) precision recall f1-score support 0 0.88 0.83 0.86 18 1 0.77 0.83 0.80 12 avg / total 0.84 0.83 0.83 30 These metrics are helpful in two particular cases that come up often in practice: 1. Imbalanced classes, that is one class might be much more frequent than the other. 2. Asymmetric costs, that is one kind of error is much more \"costly\" than the other. ROC and AUC [ back to top ] A receiver operating characteristic curve, or ROC curve , is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate. \u5982\u679c\u5206\u7c7b\u5668\u6548\u679c\u5f88\u597d, \u90a3\u4e48\u56fe\u5e94\u8be5\u4f1a\u5728\u5de6\u4e0a\u89d2. \u5728 ROC curve \u7684\u57fa\u7840\u4e0a, \u53ef\u4ee5\u8ba1\u7b97 AUC -- area under the curve. Area Under Curve The AUC is a common evaluation metric for binary classification problems. Consider a plot of the true positive rate vs the false positive rate as the threshold value for classifying an item as 0 or is increased from 0 to 1: if the classifier is very good, the true positive rate will increase quickly and the area under the curve will be close to 1. If the classifier is no better than random guessing, the true positive rate will increase linearly with the false positive rate and the area under the curve will be around 0.5. One characteristic of the AUC is that it is independent of the fraction of the test population which is class 0 or class 1: this makes the AUC useful for evaluating the performance of classifiers on unbalanced data sets. def roc_curve(true_labels, predicted_probs, n_points=100, pos_class=1): thr = np.linspace(0,1,n_points) tpr = np.zeros(n_points) fpr = np.zeros(n_points) pos = true_labels == pos_class neg = np.logical_not(pos) n_pos = np.count_nonzero(pos) n_neg = np.count_nonzero(neg) for i,t in enumerate(thr): tpr[i] = np.count_nonzero(np.logical_and(predicted_probs = t, pos)) / float(n_pos) fpr[i] = np.count_nonzero(np.logical_and(predicted_probs = t, neg)) / float(n_neg) return fpr, tpr, thr df_imputed = pd.read_csv('df_imputed') features = ['revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents', 'income_bins', 'age_bin', 'monthly_income_scaled'] y = df_imputed.serious_dlqin2yrs X = pd.get_dummies(df_imputed[features], columns = ['income_bins', 'age_bin']) from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(X, y ,train_size=0.7,random_state=1) # Randomly generated predictions should give us a diagonal ROC curve preds = np.random.rand(len(test_y)) fpr, tpr, thr = roc_curve(test_y, preds) plt.plot(fpr, tpr); from sklearn.linear_model import LogisticRegression clf = LogisticRegression() clf.fit(train_X,train_y) preds = clf.predict_proba(test_X)[:,1] fpr, tpr, thr = roc_curve(test_y, preds) plt.plot(fpr, tpr); Log loss [ back to top ] Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs ( predict_proba ) of a classifier instead of its discrete predictions. For binary classification with a true label $$y \\in {0,1}$$ and a probability estimate $$p = \\operatorname{Pr}(y = 1)$$, the log loss per sample is the negative log-likelihood of the classifier given the true label: $$ L_{log}(y, p) = -log Pr(y|p) = -(y log(p) + (1-y) log(1-p)) $$ This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix $$Y$$, i.e., $$y_{i,k} = 1$$ if sample $$i$$ has label $$k$$ taken from a set of $$K$$ labels. Let $$P$$ be a matrix of probability estimates, with $$p_{i,k} = \\operatorname{Pr}(t_{i,k} = 1)$$. Then the log loss of the whole set is $$L_{log}(Y, P) = -logPr(Y|P) = - \\frac 1 N \\sum^{N-1} {i=0} \\sum^{K-1} {k=0} y_{i,k}logp_{i,k} $$ To see how this generalizes the binary log loss given above, note that in the binary case, we have $$p_{i,0} = 1 - p_{i,1}$$ and $$y_{i,0} = 1 - y_{i,1}$$, so expanding the inner sum over $$y_{i,k} \\in {0,1}$$ gives the binary log loss. The log_loss function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator\u2019s predict_proba method. from sklearn.metrics import log_loss y_true = [0, 0, 1, 1] y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]] log_loss(y_true, y_pred) 0.17380733669106749 Hinge loss [ back to top ] The hinge_loss function computes the average distance between the model and the data using hinge loss , a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.) If the labels are encoded with +1 and -1, $$y$$: is the true value, and $w$ is the predicted decisions as output by decision_function, then the hinge loss is defined as: $$ L_{Hinge}(y, w) = max{1-wy, 0} = |1-wy|_+ $$ If there are more than two labels, hinge_loss uses a multiclass variant due to Crammer Singer. If $$y_w$$ is the predicted decision for true label and $$y_t$$ is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by: $$ L_{Hinge}(y_w, y_t) = max{1+y_t-y_w,0}$$ # Here a small example demonstrating the use of the hinge_loss function # with a svm classifier in a binary class problem: from sklearn import svm from sklearn.metrics import hinge_loss X = [[0], [1]] y = [-1, 1] est = svm.LinearSVC(random_state=0) est.fit(X, y) pred_decision = est.decision_function([[-2], [3], [0.5]]) print(pred_decision) print(hinge_loss([-1, 1, 1], pred_decision)) [-2.18173682 2.36360149 0.09093234] 0.303022554204 # Here is an example demonstrating the use of the hinge_loss function # with a svm classifier in a multiclass problem: X = np.array([[0], [1], [2], [3]]) Y = np.array([0, 1, 2, 3]) labels = np.array([0, 1, 2, 3]) est = svm.LinearSVC() est.fit(X, Y) pred_decision = est.decision_function([[-1], [2], [3]]) y_true = [0, 2, 3] hinge_loss(y_true, pred_decision, labels) 0.56412359941917456 \u7ec3\u4e60\uff1a\u5c1d\u8bd5\u5728\u4fe1\u8d37\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u6b63\u5219\u5316\u65b9\u6cd5\uff0c \u753b\u51fa\u7cfb\u6570\u7684\u53d8\u5316\uff0c\u4ee5\u53ca\u6700\u7ec8\u7684\u9884\u6d4b\u6548\u679c","title":"2w"},{"location":"w2-preceptron-and-logistic-regression/2w/#sections","text":"Implementing a perceptron learning algorithm in Python Training a perceptron model on the Iris dataset Adaptive linear neurons and the convergence of learning Implementing an adaptive linear neuron in Python Implementing logistic regression in Python Classification with scikit-learn Loading and preprocessing the data Other Available Data Training a perceptron via scikit-learn Modeling class probabilities via logistic regression Maximum margin classification with support vector machines Solving non-linear problems using a kernel SVM K-nearest neighbors - a lazy learning algorithm Scoring metrics for classification Classification metrics in Scikit-learn Reading a confusion matrix Precision, recall and F-measures ROC and AUC Hinge loss Log loss","title":"Sections"},{"location":"w2-preceptron-and-logistic-regression/2w/#_1","text":"\u6700\u7b80\u5355\u5f62\u5f0f\u7684\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u662f\u4e00\u79cd\u4e8c\u5143\u7ebf\u6027\u5206\u7c7b\u5668, \u628a\u77e9\u9635\u4e0a\u7684\u8f93\u5165 $$\\displaystyle x$$ \uff08\u5b9e\u6570\u503c\u5411\u91cf\uff09\u6620\u5c04\u5230\u8f93\u51fa\u503c $$\\displaystyle f(x)$$ \u4e0a\uff08\u4e00\u4e2a\u4e8c\u5143\u7684\u503c\uff09\u3002 $$\\displaystyle f(x)={\\begin{cases}+1 {\\text{if }}w\\cdot x+b 0\\-1 {\\text{else}}\\end{cases}}$$","title":"\u4ec0\u4e48\u662f\u611f\u77e5\u673a\u5206\u7c7b"},{"location":"w2-preceptron-and-logistic-regression/2w/#_2","text":"\u6211\u4eec\u9996\u5148\u5b9a\u4e49\u4e00\u4e9b\u53d8\u91cf\uff1a - $$\\displaystyle x(j)$$ \u8868\u793an\u7ef4\u8f93\u5165\u5411\u91cf\u4e2d\u7684\u7b2cj\u9879 - $$\\displaystyle w(j)$$ \u8868\u793a\u6743\u91cd\u5411\u91cf\u7684\u7b2cj\u9879 - $$\\displaystyle f(x)$$ \u8868\u793a\u795e\u7ecf\u5143\u63a5\u53d7\u8f93\u5165 $$\\displaystyle x$$ \u4ea7\u751f\u7684\u8f93\u51fa - $$\\displaystyle \\alpha $$ \u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u7b26\u5408 $$\\displaystyle 0 \\alpha \\leq 1$$ \uff08\u63a5\u53d7\u7387\uff09 - \u66f4\u8fdb\u4e00\u6b65\uff0c\u4e3a\u4e86\u7b80\u4fbf\u6211\u4eec\u5047\u5b9a\u504f\u7f6e\u91cf $$\\displaystyle b$$ \u7b49\u4e8e0\u3002\u56e0\u4e3a\u4e00\u4e2a\u989d\u5916\u7684\u7ef4\u5ea6 $$\\displaystyle n+1$$ \u7ef4\uff0c\u53ef\u4ee5\u7528 $$\\displaystyle x(n+1)=1$$ \u7684\u5f62\u5f0f\u52a0\u5230\u8f93\u5165\u5411\u91cf\uff0c\u8fd9\u6837\u6211\u4eec\u5c31\u53ef\u4ee5\u7528 $$\\displaystyle w(n+1)$$ \u4ee3\u66ff\u504f\u7f6e\u91cf\u3002 \u611f\u77e5\u5668\u7684\u5b66\u4e60\u901a\u8fc7\u5bf9 \u6240\u6709\u8bad\u7ec3\u5b9e\u4f8b \u8fdb\u884c \u591a\u6b21\u7684\u8fed\u4ee3\u8fdb\u884c\u66f4\u65b0 \u7684\u65b9\u5f0f\u6765\u5efa\u6a21\u3002 \u4ee4 $$\\displaystyle D_{m}={(x_{1},y_{1}),\\dots ,(x_{m},y_{m})}$$ \u8868\u793a\u4e00\u4e2a\u6709 $$\\displaystyle m$$ \u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u7684\u8bad\u7ec3\u96c6\u3002 \u6bcf\u6b21\u8fed\u4ee3\u6743\u91cd\u5411\u91cf\u4ee5\u5982\u4e0b\u65b9\u5f0f\u66f4\u65b0\uff1a \u5bf9\u4e8e\u6bcf\u4e2a $$\\displaystyle D_{m}={(x_{1},y_{1}),\\dots ,(x_{m},y_{m})}$$ \u4e2d\u7684\u6bcf\u4e2a $$\\displaystyle (x,y)$$ \u5bf9\uff0c $$\\displaystyle w(j):=w(j)+{\\alpha (y-f(x))}{x(j)}\\quad (j=1,\\ldots ,n)$$ \u6ce8\u610f\u8fd9\u610f\u5473\u7740\uff0c\u4ec5\u5f53\u9488\u5bf9\u7ed9\u5b9a\u8bad\u7ec3\u5b9e\u4f8b $$\\displaystyle (x,y)$$ \u4ea7\u751f\u7684\u8f93\u51fa\u503c $$\\displaystyle f(x)$$ \u4e0e\u9884\u671f\u7684\u8f93\u51fa\u503c $$\\displaystyle y$$ \u4e0d\u540c\u65f6\uff0c\u6743\u91cd\u5411\u91cf\u624d\u4f1a\u53d1\u751f\u6539\u53d8\u3002 \u5982\u679c\u5b58\u5728\u4e00\u4e2a\u6b63\u7684\u5e38\u6570 $$\\displaystyle \\gamma $$ \u548c\u6743\u91cd\u5411\u91cf $$\\displaystyle w$$ \uff0c\u5bf9\u6240\u6709\u7684 $$\\displaystyle i$$ \u6ee1\u8db3 $$\\displaystyle y_{i}\\cdot \\left(\\langle w,x_{i}\\rangle +b\\right) \\gamma $$ \uff0c\u8bad\u7ec3\u96c6 $$\\displaystyle D_{m}$$ \u5c31\u88ab\u53eb\u505a\u7ebf\u6027\u5206\u9694\u3002 \u7136\u800c\uff0c\u5982\u679c\u8bad\u7ec3\u96c6\u4e0d\u662f\u7ebf\u6027\u5206\u9694\u7684\uff0c\u90a3\u4e48\u8fd9\u4e2a\u7b97\u6cd5\u5219\u4e0d\u80fd\u786e\u4fdd\u4f1a\u6536\u655b\u3002","title":"\u5b66\u4e60\u7b97\u6cd5"},{"location":"w2-preceptron-and-logistic-regression/2w/#implementing-a-perceptron-learning-algorithm-in-python","text":"[ back to top ] import numpy as np class Perceptron(object): Perceptron classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. def __init__(self, eta=0.01, n_iter=10): self.eta = eta self.n_iter = n_iter # the number of epochs def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) # weights, \u521d\u59cb\u503c0 self.errors_ = [] # \u5bf9\u6bcf\u4e2a sample \u5faa\u73af\u66f4\u65b0 for _ in range(self.n_iter): errors = 0 for xi, target in zip(X, y): update = self.eta * (target - self.predict(xi)) #learning rate*error self.w_[1:] += update * xi self.w_[0] += update errors += int(update != 0.0) self.errors_.append(errors) # \u9519\u8bef\u7684\u5206\u7c7b\u7ed3\u679c return self def net_input(self, X): Calculate net input w*x return np.dot(X, self.w_[1:]) + self.w_[0] def predict(self, X): Return class label after unit step return np.where(self.net_input(X) = 0.0, 1, -1)","title":"Implementing a perceptron learning algorithm in Python"},{"location":"w2-preceptron-and-logistic-regression/2w/#training-a-perceptron-model-on-the-iris-dataset","text":"\u8fd9\u91cc\u53ea\u8003\u8651\u4e24\u79cd\u82b1 Setosa \u548c Versicolor , \u4ee5\u53ca\u4e24\u79cd\u7279\u5f81 sepal length \u548c petal length. \u4f46\u662f Perceptron Model \u53ef\u4ee5\u89e3\u51b3\u591a\u7c7b\u522b\u5206\u7c7b\u95ee\u9898, \u53c2\u8003 one-vs-all [ back to top ]","title":"Training a perceptron model on the Iris dataset"},{"location":"w2-preceptron-and-logistic-regression/2w/#reading-in-the-iris-data","text":"import pandas as pd df = pd.read_csv('data/iris.csv', header=None) df.tail() 0 1 2 3 4 146 6.7 3 5.2 2.3 virginica 147 6.3 2.5 5 1.9 virginica 148 6.5 3 5.2 2 virginica 149 6.2 3.4 5.4 2.3 virginica 150 5.9 3 5.1 1.8 virginica","title":"Reading-in the Iris data"},{"location":"w2-preceptron-and-logistic-regression/2w/#plotting-the-iris-data","text":"# \u5c06\u4e24\u4e2a\u5206\u7c7b\u5148\u53ef\u89c6\u5316 %matplotlib inline import matplotlib.pyplot as plt import numpy as np # select setosa and versicolor # \u4e24\u79cd\u5404\u9009\u62e950\u4e2a, \u628a\u7c7b\u522b\u6539\u4e3a -1 \u548c 1, \u65b9\u4fbf\u753b\u56fe y = df.iloc[0:100, 4].values y = np.where(y == 'Iris-setosa', -1, 1) # extract sepal length and petal length X = df.iloc[0:100, [0, 2]].values # plot data plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa') plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor') plt.xlabel('petal length [cm]') plt.ylabel('sepal length [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./iris_1.png', dpi=300)","title":"Plotting the Iris data"},{"location":"w2-preceptron-and-logistic-regression/2w/#training-the-perceptron-model","text":"ppn = Perceptron(eta=0.1, n_iter=10) ppn.fit(X, y) ppn.errors_ [2, 2, 3, 2, 1, 0, 0, 0, 0, 0] # error \u753b\u56fe, \u68c0\u67e5\u662f\u5426 error \u8d8b\u8fd1\u4e8e0 \u5728\u591a\u6b21 loop \u66f4\u65b0\u540e plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o') plt.xlabel('Epochs') plt.ylabel('Number of misclassifications') plt.tight_layout() # plt.savefig('./perceptron_1.png', dpi=300) \u7ed3\u679c error \u7684\u786e\u6700\u540e\u4e3a 0, \u8bc1\u660e\u662f convergent \u7684, \u4e14\u5206\u7c7b\u6548\u679c\u5e94\u8be5\u8bf4\u662f\u975e\u5e38\u51c6\u786e\u4e86","title":"Training the perceptron model"},{"location":"w2-preceptron-and-logistic-regression/2w/#a-function-for-plotting-decision-regions","text":"\u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u753b\u51b3\u7b56\u8fb9\u754c from matplotlib.colors import ListedColormap # Colormap object generated from a list of colors. def plot_decision_regions(X, y, classifier, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface \u786e\u5b9a\u6a2a\u7eb5\u8f74\u8fb9\u754c x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 # \u6700\u5c0f-1, \u6700\u5927+1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 # create a pair of grid arrays # flatten the grid arrays then predict xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) # maps the different decision regions to different colors plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot class samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) plot_decision_regions(X, y, classifier=ppn) plt.xlabel('sepal length [cm]') plt.ylabel('petal length [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./perceptron_2.png', dpi=300) \u867d\u7136 Perceptron Model \u5728\u4e0a\u9762 Iris \u4f8b\u5b50\u91cc\u8868\u73b0\u5f97\u5f88\u597d\uff0c\u4f46\u5728\u5176\u4ed6\u95ee\u9898\u4e0a\u5374\u4e0d\u4e00\u5b9a\u8868\u73b0\u5f97\u597d\u3002 Frank Rosenblatt \u4ece\u6570\u5b66\u4e0a\u8bc1\u660e\u4e86\uff0c\u5728\u7ebf\u6027\u53ef\u5206\u7684\u6570\u636e\u91cc\uff0cPerceptron \u7684\u5b66\u4e60\u89c4\u5219\u4f1a converge\uff0c\u4f46\u5728\u7ebf\u6027\u4e0d\u53ef\u5206\u7684\u60c5\u51b5\u4e0b\uff0c\u5374\u65e0\u6cd5 converge","title":"A function for plotting decision regions"},{"location":"w2-preceptron-and-logistic-regression/2w/#adaptive-linear-neurons-and-the-convergence-of-learning-adaline","text":"[ back to top ]","title":"Adaptive linear neurons and the convergence of learning (Adaline)"},{"location":"w2-preceptron-and-logistic-regression/2w/#implementing-an-adaptive-linear-neuron-in-python","text":"[ back to top ] ADAptive LInear NEuron classifier \u4e5f\u662f\u4e00\u4e2a\u5355\u5c42\u795e\u7ecf\u7f51\u7edc. \u5b83\u7684\u91cd\u70b9\u5c31\u662f\u5b9a\u4e49\u53ca\u6700\u4f18\u5316 cost function, \u5bf9\u4e8e\u7406\u89e3\u66f4\u9ad8\u5c42\u6b21\u66f4\u96be\u7684\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u6a21\u578b\u662f\u975e\u5e38\u597d\u7684\u5165\u95e8. \u5b83\u4e0e Perceptron \u4e0d\u540c\u7684\u5730\u65b9\u5728\u4e8e\u66f4\u65b0 weights \u65f6\u662f\u7528\u7684 linear activation function, \u800c\u4e0d\u662funit step function. Adaline \u4e2d\u8fd9\u4e2a linear activation function \u8f93\u51fa\u7b49\u4e8e\u8f93\u5165, $$ \u03c6(w^T x)=w^T x$$. \u7136\u540e activation \u540e\u4f1a\u6709\u4e00\u4e2a quantizer \u7528\u6765\u5b66\u4e60\u66f4\u65b0 weights \u5b9a\u4e49 cost function \u4e3a SSE: Sum of Squared Errors $$ \\displaystyle J(w)= \\frac 1 2 \\sum_i(y^{(i)} \u2212\\phi(z^{(i)}))^2$$ \u8fd9\u4e2a function \u662f\u53ef\u5bfc\u7684, \u5e76\u4e14\u662f convex \u7684, \u53ef\u4ee5\u8fdb\u884c\u6700\u4f18\u5316, \u4f7f\u7528 gradient descent \u7b97\u6cd5. class AdalineGD(object): ADAptive LInear NEuron classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. def __init__(self, eta=0.01, n_iter=50): self.eta = eta self.n_iter = n_iter def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] # gradient descent for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) self.w_[0] += self.eta * errors.sum() cost = (errors**2).sum() / 2.0 self.cost_.append(cost) # cost list, to check algorithm convergence return self def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): Compute linear activation return self.net_input(X) def predict(self, X): Return class label after unit step return np.where(self.activation(X) = 0.0, 1, -1) # \u6d4b\u8bd5\u4e24\u79cd learning rate, 0.01 \u548c 0.0001 fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8, 4)) ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y) ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o') ax[0].set_xlabel('Epochs') ax[0].set_ylabel('log(Sum-squared-error)') ax[0].set_title('Adaline - Learning rate 0.01') ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y) ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o') ax[1].set_xlabel('Epochs') ax[1].set_ylabel('Sum-squared-error') ax[1].set_title('Adaline - Learning rate 0.0001') plt.tight_layout() # plt.savefig('./adaline_1.png', dpi=300) \u5de6\u56fe\u663e\u793a learning rate \u592a\u5927, error \u6ca1\u6709\u53d8\u5c0f, \u53cd\u800c\u53d8\u5927\u4e86. \u53f3\u56fe\u663e\u793a learning rate \u592a\u5c0f, error \u53d8\u5316\u901f\u5ea6\u592a\u5c0f","title":"Implementing an adaptive linear neuron in Python"},{"location":"w2-preceptron-and-logistic-regression/2w/#standardizing-features-and-re-training-adaline","text":"$$ x^\\prime_j = \\frac {x_j - \\mu_j} {\\sigma_j} $$ # standardize features X_std = np.copy(X) X_std[:,0] = (X[:,0] - X[:,0].mean()) / X[:,0].std() X_std[:,1] = (X[:,1] - X[:,1].mean()) / X[:,1].std() ada = AdalineGD(n_iter=15, eta=0.01) ada.fit(X_std, y) plot_decision_regions(X_std, y, classifier=ada) plt.title('Adaline - Gradient Descent') plt.xlabel('sepal length [standardized]') plt.ylabel('petal length [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./adaline_2.png', dpi=300) plt.show() plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o') plt.xlabel('Epochs') plt.ylabel('Sum-squared-error') plt.tight_layout() # plt.savefig('./adaline_3.png', dpi=300) \u5206\u7c7b\u6548\u679c\u4e0d\u9519, error \u6700\u7ec8\u63a5\u8fd1\u4e8e0 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u867d\u7136\u6211\u4eec\u7684\u5206\u7c7b\u5168\u90e8\u6b63\u786e\uff0c\u4f46 error \u4e5f\u4e0d\u7b49\u4e8e0 ada.w_ #weights array([ 1.36557432e-16, -1.26256159e-01, 1.10479201e+00])","title":"Standardizing features and re-training adaline"},{"location":"w2-preceptron-and-logistic-regression/2w/#large-scale-machine-learning-and-stochastic-gradient-descent","text":"Stochastic gradient descent \u968f\u673a\u68af\u5ea6\u4e0b\u964d \u6bd4\u4e00\u822c\u7684\u68af\u5ea6\u4e0b\u964d\u66f4\u6709\u4f18\u52bf, \u56e0\u4e3a\u6bcf\u4e00\u6b65\u8ba1\u7b97\u7684 cost \u66f4\u5c0f, \u6bcf\u4e00\u6b65\u66f4\u65b0\u90fd\u662f\u968f\u673a\u53d6\u5176\u4e2d\u4e00\u5c0f\u6b65\u66f4\u65b0\u5c31\u53ef. batch gradient descent \u4e00\u6b21\u66f4\u65b0\u9700\u8981\u8ba1\u7b97\u4e00\u904d\u6574\u4e2a\u6570\u636e\u96c6 $$ \\Delta w = \\eta \\sum_i (y^{(i)} - \\phi(z^{(i)}))x^{(i)} $$ stochastic gradient descent \u4e00\u6b21\u66f4\u65b0\u53ea\u9700\u8ba1\u7b97\u4e00\u4e2a\u6570\u636e\u70b9 $$ \\Delta w = \\eta(y^{(i)} - \\phi(z^{(i)}))x^{(i)} $$ class AdalineSGD(object): ADAptive LInear NEuron classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. shuffle : bool (default: True) Shuffles training data every epoch if True to prevent cycles. random_state : int (default: None) Set random state for shuffling and initializing the weights. def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None): self.eta = eta self.n_iter = n_iter self.w_initialized = False self.shuffle = shuffle if random_state: # allow the specication of a random seed for consistency np.random.seed(random_state) def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self._initialize_weights(X.shape[1]) self.cost_ = [] for i in range(self.n_iter): if self.shuffle: X, y = self._shuffle(X, y) cost = [] for xi, target in zip(X, y): cost.append(self._update_weights(xi, target)) avg_cost = sum(cost) / len(y) self.cost_.append(avg_cost) return self # \u5728\u6bcf\u4e2a epoch \u524d\u662f\u5426 shuffle data def _shuffle(self, X, y): Shuffle training data r = np.random.permutation(len(y)) return X[r], y[r] def _initialize_weights(self, m): Initialize weights to zeros self.w_ = np.zeros(1 + m) self.w_initialized = True #stochastic gradient descent def _update_weights(self, xi, target): Apply Adaline learning rule to update the weights output = self.net_input(xi) error = (target - output) self.w_[1:] += self.eta * xi.dot(error) # \u4ec5\u4e00\u4e2a error \u76f8\u4e58 self.w_[0] += self.eta * error # \u4ec5\u4ec5\u662f\u4e00\u4e2a error, \u800c\u975e sum cost = 0.5 * error**2 return cost def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): Compute linear activation return self.net_input(X) def predict(self, X): Return class label after unit step return np.where(self.activation(X) = 0.0, 1, -1) # plot result ada = AdalineSGD(n_iter=15, eta=0.01, random_state=1) ada.fit(X_std, y) plot_decision_regions(X_std, y, classifier=ada) plt.title('Adaline - Stochastic Gradient Descent') plt.xlabel('sepal length [standardized]') plt.ylabel('petal length [standardized]') plt.legend(loc='upper left') plt.tight_layout() #plt.savefig('./adaline_4.png', dpi=300) plt.show() plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o') plt.xlabel('Epochs') plt.ylabel('Average Cost') plt.tight_layout() # plt.savefig('./adaline_5.png', dpi=300) \u5bf9\u6bd4\u53ef\u4ee5\u53d1\u73b0\uff0cStochastic gradient descent \u7684\u5b66\u4e60\u901f\u7387\u6bd4 Batch gradient descnent \u7684\u9ad8\u5f88\u591a","title":"Large scale machine learning and stochastic gradient descent"},{"location":"w2-preceptron-and-logistic-regression/2w/#implementing-logistic-regression-in-python","text":"[ back to top ] logistic regression: powerful algorithm for linear and binary classication problems odds ratio: the odds in favor of a particular event.$$\\displaystyle \\frac{p}{(1-p)}$$, if p stands for the probability of the positive event, we want to predict.$$\\displaystyle logit(p) = log\\frac{p}{1-p}$$ and $$\\displaystyle p(y=1|x)$$ is conditional probability that a particular sample belongs to class 1 given its features x. Therefore, the logit is $$\\displaystyle logit(p(y=1|x)) = w_0x_0 + w_1x_1 + ... + w_mx_m = \\sum_{i=0}^nw_ix_i$$ we want to know the probability, which is inverse of logit function, we call it logistic funcition, or sigmoid function . $$\\displaystyle \\phi (z) = \\frac{1}{1+e^{-z}}$$ output of sigmoid function is as the probability of particular sample belonging to class 1","title":"Implementing logistic regression in Python"},{"location":"w2-preceptron-and-logistic-regression/2w/#plot-sigmoid-function","text":"%matplotlib inline import matplotlib.pyplot as plt import numpy as np def sigmoid(z): return 1.0 / (1.0 + np.exp(-z)) z = np.arange(-7, 7, 0.1) phi_z = sigmoid(z) plt.plot(z, phi_z) plt.axvline(0.0, color='k') plt.ylim(-0.1, 1.1) plt.xlabel('z') plt.ylabel('$\\phi (z)$') # y axis ticks and gridline plt.yticks([0.0, 0.5, 1.0]) ax = plt.gca() ax.yaxis.grid(True) plt.tight_layout() # plt.savefig('./figures/sigmoid.png', dpi=300) when $$\\phi(z)$$ approached 1 if z\u2192$$\\infty$$, goes to 1 if z\u2192$$-\\infty$$","title":"Plot sigmoid function:"},{"location":"w2-preceptron-and-logistic-regression/2w/#plot-cost-function","text":"use log-likelihood function to redefine cost function then $$\\displaystyle J(\\phi(z),y;w)={\\begin{cases}-log(\\phi(z)) {\\text{if }}y=1\\-log(1-\\phi(z)) {\\text{if }}y=0\\end{cases}}$$ def cost_1(z): return - np.log(sigmoid(z)) def cost_0(z): return - np.log(1 - sigmoid(z)) z = np.arange(-10, 10, 0.1) phi_z = sigmoid(z) c1 = [cost_1(x) for x in z] plt.plot(phi_z, c1, label='J(w) if y=1') c0 = [cost_0(x) for x in z] plt.plot(phi_z, c0, linestyle='--', label='J(w) if y=0') plt.ylim(0.0, 5.1) plt.xlim([0, 1]) plt.xlabel('$\\phi$(z)') plt.ylabel('J(w)') plt.legend(loc='best') plt.tight_layout() # plt.savefig('./figures/log_cost.png', dpi=300) # this illustrates the cost for the classification of a single-sample instance for diff values of phi(z) cost \u8d8b\u8fd1\u4e8e0 \u5982\u679c\u6b63\u786e\u9884\u6d4b class.","title":"Plot cost function:"},{"location":"w2-preceptron-and-logistic-regression/2w/#implement-in-python","text":"The following implementation is similar to the Adaline implementation except that we replace the sum of squared errors cost function with the logistic cost function $$J(\\mathbf{w}) = \\sum_{i=1}^{m} - y^{(i)} log \\bigg( \\phi\\big(z^{(i)}\\big) \\bigg) - \\big(1 - y^{(i)}\\big) log\\bigg(1-\\phi\\big(z^{(i)}\\big)\\bigg).$$ class LogisticRegression(object): LogisticRegression classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. cost_ : list Cost in every epoch. def __init__(self, eta=0.01, n_iter=50): self.eta = eta self.n_iter = n_iter def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): y_val = self.activation(X) errors = (y - y_val) neg_grad = X.T.dot(errors) self.w_[1:] += self.eta * neg_grad self.w_[0] += self.eta * errors.sum() self.cost_.append(self._logit_cost(y, self.activation(X))) return self def _logit_cost(self, y, y_val): logit = -y.dot(np.log(y_val)) - ((1 - y).dot(np.log(1 - y_val))) return logit def _sigmoid(self, z): return 1.0 / (1.0 + np.exp(-z)) def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def activation(self, X): Activate the logistic neuron z = self.net_input(X) return self._sigmoid(z) def predict_proba(self, X): Predict class probabilities for X. Parameters ---------- X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns ---------- Class 1 probability : float return activation(X) def predict(self, X): Predict class labels for X. Parameters ---------- X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns ---------- class : int Predicted class label. # equivalent to np.where(self.activation(X) = 0.5, 1, 0) return np.where(self.net_input(X) = 0.0, 1, 0) y[y == -1] = 0 # \u5c06\u8d1f\u6027\u6807\u7b7e\u7f16\u7801\u4e3a 0 y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) lr = LogisticRegression(n_iter=500, eta=0.02).fit(X_std, y) plt.plot(range(1, len(lr.cost_) + 1), np.log10(lr.cost_)) plt.xlabel('Epochs') plt.ylabel('Cost') plt.title('Logistic Regression - Learning rate 0.02') plt.tight_layout() plot_decision_regions(X_std, y, classifier=lr) plt.title('Logistic Regression - Gradient Descent') plt.xlabel('sepal length [standardized]') plt.ylabel('petal length [standardized]') plt.legend(loc='upper left') plt.tight_layout()","title":"Implement in Python"},{"location":"w2-preceptron-and-logistic-regression/2w/#classification-with-scikit-learn","text":"[ back to top ]","title":"Classification with scikit-learn"},{"location":"w2-preceptron-and-logistic-regression/2w/#loading-and-preprocessing-the-data","text":"[ back to top ] Loading the Iris dataset from scikit-learn. Here, the third column represents the petal length, and the fourth column the petal width of the flower samples. The classes are already converted to integer labels where 0=Iris-Setosa, 1=Iris-Versicolor, 2=Iris-Virginica. from sklearn import datasets import numpy as np iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target print('Class labels:', np.unique(y)) print(iris.target_names) ('Class labels:', array([0, 1, 2])) ['setosa' 'versicolor' 'virginica'] Splitting data into 70% training and 30% test data: from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) Standardizing the features: from sklearn.preprocessing import StandardScaler sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) # standardize by mean std X_test_std = sc.transform(X_test)","title":"Loading and preprocessing the data"},{"location":"w2-preceptron-and-logistic-regression/2w/#other-available-data","text":"[ back to top ] Scikit-learn makes available a host of datasets for testing learning algorithms . They come in three flavors: Packaged Data: these small datasets are packaged with the scikit-learn installation, and can be downloaded using the tools in sklearn.datasets.load_* Downloadable Data: these larger datasets are available for download, and scikit-learn includes tools which streamline this process. These tools can be found in sklearn.datasets.fetch_* Generated Data: there are several datasets which are generated from models based on a random seed. These are available in the sklearn.datasets.make_* You can explore the available dataset loaders, fetchers, and generators using IPython's tab-completion functionality. After importing the datasets submodule from sklearn , type datasets.load_ TAB or datasets.fetch_ TAB or datasets.make_ TAB to see a list of available functions. The data downloaded using the fetch_ scripts are stored locally, within a subdirectory of your home directory. You can use the following to determine where it is: from sklearn.datasets import get_data_home get_data_home()","title":"Other Available Data"},{"location":"w2-preceptron-and-logistic-regression/2w/#training-a-perceptron-via-scikit-learn","text":"[ back to top ] from sklearn.linear_model import Perceptron # sklearn \u4e2d\u6709\u5c01\u88c5\u597d\u7684 Perceptron \u51fd\u6570 ppn = Perceptron(n_iter=40, eta0=0.1, random_state=0) ppn.fit(X_train_std, y_train) Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True, n_iter=40, n_jobs=1, penalty=None, random_state=0, shuffle=True, verbose=0, warm_start=False) y_test.shape (45,) y_pred = ppn.predict(X_test_std) # predict print('Misclassified samples: %d' % (y_test != y_pred).sum()) # \u9519\u8bef\u4e2a\u6570 Misclassified samples: 4 from sklearn.metrics import accuracy_score print('Accuracy: %.2f' % accuracy_score(y_test, y_pred)) # 91% \u7684\u51c6\u786e\u7387 Accuracy: 0.91 from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt %matplotlib inline # \u91cd\u65b0\u5b9a\u4e49\u753b\u51b3\u7b56\u8fb9\u754c\u51fd\u6570, \u4f7f\u5f97\u80fd\u533a\u5206\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot all samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) # highlight test samples if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidth=1, marker='o', s=55, label='test set') Training a perceptron model using the standardized training data: X_combined_std = np.vstack((X_train_std, X_test_std)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X=X_combined_std, y=y_combined, classifier=ppn, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/iris_perceptron_scikit.png', dpi=300) # \u8fd9\u6b21\u662f3\u4e2a\u5206\u7c7b\u4e00\u8d77 Peceptron \u6a21\u578b\u5bf9\u4e8e\u5e76\u4e0d\u662f\u5b8c\u5168\u7ebf\u6027\u9694\u79bb\u7684 dataset \u4e0d\u80fd converge, \u6240\u4ee5\u5b9e\u9645\u5e94\u7528\u4e2d\u5e76\u4e0d\u591a\u7528.","title":"Training a perceptron via scikit-learn"},{"location":"w2-preceptron-and-logistic-regression/2w/#modeling-class-probabilities-via-logistic-regression","text":"[ back to top ] # use Logistic Regression from sklearn.linear_model import LogisticRegression # C parameter \u662f\u4ec0\u4e48\u5462? lr = LogisticRegression(C=1000.0, random_state=0) lr.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=lr, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/logistic_regression.png', dpi=300) lr.predict_proba(X_test_std[0,:].reshape(1,-1)) # predict probability array([[ 2.05743774e-11, 6.31620264e-02, 9.36837974e-01]])","title":"Modeling class probabilities via logistic regression"},{"location":"w2-preceptron-and-logistic-regression/2w/#regularization-path","text":"\u89e3\u51b3 overfitting: \u6a21\u578b\u62df\u5408\u7684\u8fc7\u597d, \u4ee5\u81f4\u4e8e\u6ca1\u6709\u4e00\u822c\u6027, \u9884\u6d4b\u65b0\u7684\u6837\u672c\u7684\u7ed3\u679c\u5c31\u4f1a\u5f88\u5dee \u4e00\u822c\u8fc7\u62df\u5408\u7684\u6a21\u578b\u4f1a\u6709 high variance \u6700\u5e38\u7528\u7684\u89e3\u51b3\u65b9\u6cd5\u5c31\u53eb\u505a L2 regulatization$$\\displaystyle \\frac{\\lambda}{2} \\lVert w \\rVert^2 = \\frac{\\lambda}{2}\\sum_{j=1}^m w_j^2$$ \u5176\u4e2d$$\\lambda$$ \u5c31\u662f regularization parameter, \u53ef\u4ee5\u7528\u6765\u63a7\u5236\u62df\u5408\u8bad\u7ec3\u6570\u636e\u7684\u597d\u574f, \u800c $$C = \\frac{1}{\\lambda}$$ \u5c31\u662f\u524d\u9762\u63d0\u5230\u8fc7\u7684 parameter weights, params = [], [] for c in np.arange(-5, 5): lr = LogisticRegression(C=10**c, random_state=0) lr.fit(X_train_std, y_train) weights.append(lr.coef_[1]) params.append(10**c) weights = np.array(weights) plt.plot(params, weights[:, 0], label='petal length') plt.plot(params, weights[:, 1], linestyle='--', label='petal width') plt.ylabel('weight coefficient') plt.xlabel('C') plt.legend(loc='upper left') plt.xscale('log') # plt.savefig('./figures/regression_path.png', dpi=300) # C \u51cf\u5c0f\u7684\u8bdd, \u5c31\u662f\u589e\u52a0 regularization","title":"Regularization path"},{"location":"w2-preceptron-and-logistic-regression/2w/#logistic-regression-with-regularization","text":"class LogitGD(object): Logistic Regression classifier. Parameters ------------ eta : float Learning rate (between 0.0 and 1.0) n_iter : int Passes over the training dataset. Attributes ----------- w_ : 1d-array Weights after fitting. errors_ : list Number of misclassifications in every epoch. def __init__(self, eta=0.01, lamb = 0.01, n_iter=50): self.eta = eta self.n_iter = n_iter self.lamb = lamb def fit(self, X, y): Fit training data. Parameters ---------- X : {array-like}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns ------- self : object self.w_ = np.zeros(1 + X.shape[1]) self.cost_ = [] for i in range(self.n_iter): output = self.net_input(X) errors = (y - output) self.w_[1:] += self.eta * X.T.dot(errors) - self.lamb* self.w_[1:] self.w_[0] += self.eta * errors.sum() cost = (errors**2).sum() / 2.0 + self.lamb* np.sum(self.w_[1:]**2) self.cost_.append(cost) return self def net_input(self, X): Calculate net input return np.dot(X, self.w_[1:]) + self.w_[0] def sigmoid(z): return 1.0 / (1.0 + np.exp(-z)) def activation(self, X): Compute linear activation return sigmoid(self.net_input(X)) def predict(self, X): Return class label after unit step return np.where(self.activation(X) = 0.5, 1, -1)","title":"Logistic regression with regularization"},{"location":"w2-preceptron-and-logistic-regression/2w/#_3","text":"","title":"\u5176\u5b83\u7684\u5206\u7c7b\u5668\u7b80\u4ecb"},{"location":"w2-preceptron-and-logistic-regression/2w/#maximum-margin-classification-with-support-vector-machines","text":"\u76ee\u7684\u662f maximize the margin , margin \u662f\u5206\u79bb\u51b3\u7b56\u8fb9\u754c\u4e0e\u79bb\u4e4b\u6700\u8fd1\u7684\u8bad\u7ec3\u6837\u672c\u4e4b\u95f4\u7684\u8ddd\u79bb. [ back to top ] # train SVC from sklearn.svm import SVC svm = SVC(kernel='linear', C=1.0, random_state=0) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_linear.png', dpi=300)","title":"Maximum margin classification with support vector machines"},{"location":"w2-preceptron-and-logistic-regression/2w/#solving-non-linear-problems-using-a-kernel-svm","text":"SVM \u53ef\u4ee5\u89e3\u51b3\u975e\u7ebf\u6027\u95ee\u9898 [ back to top ] # create a simple dataset np.random.seed(0) X_xor = np.random.randn(200, 2) y_xor = np.logical_xor(X_xor[:, 0] 0, X_xor[:, 1] 0) # 100\u4e2a with label 1, 100 withlabel 0 y_xor = np.where(y_xor, 1, -1) plt.scatter(X_xor[y_xor==1, 0], X_xor[y_xor==1, 1], c='b', marker='x', label='1') plt.scatter(X_xor[y_xor==-1, 0], X_xor[y_xor==-1, 1], c='r', marker='s', label='-1') plt.xlim([-3, 3]) plt.ylim([-3, 3]) plt.legend(loc='best') plt.tight_layout() # plt.savefig('./figures/xor.png', dpi=300) # \u4f7f\u7528\u666e\u901a\u7684 linear logistic Regression \u4e0d\u80fd\u5f88\u597d\u5c06\u6837\u672c\u5206\u4e3a+ve \u548c-ve rbf \u662f\u6307 radial basis function kernel \u6216\u8005 Gaussian kernel $$\\displaystyle k(x^{(i)}, x^{(j)}) = exp (- \\frac{\\lVert x^{(i)}-x^{(j)} \\rVert^2}{2 \\sigma^2})$$ simplified to $$\\displaystyle exp(-\\gamma \\lVert x^{(i)}-x^{(j)} \\rVert^2)$$ with $$\\gamma = \\frac{1}{2\\sigma^2}$$ # \u4f7f\u7528 svm kernel \u65b9\u6cd5, \u6295\u5c04\u5230\u9ad8\u7eac\u5ea6\u4e2d, \u4f7f\u4e4b\u6210\u4e3a\u7ebf\u6027\u53ef\u5206\u79bb\u7684 svm = SVC(kernel='rbf', random_state=0, gamma=0.10, C=10.0) svm.fit(X_xor, y_xor) plot_decision_regions(X_xor, y_xor, classifier=svm) plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_rbf_xor.png', dpi=300) \u5176\u4e2d$$\\gamma$$ parameter \u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a cut-off parameter for Gaussian sphere \u5f53$$\\gamma$$ \u589e\u52a0, \u4e5f\u5c31\u589e\u52a0\u4e86\u8bad\u7ec3\u6837\u672c\u7684\u5f71\u54cd, \u4e5f\u5c31\u4f1a\u4f7f\u51b3\u7b56\u8fb9\u754c\u53d8\u5f97\u6a21\u7cca from sklearn.svm import SVC ## gamma \u8f83\u5c0f svm = SVC(kernel='rbf', random_state=0, gamma=0.2, C=1.0) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_rbf_iris_1.png', dpi=300) # gamma \u5f88\u5927, \u8fb9\u754c tight svm = SVC(kernel='rbf', random_state=0, gamma=100.0, C=1.0) svm.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/support_vector_machine_rbf_iris_2.png', dpi=300)","title":"Solving non-linear problems using a kernel SVM"},{"location":"w2-preceptron-and-logistic-regression/2w/#k-nearest-neighbors-a-lazy-learning-algorithm","text":"it doesn't learn a discriminative function from the training data but memorizes the training dataset instead. Choose the number of k and a distance metric. Find the k nearest neighbors of the sample that we want to classify. Assign the class label by majority vote. \u8fd9\u79cd\u65b9\u6cd5\u597d\u5904\u5728\u4e8e\u65b0\u6570\u636e\u8fdb\u6765, \u5206\u7c7b\u5668\u53ef\u4ee5\u9a6c\u4e0a\u5b66\u4e60\u5e76\u9002\u5e94, \u4f46\u662f\u8ba1\u7b97\u6210\u672c\u4e5f\u662f\u7ebf\u6027\u589e\u957f, \u5b58\u50a8\u4e5f\u662f\u95ee\u9898. [ back to top ] from sklearn.neighbors import KNeighborsClassifier # \u5bfb\u627e5\u4e2a\u90bb\u5c45 knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski') knn.fit(X_train_std, y_train) plot_decision_regions(X_combined_std, y_combined, classifier=knn, test_idx=range(105,150)) plt.xlabel('petal length [standardized]') plt.ylabel('petal width [standardized]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/k_nearest_neighbors.png', dpi=300) \u5982\u4f55\u9009\u62e9 k \u662f\u4e00\u4e2a\u91cd\u70b9, \u5e76\u4e14\u9700\u8981\u6807\u51c6\u5316\u6570\u636e. \u4f8b\u5b50\u4e2d\u7528\u5230\u7684'minkowski' distance \u662f\u666e\u901a\u7684 Euclidean \u548c Manhattan distance \u7684\u6269\u5c55. $$\\displaystyle d(x^{(i)}, x^{(j)}) = \\sqrt[p]{\\sum_k \\left|x_k^{(i)}x_k^{(j)}\\right|^p} $$","title":"K-nearest neighbors - a lazy learning algorithm"},{"location":"w2-preceptron-and-logistic-regression/2w/#scoring-metrics-for-classification","text":"[ back to top ]","title":"Scoring metrics for classification"},{"location":"w2-preceptron-and-logistic-regression/2w/#classification-metrics-in-scikit-learn","text":"[ back to top ] The sklearn.metrics module implements several loss, score, and utility functions to measure classification performance. Some metrics might require probability estimates of the positive class, confidence values, or binary decisions values. Most implementations allow each sample to provide a weighted contribution to the overall score, through the sample_weight parameter. Some of these are restricted to the binary classification case: matthews_corrcoef (y_true, y_pred) Compute the Matthews correlation coefficient (MCC) for binary classes precision_recall_curve (y_true, probas_pred) Compute precision-recall pairs for different probability thresholds roc_curve (y_true, y_score[, pos_label, ...]) Compute Receiver operating characteristic (ROC) Others also work in the multiclass case: confusion_matrix (y_true, y_pred[, labels]) Compute confusion matrix to evaluate the accuracy of a classification hinge_loss (y_true, pred_decision[, labels, ...]) Average hinge loss (non-regularized) Some also work in the multilabel case: accuracy_score (y_true, y_pred[, normalize, ...]) Accuracy classification score. classification_report (y_true, y_pred[, ...]) Build a text report showing the main classification metrics f1_score (y_true, y_pred[, labels, ...]) Compute the F1 score, also known as balanced F-score or F-measure fbeta_score (y_true, y_pred, beta[, labels, ...]) Compute the F-beta score hamming_loss (y_true, y_pred[, classes]) Compute the average Hamming loss. jaccard_similarity_score (y_true, y_pred[, ...]) Jaccard similarity coefficient score log_loss (y_true, y_pred[, eps, normalize, ...]) Log loss, aka logistic loss or cross-entropy loss. precision_recall_fscore_support (y_true, y_pred) Compute precision, recall, F-measure and support for each class precision_score (y_true, y_pred[, labels, ...]) Compute the precision recall_score (y_true, y_pred[, labels, ...]) Compute the recall zero_one_loss (y_true, y_pred[, normalize, ...]) Zero-one classification loss. And some work with binary and multilabel (but not multiclass) problems: average_precision_score (y_true, y_score[, ...]) Compute average precision (AP) from prediction scores roc_auc_score (y_true, y_score[, average, ...]) Compute Area Under the Curve (AUC) from prediction scores # \u6784\u5efa\u6570\u636e from sklearn import datasets from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC X, y = datasets.make_classification(n_classes=2, random_state=0) X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) # standardize by mean std X_test_std = sc.transform(X_test) model = SVC(probability=True, random_state=0) model.fit(X_train_std, y_train); default score for classification in sklearn is accuracy (\u6807\u7b7e\u9884\u6d4b\u6b63\u786e\u7684\u6bd4\u4f8b) $$ accuracy(y, \\hat y) = \\frac 1 n \\sum^{n - 1}_{i=0} 1 (\\hat y_i = y_i)$$ where $$1(x)$$ is the indicator function model.score(X_test_std, y_test) 0.83333333333333337 from sklearn.metrics import accuracy_score y_pred = model.predict(X_test_std) accuracy_score(y_test, y_pred) 0.83333333333333337","title":"Classification metrics in Scikit-learn"},{"location":"w2-preceptron-and-logistic-regression/2w/#reading-a-confusion-matrix","text":"[ back to top ] For multi-class problems, it is often interesting to know which of the classes are hard to predict, and which are easy, or which classes get confused. One way to get more information about misclassifications is the confusion_matrix , which shows for each true class, how frequent a given predicted outcome is. from sklearn.metrics import confusion_matrix y_test_pred = model.predict(X_test_std) confmat = confusion_matrix(y_test, y_test_pred) print(confmat) [[15 3] [ 2 10]] plt.matshow(confusion_matrix(y_test, y_test_pred), cmap=plt.cm.Blues) plt.colorbar() plt.xlabel( Predicted label ) plt.ylabel( True label );","title":"Reading a confusion matrix"},{"location":"w2-preceptron-and-logistic-regression/2w/#precision-recall-and-f-measures","text":"[ back to top ] Precision is how many of the predictions for a class are actually that class. Recall is how many of the true positives were recovered: f1-score is the geometric average of precision and recall: With TP, FP, TN, FN, FPR, TPR standing for \"true positive\", \"false positive\", \"true negative\" and \"false negative\", \"false positive rate\", \"true positive rate\" repectively: \\begin{align} PRE = \\frac{TP}{TP+FP} \\ REC = TPR = \\frac{TP}{FN+TP} \\ F1 = 2 \\frac{PRE \\times REC}{PRE+REC} \\ F_\\beta = (1+\\beta^2)\\frac{PRE \\times REC}{\\beta^2 PRE+REC} \\ FPR = \\frac{FP}{FP+TN} \\ TPR = \\frac{TP}{FN+TP} \\end{align} from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_test_pred)) print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_test_pred)) print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_test_pred)) print('F_beta2: %.3f' % fbeta_score(y_true=y_test, y_pred=y_test_pred, beta=2)) Precision: 0.769 Recall: 0.833 F1: 0.800 F_beta2: 0.820 Another useful function is the classification_report which provides precision, recall, fscore and support for all classes. from sklearn.metrics import classification_report print(classification_report(y_test, y_test_pred)) precision recall f1-score support 0 0.88 0.83 0.86 18 1 0.77 0.83 0.80 12 avg / total 0.84 0.83 0.83 30 These metrics are helpful in two particular cases that come up often in practice: 1. Imbalanced classes, that is one class might be much more frequent than the other. 2. Asymmetric costs, that is one kind of error is much more \"costly\" than the other.","title":"Precision, recall and F-measures"},{"location":"w2-preceptron-and-logistic-regression/2w/#roc-and-auc","text":"[ back to top ] A receiver operating characteristic curve, or ROC curve , is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate. \u5982\u679c\u5206\u7c7b\u5668\u6548\u679c\u5f88\u597d, \u90a3\u4e48\u56fe\u5e94\u8be5\u4f1a\u5728\u5de6\u4e0a\u89d2. \u5728 ROC curve \u7684\u57fa\u7840\u4e0a, \u53ef\u4ee5\u8ba1\u7b97 AUC -- area under the curve.","title":"ROC and AUC"},{"location":"w2-preceptron-and-logistic-regression/2w/#area-under-curve","text":"The AUC is a common evaluation metric for binary classification problems. Consider a plot of the true positive rate vs the false positive rate as the threshold value for classifying an item as 0 or is increased from 0 to 1: if the classifier is very good, the true positive rate will increase quickly and the area under the curve will be close to 1. If the classifier is no better than random guessing, the true positive rate will increase linearly with the false positive rate and the area under the curve will be around 0.5. One characteristic of the AUC is that it is independent of the fraction of the test population which is class 0 or class 1: this makes the AUC useful for evaluating the performance of classifiers on unbalanced data sets. def roc_curve(true_labels, predicted_probs, n_points=100, pos_class=1): thr = np.linspace(0,1,n_points) tpr = np.zeros(n_points) fpr = np.zeros(n_points) pos = true_labels == pos_class neg = np.logical_not(pos) n_pos = np.count_nonzero(pos) n_neg = np.count_nonzero(neg) for i,t in enumerate(thr): tpr[i] = np.count_nonzero(np.logical_and(predicted_probs = t, pos)) / float(n_pos) fpr[i] = np.count_nonzero(np.logical_and(predicted_probs = t, neg)) / float(n_neg) return fpr, tpr, thr df_imputed = pd.read_csv('df_imputed') features = ['revolving_utilization_of_unsecured_lines', 'age', 'number_of_time30-59_days_past_due_not_worse', 'debt_ratio', 'monthly_income', 'number_of_open_credit_lines_and_loans', 'number_of_times90_days_late', 'number_real_estate_loans_or_lines', 'number_of_time60-89_days_past_due_not_worse', 'number_of_dependents', 'income_bins', 'age_bin', 'monthly_income_scaled'] y = df_imputed.serious_dlqin2yrs X = pd.get_dummies(df_imputed[features], columns = ['income_bins', 'age_bin']) from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(X, y ,train_size=0.7,random_state=1) # Randomly generated predictions should give us a diagonal ROC curve preds = np.random.rand(len(test_y)) fpr, tpr, thr = roc_curve(test_y, preds) plt.plot(fpr, tpr); from sklearn.linear_model import LogisticRegression clf = LogisticRegression() clf.fit(train_X,train_y) preds = clf.predict_proba(test_X)[:,1] fpr, tpr, thr = roc_curve(test_y, preds) plt.plot(fpr, tpr);","title":"Area Under Curve"},{"location":"w2-preceptron-and-logistic-regression/2w/#log-loss","text":"[ back to top ] Log loss, also called logistic regression loss or cross-entropy loss, is defined on probability estimates. It is commonly used in (multinomial) logistic regression and neural networks, as well as in some variants of expectation-maximization, and can be used to evaluate the probability outputs ( predict_proba ) of a classifier instead of its discrete predictions. For binary classification with a true label $$y \\in {0,1}$$ and a probability estimate $$p = \\operatorname{Pr}(y = 1)$$, the log loss per sample is the negative log-likelihood of the classifier given the true label: $$ L_{log}(y, p) = -log Pr(y|p) = -(y log(p) + (1-y) log(1-p)) $$ This extends to the multiclass case as follows. Let the true labels for a set of samples be encoded as a 1-of-K binary indicator matrix $$Y$$, i.e., $$y_{i,k} = 1$$ if sample $$i$$ has label $$k$$ taken from a set of $$K$$ labels. Let $$P$$ be a matrix of probability estimates, with $$p_{i,k} = \\operatorname{Pr}(t_{i,k} = 1)$$. Then the log loss of the whole set is $$L_{log}(Y, P) = -logPr(Y|P) = - \\frac 1 N \\sum^{N-1} {i=0} \\sum^{K-1} {k=0} y_{i,k}logp_{i,k} $$ To see how this generalizes the binary log loss given above, note that in the binary case, we have $$p_{i,0} = 1 - p_{i,1}$$ and $$y_{i,0} = 1 - y_{i,1}$$, so expanding the inner sum over $$y_{i,k} \\in {0,1}$$ gives the binary log loss. The log_loss function computes log loss given a list of ground-truth labels and a probability matrix, as returned by an estimator\u2019s predict_proba method. from sklearn.metrics import log_loss y_true = [0, 0, 1, 1] y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]] log_loss(y_true, y_pred) 0.17380733669106749","title":"Log loss"},{"location":"w2-preceptron-and-logistic-regression/2w/#hinge-loss","text":"[ back to top ] The hinge_loss function computes the average distance between the model and the data using hinge loss , a one-sided metric that considers only prediction errors. (Hinge loss is used in maximal margin classifiers such as support vector machines.) If the labels are encoded with +1 and -1, $$y$$: is the true value, and $w$ is the predicted decisions as output by decision_function, then the hinge loss is defined as: $$ L_{Hinge}(y, w) = max{1-wy, 0} = |1-wy|_+ $$ If there are more than two labels, hinge_loss uses a multiclass variant due to Crammer Singer. If $$y_w$$ is the predicted decision for true label and $$y_t$$ is the maximum of the predicted decisions for all other labels, where predicted decisions are output by decision function, then multiclass hinge loss is defined by: $$ L_{Hinge}(y_w, y_t) = max{1+y_t-y_w,0}$$ # Here a small example demonstrating the use of the hinge_loss function # with a svm classifier in a binary class problem: from sklearn import svm from sklearn.metrics import hinge_loss X = [[0], [1]] y = [-1, 1] est = svm.LinearSVC(random_state=0) est.fit(X, y) pred_decision = est.decision_function([[-2], [3], [0.5]]) print(pred_decision) print(hinge_loss([-1, 1, 1], pred_decision)) [-2.18173682 2.36360149 0.09093234] 0.303022554204 # Here is an example demonstrating the use of the hinge_loss function # with a svm classifier in a multiclass problem: X = np.array([[0], [1], [2], [3]]) Y = np.array([0, 1, 2, 3]) labels = np.array([0, 1, 2, 3]) est = svm.LinearSVC() est.fit(X, Y) pred_decision = est.decision_function([[-1], [2], [3]]) y_true = [0, 2, 3] hinge_loss(y_true, pred_decision, labels) 0.56412359941917456","title":"Hinge loss"},{"location":"w2-preceptron-and-logistic-regression/2w/#_4","text":"","title":"\u7ec3\u4e60\uff1a\u5c1d\u8bd5\u5728\u4fe1\u8d37\u6570\u636e\u96c6\u4e2d\u4f7f\u7528\u6b63\u5219\u5316\u65b9\u6cd5\uff0c \u753b\u51fa\u7cfb\u6570\u7684\u53d8\u5316\uff0c\u4ee5\u53ca\u6700\u7ec8\u7684\u9884\u6d4b\u6548\u679c"},{"location":"w3-decision-tree-and-ensemble-learning/3w/","text":"Sections Decision trees learning Building a decision tree Visualize a decision tree Different impurity criteria Implement a binary decision tree in python Combining weak to strong learners via random forests Learning with ensembles Majority vote classifier VotingClassifier in Sklearn Combining different algorithms for classification with majority vote Evaluating the ensemble classifier Bagging -- Building an ensemble of classifiers from bootstrap samples Leveraging weak learners via adaptive boosting Algorithm implementation Decision trees learning [ back to top ] Here we'll explore a class of algorithms based on decision trees. Decision trees at their root are extremely intuitive. They encode a series of \"if\" and \"else\" choices, similar to how a person might make a decision. However, which questions to ask, and how to proceed for each answer is entirely learned from the data. For example, if you wanted to create a guide to identifying an animal found in nature, you might ask the following series of questions: Is the animal bigger or smaller than a meter long? bigger : does the animal have horns? yes : are the horns longer than ten centimeters? no : is the animal wearing a collar smaller : does the animal have two or four legs? two : does the animal have wings? four : does the animal have a bushy tail? and so on. This binary splitting of questions is the essence of a decision tree. One of the main benefit of tree-based models is that they require little preprocessing of the data. They can work with variables of different types (continuous and discrete) and are invariant to scaling of the features. Another benefit is that tree-based models are what is called \"nonparametric\", which means they don't have a fix set of parameters to learn. Instead, a tree model can become more and more flexible, if given more data. In other words, the number of free parameters grows with the number of samples and is not fixed, as for example in linear models. Building a decision tree [ back to top ] import numpy as np from sklearn import datasets from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) # standardize by mean std X_test_std = sc.transform(X_test) from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt %matplotlib inline def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot all samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) # highlight test samples if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidth=1, marker='o', s=55, label='test set') from sklearn.tree import DecisionTreeClassifier # max depth 3 using entropy for impurofy tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) tree.fit(X_train, y_train) X_combined = np.vstack((X_train, X_test)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X_combined, y_combined, classifier=tree, test_idx=range(105,150)) plt.xlabel('petal length [cm]') plt.ylabel('petal width [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/decision_tree_decision.png', dpi=300) Visualize a decision tree [ back to top ] from sklearn.tree import export_graphviz # export tree as .dot file, install GraphViz to transfer the format export_graphviz(tree, out_file='tree.dot', feature_names=['petal length', 'petal width']) # pip install pydotplus import pydotplus from sklearn.externals.six import StringIO from IPython.display import Image dot_data = StringIO() export_graphviz(tree, out_file=dot_data, feature_names=['petal length', 'petal width'], class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) Image(graph.create_png()) Different impurity criteria [ back to top ] def gini(p): return (p)*(1 - (p)) + (1-p)*(1 - (1-p)) def entropy(p): return - p*np.log2(p) - (1 - p)*np.log2((1 - p)) def error(p): return 1 - np.max([p, 1 - p]) x = np.arange(0.0, 1.0, 0.01) ent = [entropy(p) if p != 0 else None for p in x] sc_ent = [e*0.5 if e else None for e in ent] err = [error(i) for i in x] fig = plt.figure() ax = plt.subplot(111) for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], ['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'], ['-', '-', '--', '-.'], ['black', 'lightgray', 'red', 'green', 'cyan']): line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c) # \u753b\u56fe ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3, fancybox=True, shadow=False) ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--') ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--') plt.ylim([0, 1.1]) plt.xlabel('p(i=1)') plt.ylabel('Impurity Index') plt.tight_layout() # plt.savefig('./figures/impurity.png', dpi=300, bbox_inches='tight') Implement a binary decision tree in python [ back to top ] from collections import Counter import numpy as np # \u6784\u5efa\u4e00\u4e2a\u7c7b\uff0c\u6765\u8868\u5f81\u4e8c\u5206\u6811\u7684\u7ed3\u6784 # Tree \u91cc\u7684\u5c5e\u6027\u9664\u4e86\u5305\u62ec\u5de6\u53f3\u8282\u70b9\u7684 Tree \u4e4b\u5916\uff0c\u8fd8\u6709\u6b64\u8282\u70b9\u4e2d\u5305\u62ec\u6570\u636e\u7684\u6807\u7b7e\u53ca\u5176\u71b5\u503c\uff0c\u7136\u540e\u8fd8\u6709\u8981\u5207\u5206 feature \u7684 idex class Tree: Binary Tree def __init__(self, labels, split_idx=None, children_left=None, children_right=None): self.children_left = children_left self.children_right = children_right self.labels = labels self.split_idx = split_idx self.entropy = calc_entropy(self.labels) def predict(self): most_freq = np.bincount(self.labels).argmax() # find most frequent element return most_freq def __repr__(self, level=0): make it easy to visualize a tree prefix = \\t * level string = prefix + entropy = {}, labels = {}, [0s, 1s] = {}\\n .format( self.entropy, self.labels, np.bincount(self.labels, minlength=2)) if self.split_idx is not None: string += prefix + split on Column {}\\n .format(self.split_idx) string += prefix + True:\\n string += self.children_left.__repr__(level+1) string += prefix + False:\\n string += self.children_right.__repr__(level+1) return string # \u8ba1\u7b97\u4e00\u7ec4\u6570\u636e\u91cc\u7684\u71b5\u503c def calc_entropy(labels): calculate entropy from an array of labels size = float(len(labels)) cnt = Counter(labels) entropy = 0 for label in set(labels): prob = cnt[label] / size entropy += -1 * prob * np.log2(prob) return entropy # \u4e0d\u540c\u7684\u51b3\u7b56\u6811\u7b97\u6cd5 (\u5982 ID3, C4.5, CART \u7b49) \u4f1a\u7528\u4e0d\u540c\u7684\u6807\u51c6\u6765\u9009\u62e9\u8981\u5207\u5206\u7684 feature # \u8fd9\u91cc\u4f7f\u7528\u7684\u662f Information Gain\uff0c\u5373 feature \u5207\u5206\u524d\u540e\u7684\u71b5\u503c\u53d8\u5316 def choose_best_feature_to_split(features, labels): choose the best split feature which maximize information gain num_features = features.shape[1] base_entropy = calc_entropy(labels) best_info_gain = 0 best_feature = None for i in range(num_features): new_entropy = 0 for value in [0, 1]: new_labels = labels[features[:, i] == value] weight = float(len(new_labels)) / len(labels) new_entropy += weight * calc_entropy(new_labels) info_gain = base_entropy - new_entropy if info_gain best_info_gain: best_info_gain = info_gain best_feature = i return best_feature def create_decision_tree(features, labels, current_depth=0, max_depth=10): recursively create tree tree = Tree(labels) # define stop condition # stop when all data in this node are from the same class if len(set(labels)) == 1: return tree # stop when max_depth are reached if current_depth = max_depth: return tree # split on the best feature found best_feature = choose_best_feature_to_split(features, labels) if best_feature is None: return tree # recursively build subtrees msk = (features[:, best_feature] == 1) tree.split_idx = best_feature tree.children_left = create_decision_tree( features[msk], labels[msk], current_depth+1) tree.children_right = create_decision_tree( features[~msk], labels[~msk], current_depth+1) return tree # \u6a21\u62df\u4e00\u7ec4\u6570\u636e\u6765\u6d4b\u8bd5 data = np.array([[1, 0], [1, 1], [0, 1], [0, 0]]) labels = np.array([1, 0, 0, 0]) tree = create_decision_tree(data, labels) tree entropy = 0.811278124459, labels = [1 0 0 0], [0s, 1s] = [3 1] split on Column 0 True: entropy = 1.0, labels = [1 0], [0s, 1s] = [1 1] split on Column 1 True: entropy = 0.0, labels = [0], [0s, 1s] = [1 0] False: entropy = 0.0, labels = [1], [0s, 1s] = [0 1] False: entropy = 0.0, labels = [0 0], [0s, 1s] = [2 0] # \u904d\u5386\u4e8c\u5206\u6811\uff0c\u6765\u5f97\u5230\u5206\u7c7b\u9884\u6d4b def _classify(tree, data_row): prediction for new data if tree.split_idx is None: # if it's a leaf return tree.predict() split_idx = tree.split_idx if data_row[split_idx]: # if True for split condition return _classify(tree.children_left, data_row) else: return _classify(tree.children_right, data_row) def classify(tree, data): data = np.array(data) num_row = data.shape[0] results = np.empty(shape=num_row) for i in range(num_row): results[i] = _classify(tree, data[i, :]) return results new_data = [[0, 0], [1, 0]] classify(tree, new_data) array([ 0., 1.]) Combining weak to strong learners via random forests [ back to top ] \u53ef\u4ee5\u770b\u505a\u662f ensemble of decision trees, \u5c06\u5f31\u7684\u6a21\u578b\u7ed3\u5408\u5728\u4e00\u8d77\u53d8\u6210\u5f3a\u6a21\u578b. \u66f4\u6613\u6269\u5c55, \u4e14\u8f83\u5c11\u4f1a overfitting. draw a random bootstrap sample of size n (with replacement) grow decision tree from bootstrap sample, at each node: randomly select d features without replacement split node using feature that provides best split repeat 1 2 k times. aggregate the prediction by each tree to assign the class label by majority vote from sklearn.ensemble import RandomForestClassifier # from 10 decision trees, n_jobs \u503c\u4f7f\u7528 cpu \u4e2a\u6570 forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2) forest.fit(X_train, y_train) plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105,150)) plt.xlabel('petal length [cm]') plt.ylabel('petal width [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/random_forest.png', dpi=300) Learning with ensembles \u5c06\u4e00\u7cfb\u5217\u5206\u7c7b\u5668\u96c6\u5408\u8d77\u6765, \u53d6\u591a\u6570\u4e3a\u5206\u7c7b\u7ed3\u679c Build powerful models from weak learners that learn from their mistakes ensemble method \u5c31\u662f\u8bb2\u591a\u4e2a\u4e0d\u540c\u7684\u5206\u7c7b\u5668\u96c6\u5408\u7ec4\u5408\u4e3a\u4e00\u4e2a\u5927\u7684\u5206\u7c7b\u5668. \u9009\u62e9\u6700\u7ec8\u7ed3\u679c\u662f\u4ee5 majority voting \u5373\u4f7f\u6bcf\u4e2a\u5355\u72ec\u7684\u5206\u7c7b\u5668\u9519\u8bef\u7387\u8f83\u9ad8, \u4f46\u5c06\u591a\u4e2a\u5206\u7c7b\u5668\u7ec4\u5408\u4e4b\u540e, \u9519\u8bef\u7387\u5c31\u4f1a\u5927\u5927\u964d\u4f4e [ back to top ] \u5047\u8bbe\u6211\u4eec\u7ec4\u5408\u4e86 n \u4e2a\u5206\u7c7b\u5668\uff0c\u5b83\u4eec\u7684\u9519\u8bef\u7387\u90fd\u4e3a $$ \\varepsilon $$, \u5404\u4e2a\u5206\u7c7b\u5668\u4e4b\u95f4\u72ec\u7acb\u3002 \u5219\u8fd9 n \u4e2a\u5206\u7c7b\u5668\u91cc, \u591a\u4e8e k \u4e2a\u5206\u7c7b\u5668\u5206\u7c7b\u9519\u8bef\u7684\u6982\u7387\u4e3a $$ P(y \\geq k) = \\sum^n_k \\binom{n}{k} \\varepsilon^k (1-\\varepsilon)^{n-k}$$ from scipy.misc import comb import math # emsemble error rate def ensemble_error(n_classifier, error): k_start = int(math.ceil(n_classifier / 2.0)) probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k) for k in range(k_start, n_classifier + 1)] return sum(probs) # 11\u4e2a\u5206\u7c7b\u5668, \u6bcf\u4e2a\u5206\u7c7b\u5668\u7684 error rate \u662f0.25\u7684\u8bdd, \u901a\u8fc7 combinator \u4e4b\u540e\u7684 error rate ensemble_error(n_classifier=11, error=0.25) 0.034327507019042969 # ensemble error \u548c base error \u7684\u5173\u7cfb error_range = np.arange(0.0, 1.01, 0.01) ens_errors = [ensemble_error(n_classifier=11, error=error) for error in error_range] plt.plot(error_range, ens_errors, label='Ensemble error', linewidth=2) plt.plot(error_range, error_range, linestyle='--', label='Base error',linewidth=2) plt.xlabel('Base error') plt.ylabel('Base/Ensemble error') plt.legend(loc='upper left') plt.grid() plt.tight_layout() # plt.savefig('./figures/ensemble_err.png', dpi=300) $$\\varepsilon$$ 0.5 \u65f6, emsemble error \u90fd\u8981\u4f4e\u4e8e base error, $$\\varepsilon$$ 0.5 \u65f6, emsemble error \u5c31\u4f1a\u5927\u4e8e base error Majority vote classifier combine different classication algorithms associated with individual weights for confidence [ back to top ] \u5f53\u591a\u4e2a\u5206\u7c7b\u5668 C \u62e5\u6709\u76f8\u540c\u6743\u91cd\u65f6\uff0censemble \u7ed9\u51fa\u7684\u9884\u6d4b $$\\hat y$$ \u4e3a\u4f17\u6570\uff1a $$ \\hat y = mode{C_1(x), C_2(x), \\dotso, C_m(x)} $$ \u82e5\u5206\u7c7b\u5668 $$C_j$$ \u5bf9\u5e94\u4e0d\u540c\u7684\u6743\u91cd $$w_j$$, \u5219 $$ \\hat y = \\arg\\max_i \\sum^m_{j=1}w_j P_{ij} $$ \u5176\u4e2d $$P_{ij}$$ \u662f $$C_j$$ \u9884\u6d4b\u7ed3\u679c\u4e3a i \u7684\u6982\u7387 import numpy as np np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) # np.argmax: returns the indices of the maximum values along an axis. # np.bincount: Count number of occurrences of each value in array of non-negative ints 1 np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6]) array([ 0.4, 0.6]) ex = np.array([[0.9, 0.1], # C1 \u7684\u9884\u6d4b\u7ed3\u679c [0.8, 0.2], # C2 \u7684\u9884\u6d4b\u7ed3\u679c [0.4, 0.6]]) # C3 \u7684\u9884\u6d4b\u7ed3\u679c p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6]) # C1, C2, C3 \u7684\u6743\u91cd p array([ 0.58, 0.42]) $$p(i_0| x) = 0.58$$ $$p(i_1 | x) = 0.42$$ $$\\hat y = \\arg\\max_i [p(i_0 | x), p(i_1 | x)] = 0$$ np.argmax(p) 0 VotingClassifier in Sklearn \u4f7f\u7528 Sklearn \u4e2d\u81ea\u5e26\u7684 VotingClassifier [ back to top ] import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier # \u4f7f\u7528 3 \u4e2a\u5206\u7c7b\u5668 clf1 = LogisticRegression(random_state=1) clf2 = RandomForestClassifier(random_state=1) clf3 = GaussianNB() # \u751f\u6210\u6570\u636e X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y = np.array([1, 1, 1, 2, 2, 2]) # voting='hard', uses predicted class labels for majority rule voting. eclf1 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard') eclf1 = eclf1.fit(X, y) print(eclf1.predict(X)) # voting='soft', predicts the class label based on the argmax of the sums of the predicted probalities eclf2 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft') eclf2 = eclf2.fit(X, y) print(eclf2.predict(X)) # add weight eclf3 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,1,1]) eclf3 = eclf3.fit(X, y) print(eclf3.predict(X)) [1 1 1 2 2 2] [1 1 1 2 2 2] [1 1 1 2 2 2] Combining different algorithms for classification with majority vote [ back to top ] from sklearn import datasets from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder # load iris data iris = datasets.load_iris() X, y = iris.data[50:, [1, 2]], iris.target[50:] st = StandardScaler() X = st.fit_transform(X) le = LabelEncoder() y = le.fit_transform(y) # 50% train, 50% test X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.5, random_state=1) from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.cross_validation import cross_val_score clf1 = LogisticRegression(C=0.01, random_state=42) clf2 = KNeighborsClassifier(n_neighbors=1) clf3 = DecisionTreeClassifier(max_depth=1, random_state=42) clf_labels = ['Logistic Regression', 'KNN', 'Decision Tree'] all_clf = [clf1, clf2, clf3] print('10-fold cross validation:\\n') for clf, label in zip(all_clf, clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc') print( ROC AUC: %0.2f (+/- %0.2f) [%s] % (scores.mean(), scores.std(), label)) 10-fold cross validation: ROC AUC: 0.93 (+/- 0.15) [Logistic Regression] ROC AUC: 0.93 (+/- 0.10) [KNN] ROC AUC: 0.92 (+/- 0.15) [Decision Tree] from sklearn.ensemble import VotingClassifier mv_clf = VotingClassifier( estimators=[('c1', clf1), ('c2', clf2), ('c3', clf3)], voting='soft') clf_labels += ['Majority Voting'] all_clf += [mv_clf] print('10-fold cross validation:\\n') for clf, label in zip(all_clf, clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc') print( ROC AUC: %0.2f (+/- %0.2f) [%s] % (scores.mean(), scores.std(), label)) 10-fold cross validation: ROC AUC: 0.93 (+/- 0.15) [Logistic Regression] ROC AUC: 0.93 (+/- 0.10) [KNN] ROC AUC: 0.92 (+/- 0.15) [Decision Tree] ROC AUC: 0.97 (+/- 0.10) [Majority Voting] \u6700\u540e\u4e00\u4e2a\u662f majority voting, \u660e\u663e\u6bd4\u5355\u72ec\u7684\u5206\u7c7b\u5668\u7ed3\u679c\u597d Evaluating the ensemble classifier [ back to top ] \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u5404\u4e2a\u5206\u7c7b\u5668\u7684 ROC AUC from sklearn.metrics import roc_curve from sklearn.metrics import auc colors = ['black', 'orange', 'blue', 'green'] linestyles = [':', '--', '-.', '-'] for clf, label, clr, ls \\ in zip(all_clf, clf_labels, colors, linestyles): # assuming the label of the positive class is 1 y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:, 1] fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred) roc_auc = auc(x=fpr, y=tpr) plt.plot(fpr, tpr, color=clr, linestyle=ls, label='%s (auc = %0.2f)' % (label, roc_auc)) plt.legend(loc='lower right') plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=2) plt.xlim([-0.1, 1.1]) plt.ylim([-0.1, 1.1]) plt.grid() plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.tight_layout() # plt.savefig('./figures/roc.png', dpi=300) ROC \u53ef\u770b\u51fa, ensemble classfifier \u5728 test set \u4e0a\u8868\u73b0\u4e0d\u9519 \u5bf9\u6bd4\u51b3\u7b56\u8fb9\u754c from itertools import product x_min = X_train[:, 0].min() - 1 x_max = X_train[:, 0].max() + 1 y_min = X_train[:, 1].min() - 1 y_max = X_train[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) # \u6dfb\u52a0 subplots f, axarr = plt.subplots(nrows=2, ncols=2, sharex='col', sharey='row', figsize=(7, 5)) for idx, clf, tt in zip(product([0, 1], [0, 1]), all_clf, clf_labels): clf.fit(X_train, y_train) Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T) Z = Z.reshape(xx.shape) axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3) axarr[idx[0], idx[1]].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', marker='^', s=50) axarr[idx[0], idx[1]].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', marker='o', s=50) axarr[idx[0], idx[1]].set_title(tt) plt.text(-3.5, -4.5, s='Sepal width [standardized]', ha='center', va='center', fontsize=12) plt.text(-10.5, 4.5, s='Petal length [standardized]', ha='center', va='center', fontsize=12, rotation=90) plt.tight_layout() # plt.savefig('./figures/voting_panel', bbox_inches='tight', dpi=300) Bagging -- Building an ensemble of classifiers from bootstrap samples draw bootstrap samples (random samples with replacement) from initial training set random forests are a special case of bagging where we also use random feature subsets to fit the individual decision trees [ back to top ] import pandas as pd # wine dataset df_wine = pd.read_csv('ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine/wine.data', header=None) df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'] # only consider Wine classes 2 and 3 df_wine = df_wine[df_wine['Class label'] != 1] y = df_wine['Class label'].values X = df_wine[['Alcohol', 'Hue']].values from sklearn.preprocessing import LabelEncoder from sklearn.cross_validation import train_test_split # \u8f6c\u6362 label le = LabelEncoder() y = le.fit_transform(y) # 60% train, 40% test X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.40, random_state=1) # sklearn \u63d0\u4f9b\u7684 BaggingClassifier\uff0c \u5176\u5b9e\u529f\u80fd\u5df2\u7ecf\u8d85\u8fc7 Bagging \u4e86 # \u5b83\u65e2\u80fd\u5bf9 samples \u91c7\u6837\uff0c\u4e5f\u80fd\u5bf9 features \u91c7\u6837 from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier tree = DecisionTreeClassifier(criterion='entropy') # \u7528 Decision Tree \u4f5c base bag = BaggingClassifier(base_estimator=tree, n_estimators=500, max_samples=1.0, # \u5b50\u91c7\u6837 samples \u7684\u6bd4\u4f8b max_features=1.0, # \u5b50\u91c7\u6837 features \u7684\u6bd4\u4f8b bootstrap=True, # \u91c7\u6837 samples \u65f6\u662f\u5426\u4f7f\u7528 bootstrap bootstrap_features=False, # \u91c7\u6837 features \u65f6\u662f\u5426\u4f7f\u7528 bootstrap random_state=1) from sklearn.metrics import accuracy_score tree = tree.fit(X_train, y_train) y_train_pred = tree.predict(X_train) y_test_pred = tree.predict(X_test) tree_train = accuracy_score(y_train, y_train_pred) tree_test = accuracy_score(y_test, y_test_pred) print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train, tree_test)) bag = bag.fit(X_train, y_train) y_train_pred = bag.predict(X_train) y_test_pred = bag.predict(X_test) bag_train = accuracy_score(y_train, y_train_pred) bag_test = accuracy_score(y_test, y_test_pred) print('Bagging train/test accuracies %.3f/%.3f' % (bag_train, bag_test)) Decision tree train/test accuracies 1.000/0.854 Bagging train/test accuracies 1.000/0.896 \u4f7f\u7528 Bagging \u4e4b\u540e\uff0c\u6d4b\u8bd5\u96c6\u7684\u51c6\u786e\u7387\u6709\u63d0\u5347 # \u753b\u51b3\u7b56\u8fb9\u754c %matplotlib inline import numpy as np import matplotlib.pyplot as plt x_min = X_train[:, 0].min() - 1 x_max = X_train[:, 0].max() + 1 y_min = X_train[:, 1].min() - 1 y_max = X_train[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) f, axarr = plt.subplots(nrows=1, ncols=2, sharex='col', sharey='row', figsize=(8, 3)) for idx, clf, tt in zip([0, 1], [tree, bag], ['Decision Tree', 'Bagging']): clf.fit(X_train, y_train) Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.3) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', marker='^') axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', marker='o') axarr[idx].set_title(tt) axarr[0].set_ylabel('Alcohol', fontsize=12) plt.text(10.2, -1.2, s='Hue', ha='center', va='center', fontsize=12) plt.tight_layout() #plt.savefig('./figures/bagging_region.png', # dpi=300, # bbox_inches='tight') Bagging \u51cf\u5c11\u4e86 overfitting\uff0c \u4f7f\u51b3\u7b56\u8fb9\u754c\u66f4\u5e73\u6ed1 Leveraging of weak learners via adaptive boosting let the weak learners subsequently learn from misclassified training samples to improve the performance of the ensemble [ back to top ] Adaptive boosting (AdaBoost) 1. Set weight vector w to uniform weights where $$\\sum_i w_i = 1$$ 2. For j in m boosting rounds, do the following: 3. Train a weighted weak learner: $$C_j = train(X,y,w)$$. 4. Predict class labels: $$\\hat{y} = predict(C_j, X)$$ . 5. Compute weighted error rate: $$\\epsilon = w \\cdot (\\hat{y} \\neq y)$$. 6. Compute coefficient: $$\\alpha_j = \\frac 1 2 ln\\frac{1-\\epsilon}{\\epsilon}$$ . 7. Update weights: $$w:= w \\times exp(-\\alpha_j \\times \\hat{y} \\times y)$$ . 8. Normalize weights to sum to 1: $$w:= \\frac{w}{\\sum_i w_i}$$ . 9. Compute final prediction: $$\\hat{y} = (\\sum_{j=1}^{m}(\\alpha_j \\times predict(C_j, X)) 0)$$. $$\\cdot$$ denotes dot product between two vectors $$\\times$$ denotes element-wise multiplication of two vectors from sklearn.ensemble import AdaBoostClassifier tree = DecisionTreeClassifier(criterion='entropy', max_depth=1) ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=0) tree = tree.fit(X_train, y_train) y_train_pred = tree.predict(X_train) y_test_pred = tree.predict(X_test) tree_train = accuracy_score(y_train, y_train_pred) tree_test = accuracy_score(y_test, y_test_pred) print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train, tree_test)) ada = ada.fit(X_train, y_train) y_train_pred = ada.predict(X_train) y_test_pred = ada.predict(X_test) ada_train = accuracy_score(y_train, y_train_pred) ada_test = accuracy_score(y_test, y_test_pred) print('AdaBoost train/test accuracies %.3f/%.3f' % (ada_train, ada_test)) Decision tree train/test accuracies 0.845/0.854 AdaBoost train/test accuracies 1.000/0.875 Adaboost \u53ef\u4ee5\u51cf\u5c11 Bias\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u66f4\u591a\u7684 Variance x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(8, 3)) for idx, clf, tt in zip([0, 1], [tree, ada], ['Decision Tree', 'AdaBoost']): clf.fit(X_train, y_train) Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.3) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', marker='^') axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', marker='o') axarr[idx].set_title(tt) axarr[0].set_ylabel('Alcohol', fontsize=12) plt.text(10.2, -1.2, s='Hue', ha='center', va='center', fontsize=12) plt.tight_layout() #plt.savefig('./figures/adaboost_region.png', # dpi=300, # bbox_inches='tight') Adaboost \u7684\u51b3\u7b56\u8fb9\u754c\u6bd4 tree \u590d\u6742, \u4e0e BaggingClassifier \u76f8\u4f3c. Ensemble method \u9700\u8981\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90, \u8fd9\u4e2a\u5728\u5b9e\u9645\u8fd0\u7528\u4e2d\u4e5f\u662f\u8981\u8003\u8651\u7684. Algorithm implementation [ back to top ] \u5e7f\u4e49\u63d0\u5347\u6811\u7b97\u6cd5\u8be6\u89e3 import numpy import matplotlib.pyplot as plot %matplotlib inline from sklearn import tree from sklearn.tree import DecisionTreeRegressor from math import floor import random # Build a simple data set with y = x + random n_points = 1000 # x values for plotting x_plot = [(float(i) / float(n_points) - 0.5) for i in range(n_points + 1)] # x needs to be list of lists. x = [[s] for s in x_plot] # y (labels) has random noise added to x-value # set seed numpy.random.seed(1) y = [s + numpy.random.normal(scale=0.1) for s in x_plot] # take fixed test set 30% of sample n_sample = int(n_points * 0.30) idx_test = random.sample(range(n_points), n_sample) idx_test.sort() idx_train = [idx for idx in range(n_points) if not (idx in idx_test)] # Define test and training attribute and label sets x_train = [x[r] for r in idx_train] x_test = [x[r] for r in idx_test] y_train = [y[r] for r in idx_train] y_test = [y[r] for r in idx_test] # train a series of models on random subsets of the training data # collect the models in a list and check error of composite as list grows # maximum number of models to generate num_trees_max = 30 # tree depth - typically at the high end tree_depth = 5 # initialize a list to hold models mode_list = [] pred_list = [] eps = 0.3 # initialize residuals to be the labels y residuals = list(y_train) for i_trees in range(num_trees_max): mode_list.append(DecisionTreeRegressor(max_depth=tree_depth)) mode_list[-1].fit(x_train, residuals) # make prediction with latest model and add to list of predictions latest_in_sample_prediction = mode_list[-1].predict(x_train) # use new predictions to update residuals residuals = [residuals[i] - eps * latest_in_sample_prediction[i] for i in range(len(residuals))] latest_out_sample_prediction = mode_list[-1].predict(x_test) pred_list.append(list(latest_out_sample_prediction)) # build cumulative prediction from first n models mse = [] all_predictions = [] for i_models in range(len(mode_list)): # add the first i_models of the predictions and multiply by eps prediction = [] for i_pred in range(len(x_test)): prediction.append( sum([pred_list[i][i_pred] for i in range(i_models + 1)]) * eps) all_predictions.append(prediction) errors = [(y_test[i] - prediction[i]) for i in range(len(y_test))] mse.append(sum([e * e for e in errors]) / len(y_test)) n_models = [i + 1 for i in range(len(mode_list))] plot.plot(n_models, mse) plot.axis('tight') plot.xlabel('Number of Models in Ensemble') plot.ylabel('Mean Squared Error') plot.ylim((0.0, max(mse))) plot.show() plot_list = [0, 14, 29] line_type = [':', '-.', '--'] plot.figure() for i in range(len(plot_list)): i_plot = plot_list[i] text_legend = 'Prediction with ' + str(i_plot) + ' Trees' plot.plot(x_test, all_predictions[i_plot], label=text_legend, linestyle=line_type[i]) plot.plot(x_test, y_test, label='True y Value', alpha=0.25) plot.legend(bbox_to_anchor=(1, 0.3)) plot.axis('tight') plot.xlabel('x value') plot.ylabel('Predictions'); \u968f\u673a\u68ee\u6797\u7b97\u6cd5\u8be6\u89e3 import urllib2 import numpy from sklearn import tree from sklearn.tree import DecisionTreeRegressor import random from math import sqrt import matplotlib.pyplot as plot # read data into iterable target_url = ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine-quality/winequality-red.csv data = urllib2.urlopen(target_url) x_list = [] labels = [] names = [] first_line = True for line in data: if first_line: names = line.strip().split( ; ) first_line = False else: # split on semi-colon row = line.strip().split( ; ) # put labels in separate array labels.append(float(row[-1])) # remove label from row row.pop() # convert row to floats float_row = [float(num) for num in row] x_list.append(float_row) nrows = len(x_list) ncols = len(x_list[0]) # take fixed test set 30% of sample random.seed(1) # set seed so results are the same each run n_sample = int(nrows * 0.30) idx_test = random.sample(range(nrows), n_sample) idx_test.sort() idx_train = [idx for idx in range(nrows) if not (idx in idx_test)] # Define test and training attribute and label sets x_train = [x_list[r] for r in idx_train] x_test = [x_list[r] for r in idx_test] y_train = [labels[r] for r in idx_train] y_test = [labels[r] for r in idx_test] # train a series of models on random subsets of the training data # collect the models in a list and check error of composite as list grows # maximum number of models to generate num_trees_max = 30 # tree depth - typically at the high end tree_depth = 12 # pick how many attributes will be used in each model. # authors recommend 1/3 for regression problem n_attr = 4 # initialize a list to hold models mode_list = [] index_list = [] pred_list = [] n_train_rows = len(y_train) for i_trees in range(num_trees_max): mode_list.append(DecisionTreeRegressor(max_depth=tree_depth)) # take random sample of attributes idx_attr = random.sample(range(ncols), n_attr) idx_attr.sort() index_list.append(idx_attr) # take a random sample of training rows idx_rows = [] for i in range(int(0.5 * n_train_rows)): idx_rows.append(random.choice(range(len(x_train)))) idx_rows.sort() # build training set x_rf_train = [] y_rf_train = [] for i in range(len(idx_rows)): temp = [x_train[idx_rows[i]][j] for j in idx_attr] x_rf_train.append(temp) y_rf_train.append(y_train[idx_rows[i]]) mode_list[-1].fit(x_rf_train, y_rf_train) # restrict xTest to attributes selected for training x_rf_test = [] for xx in x_test: temp = [xx[i] for i in idx_attr] x_rf_test.append(temp) latest_out_sample_prediction = mode_list[-1].predict(x_rf_test) pred_list.append(list(latest_out_sample_prediction)) # build cumulative prediction from first n models mse = [] all_predictions = [] for i_models in range(len(mode_list)): # add the first iModels of the predictions and multiply by eps prediction = [] for i_pred in range(len(x_test)): prediction.append( sum([pred_list[i][i_pred] for i in range(i_models + 1)]) / ( i_models + 1)) all_predictions.append(prediction) errors = [(y_test[i] - prediction[i]) for i in range(len(y_test))] mse.append(sum([e * e for e in errors]) / len(y_test)) n_models = [i + 1 for i in range(len(mode_list))] plot.plot(n_models, mse) plot.axis('tight') plot.xlabel('Number of Trees in Ensemble') plot.ylabel('Mean Squared Error') plot.ylim((0.0, max(mse))) plot.show() print('Minimum MSE') print(min(mse)) Minimum MSE 0.389088116065 \u7ec3\u4e60\u9898\uff1a \u4f7f\u7528\u63d0\u5347\u6811\u548c\u968f\u673a\u68ee\u6797\u4e24\u79cd\u65b9\u6cd5\uff0c\u5bf9\u4fe1\u8d37\u6570\u636e\u96c6\u8fdb\u884c\u5efa\u6a21\u6bd4\u8f83","title":"3w"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#sections","text":"Decision trees learning Building a decision tree Visualize a decision tree Different impurity criteria Implement a binary decision tree in python Combining weak to strong learners via random forests Learning with ensembles Majority vote classifier VotingClassifier in Sklearn Combining different algorithms for classification with majority vote Evaluating the ensemble classifier Bagging -- Building an ensemble of classifiers from bootstrap samples Leveraging weak learners via adaptive boosting Algorithm implementation","title":"Sections"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#decision-trees-learning","text":"[ back to top ] Here we'll explore a class of algorithms based on decision trees. Decision trees at their root are extremely intuitive. They encode a series of \"if\" and \"else\" choices, similar to how a person might make a decision. However, which questions to ask, and how to proceed for each answer is entirely learned from the data. For example, if you wanted to create a guide to identifying an animal found in nature, you might ask the following series of questions: Is the animal bigger or smaller than a meter long? bigger : does the animal have horns? yes : are the horns longer than ten centimeters? no : is the animal wearing a collar smaller : does the animal have two or four legs? two : does the animal have wings? four : does the animal have a bushy tail? and so on. This binary splitting of questions is the essence of a decision tree. One of the main benefit of tree-based models is that they require little preprocessing of the data. They can work with variables of different types (continuous and discrete) and are invariant to scaling of the features. Another benefit is that tree-based models are what is called \"nonparametric\", which means they don't have a fix set of parameters to learn. Instead, a tree model can become more and more flexible, if given more data. In other words, the number of free parameters grows with the number of samples and is not fixed, as for example in linear models.","title":"Decision trees learning"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#building-a-decision-tree","text":"[ back to top ] import numpy as np from sklearn import datasets from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=0) sc = StandardScaler() sc.fit(X_train) X_train_std = sc.transform(X_train) # standardize by mean std X_test_std = sc.transform(X_test) from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt %matplotlib inline def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot all samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) # highlight test samples if test_idx: X_test, y_test = X[test_idx, :], y[test_idx] plt.scatter(X_test[:, 0], X_test[:, 1], c='', alpha=1.0, linewidth=1, marker='o', s=55, label='test set') from sklearn.tree import DecisionTreeClassifier # max depth 3 using entropy for impurofy tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0) tree.fit(X_train, y_train) X_combined = np.vstack((X_train, X_test)) y_combined = np.hstack((y_train, y_test)) plot_decision_regions(X_combined, y_combined, classifier=tree, test_idx=range(105,150)) plt.xlabel('petal length [cm]') plt.ylabel('petal width [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/decision_tree_decision.png', dpi=300)","title":"Building a decision tree"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#visualize-a-decision-tree","text":"[ back to top ] from sklearn.tree import export_graphviz # export tree as .dot file, install GraphViz to transfer the format export_graphviz(tree, out_file='tree.dot', feature_names=['petal length', 'petal width']) # pip install pydotplus import pydotplus from sklearn.externals.six import StringIO from IPython.display import Image dot_data = StringIO() export_graphviz(tree, out_file=dot_data, feature_names=['petal length', 'petal width'], class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) Image(graph.create_png())","title":"Visualize a decision tree"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#different-impurity-criteria","text":"[ back to top ] def gini(p): return (p)*(1 - (p)) + (1-p)*(1 - (1-p)) def entropy(p): return - p*np.log2(p) - (1 - p)*np.log2((1 - p)) def error(p): return 1 - np.max([p, 1 - p]) x = np.arange(0.0, 1.0, 0.01) ent = [entropy(p) if p != 0 else None for p in x] sc_ent = [e*0.5 if e else None for e in ent] err = [error(i) for i in x] fig = plt.figure() ax = plt.subplot(111) for i, lab, ls, c, in zip([ent, sc_ent, gini(x), err], ['Entropy', 'Entropy (scaled)', 'Gini Impurity', 'Misclassification Error'], ['-', '-', '--', '-.'], ['black', 'lightgray', 'red', 'green', 'cyan']): line = ax.plot(x, i, label=lab, linestyle=ls, lw=2, color=c) # \u753b\u56fe ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3, fancybox=True, shadow=False) ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--') ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--') plt.ylim([0, 1.1]) plt.xlabel('p(i=1)') plt.ylabel('Impurity Index') plt.tight_layout() # plt.savefig('./figures/impurity.png', dpi=300, bbox_inches='tight')","title":"Different impurity criteria"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#implement-a-binary-decision-tree-in-python","text":"[ back to top ] from collections import Counter import numpy as np # \u6784\u5efa\u4e00\u4e2a\u7c7b\uff0c\u6765\u8868\u5f81\u4e8c\u5206\u6811\u7684\u7ed3\u6784 # Tree \u91cc\u7684\u5c5e\u6027\u9664\u4e86\u5305\u62ec\u5de6\u53f3\u8282\u70b9\u7684 Tree \u4e4b\u5916\uff0c\u8fd8\u6709\u6b64\u8282\u70b9\u4e2d\u5305\u62ec\u6570\u636e\u7684\u6807\u7b7e\u53ca\u5176\u71b5\u503c\uff0c\u7136\u540e\u8fd8\u6709\u8981\u5207\u5206 feature \u7684 idex class Tree: Binary Tree def __init__(self, labels, split_idx=None, children_left=None, children_right=None): self.children_left = children_left self.children_right = children_right self.labels = labels self.split_idx = split_idx self.entropy = calc_entropy(self.labels) def predict(self): most_freq = np.bincount(self.labels).argmax() # find most frequent element return most_freq def __repr__(self, level=0): make it easy to visualize a tree prefix = \\t * level string = prefix + entropy = {}, labels = {}, [0s, 1s] = {}\\n .format( self.entropy, self.labels, np.bincount(self.labels, minlength=2)) if self.split_idx is not None: string += prefix + split on Column {}\\n .format(self.split_idx) string += prefix + True:\\n string += self.children_left.__repr__(level+1) string += prefix + False:\\n string += self.children_right.__repr__(level+1) return string # \u8ba1\u7b97\u4e00\u7ec4\u6570\u636e\u91cc\u7684\u71b5\u503c def calc_entropy(labels): calculate entropy from an array of labels size = float(len(labels)) cnt = Counter(labels) entropy = 0 for label in set(labels): prob = cnt[label] / size entropy += -1 * prob * np.log2(prob) return entropy # \u4e0d\u540c\u7684\u51b3\u7b56\u6811\u7b97\u6cd5 (\u5982 ID3, C4.5, CART \u7b49) \u4f1a\u7528\u4e0d\u540c\u7684\u6807\u51c6\u6765\u9009\u62e9\u8981\u5207\u5206\u7684 feature # \u8fd9\u91cc\u4f7f\u7528\u7684\u662f Information Gain\uff0c\u5373 feature \u5207\u5206\u524d\u540e\u7684\u71b5\u503c\u53d8\u5316 def choose_best_feature_to_split(features, labels): choose the best split feature which maximize information gain num_features = features.shape[1] base_entropy = calc_entropy(labels) best_info_gain = 0 best_feature = None for i in range(num_features): new_entropy = 0 for value in [0, 1]: new_labels = labels[features[:, i] == value] weight = float(len(new_labels)) / len(labels) new_entropy += weight * calc_entropy(new_labels) info_gain = base_entropy - new_entropy if info_gain best_info_gain: best_info_gain = info_gain best_feature = i return best_feature def create_decision_tree(features, labels, current_depth=0, max_depth=10): recursively create tree tree = Tree(labels) # define stop condition # stop when all data in this node are from the same class if len(set(labels)) == 1: return tree # stop when max_depth are reached if current_depth = max_depth: return tree # split on the best feature found best_feature = choose_best_feature_to_split(features, labels) if best_feature is None: return tree # recursively build subtrees msk = (features[:, best_feature] == 1) tree.split_idx = best_feature tree.children_left = create_decision_tree( features[msk], labels[msk], current_depth+1) tree.children_right = create_decision_tree( features[~msk], labels[~msk], current_depth+1) return tree # \u6a21\u62df\u4e00\u7ec4\u6570\u636e\u6765\u6d4b\u8bd5 data = np.array([[1, 0], [1, 1], [0, 1], [0, 0]]) labels = np.array([1, 0, 0, 0]) tree = create_decision_tree(data, labels) tree entropy = 0.811278124459, labels = [1 0 0 0], [0s, 1s] = [3 1] split on Column 0 True: entropy = 1.0, labels = [1 0], [0s, 1s] = [1 1] split on Column 1 True: entropy = 0.0, labels = [0], [0s, 1s] = [1 0] False: entropy = 0.0, labels = [1], [0s, 1s] = [0 1] False: entropy = 0.0, labels = [0 0], [0s, 1s] = [2 0] # \u904d\u5386\u4e8c\u5206\u6811\uff0c\u6765\u5f97\u5230\u5206\u7c7b\u9884\u6d4b def _classify(tree, data_row): prediction for new data if tree.split_idx is None: # if it's a leaf return tree.predict() split_idx = tree.split_idx if data_row[split_idx]: # if True for split condition return _classify(tree.children_left, data_row) else: return _classify(tree.children_right, data_row) def classify(tree, data): data = np.array(data) num_row = data.shape[0] results = np.empty(shape=num_row) for i in range(num_row): results[i] = _classify(tree, data[i, :]) return results new_data = [[0, 0], [1, 0]] classify(tree, new_data) array([ 0., 1.])","title":"Implement a binary decision tree in python"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#combining-weak-to-strong-learners-via-random-forests","text":"[ back to top ] \u53ef\u4ee5\u770b\u505a\u662f ensemble of decision trees, \u5c06\u5f31\u7684\u6a21\u578b\u7ed3\u5408\u5728\u4e00\u8d77\u53d8\u6210\u5f3a\u6a21\u578b. \u66f4\u6613\u6269\u5c55, \u4e14\u8f83\u5c11\u4f1a overfitting. draw a random bootstrap sample of size n (with replacement) grow decision tree from bootstrap sample, at each node: randomly select d features without replacement split node using feature that provides best split repeat 1 2 k times. aggregate the prediction by each tree to assign the class label by majority vote from sklearn.ensemble import RandomForestClassifier # from 10 decision trees, n_jobs \u503c\u4f7f\u7528 cpu \u4e2a\u6570 forest = RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=1, n_jobs=2) forest.fit(X_train, y_train) plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105,150)) plt.xlabel('petal length [cm]') plt.ylabel('petal width [cm]') plt.legend(loc='upper left') plt.tight_layout() # plt.savefig('./figures/random_forest.png', dpi=300)","title":"Combining weak to strong learners via random forests"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#learning-with-ensembles","text":"\u5c06\u4e00\u7cfb\u5217\u5206\u7c7b\u5668\u96c6\u5408\u8d77\u6765, \u53d6\u591a\u6570\u4e3a\u5206\u7c7b\u7ed3\u679c Build powerful models from weak learners that learn from their mistakes ensemble method \u5c31\u662f\u8bb2\u591a\u4e2a\u4e0d\u540c\u7684\u5206\u7c7b\u5668\u96c6\u5408\u7ec4\u5408\u4e3a\u4e00\u4e2a\u5927\u7684\u5206\u7c7b\u5668. \u9009\u62e9\u6700\u7ec8\u7ed3\u679c\u662f\u4ee5 majority voting \u5373\u4f7f\u6bcf\u4e2a\u5355\u72ec\u7684\u5206\u7c7b\u5668\u9519\u8bef\u7387\u8f83\u9ad8, \u4f46\u5c06\u591a\u4e2a\u5206\u7c7b\u5668\u7ec4\u5408\u4e4b\u540e, \u9519\u8bef\u7387\u5c31\u4f1a\u5927\u5927\u964d\u4f4e [ back to top ] \u5047\u8bbe\u6211\u4eec\u7ec4\u5408\u4e86 n \u4e2a\u5206\u7c7b\u5668\uff0c\u5b83\u4eec\u7684\u9519\u8bef\u7387\u90fd\u4e3a $$ \\varepsilon $$, \u5404\u4e2a\u5206\u7c7b\u5668\u4e4b\u95f4\u72ec\u7acb\u3002 \u5219\u8fd9 n \u4e2a\u5206\u7c7b\u5668\u91cc, \u591a\u4e8e k \u4e2a\u5206\u7c7b\u5668\u5206\u7c7b\u9519\u8bef\u7684\u6982\u7387\u4e3a $$ P(y \\geq k) = \\sum^n_k \\binom{n}{k} \\varepsilon^k (1-\\varepsilon)^{n-k}$$ from scipy.misc import comb import math # emsemble error rate def ensemble_error(n_classifier, error): k_start = int(math.ceil(n_classifier / 2.0)) probs = [comb(n_classifier, k) * error**k * (1-error)**(n_classifier - k) for k in range(k_start, n_classifier + 1)] return sum(probs) # 11\u4e2a\u5206\u7c7b\u5668, \u6bcf\u4e2a\u5206\u7c7b\u5668\u7684 error rate \u662f0.25\u7684\u8bdd, \u901a\u8fc7 combinator \u4e4b\u540e\u7684 error rate ensemble_error(n_classifier=11, error=0.25) 0.034327507019042969 # ensemble error \u548c base error \u7684\u5173\u7cfb error_range = np.arange(0.0, 1.01, 0.01) ens_errors = [ensemble_error(n_classifier=11, error=error) for error in error_range] plt.plot(error_range, ens_errors, label='Ensemble error', linewidth=2) plt.plot(error_range, error_range, linestyle='--', label='Base error',linewidth=2) plt.xlabel('Base error') plt.ylabel('Base/Ensemble error') plt.legend(loc='upper left') plt.grid() plt.tight_layout() # plt.savefig('./figures/ensemble_err.png', dpi=300) $$\\varepsilon$$ 0.5 \u65f6, emsemble error \u90fd\u8981\u4f4e\u4e8e base error, $$\\varepsilon$$ 0.5 \u65f6, emsemble error \u5c31\u4f1a\u5927\u4e8e base error","title":"Learning with ensembles"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#majority-vote-classifier","text":"combine different classication algorithms associated with individual weights for confidence [ back to top ] \u5f53\u591a\u4e2a\u5206\u7c7b\u5668 C \u62e5\u6709\u76f8\u540c\u6743\u91cd\u65f6\uff0censemble \u7ed9\u51fa\u7684\u9884\u6d4b $$\\hat y$$ \u4e3a\u4f17\u6570\uff1a $$ \\hat y = mode{C_1(x), C_2(x), \\dotso, C_m(x)} $$ \u82e5\u5206\u7c7b\u5668 $$C_j$$ \u5bf9\u5e94\u4e0d\u540c\u7684\u6743\u91cd $$w_j$$, \u5219 $$ \\hat y = \\arg\\max_i \\sum^m_{j=1}w_j P_{ij} $$ \u5176\u4e2d $$P_{ij}$$ \u662f $$C_j$$ \u9884\u6d4b\u7ed3\u679c\u4e3a i \u7684\u6982\u7387 import numpy as np np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6])) # np.argmax: returns the indices of the maximum values along an axis. # np.bincount: Count number of occurrences of each value in array of non-negative ints 1 np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6]) array([ 0.4, 0.6]) ex = np.array([[0.9, 0.1], # C1 \u7684\u9884\u6d4b\u7ed3\u679c [0.8, 0.2], # C2 \u7684\u9884\u6d4b\u7ed3\u679c [0.4, 0.6]]) # C3 \u7684\u9884\u6d4b\u7ed3\u679c p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6]) # C1, C2, C3 \u7684\u6743\u91cd p array([ 0.58, 0.42]) $$p(i_0| x) = 0.58$$ $$p(i_1 | x) = 0.42$$ $$\\hat y = \\arg\\max_i [p(i_0 | x), p(i_1 | x)] = 0$$ np.argmax(p) 0","title":"Majority vote classifier"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#votingclassifier-in-sklearn","text":"\u4f7f\u7528 Sklearn \u4e2d\u81ea\u5e26\u7684 VotingClassifier [ back to top ] import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import VotingClassifier # \u4f7f\u7528 3 \u4e2a\u5206\u7c7b\u5668 clf1 = LogisticRegression(random_state=1) clf2 = RandomForestClassifier(random_state=1) clf3 = GaussianNB() # \u751f\u6210\u6570\u636e X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y = np.array([1, 1, 1, 2, 2, 2]) # voting='hard', uses predicted class labels for majority rule voting. eclf1 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard') eclf1 = eclf1.fit(X, y) print(eclf1.predict(X)) # voting='soft', predicts the class label based on the argmax of the sums of the predicted probalities eclf2 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft') eclf2 = eclf2.fit(X, y) print(eclf2.predict(X)) # add weight eclf3 = VotingClassifier(estimators=[ ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft', weights=[2,1,1]) eclf3 = eclf3.fit(X, y) print(eclf3.predict(X)) [1 1 1 2 2 2] [1 1 1 2 2 2] [1 1 1 2 2 2]","title":"VotingClassifier in Sklearn"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#combining-different-algorithms-for-classification-with-majority-vote","text":"[ back to top ] from sklearn import datasets from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder # load iris data iris = datasets.load_iris() X, y = iris.data[50:, [1, 2]], iris.target[50:] st = StandardScaler() X = st.fit_transform(X) le = LabelEncoder() y = le.fit_transform(y) # 50% train, 50% test X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.5, random_state=1) from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.cross_validation import cross_val_score clf1 = LogisticRegression(C=0.01, random_state=42) clf2 = KNeighborsClassifier(n_neighbors=1) clf3 = DecisionTreeClassifier(max_depth=1, random_state=42) clf_labels = ['Logistic Regression', 'KNN', 'Decision Tree'] all_clf = [clf1, clf2, clf3] print('10-fold cross validation:\\n') for clf, label in zip(all_clf, clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc') print( ROC AUC: %0.2f (+/- %0.2f) [%s] % (scores.mean(), scores.std(), label)) 10-fold cross validation: ROC AUC: 0.93 (+/- 0.15) [Logistic Regression] ROC AUC: 0.93 (+/- 0.10) [KNN] ROC AUC: 0.92 (+/- 0.15) [Decision Tree] from sklearn.ensemble import VotingClassifier mv_clf = VotingClassifier( estimators=[('c1', clf1), ('c2', clf2), ('c3', clf3)], voting='soft') clf_labels += ['Majority Voting'] all_clf += [mv_clf] print('10-fold cross validation:\\n') for clf, label in zip(all_clf, clf_labels): scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc') print( ROC AUC: %0.2f (+/- %0.2f) [%s] % (scores.mean(), scores.std(), label)) 10-fold cross validation: ROC AUC: 0.93 (+/- 0.15) [Logistic Regression] ROC AUC: 0.93 (+/- 0.10) [KNN] ROC AUC: 0.92 (+/- 0.15) [Decision Tree] ROC AUC: 0.97 (+/- 0.10) [Majority Voting] \u6700\u540e\u4e00\u4e2a\u662f majority voting, \u660e\u663e\u6bd4\u5355\u72ec\u7684\u5206\u7c7b\u5668\u7ed3\u679c\u597d","title":"Combining different algorithms for classification with majority vote"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#evaluating-the-ensemble-classifier","text":"[ back to top ] \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u5404\u4e2a\u5206\u7c7b\u5668\u7684 ROC AUC from sklearn.metrics import roc_curve from sklearn.metrics import auc colors = ['black', 'orange', 'blue', 'green'] linestyles = [':', '--', '-.', '-'] for clf, label, clr, ls \\ in zip(all_clf, clf_labels, colors, linestyles): # assuming the label of the positive class is 1 y_pred = clf.fit(X_train, y_train).predict_proba(X_test)[:, 1] fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred) roc_auc = auc(x=fpr, y=tpr) plt.plot(fpr, tpr, color=clr, linestyle=ls, label='%s (auc = %0.2f)' % (label, roc_auc)) plt.legend(loc='lower right') plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=2) plt.xlim([-0.1, 1.1]) plt.ylim([-0.1, 1.1]) plt.grid() plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.tight_layout() # plt.savefig('./figures/roc.png', dpi=300) ROC \u53ef\u770b\u51fa, ensemble classfifier \u5728 test set \u4e0a\u8868\u73b0\u4e0d\u9519 \u5bf9\u6bd4\u51b3\u7b56\u8fb9\u754c from itertools import product x_min = X_train[:, 0].min() - 1 x_max = X_train[:, 0].max() + 1 y_min = X_train[:, 1].min() - 1 y_max = X_train[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) # \u6dfb\u52a0 subplots f, axarr = plt.subplots(nrows=2, ncols=2, sharex='col', sharey='row', figsize=(7, 5)) for idx, clf, tt in zip(product([0, 1], [0, 1]), all_clf, clf_labels): clf.fit(X_train, y_train) Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T) Z = Z.reshape(xx.shape) axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.3) axarr[idx[0], idx[1]].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', marker='^', s=50) axarr[idx[0], idx[1]].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', marker='o', s=50) axarr[idx[0], idx[1]].set_title(tt) plt.text(-3.5, -4.5, s='Sepal width [standardized]', ha='center', va='center', fontsize=12) plt.text(-10.5, 4.5, s='Petal length [standardized]', ha='center', va='center', fontsize=12, rotation=90) plt.tight_layout() # plt.savefig('./figures/voting_panel', bbox_inches='tight', dpi=300)","title":"Evaluating the ensemble classifier"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#bagging-building-an-ensemble-of-classifiers-from-bootstrap-samples","text":"draw bootstrap samples (random samples with replacement) from initial training set random forests are a special case of bagging where we also use random feature subsets to fit the individual decision trees [ back to top ] import pandas as pd # wine dataset df_wine = pd.read_csv('ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine/wine.data', header=None) df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'] # only consider Wine classes 2 and 3 df_wine = df_wine[df_wine['Class label'] != 1] y = df_wine['Class label'].values X = df_wine[['Alcohol', 'Hue']].values from sklearn.preprocessing import LabelEncoder from sklearn.cross_validation import train_test_split # \u8f6c\u6362 label le = LabelEncoder() y = le.fit_transform(y) # 60% train, 40% test X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.40, random_state=1) # sklearn \u63d0\u4f9b\u7684 BaggingClassifier\uff0c \u5176\u5b9e\u529f\u80fd\u5df2\u7ecf\u8d85\u8fc7 Bagging \u4e86 # \u5b83\u65e2\u80fd\u5bf9 samples \u91c7\u6837\uff0c\u4e5f\u80fd\u5bf9 features \u91c7\u6837 from sklearn.ensemble import BaggingClassifier from sklearn.tree import DecisionTreeClassifier tree = DecisionTreeClassifier(criterion='entropy') # \u7528 Decision Tree \u4f5c base bag = BaggingClassifier(base_estimator=tree, n_estimators=500, max_samples=1.0, # \u5b50\u91c7\u6837 samples \u7684\u6bd4\u4f8b max_features=1.0, # \u5b50\u91c7\u6837 features \u7684\u6bd4\u4f8b bootstrap=True, # \u91c7\u6837 samples \u65f6\u662f\u5426\u4f7f\u7528 bootstrap bootstrap_features=False, # \u91c7\u6837 features \u65f6\u662f\u5426\u4f7f\u7528 bootstrap random_state=1) from sklearn.metrics import accuracy_score tree = tree.fit(X_train, y_train) y_train_pred = tree.predict(X_train) y_test_pred = tree.predict(X_test) tree_train = accuracy_score(y_train, y_train_pred) tree_test = accuracy_score(y_test, y_test_pred) print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train, tree_test)) bag = bag.fit(X_train, y_train) y_train_pred = bag.predict(X_train) y_test_pred = bag.predict(X_test) bag_train = accuracy_score(y_train, y_train_pred) bag_test = accuracy_score(y_test, y_test_pred) print('Bagging train/test accuracies %.3f/%.3f' % (bag_train, bag_test)) Decision tree train/test accuracies 1.000/0.854 Bagging train/test accuracies 1.000/0.896 \u4f7f\u7528 Bagging \u4e4b\u540e\uff0c\u6d4b\u8bd5\u96c6\u7684\u51c6\u786e\u7387\u6709\u63d0\u5347 # \u753b\u51b3\u7b56\u8fb9\u754c %matplotlib inline import numpy as np import matplotlib.pyplot as plt x_min = X_train[:, 0].min() - 1 x_max = X_train[:, 0].max() + 1 y_min = X_train[:, 1].min() - 1 y_max = X_train[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) f, axarr = plt.subplots(nrows=1, ncols=2, sharex='col', sharey='row', figsize=(8, 3)) for idx, clf, tt in zip([0, 1], [tree, bag], ['Decision Tree', 'Bagging']): clf.fit(X_train, y_train) Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.3) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', marker='^') axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', marker='o') axarr[idx].set_title(tt) axarr[0].set_ylabel('Alcohol', fontsize=12) plt.text(10.2, -1.2, s='Hue', ha='center', va='center', fontsize=12) plt.tight_layout() #plt.savefig('./figures/bagging_region.png', # dpi=300, # bbox_inches='tight') Bagging \u51cf\u5c11\u4e86 overfitting\uff0c \u4f7f\u51b3\u7b56\u8fb9\u754c\u66f4\u5e73\u6ed1","title":"Bagging -- Building an ensemble of classifiers from bootstrap samples"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#leveraging-of-weak-learners-via-adaptive-boosting","text":"let the weak learners subsequently learn from misclassified training samples to improve the performance of the ensemble [ back to top ] Adaptive boosting (AdaBoost) 1. Set weight vector w to uniform weights where $$\\sum_i w_i = 1$$ 2. For j in m boosting rounds, do the following: 3. Train a weighted weak learner: $$C_j = train(X,y,w)$$. 4. Predict class labels: $$\\hat{y} = predict(C_j, X)$$ . 5. Compute weighted error rate: $$\\epsilon = w \\cdot (\\hat{y} \\neq y)$$. 6. Compute coefficient: $$\\alpha_j = \\frac 1 2 ln\\frac{1-\\epsilon}{\\epsilon}$$ . 7. Update weights: $$w:= w \\times exp(-\\alpha_j \\times \\hat{y} \\times y)$$ . 8. Normalize weights to sum to 1: $$w:= \\frac{w}{\\sum_i w_i}$$ . 9. Compute final prediction: $$\\hat{y} = (\\sum_{j=1}^{m}(\\alpha_j \\times predict(C_j, X)) 0)$$. $$\\cdot$$ denotes dot product between two vectors $$\\times$$ denotes element-wise multiplication of two vectors from sklearn.ensemble import AdaBoostClassifier tree = DecisionTreeClassifier(criterion='entropy', max_depth=1) ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=0) tree = tree.fit(X_train, y_train) y_train_pred = tree.predict(X_train) y_test_pred = tree.predict(X_test) tree_train = accuracy_score(y_train, y_train_pred) tree_test = accuracy_score(y_test, y_test_pred) print('Decision tree train/test accuracies %.3f/%.3f' % (tree_train, tree_test)) ada = ada.fit(X_train, y_train) y_train_pred = ada.predict(X_train) y_test_pred = ada.predict(X_test) ada_train = accuracy_score(y_train, y_train_pred) ada_test = accuracy_score(y_test, y_test_pred) print('AdaBoost train/test accuracies %.3f/%.3f' % (ada_train, ada_test)) Decision tree train/test accuracies 0.845/0.854 AdaBoost train/test accuracies 1.000/0.875 Adaboost \u53ef\u4ee5\u51cf\u5c11 Bias\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u66f4\u591a\u7684 Variance x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1 y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(8, 3)) for idx, clf, tt in zip([0, 1], [tree, ada], ['Decision Tree', 'AdaBoost']): clf.fit(X_train, y_train) Z = clf.predict(np.vstack([xx.ravel(), yy.ravel()]).T) Z = Z.reshape(xx.shape) axarr[idx].contourf(xx, yy, Z, alpha=0.3) axarr[idx].scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], c='blue', marker='^') axarr[idx].scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], c='red', marker='o') axarr[idx].set_title(tt) axarr[0].set_ylabel('Alcohol', fontsize=12) plt.text(10.2, -1.2, s='Hue', ha='center', va='center', fontsize=12) plt.tight_layout() #plt.savefig('./figures/adaboost_region.png', # dpi=300, # bbox_inches='tight') Adaboost \u7684\u51b3\u7b56\u8fb9\u754c\u6bd4 tree \u590d\u6742, \u4e0e BaggingClassifier \u76f8\u4f3c. Ensemble method \u9700\u8981\u66f4\u591a\u7684\u8ba1\u7b97\u8d44\u6e90, \u8fd9\u4e2a\u5728\u5b9e\u9645\u8fd0\u7528\u4e2d\u4e5f\u662f\u8981\u8003\u8651\u7684.","title":"Leveraging of weak learners via adaptive boosting"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#algorithm-implementation","text":"[ back to top ]","title":"Algorithm implementation"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#_1","text":"import numpy import matplotlib.pyplot as plot %matplotlib inline from sklearn import tree from sklearn.tree import DecisionTreeRegressor from math import floor import random # Build a simple data set with y = x + random n_points = 1000 # x values for plotting x_plot = [(float(i) / float(n_points) - 0.5) for i in range(n_points + 1)] # x needs to be list of lists. x = [[s] for s in x_plot] # y (labels) has random noise added to x-value # set seed numpy.random.seed(1) y = [s + numpy.random.normal(scale=0.1) for s in x_plot] # take fixed test set 30% of sample n_sample = int(n_points * 0.30) idx_test = random.sample(range(n_points), n_sample) idx_test.sort() idx_train = [idx for idx in range(n_points) if not (idx in idx_test)] # Define test and training attribute and label sets x_train = [x[r] for r in idx_train] x_test = [x[r] for r in idx_test] y_train = [y[r] for r in idx_train] y_test = [y[r] for r in idx_test] # train a series of models on random subsets of the training data # collect the models in a list and check error of composite as list grows # maximum number of models to generate num_trees_max = 30 # tree depth - typically at the high end tree_depth = 5 # initialize a list to hold models mode_list = [] pred_list = [] eps = 0.3 # initialize residuals to be the labels y residuals = list(y_train) for i_trees in range(num_trees_max): mode_list.append(DecisionTreeRegressor(max_depth=tree_depth)) mode_list[-1].fit(x_train, residuals) # make prediction with latest model and add to list of predictions latest_in_sample_prediction = mode_list[-1].predict(x_train) # use new predictions to update residuals residuals = [residuals[i] - eps * latest_in_sample_prediction[i] for i in range(len(residuals))] latest_out_sample_prediction = mode_list[-1].predict(x_test) pred_list.append(list(latest_out_sample_prediction)) # build cumulative prediction from first n models mse = [] all_predictions = [] for i_models in range(len(mode_list)): # add the first i_models of the predictions and multiply by eps prediction = [] for i_pred in range(len(x_test)): prediction.append( sum([pred_list[i][i_pred] for i in range(i_models + 1)]) * eps) all_predictions.append(prediction) errors = [(y_test[i] - prediction[i]) for i in range(len(y_test))] mse.append(sum([e * e for e in errors]) / len(y_test)) n_models = [i + 1 for i in range(len(mode_list))] plot.plot(n_models, mse) plot.axis('tight') plot.xlabel('Number of Models in Ensemble') plot.ylabel('Mean Squared Error') plot.ylim((0.0, max(mse))) plot.show() plot_list = [0, 14, 29] line_type = [':', '-.', '--'] plot.figure() for i in range(len(plot_list)): i_plot = plot_list[i] text_legend = 'Prediction with ' + str(i_plot) + ' Trees' plot.plot(x_test, all_predictions[i_plot], label=text_legend, linestyle=line_type[i]) plot.plot(x_test, y_test, label='True y Value', alpha=0.25) plot.legend(bbox_to_anchor=(1, 0.3)) plot.axis('tight') plot.xlabel('x value') plot.ylabel('Predictions');","title":"\u5e7f\u4e49\u63d0\u5347\u6811\u7b97\u6cd5\u8be6\u89e3"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#_2","text":"import urllib2 import numpy from sklearn import tree from sklearn.tree import DecisionTreeRegressor import random from math import sqrt import matplotlib.pyplot as plot # read data into iterable target_url = ftp://ftp.ics.uci.edu/pub/machine-learning-databases/wine-quality/winequality-red.csv data = urllib2.urlopen(target_url) x_list = [] labels = [] names = [] first_line = True for line in data: if first_line: names = line.strip().split( ; ) first_line = False else: # split on semi-colon row = line.strip().split( ; ) # put labels in separate array labels.append(float(row[-1])) # remove label from row row.pop() # convert row to floats float_row = [float(num) for num in row] x_list.append(float_row) nrows = len(x_list) ncols = len(x_list[0]) # take fixed test set 30% of sample random.seed(1) # set seed so results are the same each run n_sample = int(nrows * 0.30) idx_test = random.sample(range(nrows), n_sample) idx_test.sort() idx_train = [idx for idx in range(nrows) if not (idx in idx_test)] # Define test and training attribute and label sets x_train = [x_list[r] for r in idx_train] x_test = [x_list[r] for r in idx_test] y_train = [labels[r] for r in idx_train] y_test = [labels[r] for r in idx_test] # train a series of models on random subsets of the training data # collect the models in a list and check error of composite as list grows # maximum number of models to generate num_trees_max = 30 # tree depth - typically at the high end tree_depth = 12 # pick how many attributes will be used in each model. # authors recommend 1/3 for regression problem n_attr = 4 # initialize a list to hold models mode_list = [] index_list = [] pred_list = [] n_train_rows = len(y_train) for i_trees in range(num_trees_max): mode_list.append(DecisionTreeRegressor(max_depth=tree_depth)) # take random sample of attributes idx_attr = random.sample(range(ncols), n_attr) idx_attr.sort() index_list.append(idx_attr) # take a random sample of training rows idx_rows = [] for i in range(int(0.5 * n_train_rows)): idx_rows.append(random.choice(range(len(x_train)))) idx_rows.sort() # build training set x_rf_train = [] y_rf_train = [] for i in range(len(idx_rows)): temp = [x_train[idx_rows[i]][j] for j in idx_attr] x_rf_train.append(temp) y_rf_train.append(y_train[idx_rows[i]]) mode_list[-1].fit(x_rf_train, y_rf_train) # restrict xTest to attributes selected for training x_rf_test = [] for xx in x_test: temp = [xx[i] for i in idx_attr] x_rf_test.append(temp) latest_out_sample_prediction = mode_list[-1].predict(x_rf_test) pred_list.append(list(latest_out_sample_prediction)) # build cumulative prediction from first n models mse = [] all_predictions = [] for i_models in range(len(mode_list)): # add the first iModels of the predictions and multiply by eps prediction = [] for i_pred in range(len(x_test)): prediction.append( sum([pred_list[i][i_pred] for i in range(i_models + 1)]) / ( i_models + 1)) all_predictions.append(prediction) errors = [(y_test[i] - prediction[i]) for i in range(len(y_test))] mse.append(sum([e * e for e in errors]) / len(y_test)) n_models = [i + 1 for i in range(len(mode_list))] plot.plot(n_models, mse) plot.axis('tight') plot.xlabel('Number of Trees in Ensemble') plot.ylabel('Mean Squared Error') plot.ylim((0.0, max(mse))) plot.show() print('Minimum MSE') print(min(mse)) Minimum MSE 0.389088116065","title":"\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u8be6\u89e3"},{"location":"w3-decision-tree-and-ensemble-learning/3w/#_3","text":"","title":"\u7ec3\u4e60\u9898\uff1a \u4f7f\u7528\u63d0\u5347\u6811\u548c\u968f\u673a\u68ee\u6797\u4e24\u79cd\u65b9\u6cd5\uff0c\u5bf9\u4fe1\u8d37\u6570\u636e\u96c6\u8fdb\u884c\u5efa\u6a21\u6bd4\u8f83"},{"location":"w4-feature-engineering/4w/","text":"Sections What is Feature Engineering? Data preprocessing Dealing with missing data Eliminating samples or features with missing values Imputing missing values Handling categorical data Mapping ordinal features Encoding class labels Performing one-hot encoding on nominal features Partitioning a dataset in training and test sets Bringing features onto the same scale Feature selection Univariate statistics Recursive feature elimination Feature selection using SelectFromModel L1-based feature selection Tree-based feature selection Feature extraction Unsupervised dimensionality reduction via principal component analysis Total and explained variance Feature transformation Principal component analysis in scikit-learn Supervised data compression via linear discriminant analysis Computing the scatter matrices Selecting linear discriminants for the new feature subspace Projecting samples onto the new feature space LDA via scikit-learn Using kernel principal component analysis for nonlinear mappings Implementing a kernel principal component analysis in Python Example 1: Separating half-moon shapes Example 2: Separating concentric circles Projecting new data points Kernel principal component analysis in scikit-learn Using regularization Ridge Regression LASSO Regression Logistic regression with regularization What is Feature Engineering? [ back to top ] Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data. Sub-Problems of Feature Engineering Feature Importance: An estimate of the usefulness of a feature Feature Selection: From many features to a few that are useful Feature Extraction: The automatic construction of new features from raw data Feature Construction: The manual construction of new features from raw data Iterative Process of Feature Engineering Brainstorm features: Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal. Devise features: Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two. Select features: Use different feature importance scorings and feature selection methods to prepare one or more \u201cviews\u201d for your models to operate upon. Evaluate models: Estimate model accuracy on unseen data using the chosen features. General Examples of Feature Engineering Decompose Categorical Attributes Imagine you have a categorical attribute, like \u201cItem_Color\u201d that can be Red, Blue or Unknown. Decompose a Date-Time A date-time contains a lot of information that can be difficult for a model to take advantage of in it\u2019s native form, such as ISO 8601 (i.e. 2014-09-20T20:45:40Z). Reframe Numerical Quantities Your data is very likely to contain quantities, which can be reframed to better expose relevant structures. This may be a transform into a new unit or the decomposition of a rate into time and amount components. Data preprocessing Dealing with missing data [ back to top ] # \u6784\u9020\u542b\u7f3a\u5931\u503c\u7684\u6570\u636e, NaN \u8868\u793a Not a Number import numpy as np import pandas as pd df = pd.DataFrame(np.arange(1, 13).reshape(3, 4), columns=['A', 'B', 'C', 'D']) df.loc[1, 'C'] = None df.loc[2, 'D'] = None df A B C D 0 1 2 3.0 4.0 1 5 6 NaN 8.0 2 9 10 11.0 NaN # isnull \u4f1a\u8fd4\u56de\u4e00\u4e2a DataFrame, \u91cc\u9762\u7684 bool \u503c\u8868\u793a\u539f\u59cb\u6570\u636e\u662f\u5426\u7f3a\u5931 df.isnull() A B C D 0 False False False False 1 False False True False 2 False False False True # \u7ed3\u679c\u663e\u793a A \u548c B \u5217\u6ca1\u6709\u7f3a\u5931\u503c, C \u548c D \u5404\u6709\u4e00\u4e2a\u7f3a\u5931\u503c df.isnull().sum() A 0 B 0 C 1 D 1 dtype: int64 Eliminating samples or features with missing values \u5904\u7406\u7f3a\u5931\u503c\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u5c31\u662f\u5220\u6389\u6709\u7f3a\u5931\u7684\u884c\u6216\u8005\u5217 [ back to top ] df.dropna() # \u9ed8\u8ba4\u5220\u9664\u884c axis = 0 A B C D 0 1 2 3.0 4.0 df.dropna(axis=1) # \u5220\u9664\u5217 A B 0 1 2 1 5 6 2 9 10 # \u53ea\u5220\u9664\u5168\u662f\u7f3a\u5931\u503c\u7684\u884c df.dropna(how='all') A B C D 0 1 2 3.0 4.0 1 5 6 NaN 8.0 2 9 10 11.0 NaN # \u5220\u9664\u975e\u7f3a\u5931\u503c\u5c11\u4e8e thresh \u7684\u884c df.dropna(thresh=4) A B C D 0 1 2 3.0 4.0 # \u5220\u9664\u6709\u7f3a\u5931\u503c\u51fa\u73b0\u5728\u7279\u5b9a\u5217\u7684\u884c df.dropna(subset=['C']) A B C D 0 1 2 3.0 4.0 2 9 10 11.0 NaN \u770b\u4e0a\u53bb\u5220\u9664\u662f\u5f88\u7b80\u4fbf\u7684\u5904\u7406\u65b9\u6cd5, \u4f46\u5b9e\u9645\u4e0a\u76f4\u63a5\u5220\u9664\u53ef\u80fd\u4f1a\u4e22\u5931\u4e0d\u5c11\u4fe1\u606f, \u66f4\u597d\u7684\u9009\u62e9\u662f\u586b\u8865\u7f3a\u5931\u503c Imputing missing values \u4f30\u8ba1\u7f3a\u5931\u503c\u5e76\u586b\u5145, \u6700\u666e\u904d\u7684\u662f mean imputation, \u4e5f\u5c31\u662f\u7528\u5e73\u5747\u503c\u586b\u5145 [ back to top ] from sklearn.preprocessing import Imputer imr = Imputer(missing_values='NaN', strategy='mean', axis=0) # If axis=0, then impute along columns. # If axis=1, then impute along rows. imr = imr.fit(df.values) imputed_data = imr.transform(df.values) imputed_data array([[ 1., 2., 3., 4.], [ 5., 6., 7., 8.], [ 9., 10., 11., 6.]]) df.values # \u5e76\u6ca1\u6709\u6539\u53d8\u539f\u5148\u7684 df array([[ 1., 2., 3., 4.], [ 5., 6., nan, 8.], [ 9., 10., 11., nan]]) Handling categorical data \u5bf9 categorical \u9700\u8981\u533a\u5206 nominal \u548c ordinal \u4e24\u79cd\u7c7b\u578b, nominal \u662f\u65e0\u5e8f\u7684, \u800c ordinal \u662f\u6709\u5e8f\u7684 [ back to top ] import pandas as pd df = pd.DataFrame([ ['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']]) df.columns = ['color', 'size', 'price', 'classlabel'] df color size price classlabel 0 green M 10.1 class1 1 red L 13.5 class2 2 blue XL 15.3 class1 color : nominal feature size : ordinal feature, XL L M price : numerical feature Mapping ordinal features convert the categorical string values into integers [ back to top ] # define the mapping manually size_mapping = { 'XL': 3, 'L': 2, 'M': 1} df['size'] = df['size'].map(size_mapping) df color size price classlabel 0 green 1 10.1 class1 1 red 2 13.5 class2 2 blue 3 15.3 class1 # transform the integer values back to the original string inv_size_mapping = {v: k for k, v in size_mapping.items()} df['size'].map(inv_size_mapping) 0 M 1 L 2 XL Name: size, dtype: object Encoding class labels \u5bf9\u5e94 nominal \u7684 class labels, \u4e5f\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3a\u6570\u503c\u8868\u5f81\uff0c\u8bb0\u4f4f\u6b64\u65f6\u7684\u6570\u503c\u53ea\u4ee3\u8868\u4e00\u4e2a\u7c7b\u522b\uff0c\u5e76\u4e0d\u8868\u5f81\u6570\u503c\u5173\u7cfb [ back to top ] import numpy as np class_mapping = {label:idx for idx,label in enumerate(np.unique(df['classlabel']))} class_mapping {'class1': 0, 'class2': 1} # \u6700\u7ec8\u628a classlabel \u4e5f\u8f6c\u5316\u4e3a interger df['classlabel'] = df['classlabel'].map(class_mapping) df color size price classlabel 0 green 1 10.1 0 1 red 2 13.5 1 2 blue 3 15.3 0 # \u8f6c\u5316\u56de\u6765\u4e5f\u662f ok \u7684 inv_class_mapping = {v: k for k, v in class_mapping.items()} df['classlabel'] = df['classlabel'].map(inv_class_mapping) df color size price classlabel 0 green 1 10.1 class1 1 red 2 13.5 class2 2 blue 3 15.3 class1 # sklearn \u4e2d\u4e5f\u6709\u76f8\u5e94\u51fd\u6570 from sklearn.preprocessing import LabelEncoder class_le = LabelEncoder() y = class_le.fit_transform(df['classlabel'].values) y array([0, 1, 0]) # \u540c\u6837\u4e5f\u53ef\u4ee5\u53cd\u5411\u8f6c\u6362 class_le.inverse_transform(y) array(['class1', 'class2', 'class1'], dtype=object) Performing one-hot encoding on nominal features [ back to top ] X = df[['color', 'size', 'price']].values # color column color_le = LabelEncoder() X[:, 0] = color_le.fit_transform(X[:, 0]) X #blue 0 #green 1 #red 2 array([[1, 1, 10.1], [2, 2, 13.5], [0, 3, 15.3]], dtype=object) \u867d\u7136 color \u8f6c\u5316\u4e3a\u4e86 0, 1, 2, \u4f46\u5e76\u4e0d\u80fd\u76f4\u63a5\u4f7f\u7528\u6765\u5efa\u6a21, \u56e0\u4e3a\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d, \u4f1a\u8ba4\u4e3a 2 \u5927\u4e8e 1, \u4e5f\u5c31\u662f red \u5927\u4e8e green. \u5b9e\u9645\u5374\u4e0d\u662f\u8fd9\u6837\u7684, \u6240\u4ee5\u9700\u8981\u7528\u5230 one-hot encoding, \u9700\u8981\u4f7f\u7528 dummy variable, \u6bcf\u4e00\u4e2a label \u6700\u540e\u88ab\u8868\u793a\u4e3a\u4e00\u4e2a\u5411\u91cf. \u4f8b\u5982, blue sample can be encoded as blue=1, green=0, red=0. from sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder(categorical_features=[0], sparse=False) # \u4e0d\u8bbe\u5b9a sparse=False \u7684\u8bdd\uff0conehot \u4f1a\u8fd4\u56de\u4e00\u4e2a sparse matrix\uff0c \u53ef\u4ee5\u7528 toarray() \u5c06\u4e4b\u53d8\u56de dense ohe.fit_transform(X) # \u524d\u4e09\u5217\u4e3adummy array([[ 0. , 1. , 0. , 1. , 10.1], [ 0. , 0. , 1. , 2. , 13.5], [ 1. , 0. , 0. , 3. , 15.3]]) # pandas \u4e2d\u7684 get_dummies \u51fd\u6570\u662f\u751f\u6210 dummy variable \u66f4\u7b80\u5355\u7684\u65b9\u6cd5 pd.get_dummies(df[['price', 'color', 'size']]) price size color_blue color_green color_red 0 10.1 1 0.0 1.0 0.0 1 13.5 2 0.0 0.0 1.0 2 15.3 3 1.0 0.0 0.0 Partitioning a dataset in training and test sets the test set can be understood as the ultimate test of our model before we let it loose on the real world [ back to top ] # \u8bfb\u53d6wine\u6570\u636e df_wine = pd.read_csv('data/wine.data', header=None) df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'] print('Class labels', np.unique(df_wine['Class label'])) df_wine.head() # \u4e00\u5171\u6709\u4e09\u79cd label ('Class labels', array([1, 2, 3])) Class label Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline 0 1 14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 1 1 13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 2 1 13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 3 1 14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 4 1 13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 \u4f7f\u7528 train_test_split \u51fd\u6570\u8fdb\u884c\u8bad\u7ec3/\u6d4b\u8bd5\u96c6\u5207\u5206 from sklearn.cross_validation import train_test_split X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.3, random_state=0) # 30%\u662f test data stratified train test split stratified \u5207\u5206\uff0c \u4f7f\u5207\u5206\u540e\u7684\u6570\u636e\u96c6\u66f4\u597d\u5730\u4fdd\u7559\u6807\u7b7e\u7684\u76f8\u5bf9\u6bd4\u4f8b # \u5e2e\u52a9\u51fd\u6570\uff0c\u8ba1\u7b97\u5404\u6807\u7b7e\u6bd4\u4f8b def label_frequency(labels): counts = np.unique(labels, return_counts=True)[1] n = len(labels) return counts / float(n) # \u539f\u59cb\u6570\u636e\u4e2d\u5404\u6807\u7b7e\u7684\u6bd4\u4f8b label_frequency(y) array([ 0.33146067, 0.3988764 , 0.26966292]) # train_test_split \u540e\u7684\u6bd4\u4f8b label_frequency(y_train), label_frequency(y_test) (array([ 0.32258065, 0.39516129, 0.28225806]), array([ 0.35185185, 0.40740741, 0.24074074])) # stratified \u4e4b\u540e\u7684\u6807\u7b7e\u6bd4\u4f8b\uff0c \u66f4\u63a5\u8fd1\u539f\u59cb\u6bd4\u4f8b X_train, X_test, y_train, y_test = \\ train_test_split(X, y, stratify=y, test_size=0.3, random_state=0) label_frequency(y_train), label_frequency(y_test) (array([ 0.33333333, 0.39837398, 0.26829268]), array([ 0.32727273, 0.4 , 0.27272727])) Bringing features onto the same scale Feature Scaling \u5f88\u5bb9\u6613\u88ab\u9057\u5fd8, \u867d\u7136\u5728 Decision tree\u548c random forests \u65f6\u4e0d\u7528\u62c5\u5fc3\u8fd9\u4e2a\u95ee\u9898. \u4f46\u5728\u5f88\u591a\u7b97\u6cd5\u548c\u6a21\u578b\u4e0b\u90fd\u662f scaling \u540e\u62df\u5408\u6548\u679c\u66f4\u597d. \u4e24\u7c7b\u5e38\u7528\u65b9\u6cd5: normalization \u548c standardization. - normalization: rescaling to [0,1], \u5982 min-max scaling $$ x_{norm}^{(i)} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}$$ - standardization: more practical, \u56e0\u4e3a\u5728\u4e00\u4e9b\u7b97\u6cd5\u4e2d, weights \u521d\u59cb\u503c\u90fd\u8bbe\u7f6e\u4e3a 0, \u6216\u8005\u63a5\u8fd1 0. standardization \u4e4b\u540e\u4f1a\u66f4\u5229\u7528\u66f4\u65b0 weights. \u5e76\u4e14 standardize \u5bf9 outlier \u66f4\u4e0d\u654f\u611f\uff0c\u53d7\u5f71\u54cd\u66f4\u5c0f $$ x_{std}^{(i)} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}$$ [ back to top ] # min-max rescaling from sklearn.preprocessing import MinMaxScaler mms = MinMaxScaler() X_train_norm = mms.fit_transform(X_train) X_test_norm = mms.transform(X_test) # \u6ce8\u610f\u6d4b\u8bd5\u96c6\u662f\u6309\u7167\u8bad\u7ec3\u96c6\u7684\u53c2\u6570\u8fdb\u884c\u8f6c\u6362 # standarzation from sklearn.preprocessing import StandardScaler stdsc = StandardScaler() X_train_std = stdsc.fit_transform(X_train) X_test_std = stdsc.transform(X_test) A visual example: ex = pd.DataFrame([0, 1, 2 ,3, 4, 5]) # standardize ex[1] = (ex[0] - ex[0].mean()) / ex[0].std() # normalize ex[2] = (ex[0] - ex[0].min()) / (ex[0].max() - ex[0].min()) ex.columns = ['input', 'standardized', 'normalized'] ex input standardized normalized 0 0 -1.336306 0.0 1 1 -0.801784 0.2 2 2 -0.267261 0.4 3 3 0.267261 0.6 4 4 0.801784 0.8 5 5 1.336306 1.0 Feature selection Often we collected many features that might be related to a supervised prediction task, but we don't know which of them are actually predictive. To improve interpretability, and sometimes also generalization performance, we can use feature selection to select a subset of the original features. [ back to top ] \u6839\u636e John, Kohavi, and Pfleger (1994) \uff0c\u53ef\u5c06\u7279\u5f81\u9009\u62e9\u7684\u65b9\u6cd5\u5206\u4e3a\u4e24\u7c7b: Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model. Saeys, Inza, and Larranaga (2007) surveys filter methods. Both approaches have advantages and drawbacks. Filter methods are usually more computationally efficient than wrapper methods, but the selection criterion is not directly related to the effectiveness of the model. Also, most filter methods evaluate each predictor separately and, consequently, redundant (i.e. highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified. The downside of the wrapper method is that many models are evaluated (which may also require parameter tuning) and thus an increase in computation time. There is also an increased risk of over-fitting with wrappers. Sklearn \u4e2d\u4e3b\u8981\u4f7f\u7528 Filter methods. \u4e0b\u9762\u5c06\u4ecb\u7ecd\u5982\u4f55\u7528 sklearn \u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3002 Univariate statistics [ back to top ] The simplest method to select features is using univariate statistics, that is by looking at each feature individually and running a statistical test to see whether it is related to the target. sklearn \u4e2d\u53ef\u4ee5\u7528\u5230\u7684 Univariate statistics \u6709\uff1a + for regression: f_regression + for classification: chi2 or f_classif \u5f97\u5230\u7edf\u8ba1\u91cf\u548c p \u503c\u4e4b\u540e\uff0csklearn \u53c8\u914d\u5957\u4e86\u4e0d\u540c\u7684\u9009\u62e9\u65b9\u6cd5\uff1a + SelectKBest removes all but the k highest scoring features + SelectPercentile removes all but a user-specified highest scoring percentage of features + using common univariate statistical tests for each feature: false positive rate SelectFpr , false discovery rate SelectFdr , or family wise error SelectFwe . + GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator. # \u4ee5 chi2 \u548c SelectKbest \u4e3a\u4f8b from sklearn.feature_selection import chi2 from sklearn.feature_selection import SelectKBest select = SelectKBest(chi2, k=6) X_uni_selected = select.fit_transform(X_train, y_train) print(X_train.shape) print(X_uni_selected.shape) (123, 13) (123, 6) import matplotlib.pyplot as plt %matplotlib inline # \u67e5\u770b\u9009\u51fa\u4e86\u54ea\u51e0\u4e2a feature, \u9ed1\u8272\u662f\u9009\u51fa\u6765\u7684 mask = select.get_support() print(mask) # visualize the mask. black is True, white is False plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [False True False True True False True False False True False False True] Recursive feature elimination [ back to top ] Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. from sklearn.feature_selection import RFE from sklearn.svm import SVC svc = SVC(kernel= linear , C=1) rfe = RFE(estimator=svc, n_features_to_select=6, # \u8981\u9009\u51fa\u51e0\u4e2a feature step=1) # \u6bcf\u6b21\u5254\u9664\u51fa\u51e0\u4e2afeature rfe.fit(X_train_std, y_train) X_rfe_selected = rfe.transform(X_train_std) # \u67e5\u770b\u9009\u51fa\u4e86\u54ea\u51e0\u4e2a feature mask = rfe.get_support() print(mask) plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [ True False False True False False True False False False True True True] Feature selection using SelectFromModel [ back to top ] SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or feature_importances_ attribute after fitting. The features are considered unimportant and removed, if the corresponding coef_ or feature_importances_ values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are build-in heuristics for finding a threshold using a string argument. Available heuristics are \u201cmean\u201d, \u201cmedian\u201d and float multiples of these like \u201c0.1*mean\u201d. \u4e00\u4e9b\u6a21\u578b\u80fd\u6bd4\u8f83\u6bcf\u4e2a feature \u7684\u91cd\u8981\u7a0b\u5ea6\uff0c\u4f8b\u5982 \u7ebf\u6027\u6a21\u578b\u52a0\u4e0a L1 \u6b63\u5219\u9879\u4e4b\u540e\u4e0d\u91cd\u8981\u7684\u7279\u5f81\u7684\u7cfb\u6570\u4f1a\u60e9\u7f5a\u4e3a0\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u80fd\u8ba1\u7b97\u6bcf\u4e2a feature \u7684\u91cd\u8981\u7a0b\u5ea6\u3002 \u7136\u540e sklearn \u6709\u4e2a SelectFromModel \u51fd\u6570\u53ef\u4ee5\u914d\u5408\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u9009\u62e9 L1-based feature selection L2 norm: $$ ||w|| 2^2 = \\sum {j=1}^m w_j^2 $$ L1 norm: $$ ||w|| 1 = \\sum {j=1}^m |w_j| $$ \u4e0e L2 \u6b63\u5219\u76f8\u6bd4\uff0cL1 \u6b63\u5219\u4f1a\u8ba9\u66f4\u591a\u7cfb\u6570\u4e3a 0 \u5982\u679c\u6709\u4e2a\u9ad8\u7ef4\u6570\u636e, \u6709\u5f88\u591a\u7279\u5f81\u662f\u65e0\u7528\u7684, \u90a3\u4e48 L1 regularization \u5c31\u53ef\u4ee5\u88ab\u5f53\u505a\u4e00\u79cd\u7279\u5f81\u9009\u62e9\u7684\u65b9\u6cd5. [ back to top ] from sklearn.linear_model import LogisticRegression # sklearn \u91cc\u60f3\u7528 L1 \u6b63\u5219\uff0c\u628a penalty \u53c2\u6570\u8bbe\u4e3a 'l1' \u5373\u53ef lr = LogisticRegression(penalty='l1', C=0.1) lr.fit(X_train_std, y_train) print('Training accuracy:', lr.score(X_train_std, y_train)) print('Test accuracy:', lr.score(X_test_std, y_test)) ('Training accuracy:', 0.98373983739837401) ('Test accuracy:', 0.96363636363636362) \u52a0\u4e0a L1 \u6b63\u5219\u9879\u540e\uff0c\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u76f8\u8fd1\uff0c\u6ca1\u6709\u8fc7\u62df\u5408 lr.intercept_ array([-0.26943618, -0.12656436, -0.79402866]) # \u7528\u4e86 One-vs-Rest (OvR) \u65b9\u6cd5\uff0c\u6240\u4ee5\u4f1a\u51fa\u73b0\u4e09\u884c\u7cfb\u6570 lr.coef_ array([[ 0.18750685, 0. , 0. , 0. , 0. , 0. , 0.56622652, 0. , 0. , 0. , 0. , 0. , 1.60382013], [-0.74867392, -0.04330592, -0.00242426, 0. , 0. , 0. , 0. , 0. , 0. , -0.80946123, 0. , 0.04873335, -0.44621713], [ 0. , 0. , 0. , 0. , 0. , 0. , -0.7299406 , 0. , 0. , 0.42356047, -0.33037171, -0.52828297, 0. ]]) \u53ef\u4ee5\u770b\u51fa\u7cfb\u6570\u77e9\u9635\u662f\u7a00\u758f\u7684 (\u53ea\u6709\u5c11\u6570\u975e\u96f6\u7cfb\u6570) # weights coeff of the different features for different regularization strengths import matplotlib.pyplot as plt %matplotlib inline fig = plt.figure() ax = plt.subplot(111) colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'pink', 'lightgreen', 'lightblue', 'gray', 'indigo', 'orange'] weights, params = [], [] for c in np.arange(-4, 6): lr = LogisticRegression(penalty='l1', C=10**c, random_state=0) lr.fit(X_train_std, y_train) weights.append(lr.coef_[1]) params.append(10**c) weights = np.array(weights) for column, color in zip(range(weights.shape[1]), colors): plt.plot(params, weights[:, column], label=df_wine.columns[column+1], color=color) plt.axhline(0, color='black', linestyle='--', linewidth=3) plt.xlim([10**(-5), 10**5]) plt.ylabel('weight coefficient') plt.xlabel('C') plt.xscale('log') plt.legend(loc='upper left') ax.legend(loc='upper center', bbox_to_anchor=(1.38, 1.03), ncol=1, fancybox=True); # plt.savefig('./figures/l1_path.png', dpi=300) \u968f\u7740 L1 \u6b63\u5219\u9879\u589e\u5927\uff0c\u65e0\u5173\u7279\u5f81\u522b\u6392\u9664\u51fa\u6a21\u578b (\u7cfb\u6570\u53d8\u4e3a 0)\uff0c\u56e0\u6b64 L1 \u6b63\u5219\u53ef\u4ee5\u4f5c\u4e3a\u7279\u5f81\u9009\u62e9\u7684\u4e00\u79cd\u65b9\u6cd5 \u7ed3\u5408 sklearn \u7684 SelectFromModel \u8fdb\u884c\u9009\u62e9 from sklearn.feature_selection import SelectFromModel model_l1 = SelectFromModel(lr, threshold='median', prefit=True) X_l1_selected = model_l1.transform(X) # \u67e5\u770b\u9009\u51fa\u4e86\u54ea\u51e0\u4e2a feature, \u9ed1\u8272\u662f\u9009\u51fa\u6765\u7684 mask = model_l1.get_support() print(mask) plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [ True False True True False False True False False True True False True] Tree-based feature selection \u968f\u673a\u68ee\u6797\u7b97\u6cd5\u53ef\u4ee5\u6d4b\u91cf\u5404\u4e2a\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f5c\u4e3a\u7279\u5f81\u9009\u62e9\u7684\u4e00\u79cd\u624b\u6bb5 [ back to top ] from sklearn.ensemble import RandomForestClassifier feat_labels = df_wine.columns[1:] # \u4f7f\u7528 decision tree \u6216 random forests \u4e0d\u9700\u8981 standardization\u6216 normalization forest = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1) forest.fit(X_train, y_train) # random forest \u6bd4\u8f83\u7279\u6b8a, \u6709 feature_importances \u8fd9\u4e2a attribute importances = forest.feature_importances_ indices = np.argsort(importances)[::-1] for i, idx in enumerate(indices): print( %2d) %-*s %f % (i + 1, 30, feat_labels[idx], importances[idx])) 1) Proline 0.185412 2) Flavanoids 0.169830 3) Color intensity 0.149659 4) OD280/OD315 of diluted wines 0.127238 5) Alcohol 0.117432 6) Hue 0.057148 7) Total phenols 0.053042 8) Magnesium 0.034654 9) Malic acid 0.027965 10) Proanthocyanins 0.025731 11) Alcalinity of ash 0.021699 12) Nonflavanoid phenols 0.017372 13) Ash 0.012818 plt.title('Feature Importances') plt.bar(range(X_train.shape[1]), importances[indices], color='lightblue', align='center') plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation=90) plt.xlim([-1, X_train.shape[1]]) plt.tight_layout() #plt.savefig('./random_forest.png', dpi=300) \u7ed3\u5408 Sklearn \u7684 SelectFromModel \u8fdb\u884c\u7279\u5f81\u9009\u62e9 from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier select_rf = SelectFromModel(forest, threshold=0.1, prefit=True) # \u6216\u8005\u91cd\u65b0\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b # select = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1), threshold=0.15, prefit=True) # select.fit(X_train, y_train) X_train_rf = select_rf.transform(X_train) print(X_train.shape[1]) # \u539f\u59cb\u7279\u5f81\u7ef4\u5ea6 print(X_train_rf.shape[1]) # \u7279\u5f81\u9009\u62e9\u540e\u7279\u5f81\u7ef4\u5ea6 13 5 # \u67e5\u770b\u9009\u51fa\u7684\u7279\u5f81 mask = select_rf.get_support() for f in feat_labels[mask]: print(f) Alcohol Flavanoids Color intensity OD280/OD315 of diluted wines Proline # \u53ef\u89c6\u5316\u7279\u5f81\u9009\u62e9\u7ed3\u679c\uff0c\u9ed1\u8272\u7684\u662f\u9009\u4e2d\u7684\uff0c\u767d\u8272\u7684\u662f\u6ee4\u8fc7\u7684 mask = select_rf.get_support() print(mask) plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [ True False False False False False True False False True False True True] \u4e5f\u80fd\u5c06\u968f\u673a\u68ee\u6797\u548c Sequential selection \u7ed3\u5408\u8d77\u6765 from sklearn.feature_selection import RFE select = RFE(RandomForestClassifier(n_estimators=100, random_state=0), n_features_to_select=3) select.fit(X_train, y_train) # visualize the selected features: mask = select.get_support() plt.matshow(mask.reshape(1, -1), cmap='gray_r'); Feature extraction \u4e0a\u4e00\u8282\u6211\u4eec\u5b66\u4e60\u4e86 feature selection, \u8fd9\u4e00\u8282\u6211\u4eec\u8981\u5b66\u964d\u7ef4\u7684\u53e6\u4e00\u79cd\u65b9\u6cd5\uff0cfeature extraction [ back to top ] Unsupervised dimensionality reduction via principal component analysis improve computational efficiency help to reduce the curse of dimensionality unsupervised linear transformation technique identify patterns in data based on the correlation between features PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions that the original one. summarize PCA algorithm: 1. Standardize the d-dimensional dataset. 2. Construct the covariance matrix. 3. Decompose the covariance matrix into its eigenvectors and eigenvalues. 4. Select k eigenvectors that correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace ( k \u2264 d ). 5. Construct a projection matrix W from the \"top\" k eigenvectors. 6. Transform the d -dimensional input dataset X using the projection matrix W to obtain the new k -dimensional feature subspace. \u7b80\u5355\u6765\u8bf4\uff0cPCA \u662f\u5728\u627e\u5bfb variance \u6700\u5927\u7684\u65b9\u5411 [ back to top ] \u4ecd\u7136\u4f7f\u7528 Wine dataset import pandas as pd df_wine = pd.read_csv('data/wine.data', header=None) df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'] df_wine.head() Class label Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline 0 1 14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 1 1 13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 2 1 13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 3 1 14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 4 1 13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 Splitting the data into 70% training and 30% test subsets. from sklearn.cross_validation import train_test_split X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.3, random_state=0) Standardizing the data. from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train_std = sc.fit_transform(X_train) X_test_std = sc.transform(X_test) \u8ba1\u7b97\u534f\u65b9\u5dee\u77e9\u9635\uff1a $$ \\sigma_{jk}=\\frac 1 n \\sum^n_{i=1}(x_j^{(i)} - \\mu_j)(x_k^{(i)} - \\mu_k) $$ \u901a\u8fc7\u7279\u5f81\u5206\u89e3\u5f97\u5230\u7279\u5f81\u503c $$ \\lambda $$ \u548c\u7279\u5f81\u5411\u91cf $$ v $$ import numpy as np # compute covariance matrix cov_mat = np.cov(X_train_std.T) # get eigenvalues and eigenvectors eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) # eigen_vecs 13*13 print('Eigenvalues \\n %s' % eigen_vals) Eigenvalues [ 4.8923083 2.46635032 1.42809973 1.01233462 0.84906459 0.60181514 0.52251546 0.08414846 0.33051429 0.29595018 0.16831254 0.21432212 0.2399553 ] Total and explained variance The variance explained ratio of an eigenvalue is simply the fraction of an eigenvalue and the total sum of the eigenvalues: $$\\frac{\\lambda_j}{\\sum^d_{i=1}\\lambda_i}$$ [ back to top ] tot = sum(eigen_vals) var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) # cumulative sum of explained variance # plot variance import matplotlib.pyplot as plt %matplotlib inline plt.bar(range(1, 14), var_exp, alpha=0.5, align='center', label='individual explained variance') plt.step(range(1, 14), cum_var_exp, where='mid', label='cumulative explained variance') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components') plt.legend(loc='best') plt.tight_layout() # plt.savefig('./figures/pca1.png', dpi=300) \u7b2c\u4e00\u4e2a component \u80fd\u89e3\u91ca\u5c06\u8fd1 40% \u7684 variance, \u524d\u4e24\u4e2a components \u80fd\u89e3\u91ca\u8fd1 60% Feature transformation [ back to top ] # Make a list of (eigenvalue, eigenvector) tuples eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))] # Sort the (eigenvalue, eigenvector) tuples from high to low eigen_pairs.sort(reverse=True) we only chose two eigenvectors for the purpose of illustration, since we are going to plot the data via a two-dimensional scatter plot later in this subsection. In practice, the number of principal components has to be determined from a trade-off between computational efficiency and the performance of the classifier. w = np.column_stack([eigen_pairs[0][1], eigen_pairs[1][1]]) print(w) # 13*2 projection matrix from the top two eigenvectors [[ 0.14669811 -0.50417079] [-0.24224554 -0.24216889] [-0.02993442 -0.28698484] [-0.25519002 0.06468718] [ 0.12079772 -0.22995385] [ 0.38934455 -0.09363991] [ 0.42326486 -0.01088622] [-0.30634956 -0.01870216] [ 0.30572219 -0.03040352] [-0.09869191 -0.54527081] [ 0.30032535 0.27924322] [ 0.36821154 0.174365 ] [ 0.29259713 -0.36315461]] \u5229\u7528 projection matrix $$W$$, \u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u8f6c\u6362\u540e\u7684\u6570\u636e $$x^\\prime$$ $$x^\\prime = xW$$ # transform the entire 124\u00d713-dimensional training dataset onto the two principal components X_train_pca = X_train_std.dot(w) colors = ['r', 'b', 'g'] markers = ['s', 'x', 'o'] for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m) plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.tight_layout() # plt.savefig('./figures/pca2.png', dpi=300) data is more spread along the x-axis, a linear classier will likely be able to separate the classes well Principal component analysis in scikit-learn [ back to top ] from sklearn.decomposition import PCA pca = PCA() X_train_pca = pca.fit_transform(X_train_std) pca.explained_variance_ratio_ array([ 0.37329648, 0.18818926, 0.10896791, 0.07724389, 0.06478595, 0.04592014, 0.03986936, 0.02521914, 0.02258181, 0.01830924, 0.01635336, 0.01284271, 0.00642076]) plt.bar(range(1, 14), pca.explained_variance_ratio_, alpha=0.5, align='center') plt.step(range(1, 14), np.cumsum(pca.explained_variance_ratio_), where='mid') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components'); pca = PCA(n_components=2) X_train_pca = pca.fit_transform(X_train_std) X_test_pca = pca.transform(X_test_std) plt.scatter(X_train_pca[:,0], X_train_pca[:,1]) plt.xlabel('PC 1') plt.ylabel('PC 2'); If we compare the PCA projection via scikit-learn with our own PCA implementation, we notice that the plot above is a mirror image of the previous PCA via our step-by-step approach. Note that this is not due to an error in any of those two implementations, but the reason for this difference is that, depending on the eigensolver, eigenvectors can have either negative or positive signs. from matplotlib.colors import ListedColormap def plot_decision_regions(X, y, classifier, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot class samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) Training logistic regression classifier using the first 2 principal components. from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr = lr.fit(X_train_pca, y_train) plot_decision_regions(X_train_pca, y_train, classifier=lr) plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.tight_layout(); # plt.savefig('./figures/pca3.png', dpi=300) # \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5 plot_decision_regions(X_test_pca, y_test, classifier=lr) plt.xlabel('PC1') plt.ylabel('PC2') plt.legend(loc='lower left'); # \u5206\u7c7b\u6548\u679c\u4e5f\u5f88\u4e0d\u9519 Using kernel principal component analysis for nonlinear mappings via kernel PCA, we perform a nonlinear mapping that transforms the data onto a higher-dimensional space and use standard PCA in this higher-dimensional space to project the data back onto a lower-dimensional space where the samples can be separated by a linear classifier [ back to top ] most commonly used kernel: + polynomial kernel + hyperbolic tangent (sigmoid) kernel + Radial Basis Function (RBF) to implement RBF kernel PCA: 1. compute the kernel (similarity) matrix k 2. center the kernel matrix k 3. collect the top k eigenvectors of the centered kernel matrix based on their corresponding eigenvalues, ranked by decreasing magnitude. Implementing a kernel principal component analysis in Python [ back to top ] Radial Basis Function (RBF) or Gaussian kernel: \\begin{align} k(x^{(i)}, x^{(j)}) = exp(-\\frac{||x^{(i)} - x^{(j)}||^2}{2\\sigma^2}) \\ = exp(-\\gamma ||x^{(i)} - x^{(j)}||^2) \\end{align} from scipy.spatial.distance import pdist, squareform from scipy import exp from scipy.linalg import eigh import numpy as np def rbf_kernel_pca(X, gamma, n_components): RBF kernel PCA implementation. Parameters ------------ X: {NumPy ndarray}, shape = [n_samples, n_features] gamma: float Tuning parameter of the RBF kernel n_components: int Number of principal components to return Returns ------------ X_pc: {NumPy ndarray}, shape = [n_samples, k_features] Projected dataset # Calculate pairwise squared Euclidean distances # in the MxN dimensional dataset. sq_dists = pdist(X, 'sqeuclidean') # Convert pairwise distances into a square matrix. mat_sq_dists = squareform(sq_dists) # Compute the symmetric kernel matrix. K = exp(-gamma * mat_sq_dists) # Center the kernel matrix. N = K.shape[0] one_n = np.ones((N,N)) / N K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n) # Obtaining eigenpairs from the centered kernel matrix # numpy.eigh returns them in sorted order eigvals, eigvecs = eigh(K) # Collect the top k eigenvectors (projected samples) X_pc = np.column_stack((eigvecs[:, -i] for i in range(1, n_components + 1))) return X_pc Example 1: Separating half-moon shapes [ back to top ] \u5efa\u9020\u6708\u5f62\u6570\u636e\uff0c\u7528\u4ee5\u6f14\u793a import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import make_moons X, y = make_moons(n_samples=100, random_state=123) plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5) plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5) plt.tight_layout() # plt.savefig('./figures/half_moon_1.png', dpi=300) # standardize PCA from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_std = scaler.fit_transform(X) scikit_pca = PCA(n_components=2) X_spca = scikit_pca.fit_transform(X_std) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_spca[y==0, 0], np.zeros((50,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_spca[y==1, 0], np.zeros((50,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') plt.tight_layout() # plt.savefig('./figures/half_moon_2.png', dpi=300) a linear classier would not be able to perform well # kernel PCA function rbf_kernel_pca from matplotlib.ticker import FormatStrFormatter X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') ax[0].xaxis.set_major_formatter(FormatStrFormatter('%0.1f')) ax[1].xaxis.set_major_formatter(FormatStrFormatter('%0.1f')) plt.tight_layout() # plt.savefig('./figures/half_moon_3.png', dpi=300) two classes (circles and triangles) are linearly well separated Example 2: Separating concentric circles [ back to top ] from sklearn.datasets import make_circles X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2) plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5) plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5) plt.tight_layout() # plt.savefig('./figures/circles_1.png', dpi=300) # standard PCA scaler = StandardScaler() X_std = scaler.fit_transform(X) scikit_pca = PCA(n_components=2) X_spca = scikit_pca.fit_transform(X_std) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_spca[y==0, 0], np.zeros((500,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_spca[y==1, 0], np.zeros((500,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') plt.tight_layout() # plt.savefig('./figures/circles_2.png', dpi=300) # kernel RBF X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_kpca[y==0, 0], np.zeros((500,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_kpca[y==1, 0], np.zeros((500,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') plt.tight_layout() # plt.savefig('./figures/circles_3.png', dpi=300) Projecting new data points learn how to project data points that were not part of the training dataset [ back to top ] from scipy.spatial.distance import pdist, squareform from scipy import exp from scipy.linalg import eigh import numpy as np def rbf_kernel_pca(X, gamma, n_components): RBF kernel PCA implementation. Parameters ------------ X: {NumPy ndarray}, shape = [n_samples, n_features] gamma: float Tuning parameter of the RBF kernel n_components: int Number of principal components to return Returns ------------ X_pc: {NumPy ndarray}, shape = [n_samples, k_features] Projected dataset lambdas: list Eigenvalues # Calculate pairwise squared Euclidean distances # in the MxN dimensional dataset. sq_dists = pdist(X, 'sqeuclidean') # Convert pairwise distances into a square matrix. mat_sq_dists = squareform(sq_dists) # Compute the symmetric kernel matrix. K = exp(-gamma * mat_sq_dists) # Center the kernel matrix. N = K.shape[0] one_n = np.ones((N,N)) / N K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n) # Obtaining eigenpairs from the centered kernel matrix # numpy.eigh returns them in sorted order eigvals, eigvecs = eigh(K) # Collect the top k eigenvectors (projected samples) alphas = np.column_stack((eigvecs[:,-i] for i in range(1,n_components+1))) # Collect the corresponding eigenvalues lambdas = [eigvals[-i] for i in range(1,n_components+1)] return alphas, lambdas X, y = make_moons(n_samples=100, random_state=123) alphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1) x_new = X[25] x_new array([ 1.8713, 0.0093]) x_proj = alphas[25] # original projection x_proj array([ 0.0788]) def project_x(x_new, X, gamma, alphas, lambdas): pair_dist = np.array([np.sum((x_new-row)**2) for row in X]) k = np.exp(-gamma * pair_dist) return k.dot(alphas / lambdas) # projection of the new datapoint x_reproj = project_x(x_new, X, gamma=15, alphas=alphas, lambdas=lambdas) x_reproj array([ 0.0788]) plt.scatter(alphas[y==0, 0], np.zeros((50)), color='red', marker='^',alpha=0.5) plt.scatter(alphas[y==1, 0], np.zeros((50)), color='blue', marker='o', alpha=0.5) plt.scatter(x_proj, 0, color='black', label='original projection of point X[25]', marker='^', s=100) plt.scatter(x_reproj, 0, color='green', label='remapped point X[25]', marker='x', s=500) plt.legend(scatterpoints=1) plt.tight_layout() # plt.savefig('./figures/reproject.png', dpi=300) project correctly Kernel principal component analysis in scikit-learn [ back to top ] from sklearn.decomposition import KernelPCA X, y = make_moons(n_samples=100, random_state=123) scikit_kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15) X_skernpca = scikit_kpca.fit_transform(X) plt.scatter(X_skernpca[y==0, 0], X_skernpca[y==0, 1], color='red', marker='^', alpha=0.5) plt.scatter(X_skernpca[y==1, 0], X_skernpca[y==1, 1], color='blue', marker='o', alpha=0.5) plt.xlabel('PC1') plt.ylabel('PC2') plt.tight_layout() # plt.savefig('./figures/scikit_kpca.png', dpi=300) \u7279\u5f81\u5de5\u7a0bchecklist Do you have domain knowledge? If yes, construct a better set of ad hoc\u201d\u201d features Are your features commensurate? If no, consider normalizing them. Do you suspect interdependence of features? If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you. Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)? If no, construct disjunctive features or weighted sums of feature Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)? If yes, use a variable ranking method; else, do it anyway to get baseline results. Do you need a predictor? If no, stop Do you suspect your data is \u201cdirty\u201d (has a few meaningless input patterns and/or noisy outputs or wrong class labels)? If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them. Do you know what to try first? If no, use a linear predictor. Use a forward selection method with the \u201cprobe\u201d method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset. Do you have new ideas, time, computational resources, and enough examples? If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection Do you want a stable solution (to improve performance and/or understanding)? If yes, subsample your data and redo your analysis for several \u201cbootstrap\u201d. \u4e00\u4e2a\u4f7f\u7528\u6b63\u5219\u5316\u65b9\u6cd5\u8fdb\u884c\u53d8\u91cf\u9009\u62e9\u7684\u4f8b\u5b50 [ back to top ] from sklearn import datasets from sklearn import cross_validation from sklearn import linear_model from sklearn import metrics from sklearn import tree from sklearn import neighbors from sklearn import svm from sklearn import ensemble from sklearn import cluster import matplotlib.pyplot as plt %matplotlib inline import numpy as np import seaborn as sns np.random.seed(123) # \u6784\u5efa dataset, 50\u4e2a sample, 50\u4e2a feature X_all, y_all = datasets.make_regression(n_samples=50, n_features=50, n_informative=10) # 50% train, 50% test X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, y_all, train_size=0.5) X_train.shape, y_train.shape ((25, 50), (25,)) X_test.shape, y_test.shape ((25, 50), (25,)) Linear Regression $$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b - y_i||^2 $$ [ back to top ] # linear reg model = linear_model.LinearRegression() model.fit(X_train, y_train) /Users/alan/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver. warnings.warn(mesg, RuntimeWarning) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) def sse(resid): return sum(resid**2) # \u8ba1\u7b97 train data \u7684 SSE resid_train = y_train - model.predict(X_train) sse_train = sse(resid_train) sse_train 7.9634561748974877e-25 # \u9884\u6d4b test \u518d\u8ba1\u7b97 test data \u7684SSE resid_test = y_test - model.predict(X_test) sse_test = sse(resid_test) sse_test 213555.61203039085 \u7ed3\u679c test data \u663e\u793a\u9884\u6d4b\u6548\u679c\u5f88\u5dee, \u53ef\u80fd overfitting model.score(X_train, y_train) 1.0 model.score(X_test, y_test) 0.31407400675201724 def plot_residuals_and_coeff(resid_train, resid_test, coeff): fig, axes = plt.subplots(1, 3, figsize=(12, 3)) axes[0].bar(np.arange(len(resid_train)), resid_train) axes[0].set_xlabel( sample number ) axes[0].set_ylabel( residual ) axes[0].set_title( training data ) axes[1].bar(np.arange(len(resid_test)), resid_test) axes[1].set_xlabel( sample number ) axes[1].set_ylabel( residual ) axes[1].set_title( testing data ) axes[2].bar(np.arange(len(coeff)), coeff) axes[2].set_xlabel( coefficient number ) axes[2].set_ylabel( coefficient ) fig.tight_layout() return fig, axes # \u753b\u51fa residual fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_); Ridge Regression L2 penalized, add squared sum of the weights to least-squares cost function $$ \\text{min}_{w,b} \\sum_i || w^\\mathsf{T}x_i + b - y_i||^2 + \\alpha ||w||_2^2$$ [ back to top ] # \u4f7f\u7528 Ridge \u6b63\u5219\u5316 model = linear_model.Ridge(alpha=5) model.fit(X_train, y_train) Ridge(alpha=5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001) resid_train = y_train - model.predict(X_train) sse_train = sum(resid_train**2) sse_train 3292.9620358692705 resid_test = y_test - model.predict(X_test) sse_test = sum(resid_test**2) sse_test 209557.58585055024 train data\u7684 SSE \u63d0\u5347\u5f88\u591a # test model score \u4ecd\u7136\u4e0d\u9ad8 model.score(X_train, y_train), model.score(X_test, y_test) (0.99003021243324718, 0.32691539290134652) fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_) LASSO Regression L1-norm certain weights can become zero, useful as a supervised feature selection technique. $$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b - y_i||^2 + \\alpha ||w||_1$$ [ back to top ] model = linear_model.Lasso(alpha=1.0) model.fit(X_train, y_train) Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) resid_train = y_train - model.predict(X_train) sse_train = sse(resid_train) sse_train 309.74971389532328 resid_test = y_test - model.predict(X_test) sse_test = sse(resid_test) sse_test 1489.117606500263 \u76f8\u8f83 Ridge, SSE \u90fd\u51cf\u5c11\u5f88\u591a fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_) \u4e0a\u56fe\u53d1\u73b0, coeff \u6709\u5f88\u591a\u90fd\u662f0 alphas = np.logspace(-4, 2, 100) # \u5bfb\u627e LASSO \u7684\u6700\u4f18\u53c2\u6570 alpha coeffs = np.zeros((len(alphas), X_train.shape[1])) sse_train = np.zeros_like(alphas) sse_test = np.zeros_like(alphas) for n, alpha in enumerate(alphas): model = linear_model.Lasso(alpha=alpha) model.fit(X_train, y_train) coeffs[n, :] = model.coef_ resid = y_train - model.predict(X_train) sse_train[n] = sum(resid**2) resid = y_test - model.predict(X_test) sse_test[n] = sum(resid**2) /Users/alan/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations ConvergenceWarning) fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True) for n in range(coeffs.shape[1]): axes[0].plot(np.log10(alphas), coeffs[:, n], color='k', lw=0.5) axes[1].semilogy(np.log10(alphas), sse_train, label= train ) axes[1].semilogy(np.log10(alphas), sse_test, label= test ) axes[1].legend(loc=0) axes[0].set_xlabel(r ${\\log_{10}}\\alpha$ , fontsize=18) axes[0].set_ylabel(r coefficients , fontsize=18) axes[1].set_xlabel(r ${\\log_{10}}\\alpha$ , fontsize=18) axes[1].set_ylabel(r sse , fontsize=18) fig.tight_layout() alpha \u8d8a\u5927, coeff \u6700\u7ec8\u90fd\u4f1a\u53d8\u62100, \u800c train SSE \u4f1a\u5148\u51cf\u5c0f\u518d\u589e\u52a0, \u800c test \u662f\u4e00\u76f4\u5728\u589e\u52a0. \u5728-1\u9644\u8fd1, train SSE \u6700\u5c0f, \u800c coeff \u5927\u6982\u67098\u4e2a\u4e0d\u662f0. # \u4f7f\u7528LassoCV: Lasso linear model with iterative fitting along a regularization path model = linear_model.LassoCV() model.fit(X_all, y_all) LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True, max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False, precompute='auto', random_state=None, selection='cyclic', tol=0.0001, verbose=False) # \u8ba1\u7b97\u51fa\u7684\u6700\u4f73 alphs model.alpha_ 0.06559238747534718 resid_train = y_train - model.predict(X_train) sse_train = sse(resid_train) sse_train 1.5450589323148352 resid_test = y_test - model.predict(X_test) sse_test = sse(resid_test) sse_test 1.5321417406216176 \u53d1\u73b0 SSE \u90fd\u5df2\u7ecf\u6bd4\u8f83\u63a5\u8fd10\u4e86 model.score(X_train, y_train), model.score(X_test, y_test) # score \u90fd\u5f88\u9ad8 (0.99999532217220677, 0.99999507886570982) fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_) # 9\u4e2a non-zero coeff \u7ec3\u4e60\uff1a\u5229\u7528\u672c\u7ae0\u5b66\u5230\u7684\u65b9\u6cd5\u5bf9\u4fe1\u8d37\u6570\u636e\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b (#sections)","title":"4w"},{"location":"w4-feature-engineering/4w/#sections","text":"What is Feature Engineering? Data preprocessing Dealing with missing data Eliminating samples or features with missing values Imputing missing values Handling categorical data Mapping ordinal features Encoding class labels Performing one-hot encoding on nominal features Partitioning a dataset in training and test sets Bringing features onto the same scale Feature selection Univariate statistics Recursive feature elimination Feature selection using SelectFromModel L1-based feature selection Tree-based feature selection Feature extraction Unsupervised dimensionality reduction via principal component analysis Total and explained variance Feature transformation Principal component analysis in scikit-learn Supervised data compression via linear discriminant analysis Computing the scatter matrices Selecting linear discriminants for the new feature subspace Projecting samples onto the new feature space LDA via scikit-learn Using kernel principal component analysis for nonlinear mappings Implementing a kernel principal component analysis in Python Example 1: Separating half-moon shapes Example 2: Separating concentric circles Projecting new data points Kernel principal component analysis in scikit-learn Using regularization Ridge Regression LASSO Regression Logistic regression with regularization","title":"Sections"},{"location":"w4-feature-engineering/4w/#what-is-feature-engineering","text":"[ back to top ] Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.","title":"What is Feature Engineering?"},{"location":"w4-feature-engineering/4w/#sub-problems-of-feature-engineering","text":"Feature Importance: An estimate of the usefulness of a feature Feature Selection: From many features to a few that are useful Feature Extraction: The automatic construction of new features from raw data Feature Construction: The manual construction of new features from raw data","title":"Sub-Problems of Feature Engineering"},{"location":"w4-feature-engineering/4w/#iterative-process-of-feature-engineering","text":"Brainstorm features: Really get into the problem, look at a lot of data, study feature engineering on other problems and see what you can steal. Devise features: Depends on your problem, but you may use automatic feature extraction, manual feature construction and mixtures of the two. Select features: Use different feature importance scorings and feature selection methods to prepare one or more \u201cviews\u201d for your models to operate upon. Evaluate models: Estimate model accuracy on unseen data using the chosen features.","title":"Iterative Process of Feature Engineering"},{"location":"w4-feature-engineering/4w/#general-examples-of-feature-engineering","text":"Decompose Categorical Attributes Imagine you have a categorical attribute, like \u201cItem_Color\u201d that can be Red, Blue or Unknown. Decompose a Date-Time A date-time contains a lot of information that can be difficult for a model to take advantage of in it\u2019s native form, such as ISO 8601 (i.e. 2014-09-20T20:45:40Z). Reframe Numerical Quantities Your data is very likely to contain quantities, which can be reframed to better expose relevant structures. This may be a transform into a new unit or the decomposition of a rate into time and amount components.","title":"General Examples of Feature Engineering"},{"location":"w4-feature-engineering/4w/#data-preprocessing","text":"","title":"Data preprocessing"},{"location":"w4-feature-engineering/4w/#dealing-with-missing-data","text":"[ back to top ] # \u6784\u9020\u542b\u7f3a\u5931\u503c\u7684\u6570\u636e, NaN \u8868\u793a Not a Number import numpy as np import pandas as pd df = pd.DataFrame(np.arange(1, 13).reshape(3, 4), columns=['A', 'B', 'C', 'D']) df.loc[1, 'C'] = None df.loc[2, 'D'] = None df A B C D 0 1 2 3.0 4.0 1 5 6 NaN 8.0 2 9 10 11.0 NaN # isnull \u4f1a\u8fd4\u56de\u4e00\u4e2a DataFrame, \u91cc\u9762\u7684 bool \u503c\u8868\u793a\u539f\u59cb\u6570\u636e\u662f\u5426\u7f3a\u5931 df.isnull() A B C D 0 False False False False 1 False False True False 2 False False False True # \u7ed3\u679c\u663e\u793a A \u548c B \u5217\u6ca1\u6709\u7f3a\u5931\u503c, C \u548c D \u5404\u6709\u4e00\u4e2a\u7f3a\u5931\u503c df.isnull().sum() A 0 B 0 C 1 D 1 dtype: int64","title":"Dealing with missing data"},{"location":"w4-feature-engineering/4w/#eliminating-samples-or-features-with-missing-values","text":"\u5904\u7406\u7f3a\u5931\u503c\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u5c31\u662f\u5220\u6389\u6709\u7f3a\u5931\u7684\u884c\u6216\u8005\u5217 [ back to top ] df.dropna() # \u9ed8\u8ba4\u5220\u9664\u884c axis = 0 A B C D 0 1 2 3.0 4.0 df.dropna(axis=1) # \u5220\u9664\u5217 A B 0 1 2 1 5 6 2 9 10 # \u53ea\u5220\u9664\u5168\u662f\u7f3a\u5931\u503c\u7684\u884c df.dropna(how='all') A B C D 0 1 2 3.0 4.0 1 5 6 NaN 8.0 2 9 10 11.0 NaN # \u5220\u9664\u975e\u7f3a\u5931\u503c\u5c11\u4e8e thresh \u7684\u884c df.dropna(thresh=4) A B C D 0 1 2 3.0 4.0 # \u5220\u9664\u6709\u7f3a\u5931\u503c\u51fa\u73b0\u5728\u7279\u5b9a\u5217\u7684\u884c df.dropna(subset=['C']) A B C D 0 1 2 3.0 4.0 2 9 10 11.0 NaN \u770b\u4e0a\u53bb\u5220\u9664\u662f\u5f88\u7b80\u4fbf\u7684\u5904\u7406\u65b9\u6cd5, \u4f46\u5b9e\u9645\u4e0a\u76f4\u63a5\u5220\u9664\u53ef\u80fd\u4f1a\u4e22\u5931\u4e0d\u5c11\u4fe1\u606f, \u66f4\u597d\u7684\u9009\u62e9\u662f\u586b\u8865\u7f3a\u5931\u503c","title":"Eliminating samples or features with missing values"},{"location":"w4-feature-engineering/4w/#imputing-missing-values","text":"\u4f30\u8ba1\u7f3a\u5931\u503c\u5e76\u586b\u5145, \u6700\u666e\u904d\u7684\u662f mean imputation, \u4e5f\u5c31\u662f\u7528\u5e73\u5747\u503c\u586b\u5145 [ back to top ] from sklearn.preprocessing import Imputer imr = Imputer(missing_values='NaN', strategy='mean', axis=0) # If axis=0, then impute along columns. # If axis=1, then impute along rows. imr = imr.fit(df.values) imputed_data = imr.transform(df.values) imputed_data array([[ 1., 2., 3., 4.], [ 5., 6., 7., 8.], [ 9., 10., 11., 6.]]) df.values # \u5e76\u6ca1\u6709\u6539\u53d8\u539f\u5148\u7684 df array([[ 1., 2., 3., 4.], [ 5., 6., nan, 8.], [ 9., 10., 11., nan]])","title":"Imputing missing values"},{"location":"w4-feature-engineering/4w/#handling-categorical-data","text":"\u5bf9 categorical \u9700\u8981\u533a\u5206 nominal \u548c ordinal \u4e24\u79cd\u7c7b\u578b, nominal \u662f\u65e0\u5e8f\u7684, \u800c ordinal \u662f\u6709\u5e8f\u7684 [ back to top ] import pandas as pd df = pd.DataFrame([ ['green', 'M', 10.1, 'class1'], ['red', 'L', 13.5, 'class2'], ['blue', 'XL', 15.3, 'class1']]) df.columns = ['color', 'size', 'price', 'classlabel'] df color size price classlabel 0 green M 10.1 class1 1 red L 13.5 class2 2 blue XL 15.3 class1 color : nominal feature size : ordinal feature, XL L M price : numerical feature","title":"Handling categorical data"},{"location":"w4-feature-engineering/4w/#mapping-ordinal-features","text":"convert the categorical string values into integers [ back to top ] # define the mapping manually size_mapping = { 'XL': 3, 'L': 2, 'M': 1} df['size'] = df['size'].map(size_mapping) df color size price classlabel 0 green 1 10.1 class1 1 red 2 13.5 class2 2 blue 3 15.3 class1 # transform the integer values back to the original string inv_size_mapping = {v: k for k, v in size_mapping.items()} df['size'].map(inv_size_mapping) 0 M 1 L 2 XL Name: size, dtype: object","title":"Mapping ordinal features"},{"location":"w4-feature-engineering/4w/#encoding-class-labels","text":"\u5bf9\u5e94 nominal \u7684 class labels, \u4e5f\u9700\u8981\u5c06\u5176\u8f6c\u6362\u4e3a\u6570\u503c\u8868\u5f81\uff0c\u8bb0\u4f4f\u6b64\u65f6\u7684\u6570\u503c\u53ea\u4ee3\u8868\u4e00\u4e2a\u7c7b\u522b\uff0c\u5e76\u4e0d\u8868\u5f81\u6570\u503c\u5173\u7cfb [ back to top ] import numpy as np class_mapping = {label:idx for idx,label in enumerate(np.unique(df['classlabel']))} class_mapping {'class1': 0, 'class2': 1} # \u6700\u7ec8\u628a classlabel \u4e5f\u8f6c\u5316\u4e3a interger df['classlabel'] = df['classlabel'].map(class_mapping) df color size price classlabel 0 green 1 10.1 0 1 red 2 13.5 1 2 blue 3 15.3 0 # \u8f6c\u5316\u56de\u6765\u4e5f\u662f ok \u7684 inv_class_mapping = {v: k for k, v in class_mapping.items()} df['classlabel'] = df['classlabel'].map(inv_class_mapping) df color size price classlabel 0 green 1 10.1 class1 1 red 2 13.5 class2 2 blue 3 15.3 class1 # sklearn \u4e2d\u4e5f\u6709\u76f8\u5e94\u51fd\u6570 from sklearn.preprocessing import LabelEncoder class_le = LabelEncoder() y = class_le.fit_transform(df['classlabel'].values) y array([0, 1, 0]) # \u540c\u6837\u4e5f\u53ef\u4ee5\u53cd\u5411\u8f6c\u6362 class_le.inverse_transform(y) array(['class1', 'class2', 'class1'], dtype=object)","title":"Encoding class labels"},{"location":"w4-feature-engineering/4w/#performing-one-hot-encoding-on-nominal-features","text":"[ back to top ] X = df[['color', 'size', 'price']].values # color column color_le = LabelEncoder() X[:, 0] = color_le.fit_transform(X[:, 0]) X #blue 0 #green 1 #red 2 array([[1, 1, 10.1], [2, 2, 13.5], [0, 3, 15.3]], dtype=object) \u867d\u7136 color \u8f6c\u5316\u4e3a\u4e86 0, 1, 2, \u4f46\u5e76\u4e0d\u80fd\u76f4\u63a5\u4f7f\u7528\u6765\u5efa\u6a21, \u56e0\u4e3a\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d, \u4f1a\u8ba4\u4e3a 2 \u5927\u4e8e 1, \u4e5f\u5c31\u662f red \u5927\u4e8e green. \u5b9e\u9645\u5374\u4e0d\u662f\u8fd9\u6837\u7684, \u6240\u4ee5\u9700\u8981\u7528\u5230 one-hot encoding, \u9700\u8981\u4f7f\u7528 dummy variable, \u6bcf\u4e00\u4e2a label \u6700\u540e\u88ab\u8868\u793a\u4e3a\u4e00\u4e2a\u5411\u91cf. \u4f8b\u5982, blue sample can be encoded as blue=1, green=0, red=0. from sklearn.preprocessing import OneHotEncoder ohe = OneHotEncoder(categorical_features=[0], sparse=False) # \u4e0d\u8bbe\u5b9a sparse=False \u7684\u8bdd\uff0conehot \u4f1a\u8fd4\u56de\u4e00\u4e2a sparse matrix\uff0c \u53ef\u4ee5\u7528 toarray() \u5c06\u4e4b\u53d8\u56de dense ohe.fit_transform(X) # \u524d\u4e09\u5217\u4e3adummy array([[ 0. , 1. , 0. , 1. , 10.1], [ 0. , 0. , 1. , 2. , 13.5], [ 1. , 0. , 0. , 3. , 15.3]]) # pandas \u4e2d\u7684 get_dummies \u51fd\u6570\u662f\u751f\u6210 dummy variable \u66f4\u7b80\u5355\u7684\u65b9\u6cd5 pd.get_dummies(df[['price', 'color', 'size']]) price size color_blue color_green color_red 0 10.1 1 0.0 1.0 0.0 1 13.5 2 0.0 0.0 1.0 2 15.3 3 1.0 0.0 0.0","title":"Performing one-hot encoding on nominal features"},{"location":"w4-feature-engineering/4w/#partitioning-a-dataset-in-training-and-test-sets","text":"the test set can be understood as the ultimate test of our model before we let it loose on the real world [ back to top ] # \u8bfb\u53d6wine\u6570\u636e df_wine = pd.read_csv('data/wine.data', header=None) df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'] print('Class labels', np.unique(df_wine['Class label'])) df_wine.head() # \u4e00\u5171\u6709\u4e09\u79cd label ('Class labels', array([1, 2, 3])) Class label Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline 0 1 14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 1 1 13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 2 1 13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 3 1 14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 4 1 13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 \u4f7f\u7528 train_test_split \u51fd\u6570\u8fdb\u884c\u8bad\u7ec3/\u6d4b\u8bd5\u96c6\u5207\u5206 from sklearn.cross_validation import train_test_split X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.3, random_state=0) # 30%\u662f test data","title":"Partitioning a dataset in training and test sets"},{"location":"w4-feature-engineering/4w/#stratified-train-test-split","text":"stratified \u5207\u5206\uff0c \u4f7f\u5207\u5206\u540e\u7684\u6570\u636e\u96c6\u66f4\u597d\u5730\u4fdd\u7559\u6807\u7b7e\u7684\u76f8\u5bf9\u6bd4\u4f8b # \u5e2e\u52a9\u51fd\u6570\uff0c\u8ba1\u7b97\u5404\u6807\u7b7e\u6bd4\u4f8b def label_frequency(labels): counts = np.unique(labels, return_counts=True)[1] n = len(labels) return counts / float(n) # \u539f\u59cb\u6570\u636e\u4e2d\u5404\u6807\u7b7e\u7684\u6bd4\u4f8b label_frequency(y) array([ 0.33146067, 0.3988764 , 0.26966292]) # train_test_split \u540e\u7684\u6bd4\u4f8b label_frequency(y_train), label_frequency(y_test) (array([ 0.32258065, 0.39516129, 0.28225806]), array([ 0.35185185, 0.40740741, 0.24074074])) # stratified \u4e4b\u540e\u7684\u6807\u7b7e\u6bd4\u4f8b\uff0c \u66f4\u63a5\u8fd1\u539f\u59cb\u6bd4\u4f8b X_train, X_test, y_train, y_test = \\ train_test_split(X, y, stratify=y, test_size=0.3, random_state=0) label_frequency(y_train), label_frequency(y_test) (array([ 0.33333333, 0.39837398, 0.26829268]), array([ 0.32727273, 0.4 , 0.27272727]))","title":"stratified train test split"},{"location":"w4-feature-engineering/4w/#bringing-features-onto-the-same-scale","text":"Feature Scaling \u5f88\u5bb9\u6613\u88ab\u9057\u5fd8, \u867d\u7136\u5728 Decision tree\u548c random forests \u65f6\u4e0d\u7528\u62c5\u5fc3\u8fd9\u4e2a\u95ee\u9898. \u4f46\u5728\u5f88\u591a\u7b97\u6cd5\u548c\u6a21\u578b\u4e0b\u90fd\u662f scaling \u540e\u62df\u5408\u6548\u679c\u66f4\u597d. \u4e24\u7c7b\u5e38\u7528\u65b9\u6cd5: normalization \u548c standardization. - normalization: rescaling to [0,1], \u5982 min-max scaling $$ x_{norm}^{(i)} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}$$ - standardization: more practical, \u56e0\u4e3a\u5728\u4e00\u4e9b\u7b97\u6cd5\u4e2d, weights \u521d\u59cb\u503c\u90fd\u8bbe\u7f6e\u4e3a 0, \u6216\u8005\u63a5\u8fd1 0. standardization \u4e4b\u540e\u4f1a\u66f4\u5229\u7528\u66f4\u65b0 weights. \u5e76\u4e14 standardize \u5bf9 outlier \u66f4\u4e0d\u654f\u611f\uff0c\u53d7\u5f71\u54cd\u66f4\u5c0f $$ x_{std}^{(i)} = \\frac{x^{(i)} - \\mu_x}{\\sigma_x}$$ [ back to top ] # min-max rescaling from sklearn.preprocessing import MinMaxScaler mms = MinMaxScaler() X_train_norm = mms.fit_transform(X_train) X_test_norm = mms.transform(X_test) # \u6ce8\u610f\u6d4b\u8bd5\u96c6\u662f\u6309\u7167\u8bad\u7ec3\u96c6\u7684\u53c2\u6570\u8fdb\u884c\u8f6c\u6362 # standarzation from sklearn.preprocessing import StandardScaler stdsc = StandardScaler() X_train_std = stdsc.fit_transform(X_train) X_test_std = stdsc.transform(X_test) A visual example: ex = pd.DataFrame([0, 1, 2 ,3, 4, 5]) # standardize ex[1] = (ex[0] - ex[0].mean()) / ex[0].std() # normalize ex[2] = (ex[0] - ex[0].min()) / (ex[0].max() - ex[0].min()) ex.columns = ['input', 'standardized', 'normalized'] ex input standardized normalized 0 0 -1.336306 0.0 1 1 -0.801784 0.2 2 2 -0.267261 0.4 3 3 0.267261 0.6 4 4 0.801784 0.8 5 5 1.336306 1.0","title":"Bringing features onto the same scale"},{"location":"w4-feature-engineering/4w/#feature-selection","text":"Often we collected many features that might be related to a supervised prediction task, but we don't know which of them are actually predictive. To improve interpretability, and sometimes also generalization performance, we can use feature selection to select a subset of the original features. [ back to top ] \u6839\u636e John, Kohavi, and Pfleger (1994) \uff0c\u53ef\u5c06\u7279\u5f81\u9009\u62e9\u7684\u65b9\u6cd5\u5206\u4e3a\u4e24\u7c7b: Wrapper methods evaluate multiple models using procedures that add and/or remove predictors to find the optimal combination that maximizes model performance. In essence, wrapper methods are search algorithms that treat the predictors as the inputs and utilize model performance as the output to be optimized. Filter methods evaluate the relevance of the predictors outside of the predictive models and subsequently model only the predictors that pass some criterion. For example, for classification problems, each predictor could be individually evaluated to check if there is a plausible relationship between it and the observed classes. Only predictors with important relationships would then be included in a classification model. Saeys, Inza, and Larranaga (2007) surveys filter methods. Both approaches have advantages and drawbacks. Filter methods are usually more computationally efficient than wrapper methods, but the selection criterion is not directly related to the effectiveness of the model. Also, most filter methods evaluate each predictor separately and, consequently, redundant (i.e. highly-correlated) predictors may be selected and important interactions between variables will not be able to be quantified. The downside of the wrapper method is that many models are evaluated (which may also require parameter tuning) and thus an increase in computation time. There is also an increased risk of over-fitting with wrappers. Sklearn \u4e2d\u4e3b\u8981\u4f7f\u7528 Filter methods. \u4e0b\u9762\u5c06\u4ecb\u7ecd\u5982\u4f55\u7528 sklearn \u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3002","title":"Feature selection"},{"location":"w4-feature-engineering/4w/#univariate-statistics","text":"[ back to top ] The simplest method to select features is using univariate statistics, that is by looking at each feature individually and running a statistical test to see whether it is related to the target. sklearn \u4e2d\u53ef\u4ee5\u7528\u5230\u7684 Univariate statistics \u6709\uff1a + for regression: f_regression + for classification: chi2 or f_classif \u5f97\u5230\u7edf\u8ba1\u91cf\u548c p \u503c\u4e4b\u540e\uff0csklearn \u53c8\u914d\u5957\u4e86\u4e0d\u540c\u7684\u9009\u62e9\u65b9\u6cd5\uff1a + SelectKBest removes all but the k highest scoring features + SelectPercentile removes all but a user-specified highest scoring percentage of features + using common univariate statistical tests for each feature: false positive rate SelectFpr , false discovery rate SelectFdr , or family wise error SelectFwe . + GenericUnivariateSelect allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator. # \u4ee5 chi2 \u548c SelectKbest \u4e3a\u4f8b from sklearn.feature_selection import chi2 from sklearn.feature_selection import SelectKBest select = SelectKBest(chi2, k=6) X_uni_selected = select.fit_transform(X_train, y_train) print(X_train.shape) print(X_uni_selected.shape) (123, 13) (123, 6) import matplotlib.pyplot as plt %matplotlib inline # \u67e5\u770b\u9009\u51fa\u4e86\u54ea\u51e0\u4e2a feature, \u9ed1\u8272\u662f\u9009\u51fa\u6765\u7684 mask = select.get_support() print(mask) # visualize the mask. black is True, white is False plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [False True False True True False True False False True False False True]","title":"Univariate statistics"},{"location":"w4-feature-engineering/4w/#recursive-feature-elimination","text":"[ back to top ] Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and weights are assigned to each one of them. Then, features whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. from sklearn.feature_selection import RFE from sklearn.svm import SVC svc = SVC(kernel= linear , C=1) rfe = RFE(estimator=svc, n_features_to_select=6, # \u8981\u9009\u51fa\u51e0\u4e2a feature step=1) # \u6bcf\u6b21\u5254\u9664\u51fa\u51e0\u4e2afeature rfe.fit(X_train_std, y_train) X_rfe_selected = rfe.transform(X_train_std) # \u67e5\u770b\u9009\u51fa\u4e86\u54ea\u51e0\u4e2a feature mask = rfe.get_support() print(mask) plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [ True False False True False False True False False False True True True]","title":"Recursive feature elimination"},{"location":"w4-feature-engineering/4w/#feature-selection-using-selectfrommodel","text":"[ back to top ] SelectFromModel is a meta-transformer that can be used along with any estimator that has a coef_ or feature_importances_ attribute after fitting. The features are considered unimportant and removed, if the corresponding coef_ or feature_importances_ values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are build-in heuristics for finding a threshold using a string argument. Available heuristics are \u201cmean\u201d, \u201cmedian\u201d and float multiples of these like \u201c0.1*mean\u201d. \u4e00\u4e9b\u6a21\u578b\u80fd\u6bd4\u8f83\u6bcf\u4e2a feature \u7684\u91cd\u8981\u7a0b\u5ea6\uff0c\u4f8b\u5982 \u7ebf\u6027\u6a21\u578b\u52a0\u4e0a L1 \u6b63\u5219\u9879\u4e4b\u540e\u4e0d\u91cd\u8981\u7684\u7279\u5f81\u7684\u7cfb\u6570\u4f1a\u60e9\u7f5a\u4e3a0\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u80fd\u8ba1\u7b97\u6bcf\u4e2a feature \u7684\u91cd\u8981\u7a0b\u5ea6\u3002 \u7136\u540e sklearn \u6709\u4e2a SelectFromModel \u51fd\u6570\u53ef\u4ee5\u914d\u5408\u8fd9\u4e9b\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u9009\u62e9","title":"Feature selection using SelectFromModel"},{"location":"w4-feature-engineering/4w/#l1-based-feature-selection","text":"L2 norm: $$ ||w|| 2^2 = \\sum {j=1}^m w_j^2 $$ L1 norm: $$ ||w|| 1 = \\sum {j=1}^m |w_j| $$ \u4e0e L2 \u6b63\u5219\u76f8\u6bd4\uff0cL1 \u6b63\u5219\u4f1a\u8ba9\u66f4\u591a\u7cfb\u6570\u4e3a 0 \u5982\u679c\u6709\u4e2a\u9ad8\u7ef4\u6570\u636e, \u6709\u5f88\u591a\u7279\u5f81\u662f\u65e0\u7528\u7684, \u90a3\u4e48 L1 regularization \u5c31\u53ef\u4ee5\u88ab\u5f53\u505a\u4e00\u79cd\u7279\u5f81\u9009\u62e9\u7684\u65b9\u6cd5. [ back to top ] from sklearn.linear_model import LogisticRegression # sklearn \u91cc\u60f3\u7528 L1 \u6b63\u5219\uff0c\u628a penalty \u53c2\u6570\u8bbe\u4e3a 'l1' \u5373\u53ef lr = LogisticRegression(penalty='l1', C=0.1) lr.fit(X_train_std, y_train) print('Training accuracy:', lr.score(X_train_std, y_train)) print('Test accuracy:', lr.score(X_test_std, y_test)) ('Training accuracy:', 0.98373983739837401) ('Test accuracy:', 0.96363636363636362) \u52a0\u4e0a L1 \u6b63\u5219\u9879\u540e\uff0c\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u7684\u8868\u73b0\u76f8\u8fd1\uff0c\u6ca1\u6709\u8fc7\u62df\u5408 lr.intercept_ array([-0.26943618, -0.12656436, -0.79402866]) # \u7528\u4e86 One-vs-Rest (OvR) \u65b9\u6cd5\uff0c\u6240\u4ee5\u4f1a\u51fa\u73b0\u4e09\u884c\u7cfb\u6570 lr.coef_ array([[ 0.18750685, 0. , 0. , 0. , 0. , 0. , 0.56622652, 0. , 0. , 0. , 0. , 0. , 1.60382013], [-0.74867392, -0.04330592, -0.00242426, 0. , 0. , 0. , 0. , 0. , 0. , -0.80946123, 0. , 0.04873335, -0.44621713], [ 0. , 0. , 0. , 0. , 0. , 0. , -0.7299406 , 0. , 0. , 0.42356047, -0.33037171, -0.52828297, 0. ]]) \u53ef\u4ee5\u770b\u51fa\u7cfb\u6570\u77e9\u9635\u662f\u7a00\u758f\u7684 (\u53ea\u6709\u5c11\u6570\u975e\u96f6\u7cfb\u6570) # weights coeff of the different features for different regularization strengths import matplotlib.pyplot as plt %matplotlib inline fig = plt.figure() ax = plt.subplot(111) colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', 'pink', 'lightgreen', 'lightblue', 'gray', 'indigo', 'orange'] weights, params = [], [] for c in np.arange(-4, 6): lr = LogisticRegression(penalty='l1', C=10**c, random_state=0) lr.fit(X_train_std, y_train) weights.append(lr.coef_[1]) params.append(10**c) weights = np.array(weights) for column, color in zip(range(weights.shape[1]), colors): plt.plot(params, weights[:, column], label=df_wine.columns[column+1], color=color) plt.axhline(0, color='black', linestyle='--', linewidth=3) plt.xlim([10**(-5), 10**5]) plt.ylabel('weight coefficient') plt.xlabel('C') plt.xscale('log') plt.legend(loc='upper left') ax.legend(loc='upper center', bbox_to_anchor=(1.38, 1.03), ncol=1, fancybox=True); # plt.savefig('./figures/l1_path.png', dpi=300) \u968f\u7740 L1 \u6b63\u5219\u9879\u589e\u5927\uff0c\u65e0\u5173\u7279\u5f81\u522b\u6392\u9664\u51fa\u6a21\u578b (\u7cfb\u6570\u53d8\u4e3a 0)\uff0c\u56e0\u6b64 L1 \u6b63\u5219\u53ef\u4ee5\u4f5c\u4e3a\u7279\u5f81\u9009\u62e9\u7684\u4e00\u79cd\u65b9\u6cd5 \u7ed3\u5408 sklearn \u7684 SelectFromModel \u8fdb\u884c\u9009\u62e9 from sklearn.feature_selection import SelectFromModel model_l1 = SelectFromModel(lr, threshold='median', prefit=True) X_l1_selected = model_l1.transform(X) # \u67e5\u770b\u9009\u51fa\u4e86\u54ea\u51e0\u4e2a feature, \u9ed1\u8272\u662f\u9009\u51fa\u6765\u7684 mask = model_l1.get_support() print(mask) plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [ True False True True False False True False False True True False True]","title":"L1-based feature selection"},{"location":"w4-feature-engineering/4w/#tree-based-feature-selection","text":"\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u53ef\u4ee5\u6d4b\u91cf\u5404\u4e2a\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u56e0\u6b64\u53ef\u4ee5\u4f5c\u4e3a\u7279\u5f81\u9009\u62e9\u7684\u4e00\u79cd\u624b\u6bb5 [ back to top ] from sklearn.ensemble import RandomForestClassifier feat_labels = df_wine.columns[1:] # \u4f7f\u7528 decision tree \u6216 random forests \u4e0d\u9700\u8981 standardization\u6216 normalization forest = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1) forest.fit(X_train, y_train) # random forest \u6bd4\u8f83\u7279\u6b8a, \u6709 feature_importances \u8fd9\u4e2a attribute importances = forest.feature_importances_ indices = np.argsort(importances)[::-1] for i, idx in enumerate(indices): print( %2d) %-*s %f % (i + 1, 30, feat_labels[idx], importances[idx])) 1) Proline 0.185412 2) Flavanoids 0.169830 3) Color intensity 0.149659 4) OD280/OD315 of diluted wines 0.127238 5) Alcohol 0.117432 6) Hue 0.057148 7) Total phenols 0.053042 8) Magnesium 0.034654 9) Malic acid 0.027965 10) Proanthocyanins 0.025731 11) Alcalinity of ash 0.021699 12) Nonflavanoid phenols 0.017372 13) Ash 0.012818 plt.title('Feature Importances') plt.bar(range(X_train.shape[1]), importances[indices], color='lightblue', align='center') plt.xticks(range(X_train.shape[1]), feat_labels[indices], rotation=90) plt.xlim([-1, X_train.shape[1]]) plt.tight_layout() #plt.savefig('./random_forest.png', dpi=300) \u7ed3\u5408 Sklearn \u7684 SelectFromModel \u8fdb\u884c\u7279\u5f81\u9009\u62e9 from sklearn.feature_selection import SelectFromModel from sklearn.ensemble import RandomForestClassifier select_rf = SelectFromModel(forest, threshold=0.1, prefit=True) # \u6216\u8005\u91cd\u65b0\u8bad\u7ec3\u4e00\u4e2a\u6a21\u578b # select = SelectFromModel(RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1), threshold=0.15, prefit=True) # select.fit(X_train, y_train) X_train_rf = select_rf.transform(X_train) print(X_train.shape[1]) # \u539f\u59cb\u7279\u5f81\u7ef4\u5ea6 print(X_train_rf.shape[1]) # \u7279\u5f81\u9009\u62e9\u540e\u7279\u5f81\u7ef4\u5ea6 13 5 # \u67e5\u770b\u9009\u51fa\u7684\u7279\u5f81 mask = select_rf.get_support() for f in feat_labels[mask]: print(f) Alcohol Flavanoids Color intensity OD280/OD315 of diluted wines Proline # \u53ef\u89c6\u5316\u7279\u5f81\u9009\u62e9\u7ed3\u679c\uff0c\u9ed1\u8272\u7684\u662f\u9009\u4e2d\u7684\uff0c\u767d\u8272\u7684\u662f\u6ee4\u8fc7\u7684 mask = select_rf.get_support() print(mask) plt.matshow(mask.reshape(1, -1), cmap='gray_r'); [ True False False False False False True False False True False True True] \u4e5f\u80fd\u5c06\u968f\u673a\u68ee\u6797\u548c Sequential selection \u7ed3\u5408\u8d77\u6765 from sklearn.feature_selection import RFE select = RFE(RandomForestClassifier(n_estimators=100, random_state=0), n_features_to_select=3) select.fit(X_train, y_train) # visualize the selected features: mask = select.get_support() plt.matshow(mask.reshape(1, -1), cmap='gray_r');","title":"Tree-based feature selection"},{"location":"w4-feature-engineering/4w/#feature-extraction","text":"\u4e0a\u4e00\u8282\u6211\u4eec\u5b66\u4e60\u4e86 feature selection, \u8fd9\u4e00\u8282\u6211\u4eec\u8981\u5b66\u964d\u7ef4\u7684\u53e6\u4e00\u79cd\u65b9\u6cd5\uff0cfeature extraction [ back to top ]","title":"Feature extraction"},{"location":"w4-feature-engineering/4w/#unsupervised-dimensionality-reduction-via-principal-component-analysis","text":"improve computational efficiency help to reduce the curse of dimensionality unsupervised linear transformation technique identify patterns in data based on the correlation between features PCA aims to find the directions of maximum variance in high-dimensional data and projects it onto a new subspace with equal or fewer dimensions that the original one. summarize PCA algorithm: 1. Standardize the d-dimensional dataset. 2. Construct the covariance matrix. 3. Decompose the covariance matrix into its eigenvectors and eigenvalues. 4. Select k eigenvectors that correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace ( k \u2264 d ). 5. Construct a projection matrix W from the \"top\" k eigenvectors. 6. Transform the d -dimensional input dataset X using the projection matrix W to obtain the new k -dimensional feature subspace. \u7b80\u5355\u6765\u8bf4\uff0cPCA \u662f\u5728\u627e\u5bfb variance \u6700\u5927\u7684\u65b9\u5411 [ back to top ] \u4ecd\u7136\u4f7f\u7528 Wine dataset import pandas as pd df_wine = pd.read_csv('data/wine.data', header=None) df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline'] df_wine.head() Class label Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline 0 1 14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065 1 1 13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050 2 1 13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185 3 1 14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480 4 1 13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735 Splitting the data into 70% training and 30% test subsets. from sklearn.cross_validation import train_test_split X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.3, random_state=0) Standardizing the data. from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train_std = sc.fit_transform(X_train) X_test_std = sc.transform(X_test) \u8ba1\u7b97\u534f\u65b9\u5dee\u77e9\u9635\uff1a $$ \\sigma_{jk}=\\frac 1 n \\sum^n_{i=1}(x_j^{(i)} - \\mu_j)(x_k^{(i)} - \\mu_k) $$ \u901a\u8fc7\u7279\u5f81\u5206\u89e3\u5f97\u5230\u7279\u5f81\u503c $$ \\lambda $$ \u548c\u7279\u5f81\u5411\u91cf $$ v $$ import numpy as np # compute covariance matrix cov_mat = np.cov(X_train_std.T) # get eigenvalues and eigenvectors eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) # eigen_vecs 13*13 print('Eigenvalues \\n %s' % eigen_vals) Eigenvalues [ 4.8923083 2.46635032 1.42809973 1.01233462 0.84906459 0.60181514 0.52251546 0.08414846 0.33051429 0.29595018 0.16831254 0.21432212 0.2399553 ]","title":"Unsupervised dimensionality reduction via principal component analysis"},{"location":"w4-feature-engineering/4w/#total-and-explained-variance","text":"The variance explained ratio of an eigenvalue is simply the fraction of an eigenvalue and the total sum of the eigenvalues: $$\\frac{\\lambda_j}{\\sum^d_{i=1}\\lambda_i}$$ [ back to top ] tot = sum(eigen_vals) var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) # cumulative sum of explained variance # plot variance import matplotlib.pyplot as plt %matplotlib inline plt.bar(range(1, 14), var_exp, alpha=0.5, align='center', label='individual explained variance') plt.step(range(1, 14), cum_var_exp, where='mid', label='cumulative explained variance') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components') plt.legend(loc='best') plt.tight_layout() # plt.savefig('./figures/pca1.png', dpi=300) \u7b2c\u4e00\u4e2a component \u80fd\u89e3\u91ca\u5c06\u8fd1 40% \u7684 variance, \u524d\u4e24\u4e2a components \u80fd\u89e3\u91ca\u8fd1 60%","title":"Total and explained variance"},{"location":"w4-feature-engineering/4w/#feature-transformation","text":"[ back to top ] # Make a list of (eigenvalue, eigenvector) tuples eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))] # Sort the (eigenvalue, eigenvector) tuples from high to low eigen_pairs.sort(reverse=True) we only chose two eigenvectors for the purpose of illustration, since we are going to plot the data via a two-dimensional scatter plot later in this subsection. In practice, the number of principal components has to be determined from a trade-off between computational efficiency and the performance of the classifier. w = np.column_stack([eigen_pairs[0][1], eigen_pairs[1][1]]) print(w) # 13*2 projection matrix from the top two eigenvectors [[ 0.14669811 -0.50417079] [-0.24224554 -0.24216889] [-0.02993442 -0.28698484] [-0.25519002 0.06468718] [ 0.12079772 -0.22995385] [ 0.38934455 -0.09363991] [ 0.42326486 -0.01088622] [-0.30634956 -0.01870216] [ 0.30572219 -0.03040352] [-0.09869191 -0.54527081] [ 0.30032535 0.27924322] [ 0.36821154 0.174365 ] [ 0.29259713 -0.36315461]] \u5229\u7528 projection matrix $$W$$, \u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u8f6c\u6362\u540e\u7684\u6570\u636e $$x^\\prime$$ $$x^\\prime = xW$$ # transform the entire 124\u00d713-dimensional training dataset onto the two principal components X_train_pca = X_train_std.dot(w) colors = ['r', 'b', 'g'] markers = ['s', 'x', 'o'] for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train==l, 0], X_train_pca[y_train==l, 1], c=c, label=l, marker=m) plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.tight_layout() # plt.savefig('./figures/pca2.png', dpi=300) data is more spread along the x-axis, a linear classier will likely be able to separate the classes well","title":"Feature transformation"},{"location":"w4-feature-engineering/4w/#principal-component-analysis-in-scikit-learn","text":"[ back to top ] from sklearn.decomposition import PCA pca = PCA() X_train_pca = pca.fit_transform(X_train_std) pca.explained_variance_ratio_ array([ 0.37329648, 0.18818926, 0.10896791, 0.07724389, 0.06478595, 0.04592014, 0.03986936, 0.02521914, 0.02258181, 0.01830924, 0.01635336, 0.01284271, 0.00642076]) plt.bar(range(1, 14), pca.explained_variance_ratio_, alpha=0.5, align='center') plt.step(range(1, 14), np.cumsum(pca.explained_variance_ratio_), where='mid') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components'); pca = PCA(n_components=2) X_train_pca = pca.fit_transform(X_train_std) X_test_pca = pca.transform(X_test_std) plt.scatter(X_train_pca[:,0], X_train_pca[:,1]) plt.xlabel('PC 1') plt.ylabel('PC 2'); If we compare the PCA projection via scikit-learn with our own PCA implementation, we notice that the plot above is a mirror image of the previous PCA via our step-by-step approach. Note that this is not due to an error in any of those two implementations, but the reason for this difference is that, depending on the eigensolver, eigenvectors can have either negative or positive signs. from matplotlib.colors import ListedColormap def plot_decision_regions(X, y, classifier, resolution=0.02): # setup marker generator and color map markers = ('s', 'x', 'o', '^', 'v') colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan') cmap = ListedColormap(colors[:len(np.unique(y))]) # plot the decision surface x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T) Z = Z.reshape(xx1.shape) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) # plot class samples for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx], label=cl) Training logistic regression classifier using the first 2 principal components. from sklearn.linear_model import LogisticRegression lr = LogisticRegression() lr = lr.fit(X_train_pca, y_train) plot_decision_regions(X_train_pca, y_train, classifier=lr) plt.xlabel('PC 1') plt.ylabel('PC 2') plt.legend(loc='lower left') plt.tight_layout(); # plt.savefig('./figures/pca3.png', dpi=300) # \u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5 plot_decision_regions(X_test_pca, y_test, classifier=lr) plt.xlabel('PC1') plt.ylabel('PC2') plt.legend(loc='lower left'); # \u5206\u7c7b\u6548\u679c\u4e5f\u5f88\u4e0d\u9519","title":"Principal component analysis in scikit-learn"},{"location":"w4-feature-engineering/4w/#using-kernel-principal-component-analysis-for-nonlinear-mappings","text":"via kernel PCA, we perform a nonlinear mapping that transforms the data onto a higher-dimensional space and use standard PCA in this higher-dimensional space to project the data back onto a lower-dimensional space where the samples can be separated by a linear classifier [ back to top ] most commonly used kernel: + polynomial kernel + hyperbolic tangent (sigmoid) kernel + Radial Basis Function (RBF) to implement RBF kernel PCA: 1. compute the kernel (similarity) matrix k 2. center the kernel matrix k 3. collect the top k eigenvectors of the centered kernel matrix based on their corresponding eigenvalues, ranked by decreasing magnitude.","title":"Using kernel principal component analysis for nonlinear mappings"},{"location":"w4-feature-engineering/4w/#implementing-a-kernel-principal-component-analysis-in-python","text":"[ back to top ] Radial Basis Function (RBF) or Gaussian kernel: \\begin{align} k(x^{(i)}, x^{(j)}) = exp(-\\frac{||x^{(i)} - x^{(j)}||^2}{2\\sigma^2}) \\ = exp(-\\gamma ||x^{(i)} - x^{(j)}||^2) \\end{align} from scipy.spatial.distance import pdist, squareform from scipy import exp from scipy.linalg import eigh import numpy as np def rbf_kernel_pca(X, gamma, n_components): RBF kernel PCA implementation. Parameters ------------ X: {NumPy ndarray}, shape = [n_samples, n_features] gamma: float Tuning parameter of the RBF kernel n_components: int Number of principal components to return Returns ------------ X_pc: {NumPy ndarray}, shape = [n_samples, k_features] Projected dataset # Calculate pairwise squared Euclidean distances # in the MxN dimensional dataset. sq_dists = pdist(X, 'sqeuclidean') # Convert pairwise distances into a square matrix. mat_sq_dists = squareform(sq_dists) # Compute the symmetric kernel matrix. K = exp(-gamma * mat_sq_dists) # Center the kernel matrix. N = K.shape[0] one_n = np.ones((N,N)) / N K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n) # Obtaining eigenpairs from the centered kernel matrix # numpy.eigh returns them in sorted order eigvals, eigvecs = eigh(K) # Collect the top k eigenvectors (projected samples) X_pc = np.column_stack((eigvecs[:, -i] for i in range(1, n_components + 1))) return X_pc","title":"Implementing a kernel principal component analysis in Python"},{"location":"w4-feature-engineering/4w/#example-1-separating-half-moon-shapes","text":"[ back to top ] \u5efa\u9020\u6708\u5f62\u6570\u636e\uff0c\u7528\u4ee5\u6f14\u793a import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import make_moons X, y = make_moons(n_samples=100, random_state=123) plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5) plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5) plt.tight_layout() # plt.savefig('./figures/half_moon_1.png', dpi=300) # standardize PCA from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_std = scaler.fit_transform(X) scikit_pca = PCA(n_components=2) X_spca = scikit_pca.fit_transform(X_std) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_spca[y==0, 0], np.zeros((50,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_spca[y==1, 0], np.zeros((50,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') plt.tight_layout() # plt.savefig('./figures/half_moon_2.png', dpi=300) a linear classier would not be able to perform well # kernel PCA function rbf_kernel_pca from matplotlib.ticker import FormatStrFormatter X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_kpca[y==0, 0], np.zeros((50,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_kpca[y==1, 0], np.zeros((50,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') ax[0].xaxis.set_major_formatter(FormatStrFormatter('%0.1f')) ax[1].xaxis.set_major_formatter(FormatStrFormatter('%0.1f')) plt.tight_layout() # plt.savefig('./figures/half_moon_3.png', dpi=300) two classes (circles and triangles) are linearly well separated","title":"Example 1: Separating half-moon shapes"},{"location":"w4-feature-engineering/4w/#example-2-separating-concentric-circles","text":"[ back to top ] from sklearn.datasets import make_circles X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2) plt.scatter(X[y==0, 0], X[y==0, 1], color='red', marker='^', alpha=0.5) plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', marker='o', alpha=0.5) plt.tight_layout() # plt.savefig('./figures/circles_1.png', dpi=300) # standard PCA scaler = StandardScaler() X_std = scaler.fit_transform(X) scikit_pca = PCA(n_components=2) X_spca = scikit_pca.fit_transform(X_std) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_spca[y==0, 0], np.zeros((500,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_spca[y==1, 0], np.zeros((500,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') plt.tight_layout() # plt.savefig('./figures/circles_2.png', dpi=300) # kernel RBF X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2) fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(7,3)) ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1], color='red', marker='^', alpha=0.5) ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1], color='blue', marker='o', alpha=0.5) ax[1].scatter(X_kpca[y==0, 0], np.zeros((500,1)), color='red', marker='^', alpha=0.5) ax[1].scatter(X_kpca[y==1, 0], np.zeros((500,1)), color='blue', marker='o', alpha=0.5) ax[0].set_xlabel('PC1') ax[0].set_ylabel('PC2') ax[1].set_ylim([-1, 1]) ax[1].set_yticks([]) ax[1].set_xlabel('PC1') plt.tight_layout() # plt.savefig('./figures/circles_3.png', dpi=300)","title":"Example 2: Separating concentric circles"},{"location":"w4-feature-engineering/4w/#projecting-new-data-points","text":"learn how to project data points that were not part of the training dataset [ back to top ] from scipy.spatial.distance import pdist, squareform from scipy import exp from scipy.linalg import eigh import numpy as np def rbf_kernel_pca(X, gamma, n_components): RBF kernel PCA implementation. Parameters ------------ X: {NumPy ndarray}, shape = [n_samples, n_features] gamma: float Tuning parameter of the RBF kernel n_components: int Number of principal components to return Returns ------------ X_pc: {NumPy ndarray}, shape = [n_samples, k_features] Projected dataset lambdas: list Eigenvalues # Calculate pairwise squared Euclidean distances # in the MxN dimensional dataset. sq_dists = pdist(X, 'sqeuclidean') # Convert pairwise distances into a square matrix. mat_sq_dists = squareform(sq_dists) # Compute the symmetric kernel matrix. K = exp(-gamma * mat_sq_dists) # Center the kernel matrix. N = K.shape[0] one_n = np.ones((N,N)) / N K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n) # Obtaining eigenpairs from the centered kernel matrix # numpy.eigh returns them in sorted order eigvals, eigvecs = eigh(K) # Collect the top k eigenvectors (projected samples) alphas = np.column_stack((eigvecs[:,-i] for i in range(1,n_components+1))) # Collect the corresponding eigenvalues lambdas = [eigvals[-i] for i in range(1,n_components+1)] return alphas, lambdas X, y = make_moons(n_samples=100, random_state=123) alphas, lambdas = rbf_kernel_pca(X, gamma=15, n_components=1) x_new = X[25] x_new array([ 1.8713, 0.0093]) x_proj = alphas[25] # original projection x_proj array([ 0.0788]) def project_x(x_new, X, gamma, alphas, lambdas): pair_dist = np.array([np.sum((x_new-row)**2) for row in X]) k = np.exp(-gamma * pair_dist) return k.dot(alphas / lambdas) # projection of the new datapoint x_reproj = project_x(x_new, X, gamma=15, alphas=alphas, lambdas=lambdas) x_reproj array([ 0.0788]) plt.scatter(alphas[y==0, 0], np.zeros((50)), color='red', marker='^',alpha=0.5) plt.scatter(alphas[y==1, 0], np.zeros((50)), color='blue', marker='o', alpha=0.5) plt.scatter(x_proj, 0, color='black', label='original projection of point X[25]', marker='^', s=100) plt.scatter(x_reproj, 0, color='green', label='remapped point X[25]', marker='x', s=500) plt.legend(scatterpoints=1) plt.tight_layout() # plt.savefig('./figures/reproject.png', dpi=300) project correctly","title":"Projecting new data points"},{"location":"w4-feature-engineering/4w/#kernel-principal-component-analysis-in-scikit-learn","text":"[ back to top ] from sklearn.decomposition import KernelPCA X, y = make_moons(n_samples=100, random_state=123) scikit_kpca = KernelPCA(n_components=2, kernel='rbf', gamma=15) X_skernpca = scikit_kpca.fit_transform(X) plt.scatter(X_skernpca[y==0, 0], X_skernpca[y==0, 1], color='red', marker='^', alpha=0.5) plt.scatter(X_skernpca[y==1, 0], X_skernpca[y==1, 1], color='blue', marker='o', alpha=0.5) plt.xlabel('PC1') plt.ylabel('PC2') plt.tight_layout() # plt.savefig('./figures/scikit_kpca.png', dpi=300)","title":"Kernel principal component analysis in scikit-learn"},{"location":"w4-feature-engineering/4w/#checklist","text":"Do you have domain knowledge? If yes, construct a better set of ad hoc\u201d\u201d features Are your features commensurate? If no, consider normalizing them. Do you suspect interdependence of features? If yes, expand your feature set by constructing conjunctive features or products of features, as much as your computer resources allow you. Do you need to prune the input variables (e.g. for cost, speed or data understanding reasons)? If no, construct disjunctive features or weighted sums of feature Do you need to assess features individually (e.g. to understand their influence on the system or because their number is so large that you need to do a first filtering)? If yes, use a variable ranking method; else, do it anyway to get baseline results. Do you need a predictor? If no, stop Do you suspect your data is \u201cdirty\u201d (has a few meaningless input patterns and/or noisy outputs or wrong class labels)? If yes, detect the outlier examples using the top ranking variables obtained in step 5 as representation; check and/or discard them. Do you know what to try first? If no, use a linear predictor. Use a forward selection method with the \u201cprobe\u201d method as a stopping criterion or use the 0-norm embedded method for comparison, following the ranking of step 5, construct a sequence of predictors of same nature using increasing subsets of features. Can you match or improve performance with a smaller subset? If yes, try a non-linear predictor with that subset. Do you have new ideas, time, computational resources, and enough examples? If yes, compare several feature selection methods, including your new idea, correlation coefficients, backward selection and embedded methods. Use linear and non-linear predictors. Select the best approach with model selection Do you want a stable solution (to improve performance and/or understanding)? If yes, subsample your data and redo your analysis for several \u201cbootstrap\u201d.","title":"\u7279\u5f81\u5de5\u7a0bchecklist"},{"location":"w4-feature-engineering/4w/#_1","text":"[ back to top ] from sklearn import datasets from sklearn import cross_validation from sklearn import linear_model from sklearn import metrics from sklearn import tree from sklearn import neighbors from sklearn import svm from sklearn import ensemble from sklearn import cluster import matplotlib.pyplot as plt %matplotlib inline import numpy as np import seaborn as sns np.random.seed(123) # \u6784\u5efa dataset, 50\u4e2a sample, 50\u4e2a feature X_all, y_all = datasets.make_regression(n_samples=50, n_features=50, n_informative=10) # 50% train, 50% test X_train, X_test, y_train, y_test = cross_validation.train_test_split(X_all, y_all, train_size=0.5) X_train.shape, y_train.shape ((25, 50), (25,)) X_test.shape, y_test.shape ((25, 50), (25,))","title":"\u4e00\u4e2a\u4f7f\u7528\u6b63\u5219\u5316\u65b9\u6cd5\u8fdb\u884c\u53d8\u91cf\u9009\u62e9\u7684\u4f8b\u5b50"},{"location":"w4-feature-engineering/4w/#linear-regression","text":"$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b - y_i||^2 $$ [ back to top ] # linear reg model = linear_model.LinearRegression() model.fit(X_train, y_train) /Users/alan/anaconda/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver. warnings.warn(mesg, RuntimeWarning) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False) def sse(resid): return sum(resid**2) # \u8ba1\u7b97 train data \u7684 SSE resid_train = y_train - model.predict(X_train) sse_train = sse(resid_train) sse_train 7.9634561748974877e-25 # \u9884\u6d4b test \u518d\u8ba1\u7b97 test data \u7684SSE resid_test = y_test - model.predict(X_test) sse_test = sse(resid_test) sse_test 213555.61203039085 \u7ed3\u679c test data \u663e\u793a\u9884\u6d4b\u6548\u679c\u5f88\u5dee, \u53ef\u80fd overfitting model.score(X_train, y_train) 1.0 model.score(X_test, y_test) 0.31407400675201724 def plot_residuals_and_coeff(resid_train, resid_test, coeff): fig, axes = plt.subplots(1, 3, figsize=(12, 3)) axes[0].bar(np.arange(len(resid_train)), resid_train) axes[0].set_xlabel( sample number ) axes[0].set_ylabel( residual ) axes[0].set_title( training data ) axes[1].bar(np.arange(len(resid_test)), resid_test) axes[1].set_xlabel( sample number ) axes[1].set_ylabel( residual ) axes[1].set_title( testing data ) axes[2].bar(np.arange(len(coeff)), coeff) axes[2].set_xlabel( coefficient number ) axes[2].set_ylabel( coefficient ) fig.tight_layout() return fig, axes # \u753b\u51fa residual fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_);","title":"Linear Regression"},{"location":"w4-feature-engineering/4w/#ridge-regression","text":"L2 penalized, add squared sum of the weights to least-squares cost function $$ \\text{min}_{w,b} \\sum_i || w^\\mathsf{T}x_i + b - y_i||^2 + \\alpha ||w||_2^2$$ [ back to top ] # \u4f7f\u7528 Ridge \u6b63\u5219\u5316 model = linear_model.Ridge(alpha=5) model.fit(X_train, y_train) Ridge(alpha=5, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001) resid_train = y_train - model.predict(X_train) sse_train = sum(resid_train**2) sse_train 3292.9620358692705 resid_test = y_test - model.predict(X_test) sse_test = sum(resid_test**2) sse_test 209557.58585055024 train data\u7684 SSE \u63d0\u5347\u5f88\u591a # test model score \u4ecd\u7136\u4e0d\u9ad8 model.score(X_train, y_train), model.score(X_test, y_test) (0.99003021243324718, 0.32691539290134652) fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_)","title":"Ridge Regression"},{"location":"w4-feature-engineering/4w/#lasso-regression","text":"L1-norm certain weights can become zero, useful as a supervised feature selection technique. $$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b - y_i||^2 + \\alpha ||w||_1$$ [ back to top ] model = linear_model.Lasso(alpha=1.0) model.fit(X_train, y_train) Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection='cyclic', tol=0.0001, warm_start=False) resid_train = y_train - model.predict(X_train) sse_train = sse(resid_train) sse_train 309.74971389532328 resid_test = y_test - model.predict(X_test) sse_test = sse(resid_test) sse_test 1489.117606500263 \u76f8\u8f83 Ridge, SSE \u90fd\u51cf\u5c11\u5f88\u591a fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_) \u4e0a\u56fe\u53d1\u73b0, coeff \u6709\u5f88\u591a\u90fd\u662f0 alphas = np.logspace(-4, 2, 100) # \u5bfb\u627e LASSO \u7684\u6700\u4f18\u53c2\u6570 alpha coeffs = np.zeros((len(alphas), X_train.shape[1])) sse_train = np.zeros_like(alphas) sse_test = np.zeros_like(alphas) for n, alpha in enumerate(alphas): model = linear_model.Lasso(alpha=alpha) model.fit(X_train, y_train) coeffs[n, :] = model.coef_ resid = y_train - model.predict(X_train) sse_train[n] = sum(resid**2) resid = y_test - model.predict(X_test) sse_test[n] = sum(resid**2) /Users/alan/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:466: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations ConvergenceWarning) fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharex=True) for n in range(coeffs.shape[1]): axes[0].plot(np.log10(alphas), coeffs[:, n], color='k', lw=0.5) axes[1].semilogy(np.log10(alphas), sse_train, label= train ) axes[1].semilogy(np.log10(alphas), sse_test, label= test ) axes[1].legend(loc=0) axes[0].set_xlabel(r ${\\log_{10}}\\alpha$ , fontsize=18) axes[0].set_ylabel(r coefficients , fontsize=18) axes[1].set_xlabel(r ${\\log_{10}}\\alpha$ , fontsize=18) axes[1].set_ylabel(r sse , fontsize=18) fig.tight_layout() alpha \u8d8a\u5927, coeff \u6700\u7ec8\u90fd\u4f1a\u53d8\u62100, \u800c train SSE \u4f1a\u5148\u51cf\u5c0f\u518d\u589e\u52a0, \u800c test \u662f\u4e00\u76f4\u5728\u589e\u52a0. \u5728-1\u9644\u8fd1, train SSE \u6700\u5c0f, \u800c coeff \u5927\u6982\u67098\u4e2a\u4e0d\u662f0. # \u4f7f\u7528LassoCV: Lasso linear model with iterative fitting along a regularization path model = linear_model.LassoCV() model.fit(X_all, y_all) LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True, max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False, precompute='auto', random_state=None, selection='cyclic', tol=0.0001, verbose=False) # \u8ba1\u7b97\u51fa\u7684\u6700\u4f73 alphs model.alpha_ 0.06559238747534718 resid_train = y_train - model.predict(X_train) sse_train = sse(resid_train) sse_train 1.5450589323148352 resid_test = y_test - model.predict(X_test) sse_test = sse(resid_test) sse_test 1.5321417406216176 \u53d1\u73b0 SSE \u90fd\u5df2\u7ecf\u6bd4\u8f83\u63a5\u8fd10\u4e86 model.score(X_train, y_train), model.score(X_test, y_test) # score \u90fd\u5f88\u9ad8 (0.99999532217220677, 0.99999507886570982) fig, ax = plot_residuals_and_coeff(resid_train, resid_test, model.coef_) # 9\u4e2a non-zero coeff","title":"LASSO Regression"},{"location":"w4-feature-engineering/4w/#_2","text":"(#sections)","title":"\u7ec3\u4e60\uff1a\u5229\u7528\u672c\u7ae0\u5b66\u5230\u7684\u65b9\u6cd5\u5bf9\u4fe1\u8d37\u6570\u636e\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b"},{"location":"w5-tuning-parameter/5w/","text":"Sections Loading the Breast Cancer Wisconsin dataset Streamlining workflows with pipelines Model evaluation The holdout method K-fold cross validation Stratified k-fold cross validation Learning and validation curves Diagnosing bias and variance problems with learning curves Addressing overfitting and underfitting with validation curves Grid search Tuning hyperparameters via grid search Randomized search Model selection with nested cross-validation Loading the Breast Cancer Wisconsin dataset Breast Cancer Wisconsin \u6570\u636e\u5305\u62ec 569 \u4f8b\u826f\u6027\u6216\u6076\u6027\u764c\u7ec6\u80de\u6837\u672c \u6570\u636e\u524d\u4e24\u5217\u662f\u6837\u672c ID \u53ca\u8bca\u65ad (M for malignant \u6076\u6027, B for benigh \u826f\u6027) \u540e\u9762 30 \u5217\u662f\u7ec6\u80de\u6838\u7684\u56fe\u7247\u7684\u6570\u636e [ back to top ] import pandas as pd df = pd.read_csv('data/wdbc.data', header=None) df.head() 0 1 2 3 4 5 6 7 8 9 ... 22 23 24 25 26 27 28 29 30 31 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 ... 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 ... 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 ... 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 ... 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 ... 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 5 rows \u00d7 32 columns df.shape (569, 32) # \u6570\u636e\u9884\u5904\u7406 from sklearn.preprocessing import LabelEncoder X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) le.transform(['M', 'B']) # M- 1 B- 0 array([1, 0]) # 80% train, 20% test from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.20, random_state=1) Streamlining workflows with pipelines fit a model including an arbitrary number of transformation steps and apply it to make predictions about new data. [ back to top ] # chain the StandardScaler, PCA, and LogisticRegression objects in a pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline pipe_lr = Pipeline([('scl', StandardScaler()), # \u6807\u51c6\u5316\u539f\u59cb\u6570\u636e ('pca', PCA(n_components=2)), # PCA \u964d\u7ef4 ('clf', LogisticRegression(random_state=1))]) pipe_lr.fit(X_train, y_train) print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test)) y_pred = pipe_lr.predict(X_test) Test Accuracy: 0.947 Model evaluation [ back to top ] The holdout method \u5c06\u6570\u636e\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206 + training set \u7528\u4e8e\u8bad\u7ec3\u6a21\u578b + validation set \u7528\u4e8e\u6a21\u578b\u9009\u62e9\u548c\u8c03\u53c2 + test set \u7528\u4e8e\u8bc4\u4f30\u6700\u7ec8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b [ back to top ] K-fold cross validation \u91cd\u590d hold out method k \u6b21. \u4fdd\u7559 test set\uff0c\u5269\u4e0b\u6570\u636e\u968f\u673a\u5206\u4e3a k \u7ec4, \u5c06\u5176\u4e2d\u4e00\u7ec4\u7559\u4f5c validation set, \u5176\u4f59\u505a training data, \u66f4\u6362 validation \u7ec4\u91cd\u590d\u8bad\u7ec3 k \u6b21 [ back to top ] import numpy as np from sklearn.cross_validation import KFold kfold = KFold(n=len(X_train), n_folds=10, random_state=1) scores = [] for k, (train, test) in enumerate(kfold): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(score) print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, np.bincount(y_train[train]), score)) print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) Fold: 1, Class dist.: [256 153], Acc: 0.891 Fold: 2, Class dist.: [254 155], Acc: 0.957 Fold: 3, Class dist.: [258 151], Acc: 0.978 Fold: 4, Class dist.: [257 152], Acc: 0.913 Fold: 5, Class dist.: [255 154], Acc: 0.935 Fold: 6, Class dist.: [258 152], Acc: 0.978 Fold: 7, Class dist.: [257 153], Acc: 0.933 Fold: 8, Class dist.: [254 156], Acc: 0.956 Fold: 9, Class dist.: [259 151], Acc: 0.978 Fold: 10, Class dist.: [257 153], Acc: 0.956 CV accuracy: 0.947 +/- 0.028 Stratified k-fold cross validation Stratified k-fold CV \u65b9\u6cd5\u5728\u5207\u5206\u6570\u636e\u65f6\uff0c\u4f1a\u5c3d\u91cf\u4fdd\u6301\u5404\u6807\u7b7e\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u6a21\u578b\u6548\u679c\u8bc4\u4f30 [ back to top ] from sklearn.cross_validation import StratifiedKFold kfold = StratifiedKFold(y=y_train, n_folds=10, random_state=1) scores = [] for k, (train, test) in enumerate(kfold): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(score) print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, np.bincount(y_train[train]), score)) print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) Fold: 1, Class dist.: [256 153], Acc: 0.891 Fold: 2, Class dist.: [256 153], Acc: 0.978 Fold: 3, Class dist.: [256 153], Acc: 0.978 Fold: 4, Class dist.: [256 153], Acc: 0.913 Fold: 5, Class dist.: [256 153], Acc: 0.935 Fold: 6, Class dist.: [257 153], Acc: 0.978 Fold: 7, Class dist.: [257 153], Acc: 0.933 Fold: 8, Class dist.: [257 153], Acc: 0.956 Fold: 9, Class dist.: [257 153], Acc: 0.978 Fold: 10, Class dist.: [257 153], Acc: 0.956 CV accuracy: 0.950 +/- 0.029 sklearn \u91cc cross_val_score \u51fd\u6570\u9ed8\u8ba4\u4f7f\u7528 stratified k-fold CV from sklearn.cross_validation import cross_val_score scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=-1) # \u4f7f\u7528 CPUs \u7684\u5185\u6838\u6570 print('CV accuracy scores:\\n %s' % scores) print('CV accuracy:\\n %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) CV accuracy scores: [ 0.89130435 0.97826087 0.97826087 0.91304348 0.93478261 0.97777778 0.93333333 0.95555556 0.97777778 0.95555556] CV accuracy: 0.950 +/- 0.029 Learning and validation curves diagnose if a learning algorithm has a problem with overfitting (high variance) or underfitting (high bias) [ back to top ] Diagnosing bias and variance problems with learning curves plotting the training and test accuracies as functions of the sample size [ back to top ] # \u753b learning curve, Accuracy \u4e0e training sample size\u7684\u5173\u7cfb %matplotlib inline import matplotlib.pyplot as plt from sklearn.learning_curve import learning_curve pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(penalty='l2', C=0.1, random_state=0))]) # learning_curve \u4e2d\u7684 scores \u901a\u8fc7 stratified k-fold CV \u83b7\u5f97 train_sizes, train_scores, test_scores =\\ learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), # \u8c03\u6574\u8bad\u7ec3\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf cv=10, n_jobs=-1) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) # plot train_mean plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy') # plot train_std plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue') # plot test_mean plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy') # plot test_std plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green') plt.grid() plt.xlabel('Number of training samples') plt.ylabel('Accuracy') plt.legend(loc='lower right') plt.ylim([0.9, 1.0]) plt.tight_layout() # plt.savefig('./figures/learning_curve.png', dpi=300) Addressing overfitting and underfitting with validation curves plotting the training and test accuracies as functions of the model parameters [ back to top ] # validation curve, useful tool for improving the performance of a model from sklearn.learning_curve import validation_curve # \u8bbe\u5b9a\u53c2\u6570\u9009\u9879 param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] # \u901a\u8fc7 CV \u83b7\u5f97\u4e0d\u540c\u53c2\u6570\u7684\u6a21\u578b\u6548\u679c train_scores, test_scores = \\ validation_curve(estimator=pipe_lr, X=X_train, y=y_train, param_name='clf__C', # \u7528 pipe_lr.get_params() \u627e\u5230\u53c2\u6570\u5bf9\u5e94\u7684\u540d\u79f0 param_range=param_range, cv=10) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) plt.plot(param_range, train_mean, color='blue', marker='o', markersize=5, label='training accuracy') plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue') plt.plot(param_range, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy') plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green') plt.grid() plt.xscale('log') plt.legend(loc='lower right') plt.xlabel('Parameter C') plt.ylabel('Accuracy') plt.ylim([0.9, 1.0]) plt.tight_layout() # plt.savefig('./figures/validation_curve.png', dpi=300) \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u968f\u7740 C \u589e\u52a0 (regularization \u51cf\u5c0f)\uff0c\u6a21\u578b\u7531 underfit - optimal - overfit \u6700\u4f73 C \u53c2\u6570\u503c\u5e94\u9009\u7528 0.1 Grid search [ back to top ] Tuning hyperparameters via grid search finding the optimal combination of hyperparameter values. [ back to top ] # brute-force exhaustive search, \u904d\u5386 from sklearn.grid_search import GridSearchCV from sklearn.svm import SVC svc = SVC(random_state=1) param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] param_grid = {'C': param_range} gs = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1) # use all CPU gs = gs.fit(X_train, y_train) print(gs.best_score_) # validation accuracy best print(gs.best_params_) 0.626373626374 {'C': 0.0001} \u7ed3\u5408 pipeline \u548c grid search pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))]) # linear SVM: inverse regularization parameter C # RBF kernel SVM: both C and gamma parameter param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] param_grid = [{'clf__C': param_range, 'clf__kernel': ['linear']}, {'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']}] gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1) # use all CPU gs = gs.fit(X_train, y_train) print(gs.best_score_) # validation accuracy best print(gs.best_params_) 0.978021978022 {'clf__C': 0.1, 'clf__kernel': 'linear'} # \u770b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6548\u679c clf = gs.best_estimator_ clf.fit(X_train, y_train) print('Test accuracy: %.3f' % clf.score(X_test, y_test)) Test accuracy: 0.965 Randomized search Although grid search is a powerful approach for finding the optimal set of parameters, the evaluation of all possible parameter combinations is also computationally very expensive. An alternative approach to sampling different parameter combinations using scikit-learn is randomized search . [ back to top ] from scipy.stats import expon from sklearn.grid_search import RandomizedSearchCV pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))]) param_dist = {'clf__C': expon(scale=100), 'clf__gamma': expon(scale=0.1), 'clf__kernel': ['rbf']} np.random.seed(0) rs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_dist, n_iter=20, scoring='accuracy', cv=10, n_jobs=-1) rs = rs.fit(X_train, y_train) print(rs.best_score_) print(rs.best_params_) 0.975824175824 {'clf__gamma': 0.009116102911900048, 'clf__C': 7.368535491284788, 'clf__kernel': 'rbf'} clf = rs.best_estimator_ clf.fit(X_train, y_train) print('Test accuracy: %.3f' % clf.score(X_test, y_test)) Test accuracy: 0.974 Model selection with nested cross-validation [ back to top ] \u7528 nested cross validation \u6765\u6bd4\u8f83 SVM \u548c decision tree \u6a21\u578b gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=2) scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5) print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) CV accuracy: 0.965 +/- 0.025 from sklearn.tree import DecisionTreeClassifier gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}], scoring='accuracy', cv=2) scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5) print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) CV accuracy: 0.921 +/- 0.029 \u4ece\u7ed3\u679c\u6765\u770b\uff0c\u5e94\u8be5\u9009\u7528 SVM \u6a21\u578b \u7ec3\u4e601\uff1a\u81ea\u5df1\u6765\u5199\u4e00\u4e2a\u51fd\u6570\uff0c\u5c06\u6570\u636e\u5206\u6210\u4e24\u4e2a\u90e8\u5206 \u7ec3\u4e602\uff1a\u4f7f\u7528\u4fe1\u8d37\u6570\u636e\u96c6\uff0c\u5c1d\u8bd5\u53c2\u6570\u8c03\u4f18 (#sections)","title":"5w"},{"location":"w5-tuning-parameter/5w/#sections","text":"Loading the Breast Cancer Wisconsin dataset Streamlining workflows with pipelines Model evaluation The holdout method K-fold cross validation Stratified k-fold cross validation Learning and validation curves Diagnosing bias and variance problems with learning curves Addressing overfitting and underfitting with validation curves Grid search Tuning hyperparameters via grid search Randomized search Model selection with nested cross-validation","title":"Sections"},{"location":"w5-tuning-parameter/5w/#loading-the-breast-cancer-wisconsin-dataset","text":"Breast Cancer Wisconsin \u6570\u636e\u5305\u62ec 569 \u4f8b\u826f\u6027\u6216\u6076\u6027\u764c\u7ec6\u80de\u6837\u672c \u6570\u636e\u524d\u4e24\u5217\u662f\u6837\u672c ID \u53ca\u8bca\u65ad (M for malignant \u6076\u6027, B for benigh \u826f\u6027) \u540e\u9762 30 \u5217\u662f\u7ec6\u80de\u6838\u7684\u56fe\u7247\u7684\u6570\u636e [ back to top ] import pandas as pd df = pd.read_csv('data/wdbc.data', header=None) df.head() 0 1 2 3 4 5 6 7 8 9 ... 22 23 24 25 26 27 28 29 30 31 0 842302 M 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 ... 25.38 17.33 184.60 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 1 842517 M 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 ... 24.99 23.41 158.80 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 2 84300903 M 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 ... 23.57 25.53 152.50 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 3 84348301 M 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 ... 14.91 26.50 98.87 567.7 0.2098 0.8663 0.6869 0.2575 0.6638 0.17300 4 84358402 M 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 ... 22.54 16.67 152.20 1575.0 0.1374 0.2050 0.4000 0.1625 0.2364 0.07678 5 rows \u00d7 32 columns df.shape (569, 32) # \u6570\u636e\u9884\u5904\u7406 from sklearn.preprocessing import LabelEncoder X = df.loc[:, 2:].values y = df.loc[:, 1].values le = LabelEncoder() y = le.fit_transform(y) le.transform(['M', 'B']) # M- 1 B- 0 array([1, 0]) # 80% train, 20% test from sklearn.cross_validation import train_test_split X_train, X_test, y_train, y_test = \\ train_test_split(X, y, test_size=0.20, random_state=1)","title":"Loading the Breast Cancer Wisconsin dataset"},{"location":"w5-tuning-parameter/5w/#streamlining-workflows-with-pipelines","text":"fit a model including an arbitrary number of transformation steps and apply it to make predictions about new data. [ back to top ] # chain the StandardScaler, PCA, and LogisticRegression objects in a pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline pipe_lr = Pipeline([('scl', StandardScaler()), # \u6807\u51c6\u5316\u539f\u59cb\u6570\u636e ('pca', PCA(n_components=2)), # PCA \u964d\u7ef4 ('clf', LogisticRegression(random_state=1))]) pipe_lr.fit(X_train, y_train) print('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test)) y_pred = pipe_lr.predict(X_test) Test Accuracy: 0.947","title":"Streamlining workflows with pipelines"},{"location":"w5-tuning-parameter/5w/#model-evaluation","text":"[ back to top ]","title":"Model evaluation"},{"location":"w5-tuning-parameter/5w/#the-holdout-method","text":"\u5c06\u6570\u636e\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206 + training set \u7528\u4e8e\u8bad\u7ec3\u6a21\u578b + validation set \u7528\u4e8e\u6a21\u578b\u9009\u62e9\u548c\u8c03\u53c2 + test set \u7528\u4e8e\u8bc4\u4f30\u6700\u7ec8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b [ back to top ]","title":"The holdout method"},{"location":"w5-tuning-parameter/5w/#k-fold-cross-validation","text":"\u91cd\u590d hold out method k \u6b21. \u4fdd\u7559 test set\uff0c\u5269\u4e0b\u6570\u636e\u968f\u673a\u5206\u4e3a k \u7ec4, \u5c06\u5176\u4e2d\u4e00\u7ec4\u7559\u4f5c validation set, \u5176\u4f59\u505a training data, \u66f4\u6362 validation \u7ec4\u91cd\u590d\u8bad\u7ec3 k \u6b21 [ back to top ] import numpy as np from sklearn.cross_validation import KFold kfold = KFold(n=len(X_train), n_folds=10, random_state=1) scores = [] for k, (train, test) in enumerate(kfold): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(score) print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, np.bincount(y_train[train]), score)) print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) Fold: 1, Class dist.: [256 153], Acc: 0.891 Fold: 2, Class dist.: [254 155], Acc: 0.957 Fold: 3, Class dist.: [258 151], Acc: 0.978 Fold: 4, Class dist.: [257 152], Acc: 0.913 Fold: 5, Class dist.: [255 154], Acc: 0.935 Fold: 6, Class dist.: [258 152], Acc: 0.978 Fold: 7, Class dist.: [257 153], Acc: 0.933 Fold: 8, Class dist.: [254 156], Acc: 0.956 Fold: 9, Class dist.: [259 151], Acc: 0.978 Fold: 10, Class dist.: [257 153], Acc: 0.956 CV accuracy: 0.947 +/- 0.028","title":"K-fold cross validation"},{"location":"w5-tuning-parameter/5w/#stratified-k-fold-cross-validation","text":"Stratified k-fold CV \u65b9\u6cd5\u5728\u5207\u5206\u6570\u636e\u65f6\uff0c\u4f1a\u5c3d\u91cf\u4fdd\u6301\u5404\u6807\u7b7e\u7684\u6bd4\u4f8b\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u6a21\u578b\u6548\u679c\u8bc4\u4f30 [ back to top ] from sklearn.cross_validation import StratifiedKFold kfold = StratifiedKFold(y=y_train, n_folds=10, random_state=1) scores = [] for k, (train, test) in enumerate(kfold): pipe_lr.fit(X_train[train], y_train[train]) score = pipe_lr.score(X_train[test], y_train[test]) scores.append(score) print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, np.bincount(y_train[train]), score)) print('\\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) Fold: 1, Class dist.: [256 153], Acc: 0.891 Fold: 2, Class dist.: [256 153], Acc: 0.978 Fold: 3, Class dist.: [256 153], Acc: 0.978 Fold: 4, Class dist.: [256 153], Acc: 0.913 Fold: 5, Class dist.: [256 153], Acc: 0.935 Fold: 6, Class dist.: [257 153], Acc: 0.978 Fold: 7, Class dist.: [257 153], Acc: 0.933 Fold: 8, Class dist.: [257 153], Acc: 0.956 Fold: 9, Class dist.: [257 153], Acc: 0.978 Fold: 10, Class dist.: [257 153], Acc: 0.956 CV accuracy: 0.950 +/- 0.029 sklearn \u91cc cross_val_score \u51fd\u6570\u9ed8\u8ba4\u4f7f\u7528 stratified k-fold CV from sklearn.cross_validation import cross_val_score scores = cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, cv=10, n_jobs=-1) # \u4f7f\u7528 CPUs \u7684\u5185\u6838\u6570 print('CV accuracy scores:\\n %s' % scores) print('CV accuracy:\\n %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) CV accuracy scores: [ 0.89130435 0.97826087 0.97826087 0.91304348 0.93478261 0.97777778 0.93333333 0.95555556 0.97777778 0.95555556] CV accuracy: 0.950 +/- 0.029","title":"Stratified k-fold cross validation"},{"location":"w5-tuning-parameter/5w/#learning-and-validation-curves","text":"diagnose if a learning algorithm has a problem with overfitting (high variance) or underfitting (high bias) [ back to top ]","title":"Learning and validation curves"},{"location":"w5-tuning-parameter/5w/#diagnosing-bias-and-variance-problems-with-learning-curves","text":"plotting the training and test accuracies as functions of the sample size [ back to top ] # \u753b learning curve, Accuracy \u4e0e training sample size\u7684\u5173\u7cfb %matplotlib inline import matplotlib.pyplot as plt from sklearn.learning_curve import learning_curve pipe_lr = Pipeline([('scl', StandardScaler()), ('clf', LogisticRegression(penalty='l2', C=0.1, random_state=0))]) # learning_curve \u4e2d\u7684 scores \u901a\u8fc7 stratified k-fold CV \u83b7\u5f97 train_sizes, train_scores, test_scores =\\ learning_curve(estimator=pipe_lr, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10), # \u8c03\u6574\u8bad\u7ec3\u96c6\u4e2d\u6837\u672c\u7684\u6570\u91cf cv=10, n_jobs=-1) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) # plot train_mean plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy') # plot train_std plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue') # plot test_mean plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy') # plot test_std plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green') plt.grid() plt.xlabel('Number of training samples') plt.ylabel('Accuracy') plt.legend(loc='lower right') plt.ylim([0.9, 1.0]) plt.tight_layout() # plt.savefig('./figures/learning_curve.png', dpi=300)","title":"Diagnosing bias and variance problems with learning curves"},{"location":"w5-tuning-parameter/5w/#addressing-overfitting-and-underfitting-with-validation-curves","text":"plotting the training and test accuracies as functions of the model parameters [ back to top ] # validation curve, useful tool for improving the performance of a model from sklearn.learning_curve import validation_curve # \u8bbe\u5b9a\u53c2\u6570\u9009\u9879 param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] # \u901a\u8fc7 CV \u83b7\u5f97\u4e0d\u540c\u53c2\u6570\u7684\u6a21\u578b\u6548\u679c train_scores, test_scores = \\ validation_curve(estimator=pipe_lr, X=X_train, y=y_train, param_name='clf__C', # \u7528 pipe_lr.get_params() \u627e\u5230\u53c2\u6570\u5bf9\u5e94\u7684\u540d\u79f0 param_range=param_range, cv=10) train_mean = np.mean(train_scores, axis=1) train_std = np.std(train_scores, axis=1) test_mean = np.mean(test_scores, axis=1) test_std = np.std(test_scores, axis=1) plt.plot(param_range, train_mean, color='blue', marker='o', markersize=5, label='training accuracy') plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue') plt.plot(param_range, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy') plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green') plt.grid() plt.xscale('log') plt.legend(loc='lower right') plt.xlabel('Parameter C') plt.ylabel('Accuracy') plt.ylim([0.9, 1.0]) plt.tight_layout() # plt.savefig('./figures/validation_curve.png', dpi=300) \u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u968f\u7740 C \u589e\u52a0 (regularization \u51cf\u5c0f)\uff0c\u6a21\u578b\u7531 underfit - optimal - overfit \u6700\u4f73 C \u53c2\u6570\u503c\u5e94\u9009\u7528 0.1","title":"Addressing overfitting and underfitting with validation curves"},{"location":"w5-tuning-parameter/5w/#grid-search","text":"[ back to top ]","title":"Grid search"},{"location":"w5-tuning-parameter/5w/#tuning-hyperparameters-via-grid-search","text":"finding the optimal combination of hyperparameter values. [ back to top ] # brute-force exhaustive search, \u904d\u5386 from sklearn.grid_search import GridSearchCV from sklearn.svm import SVC svc = SVC(random_state=1) param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] param_grid = {'C': param_range} gs = GridSearchCV(estimator=svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1) # use all CPU gs = gs.fit(X_train, y_train) print(gs.best_score_) # validation accuracy best print(gs.best_params_) 0.626373626374 {'C': 0.0001} \u7ed3\u5408 pipeline \u548c grid search pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))]) # linear SVM: inverse regularization parameter C # RBF kernel SVM: both C and gamma parameter param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] param_grid = [{'clf__C': param_range, 'clf__kernel': ['linear']}, {'clf__C': param_range, 'clf__gamma': param_range, 'clf__kernel': ['rbf']}] gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, n_jobs=-1) # use all CPU gs = gs.fit(X_train, y_train) print(gs.best_score_) # validation accuracy best print(gs.best_params_) 0.978021978022 {'clf__C': 0.1, 'clf__kernel': 'linear'} # \u770b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6548\u679c clf = gs.best_estimator_ clf.fit(X_train, y_train) print('Test accuracy: %.3f' % clf.score(X_test, y_test)) Test accuracy: 0.965","title":"Tuning hyperparameters via grid search"},{"location":"w5-tuning-parameter/5w/#randomized-search","text":"Although grid search is a powerful approach for finding the optimal set of parameters, the evaluation of all possible parameter combinations is also computationally very expensive. An alternative approach to sampling different parameter combinations using scikit-learn is randomized search . [ back to top ] from scipy.stats import expon from sklearn.grid_search import RandomizedSearchCV pipe_svc = Pipeline([('scl', StandardScaler()), ('clf', SVC(random_state=1))]) param_dist = {'clf__C': expon(scale=100), 'clf__gamma': expon(scale=0.1), 'clf__kernel': ['rbf']} np.random.seed(0) rs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_dist, n_iter=20, scoring='accuracy', cv=10, n_jobs=-1) rs = rs.fit(X_train, y_train) print(rs.best_score_) print(rs.best_params_) 0.975824175824 {'clf__gamma': 0.009116102911900048, 'clf__C': 7.368535491284788, 'clf__kernel': 'rbf'} clf = rs.best_estimator_ clf.fit(X_train, y_train) print('Test accuracy: %.3f' % clf.score(X_test, y_test)) Test accuracy: 0.974","title":"Randomized search"},{"location":"w5-tuning-parameter/5w/#model-selection-with-nested-cross-validation","text":"[ back to top ] \u7528 nested cross validation \u6765\u6bd4\u8f83 SVM \u548c decision tree \u6a21\u578b gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=2) scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5) print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) CV accuracy: 0.965 +/- 0.025 from sklearn.tree import DecisionTreeClassifier gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}], scoring='accuracy', cv=2) scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5) print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores))) CV accuracy: 0.921 +/- 0.029 \u4ece\u7ed3\u679c\u6765\u770b\uff0c\u5e94\u8be5\u9009\u7528 SVM \u6a21\u578b","title":"Model selection with nested cross-validation"},{"location":"w5-tuning-parameter/5w/#1","text":"","title":"\u7ec3\u4e601\uff1a\u81ea\u5df1\u6765\u5199\u4e00\u4e2a\u51fd\u6570\uff0c\u5c06\u6570\u636e\u5206\u6210\u4e24\u4e2a\u90e8\u5206"},{"location":"w5-tuning-parameter/5w/#2","text":"(#sections)","title":"\u7ec3\u4e602\uff1a\u4f7f\u7528\u4fe1\u8d37\u6570\u636e\u96c6\uff0c\u5c1d\u8bd5\u53c2\u6570\u8c03\u4f18"},{"location":"w6-unsupervised-learning/6w/","text":"\u7b2c6\u7ae0\uff1a\u975e\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5 Sections Some notable clustering routines Grouping objects by similarity using k-means k-means in Sklearn k-means++ Implementing k-means in Python Using the elbow method to find the optimal number of clusters Quantifying the quality of clustering via silhouette plots Organizing clusters as a hierarchical tree Performing hierarchical clustering on a distance matrix Attaching dendrograms to a heat map Applying agglomerative clustering via scikit-learn Applying agglomerative clustering with iris dataset Locating regions of high density via DBSCAN Learning from labeled and unlabeled data with label propagation Clustering is the task of gathering samples into groups of similar samples according to some predefined similarity or distance (dissimilarity) measure, such as the Euclidean distance. Here are some common applications of clustering algorithms: Compression for data reduction Summarizing data as a reprocessing step for recommender systems Similarly: grouping related web news (e.g. Google News) and web search results grouping related stock quotes for investment portfolio management building customer profiles for market analysis Building a code book of prototype samples for unsupervised feature extraction Some notable clustering routines [ back to top ] The following are two well-known clustering algorithms. sklearn.cluster.KMeans : The simplest, yet effective clustering algorithm. Needs to be provided with the number of clusters in advance, and assumes that the data is normalized as input (but use a PCA model as preprocessor). sklearn.cluster.MeanShift : Can find better looking clusters than KMeans but is not scalable to high number of samples. sklearn.cluster.DBSCAN : Can detect irregularly shaped clusters based on density, i.e. sparse regions in the input space are likely to become inter-cluster boundaries. Can also detect outliers (samples that are not part of a cluster). sklearn.cluster.AffinityPropagation : Clustering algorithm based on message passing between data points. sklearn.cluster.SpectralClustering : KMeans applied to a projection of the normalized graph Laplacian: finds normalized graph cuts if the affinity matrix is interpreted as an adjacency matrix of a graph. sklearn.cluster.Ward : Ward implements hierarchical clustering based on the Ward algorithm, a variance-minimizing approach. At each step, it minimizes the sum of squared differences within all clusters (inertia criterion). Of these, Ward, SpectralClustering, DBSCAN and Affinity propagation can also work with precomputed similarity matrices. Grouping objects by similarity using k-means [ back to top ] # make dataset from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=150, n_features=2, centers=3, # \u4ece\u4e09\u7c7b\u4e2d\u62bd\u53d6 cluster_std=0.5, shuffle=True, random_state=0) # \u753b\u4e2a\u56fe\u770b\u770b import matplotlib.pyplot as plt %matplotlib inline plt.scatter(X[:,0], X[:,1], c='white', marker='o', s=50) plt.grid() plt.tight_layout() #plt.savefig('./figures/spheres.png', dpi=300) k-means \u7b97\u6cd5 Randomly pick k centroids from the sample points as initial cluster centers. Assign each sample to the nearest centroid $$\\mu^{(j)}, j\u2208{1,...,k}$$. Move the centroids to the center of the samples that were assigned to it. Repeat the steps 2 and 3 until the cluster assignment do not change or a user-defined tolerance or a maximum number of iterations is reached. Visualizing K-Means Clustering \u5982\u4f55\u6765\u6d4b\u91cf\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6, similarity \u6216\u8005\u5982\u4f55\u8868\u793a\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb, distance \u6700\u5e38\u89c1\u7684\u4e00\u79cd\u8ddd\u79bb\u5ea6\u91cf\u662f squared Euclidean distance: $$ d(x,y)^2 = \\sum_{j=1}^{m} (x_j -y_j)^2 = \\lVert x - y \\rVert_2^2 $$ k-means \u53ef\u8f6c\u5316\u4e3a\u6700\u4f18\u5316\u7684\u95ee\u9898, \u6700\u5c0f\u5316 within-cluster sum of squared errors (SSE), \u53c8\u79f0\u4e3a cluster inertia $$ SSE = \\sum_{i=1}^n\\sum_{j=1}^k w^{(i,j)}\\lVert x^{(i)}-\\mu^{(j)}\\rVert_2^2 $$ \u5176\u4e2d $$\\mu^{(j)}$$ \u662f\u7b2c j \u4e2a\u805a\u7c7b\u7684\u4e2d\u5fc3\uff0c\u5982\u679c\u6837\u672c i \u5728\u805a\u7c7b j \u4e2d\uff0c$$w^{(i,j)}=1$$\uff0c\u5426\u5219 $$w^{(i,j)}=0$$ k-means in Sklearn [ back to top ] # \u4f7f\u7528 sklearn \u4e2d KMeans \u6765\u805a\u7c7b from sklearn.cluster import KMeans km = KMeans(n_clusters=3, # k \u662f\u9700\u8981\u81ea\u5df1\u8bbe\u5b9a\u7684, \u9519\u8bef\u7684 k \u53ef\u80fd\u7ed3\u679c\u5c31\u4e0d\u5bf9 init='random', n_init=10, # \u91cd\u590d\u8fd0\u884c\u7b97\u6cd5 10 \u6b21\uff0c\u9009\u5176\u4e2d\u6700\u597d\u7684\u805a\u7c7b\u6a21\u578b\uff0c\u907f\u514d\u4e0d\u597d\u7684\u521d\u59cb\u5316\u503c\u5e26\u6765\u7684\u5f71\u54cd max_iter=300, tol=1e-04, random_state=0) y_km = km.fit_predict(X) plt.scatter(X[y_km==0,0], X[y_km==0,1], s=50, c='lightgreen', marker='s', label='cluster 1') plt.scatter(X[y_km==1,0], X[y_km==1,1], s=50, c='orange', marker='o', label='cluster 2') plt.scatter(X[y_km==2,0], X[y_km==2,1], s=50, c='lightblue', marker='v', label='cluster 3') plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], s=250, marker='*', c='red', label='centroids') plt.legend() plt.grid() plt.tight_layout() #plt.savefig('./figures/centroids.png', dpi=300) k-means++ [ back to top ] k-means++ \u7b97\u6cd5\u901a\u8fc7\u6539\u5584 centroids \u521d\u59cb\u503c\u7684\u8bbe\u7f6e\uff0c\u6765\u4f18\u5316 k-means Initialize an empty set $$M$$ to store the k centroids being selected Randomly choose the first centroid $$\\mu^{(j)}$$ from the input samples and assign it to $$M$$ For each sample $$x^{(i)}$$ that is not in M, find the minimum squared distance $$d(x^{(i)}, M)^2$$ to any of the centroids in M To randomly select the next centroid $$\\mu^{(p)}$$, use a weighted probability distribution equal to $$\\frac{d(\\mu^{(p)}, M)^2}{\\sum_i d(x^{(i)}, M)^2}$$ Repeat steps 2 and 3 until $$k$$ centroids are chosen Proceed with the classic k-means algorithm sklearn \u91cc\u4f7f\u7528 k-means++ \u7b97\u6cd5\u53ea\u9700\u5728 KMeans() \u91cc\u8bbe\u7f6e init=\"k-means++\" # \u5bf9\u6bd4 k-means \u548c k-means++ import numpy as np from sklearn.utils import shuffle from sklearn.utils import check_random_state from sklearn.cluster import KMeans random_state = np.random.RandomState(0) # Number of run (with randomly generated dataset) for each strategy so as # to be able to compute an estimate of the standard deviation n_runs = 5 # k-means models can do several random inits so as to be able to trade # CPU time for convergence robustness n_init_range = np.array([1, 5, 10, 15, 20]) # Datasets generation parameters n_samples_per_center = 100 grid_size = 3 scale = 0.1 n_clusters = grid_size ** 2 def make_data(random_state, n_samples_per_center, grid_size, scale): random_state = check_random_state(random_state) centers = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)]) n_clusters_true, n_features = centers.shape noise = random_state.normal( scale=scale, size=(n_samples_per_center, centers.shape[1])) X = np.concatenate([c + noise for c in centers]) y = np.concatenate([[i] * n_samples_per_center for i in range(n_clusters_true)]) return shuffle(X, y, random_state=random_state) fig = plt.figure() plots = [] legends = [] cases = [ (KMeans, 'k-means++'), (KMeans, 'random') ] for factory, init in cases: print( Evaluation of %s with %s init % (factory.__name__, init)) inertia = np.empty((len(n_init_range), n_runs)) for run_id in range(n_runs): X_data, y = make_data(run_id, n_samples_per_center, grid_size, scale) for i, n_init in enumerate(n_init_range): kmean = factory(n_clusters=n_clusters, init=init, random_state=run_id, n_init=n_init).fit(X_data) inertia[i, run_id] = kmean.inertia_ p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1)) plots.append(p[0]) legends.append( %s with %s init % (factory.__name__, init)) plt.xlabel('n_init') plt.ylabel('inertia') plt.legend(plots, legends) plt.title( Mean inertia for various k-means init across %d runs % n_runs); Evaluation of KMeans with k-means++ init Evaluation of KMeans with random init Implementing k-means in Python [ back to top ] import numpy as np from sklearn.metrics import pairwise_distances def get_initial_centroids(data, k, seed=None): '''Randomly choose k data points as initial centroids''' if seed is not None: # useful for obtaining consistent results np.random.seed(seed) n = data.shape[0] # number of data points # Pick K indices from range [0, N). rand_indices = np.random.randint(0, n, k) # Keep centroids as dense format, as many entries will be nonzero due to averaging. # As long as at least one document in a cluster contains a word, # it will carry a nonzero weight in the TF-IDF vector of the centroid. centroids = data[rand_indices,:] return centroids def smart_initialize(data, k, seed=None): '''Use k-means++ to initialize a good set of centroids''' if seed is not None: # useful for obtaining consistent results np.random.seed(seed) centroids = np.zeros((k, data.shape[1])) # Randomly choose the first centroid. # Since we have no prior knowledge, choose uniformly at random idx = np.random.randint(data.shape[0]) centroids[0] = data[idx,:] # Compute distances from the first centroid chosen to all the other data points distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten() for i in xrange(1, k): # Choose the next centroid randomly, so that the probability for each data point to be chosen # is directly proportional to its squared distance from the nearest centroid. # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible. idx = np.random.choice(data.shape[0], 1, p=distances/sum(distances)) centroids[i] = data[idx,:] # Now compute distances from the centroids to all data points distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean'),axis=1) return centroids def assign_clusters(data, centroids): # Compute distances between each data point and the set of centroids: distances_from_centroids = pairwise_distances(data, centroids) # Compute cluster assignments for each data point: cluster_assignment = np.argmin(distances_from_centroids, axis=1) return cluster_assignment def revise_centroids(data, k, cluster_assignment): new_centroids = [] for i in xrange(k): # Select all data points that belong to cluster i. Fill in the blank (RHS only) member_data_points = data[cluster_assignment == i] # Compute the mean of the data points. Fill in the blank (RHS only) centroid = member_data_points.mean(axis=0) new_centroids.append(centroid) new_centroids = np.array(new_centroids) return new_centroids def kmeans(data, k, init='kmeans++', maxiter=100, seed=None): # Initialize centroids if init == 'kmeans++': centroids = smart_initialize(data, k, seed) else: centroids = get_initial_centroids(data, k, seed) prev_cluster_assignment = None for itr in xrange(maxiter): # 1. Make cluster assignments using nearest centroids cluster_assignment = assign_clusters(data, centroids) # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster. centroids = revise_centroids(data, k, cluster_assignment) # Check for convergence: if none of the assignments changed, stop if prev_cluster_assignment is not None and \\ (prev_cluster_assignment==cluster_assignment).all(): break prev_cluster_assignment = cluster_assignment[:] return centroids, cluster_assignment centers, y_km = kmeans(X, 3, seed=0) plt.scatter(X[y_km==0,0], X[y_km==0,1], s=50, c='lightgreen', marker='s', label='cluster 1') plt.scatter(X[y_km==1,0], X[y_km==1,1], s=50, c='orange', marker='o', label='cluster 2') plt.scatter(X[y_km==2,0], X[y_km==2,1], s=50, c='lightblue', marker='v', label='cluster 3') plt.scatter(centers[:,0], centers[:,1], s=250, marker='*', c='red', label='centroids') plt.legend() plt.grid() plt.tight_layout() #plt.savefig('./figures/centroids.png', dpi=300) Using the elbow method to find the optimal number of clusters \u901a\u5e38\u6211\u4eec\u5e76\u4e0d\u77e5\u9053\u6570\u636e\u80fd\u5206\u6210\u51e0\u4e2a\u805a\u7c7b\uff0c\u6240\u4ee5\u80fd\u6709\u529e\u6cd5\u9009\u62e9\u5408\u9002 k \u503c\u975e\u5e38\u91cd\u8981 [ back to top ] \u5224\u65ad\u805a\u7c7b\u6548\u679c\u53ef\u7528 within-cluster SSE (Distortion)\uff0c\u8fd9\u4e2a\u53ef\u7531 KMeans() \u4e2d\u7684 inertia_ \u5c5e\u6027\u83b7\u5f97 print('Distortion: %.2f' % km.inertia_) Distortion: 72.48 The Elbow method is a \"rule-of-thumb\" approach to finding the optimal number of clusters. # elbow method \u65b9\u6cd5\u5c31\u662f\u9700\u8981\u627e\u51fa\u5f53 distortion \u53d8\u5316\u975e\u5e38\u5feb\u65f6\u7684 k \u503c\uff0c\u4e5f\u5373\u627e\u62d0\u70b9 distortions = [] for i in range(1, 11): km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0) km.fit(X) distortions.append(km.inertia_) plt.plot(range(1,11), distortions , marker='o') plt.xlabel('Number of clusters') plt.ylabel('Distortion') plt.tight_layout() #plt.savefig('./figures/elbow.png', dpi=300) \u4ece\u56fe\u4e2d\u5c31\u53ef\u4ee5\u770b\u51fa, 3 \u662f\u62d0\u70b9, \u6240\u4ee5 k=3 \u662f\u6700\u597d\u7684\u9009\u62e9 Quantifying the quality of clustering via silhouette plots [ back to top ] \u53e6\u4e00\u79cd\u8bc4\u4ef7\u805a\u7c7b\u6548\u679c\u7684\u65b9\u6cd5\u662f silhouette analysis, \u8861\u91cf\u7684\u662f\u4e00\u4e2a\u7c7b\u522b\u4e2d\u7684\u6837\u672c\u662f\u5426\u8db3\u591f\u7d27\u51d1\u7ec4\u5408 \u8ba1\u7b97 $$x^{(i)}$$ \u7684 silhouette coefficient \u7684\u6b65\u9aa4\u4e3a: 1. Calculate the cluster cohesion $$a^{(i)}$$ as the average distance between a sample $$x^{(i)}$$ and all other points in the same cluster. 2. Calculate the cluster separation $$b^{(i)}$$ from the next closest cluster as the average distance between the sample $$x^{(i)}$$ and all samples in the nearest cluster. 3. Calculate the silhouette $$s^{(i)}$$ as the difference between cluster cohesion and separation divided by the greater of the two, as shown here: $$ s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{max\\big{b^{(i)}, a^{(i)}\\big}} $$ import numpy as np from matplotlib import cm from sklearn.metrics import silhouette_samples # silhouette coefficient km = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0) y_km = km.fit_predict(X) cluster_labels = np.unique(y_km) n_clusters = cluster_labels.shape[0] silhouette_vals = silhouette_samples(X, y_km, metric='euclidean') y_ax_lower, y_ax_upper = 0, 0 yticks = [] for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(i / float(n_clusters)) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color) yticks.append((y_ax_lower + y_ax_upper) / 2) y_ax_lower += len(c_silhouette_vals) silhouette_avg = np.mean(silhouette_vals) plt.axvline(silhouette_avg, color= red , linestyle= -- ) plt.yticks(yticks, cluster_labels + 1) plt.ylabel('Cluster') plt.xlabel('Silhouette coefficient') plt.tight_layout() # plt.savefig('./figures/silhouette.png', dpi=300) \u7ea2\u7ebf\u8868\u793a\u6240\u6709\u6570\u636e silhouette coef \u7684\u5e73\u5747\u503c\uff0c\u5b83\u53ef\u4f5c\u4e3a\u805a\u7c7b\u6a21\u578b\u7684\u4e00\u4e2a\u5ea6\u91cf\u6307\u6807 Comparison to \"bad\" clustering: km = KMeans(n_clusters=2, # \u8bbe\u5b9a\u4e3a2 init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0) y_km = km.fit_predict(X) plt.scatter(X[y_km==0,0], X[y_km==0,1], s=50, c='lightgreen', marker='s', label='cluster 1') plt.scatter(X[y_km==1,0], X[y_km==1,1], s=50, c='orange', marker='o', label='cluster 2') plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], s=250, marker='*', c='red', label='centroids') plt.legend() plt.grid() plt.tight_layout() #plt.savefig('./figures/centroids_bad.png', dpi=300) # evaluate the results cluster_labels = np.unique(y_km) n_clusters = cluster_labels.shape[0] silhouette_vals = silhouette_samples(X, y_km, metric='euclidean') y_ax_lower, y_ax_upper = 0, 0 yticks = [] for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(i / float(n_clusters)) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color) yticks.append((y_ax_lower + y_ax_upper) / 2) y_ax_lower += len(c_silhouette_vals) silhouette_avg = np.mean(silhouette_vals) plt.axvline(silhouette_avg, color= red , linestyle= -- ) plt.yticks(yticks, cluster_labels + 1) plt.ylabel('Cluster') plt.xlabel('Silhouette coefficient') plt.tight_layout() # plt.savefig('./figures/silhouette_bad.png', dpi=300) Organizing clusters as a hierarchical tree [ back to top ] One nice feature of hierachical clustering is that we can visualize the results as a dendrogram, a hierachical tree. Using the visualization, we can then decide how \"deep\" we want to cluster the dataset by setting a \"depth\" threshold. Or in other words, we don't need to make a decision about the number of clusters upfront. Agglomerative and divisive hierarchical clustering Furthermore, we can distinguish between 2 main approaches to hierarchical clustering: Divisive clustering and agglomerative clustering. In agglomerative clustering, we start with a single sample from our dataset and iteratively merge it with other samples to form clusters -- we can see it as a bottom-up approach for building the clustering dendrogram. In divisive clustering, however, we start with the whole dataset as one cluster, and we iteratively split it into smaller subclusters -- a top-down approach. In this notebook, we will use agglomerative clustering. Single and complete linkage Now, the next question is how we measure the similarity between samples. One approach is the familiar Euclidean distance metric that we already used via the K-Means algorithm. as a refresher, the distance between 2 m-dimensional vectors $$\\mathbf{p}$$ and $$\\mathbf{q}$$ can be computed as: \\begin{align} \\mathrm{d}(\\mathbf{q},\\mathbf{p}) = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_m-p_m)^2} \\[8pt] = \\sqrt{\\sum_{j=1}^m (q_j-p_j)^2}.\\end{align} However, that's the distance between 2 samples. Now, how do we compute the similarity between subclusters of samples in order to decide which clusters to merge when constructing the dendrogram? I.e., our goal is to iteratively merge the most similar pairs of clusters until only one big cluster remains. There are many different approaches to this, for example single and complete linkage. In single linkage, we take the pair of the most similar samples (based on the Euclidean distance, for example) in each cluster, and merge the two clusters which have the most similar 2 members into one new, bigger cluster. In complete linkage, we compare the pairs of the two most dissimilar members of each cluster with each other, and we merge the 2 clusters where the distance between its 2 most dissimilar members is smallest. # \u751f\u6210\u6570\u636e import pandas as pd import numpy as np np.random.seed(123) variables = ['X', 'Y', 'Z'] labels = ['ID_0','ID_1','ID_2','ID_3','ID_4'] X = np.random.random_sample([5,3])*10 df = pd.DataFrame(X, columns=variables, index=labels) df X Y Z ID_0 6.964692 2.861393 2.268515 ID_1 5.513148 7.194690 4.231065 ID_2 9.807642 6.848297 4.809319 ID_3 3.921175 3.431780 7.290497 ID_4 4.385722 0.596779 3.980443 Performing hierarchical clustering on a distance matrix [ back to top ] # calculate the distance matrix as input for the hierarchical clustering algorithm from scipy.spatial.distance import pdist,squareform row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')), columns=labels, index=labels) row_dist ID_0 ID_1 ID_2 ID_3 ID_4 ID_0 0.000000 4.973534 5.516653 5.899885 3.835396 ID_1 4.973534 0.000000 4.347073 5.104311 6.698233 ID_2 5.516653 4.347073 0.000000 7.244262 8.316594 ID_3 5.899885 5.104311 7.244262 0.000000 4.382864 ID_4 3.835396 6.698233 8.316594 4.382864 0.000000 We can either pass a condensed distance matrix (upper triangular) from the pdist function, or we can pass the \"original\" data array and define the metric='euclidean' argument in linkage. However, we should not pass the squareform distance matrix, which would yield different distance values although the overall clustering could be the same. # 1. incorrect approach: Squareform distance matrix from scipy.cluster.hierarchy import linkage row_clusters = linkage(row_dist, method='complete', metric='euclidean') pd.DataFrame(row_clusters, columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'], index=['cluster %d' %(i+1) for i in range(row_clusters.shape[0])]) row label 1 row label 2 distance no. of items in clust. cluster 1 0.0 4.0 6.521973 2.0 cluster 2 1.0 2.0 6.729603 2.0 cluster 3 3.0 5.0 8.539247 3.0 cluster 4 6.0 7.0 12.444824 5.0 # 2. correct approach: Condensed distance matrix row_clusters = linkage(pdist(df, metric='euclidean'), method='complete') pd.DataFrame(row_clusters, columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'], index=['cluster %d' %(i+1) for i in range(row_clusters.shape[0])]) row label 1 row label 2 distance no. of items in clust. cluster 1 0.0 4.0 3.835396 2.0 cluster 2 1.0 2.0 4.347073 2.0 cluster 3 3.0 5.0 5.899885 3.0 cluster 4 6.0 7.0 8.316594 5.0 # 3. correct approach: Input sample matrix row_clusters = linkage(df.values, method='complete', metric='euclidean') pd.DataFrame(row_clusters, columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'], index=['cluster %d' %(i+1) for i in range(row_clusters.shape[0])]) row label 1 row label 2 distance no. of items in clust. cluster 1 0.0 4.0 3.835396 2.0 cluster 2 1.0 2.0 4.347073 2.0 cluster 3 3.0 5.0 5.899885 3.0 cluster 4 6.0 7.0 8.316594 5.0 # \u53ef\u89c6\u5316\u7ed3\u679c, \u4f7f\u7528 dendrogram from scipy.cluster.hierarchy import dendrogram # make dendrogram black (part 1/2) # from scipy.cluster.hierarchy import set_link_color_palette # set_link_color_palette(['black']) row_dendr = dendrogram(row_clusters, labels=labels, # make dendrogram black (part 2/2) # color_threshold=np.inf ) plt.tight_layout() plt.ylabel('Euclidean distance'); #plt.savefig('./figures/dendrogram.png', dpi=300, bbox_inches='tight') Attaching dendrograms to a heat map [ back to top ] # plot row dendrogram fig = plt.figure(figsize=(8, 8), facecolor='white') axd = fig.add_axes([0.09, 0.1, 0.2, 0.6]) # note: for matplotlib v1.5.1, please use orientation='right' row_dendr = dendrogram(row_clusters, orientation='left') # reorder data with respect to clustering df_rowclust = df.ix[row_dendr['leaves'][::-1]] axd.set_xticks([]) axd.set_yticks([]) # remove axes spines from dendrogram for i in axd.spines.values(): i.set_visible(False) # plot heatmap axm = fig.add_axes([0.23, 0.1, 0.6, 0.6]) # x-pos, y-pos, width, height cax = axm.matshow(df_rowclust, interpolation='nearest', cmap='hot_r') fig.colorbar(cax) axm.set_xticklabels([''] + list(df_rowclust.columns)) axm.set_yticklabels([''] + list(df_rowclust.index)); # plt.savefig('./figures/heatmap.png', dpi=300) Applying agglomerative clustering via scikit-learn [ back to top ] # \u524d\u9762\u662f\u7528 scipy, \u73b0\u5728\u7528 sklearn \u6765\u805a\u7c7b from sklearn.cluster import AgglomerativeClustering ac = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete') labels = ac.fit_predict(X) print('Cluster labels: %s' % labels) Cluster labels: [0 1 1 0 0] \u7ed3\u679c\u4e0e\u524d\u9762\u4e00\u81f4 Applying agglomerative clustering with iris dataset [ back to top ] from sklearn import datasets iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target n_samples, n_features = X.shape plt.scatter(X[:, 0], X[:, 1], c=y); from scipy.cluster.hierarchy import linkage from scipy.cluster.hierarchy import dendrogram clusters = linkage(X, metric='euclidean', method='complete') dendr = dendrogram(clusters) plt.ylabel('Euclidean Distance'); from sklearn.cluster import AgglomerativeClustering ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete') prediction = ac.fit_predict(X) print('Cluster labels:\\n %s\\n' % prediction) Cluster labels: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] plt.scatter(X[:, 0], X[:, 1], c=prediction); Locating regions of high density via DBSCAN [ back to top ] Another useful approach to clustering is Density-based Spatial Clustering of Applications with Noise (DBSCAN). In essence, we can think of DBSCAN as an algorithm that divides the dataset into subgroup based on dense regions of point. In DBSCAN, we distinguish between 3 different \"points\": Core points: A core point is a point that has at least a minimum number of other points (MinPts) in its radius epsilon. Border points: A border point is a point that is not a core point, since it doesn't have enough MinPts in its neighborhood, but lies within the radius epsilon of a core point. Noise points: All other points that are neither core points nor border points. \u7ed9\u6bcf\u4e2a\u70b9 label \u4e4b\u540e, DBSCAN \u7b97\u6cd5\u5c31\u662f\u4e0b\u9762\u4e24\u6b65: 1. Form a separate cluster for each core point or a connected group of core points (core points are connected if they are no farther away than $$\\epsilon$$ ). 2. Assign each border point to the cluster of its corresponding core point. A nice feature about DBSCAN is that we don't have to specify a number of clusters upfront. However, it requires the setting of additional hyperparameters such as the value for MinPts and the radius epsilon. # \u751f\u6210\u534a\u6708\u5f62\u6570\u636e # two visible, half-moon-shaped groups consisting of 100 sample points each from sklearn.datasets import make_moons X, y = make_moons(n_samples=200, noise=0.05, random_state=0) plt.scatter(X[:,0], X[:,1]) plt.tight_layout() #plt.savefig('./figures/moons.png', dpi=300) K-means and hierarchical clustering: # complete linkage clustering f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3)) km = KMeans(n_clusters=2, random_state=0) y_km = km.fit_predict(X) ax1.scatter(X[y_km==0,0], X[y_km==0,1], c='lightblue', marker='o', s=40, label='cluster 1') ax1.scatter(X[y_km==1,0], X[y_km==1,1], c='red', marker='s', s=40, label='cluster 2') ax1.set_title('K-means clustering') ac = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete') y_ac = ac.fit_predict(X) ax2.scatter(X[y_ac==0,0], X[y_ac==0,1], c='lightblue', marker='o', s=40, label='cluster 1') ax2.scatter(X[y_ac==1,0], X[y_ac==1,1], c='red', marker='s', s=40, label='cluster 2') ax2.set_title('Agglomerative clustering') plt.legend() plt.tight_layout() #plt.savefig('./figures/kmeans_and_ac.png', dpi=300) \u6548\u679c\u5e76\u4e0d\u597d, \u5e76\u4e0d\u80fd\u5b8c\u5168 separated Density-based clustering: # DBSCAN from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean') y_db = db.fit_predict(X) plt.scatter(X[y_db==0,0], X[y_db==0,1], c='lightblue', marker='o', s=40, label='cluster 1') plt.scatter(X[y_db==1,0], X[y_db==1,1], c='red', marker='s', s=40, label='cluster 2') plt.legend() plt.tight_layout() #plt.savefig('./figures/moons_dbscan.png', dpi=300) DBSCAN \u80fd\u5f88\u597d\u5730\u5bf9\u534a\u6708\u6570\u636e\u8fdb\u884c\u805a\u7c7b \u53ef\u4ee5\u518d\u8bd5\u8bd5\u5706\u73af\u56fe\u5f62 from sklearn.datasets import make_circles X, y = make_circles(n_samples=500, factor=.6, noise=.05) plt.scatter(X[:, 0], X[:, 1], c=y); k-means from sklearn.cluster import KMeans km = KMeans(n_clusters=2) predict = km.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=predict); allomerative clustering from sklearn.cluster import AgglomerativeClustering ac = AgglomerativeClustering() predict = ac.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=predict); DBSCAN from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.15, min_samples=9, metric='euclidean') predict = db.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=predict); Learning from labeled and unlabeled data with label propagation [ back to top ] \u56e0\u4e3a\u6807\u6ce8\u6210\u672c\u6bd4\u8f83\u9ad8\uff0c\u5f53\u4f60\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u53ea\u6709\u4e00\u90e8\u5206\u6570\u636e\u662f\u6709\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u4f60\u53ea\u80fd\u6254\u6389\u90a3\u4e9b\u6ca1\u6709\u6807\u6ce8\u7684 X \u3002 \u800c\u5b9e\u9645\u4e0a\uff0c\u6709\u6807\u6ce8\u7684\u6837\u672c\u548c\u65e0\u6807\u6ce8\u7684\u6837\u672c\u4e4b\u95f4\u662f\u6709\u5173\u7cfb\u7684\uff0c\u8fd9\u79cd\u5173\u7cfb\u4fe1\u606f\u4e5f\u53ef\u4ee5\u7528\u6765\u5e2e\u52a9\u5b66\u4e60\u3002\u8fd9\u5c31\u662f\u534a\u76d1\u7763\u5b66\u4e60\u6807\u7b7e\u4f20\u64ad\uff08Label Propagation\uff09\u7b97\u6cd5\u7684\u601d\u8def\u3002 \u5b83\u7684\u57fa\u672c\u903b\u8f91\u662f\u501f\u52a9\u4e8e\u8fd1\u6731\u8005\u8d64\u7684\u601d\u8def\uff0c\u4e5f\u5c31\u662f KNN \u7684\u601d\u8def\uff0c\u5982\u679c A \u548c B \u5728 X \u7a7a\u95f4\u4e0a\u5f88\u63a5\u8fd1\uff0c\u90a3\u4e48 A \u7684 y \u6807\u7b7e\u5c31\u53ef\u4ee5\u4f20\u7ed9 B\u3002 \u8fdb\u4e00\u6b65\u8fed\u4ee3\u4e0b\u53bb\uff0c\u5982\u679c C \u548c B \u4e5f\u5f88\u63a5\u8fd1\uff0cC \u7684\u6807\u7b7e\u4e5f\u5e94\u8be5\u548c B \u4e00\u6837\u3002 \u6240\u4ee5\u57fa\u672c\u8ba1\u7b97\u903b\u8f91\u5c31\u662f\u4e24\u6b65\uff0c\u7b2c\u4e00\u6b65\u662f\u8ba1\u7b97\u6837\u672c\u95f4\u7684\u8ddd\u79bb\uff0c\u6784\u5efa\u8f6c\u79fb\u77e9\u9635\uff0c\u7b2c\u4e8c\u6b65\u662f\u5c06\u8f6c\u79fb\u77e9\u9635\u548c Y \u77e9\u9635\u76f8\u4e58\uff0cY \u91cc\u9762\u5305\u62ec\u4e86\u5df2\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u7684\u4e24\u90e8\u5206\uff0c \u901a\u8fc7\u76f8\u4e58\u53ef\u4ee5\u5c06\u5df2\u6807\u6ce8\u7684 Y \u8f6c\u64ad\u7ed9\u672a\u6807\u6ce8\u7684 Y\u3002 \u5177\u4f53\u8bba\u6587\u53ef\u4ee5 \u770b\u8fd9\u91cc \u3002 \u5728 Sklearn \u6a21\u5757\u4e2d\u5df2\u7ecf\u5185\u7f6e\u4e86\u8fd9\u79cd\u7b97\u6cd5\uff0c\u6587\u6863\u793a\u4f8b\u53ef\u4ee5 \u770b\u8fd9\u91cc \u3002 \u4e0b\u9762\u662f\u7528 Python \u7684 NumPy \u6a21\u5757\u5b9e\u73b0\u7684\u4e00\u4e2a toy demo\u3002 import pandas as pd import numpy as np # \u8bfb\u5165 iris \u6570\u636e from sklearn.datasets import load_iris iris = load_iris() X = iris.data y = iris.target n = len(y) # \u5207\u5206\u6570\u636e np.random.seed(42) train_index = np.random.choice(n, int(0.6*n), replace = False) test_index = np.setdiff1d(np.arange(n), train_index) # \u8ba1\u7b97\u6743\u91cd\u77e9\u9635 sigma = X.var(axis = 0) weights = np.zeros((n,n)) def weight_func(ind1, ind2, X=X, sigma=sigma): return np.exp(-np.sum((X[ind1] - X[ind2])**2/sigma)) for i in range(n): for j in range(n): weights[i,j] = weight_func(i,j) ## \u6807\u51c6\u5316\u4e3a\u8f6c\u79fb\u77e9\u9635 t = weights / weights.sum(axis=1) # y\u8f6c\u6362\u5f62\u5f0f y_m = np.zeros((n, len(np.unique(y)))) for i in range(n): y_m[i,y[i]] = 1 ## unlabel\u521d\u59cb\u5316, label\u8bb0\u4f4f y_m[test_index] = np.random.random(y_m[test_index].shape) clamp = y_m[train_index] ## \u8fed\u4ee3\u8ba1\u7b97 iter_n = 50 for _ in range(iter_n): y_m = t.dot(y_m) # LP y_m = (y_m.T / y_m.sum(axis=1)).T # normalize y_m[train_index] = clamp # clamp # \u9884\u6d4b\u51c6\u786e\u7387 predict = y_m[test_index].argmax(axis=1) np.sum(y[test_index] == predict) / float(len(predict)) 0.91666666666666663 Label Propagation learning a complex structure Example of LabelPropagation learning a complex internal structure to demonstrate \u201cmanifold learning\u201d. The outer circle should be labeled \u201cred\u201d and the inner circle \u201cblue\u201d. Because both label groups lie inside their own distinct shape, we can see that the labels propagate correctly around the circle. import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.semi_supervised import label_propagation from sklearn.datasets import make_circles # generate ring with inner box n_samples = 200 X, y = make_circles(n_samples=n_samples, shuffle=False) outer, inner = 0, 1 labels = -np.ones(n_samples) labels[0] = outer labels[-1] = inner # Learn with LabelSpreading label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0) label_spread.fit(X, labels) # Plot output labels output_labels = label_spread.transduction_ plt.figure(figsize=(8.5, 4)) plt.subplot(1, 2, 1) plt.scatter(X[labels == outer, 0], X[labels == outer, 1], color='navy', marker='s', lw=0, label= outer labeled , s=10) plt.scatter(X[labels == inner, 0], X[labels == inner, 1], color='c', marker='s', lw=0, label='inner labeled', s=10) plt.scatter(X[labels == -1, 0], X[labels == -1, 1], color='darkorange', marker='.', label='unlabeled') plt.legend(scatterpoints=1, shadow=False, loc='upper right') plt.title( Raw data (2 classes=outer and inner) ) plt.subplot(1, 2, 2) output_label_array = np.asarray(output_labels) outer_numbers = np.where(output_label_array == outer)[0] inner_numbers = np.where(output_label_array == inner)[0] plt.scatter(X[outer_numbers, 0], X[outer_numbers, 1], color='navy', marker='s', lw=0, s=10, label= outer learned ) plt.scatter(X[inner_numbers, 0], X[inner_numbers, 1], color='c', marker='s', lw=0, s=10, label= inner learned ) plt.legend(scatterpoints=1, shadow=False, loc='upper right') plt.title( Labels learned with Label Spreading (KNN) ) plt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92) plt.show() \u7ec3\u4e60\uff1a\u4f7f\u7528\u4fe1\u8d37\u6570\u636e\u96c6\u4e2d\u7684\u81ea\u53d8\u91cf\uff0c\u6765\u5bf9\u8fd9\u4e9b\u7528\u6237\u8fdb\u884c\u805a\u7c7b","title":"\u7b2c6\u7ae0\uff1a\u975e\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5"},{"location":"w6-unsupervised-learning/6w/#6","text":"","title":"\u7b2c6\u7ae0\uff1a\u975e\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5"},{"location":"w6-unsupervised-learning/6w/#sections","text":"Some notable clustering routines Grouping objects by similarity using k-means k-means in Sklearn k-means++ Implementing k-means in Python Using the elbow method to find the optimal number of clusters Quantifying the quality of clustering via silhouette plots Organizing clusters as a hierarchical tree Performing hierarchical clustering on a distance matrix Attaching dendrograms to a heat map Applying agglomerative clustering via scikit-learn Applying agglomerative clustering with iris dataset Locating regions of high density via DBSCAN Learning from labeled and unlabeled data with label propagation Clustering is the task of gathering samples into groups of similar samples according to some predefined similarity or distance (dissimilarity) measure, such as the Euclidean distance. Here are some common applications of clustering algorithms: Compression for data reduction Summarizing data as a reprocessing step for recommender systems Similarly: grouping related web news (e.g. Google News) and web search results grouping related stock quotes for investment portfolio management building customer profiles for market analysis Building a code book of prototype samples for unsupervised feature extraction","title":"Sections"},{"location":"w6-unsupervised-learning/6w/#some-notable-clustering-routines","text":"[ back to top ] The following are two well-known clustering algorithms. sklearn.cluster.KMeans : The simplest, yet effective clustering algorithm. Needs to be provided with the number of clusters in advance, and assumes that the data is normalized as input (but use a PCA model as preprocessor). sklearn.cluster.MeanShift : Can find better looking clusters than KMeans but is not scalable to high number of samples. sklearn.cluster.DBSCAN : Can detect irregularly shaped clusters based on density, i.e. sparse regions in the input space are likely to become inter-cluster boundaries. Can also detect outliers (samples that are not part of a cluster). sklearn.cluster.AffinityPropagation : Clustering algorithm based on message passing between data points. sklearn.cluster.SpectralClustering : KMeans applied to a projection of the normalized graph Laplacian: finds normalized graph cuts if the affinity matrix is interpreted as an adjacency matrix of a graph. sklearn.cluster.Ward : Ward implements hierarchical clustering based on the Ward algorithm, a variance-minimizing approach. At each step, it minimizes the sum of squared differences within all clusters (inertia criterion). Of these, Ward, SpectralClustering, DBSCAN and Affinity propagation can also work with precomputed similarity matrices.","title":"Some notable clustering routines"},{"location":"w6-unsupervised-learning/6w/#grouping-objects-by-similarity-using-k-means","text":"[ back to top ] # make dataset from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=150, n_features=2, centers=3, # \u4ece\u4e09\u7c7b\u4e2d\u62bd\u53d6 cluster_std=0.5, shuffle=True, random_state=0) # \u753b\u4e2a\u56fe\u770b\u770b import matplotlib.pyplot as plt %matplotlib inline plt.scatter(X[:,0], X[:,1], c='white', marker='o', s=50) plt.grid() plt.tight_layout() #plt.savefig('./figures/spheres.png', dpi=300)","title":"Grouping objects by similarity using k-means"},{"location":"w6-unsupervised-learning/6w/#k-means","text":"Randomly pick k centroids from the sample points as initial cluster centers. Assign each sample to the nearest centroid $$\\mu^{(j)}, j\u2208{1,...,k}$$. Move the centroids to the center of the samples that were assigned to it. Repeat the steps 2 and 3 until the cluster assignment do not change or a user-defined tolerance or a maximum number of iterations is reached. Visualizing K-Means Clustering \u5982\u4f55\u6765\u6d4b\u91cf\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6, similarity \u6216\u8005\u5982\u4f55\u8868\u793a\u4e24\u4e2a\u7269\u4f53\u4e4b\u95f4\u7684\u8ddd\u79bb, distance \u6700\u5e38\u89c1\u7684\u4e00\u79cd\u8ddd\u79bb\u5ea6\u91cf\u662f squared Euclidean distance: $$ d(x,y)^2 = \\sum_{j=1}^{m} (x_j -y_j)^2 = \\lVert x - y \\rVert_2^2 $$ k-means \u53ef\u8f6c\u5316\u4e3a\u6700\u4f18\u5316\u7684\u95ee\u9898, \u6700\u5c0f\u5316 within-cluster sum of squared errors (SSE), \u53c8\u79f0\u4e3a cluster inertia $$ SSE = \\sum_{i=1}^n\\sum_{j=1}^k w^{(i,j)}\\lVert x^{(i)}-\\mu^{(j)}\\rVert_2^2 $$ \u5176\u4e2d $$\\mu^{(j)}$$ \u662f\u7b2c j \u4e2a\u805a\u7c7b\u7684\u4e2d\u5fc3\uff0c\u5982\u679c\u6837\u672c i \u5728\u805a\u7c7b j \u4e2d\uff0c$$w^{(i,j)}=1$$\uff0c\u5426\u5219 $$w^{(i,j)}=0$$","title":"k-means \u7b97\u6cd5"},{"location":"w6-unsupervised-learning/6w/#k-means-in-sklearn","text":"[ back to top ] # \u4f7f\u7528 sklearn \u4e2d KMeans \u6765\u805a\u7c7b from sklearn.cluster import KMeans km = KMeans(n_clusters=3, # k \u662f\u9700\u8981\u81ea\u5df1\u8bbe\u5b9a\u7684, \u9519\u8bef\u7684 k \u53ef\u80fd\u7ed3\u679c\u5c31\u4e0d\u5bf9 init='random', n_init=10, # \u91cd\u590d\u8fd0\u884c\u7b97\u6cd5 10 \u6b21\uff0c\u9009\u5176\u4e2d\u6700\u597d\u7684\u805a\u7c7b\u6a21\u578b\uff0c\u907f\u514d\u4e0d\u597d\u7684\u521d\u59cb\u5316\u503c\u5e26\u6765\u7684\u5f71\u54cd max_iter=300, tol=1e-04, random_state=0) y_km = km.fit_predict(X) plt.scatter(X[y_km==0,0], X[y_km==0,1], s=50, c='lightgreen', marker='s', label='cluster 1') plt.scatter(X[y_km==1,0], X[y_km==1,1], s=50, c='orange', marker='o', label='cluster 2') plt.scatter(X[y_km==2,0], X[y_km==2,1], s=50, c='lightblue', marker='v', label='cluster 3') plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], s=250, marker='*', c='red', label='centroids') plt.legend() plt.grid() plt.tight_layout() #plt.savefig('./figures/centroids.png', dpi=300)","title":"k-means in Sklearn"},{"location":"w6-unsupervised-learning/6w/#k-means_1","text":"[ back to top ] k-means++ \u7b97\u6cd5\u901a\u8fc7\u6539\u5584 centroids \u521d\u59cb\u503c\u7684\u8bbe\u7f6e\uff0c\u6765\u4f18\u5316 k-means Initialize an empty set $$M$$ to store the k centroids being selected Randomly choose the first centroid $$\\mu^{(j)}$$ from the input samples and assign it to $$M$$ For each sample $$x^{(i)}$$ that is not in M, find the minimum squared distance $$d(x^{(i)}, M)^2$$ to any of the centroids in M To randomly select the next centroid $$\\mu^{(p)}$$, use a weighted probability distribution equal to $$\\frac{d(\\mu^{(p)}, M)^2}{\\sum_i d(x^{(i)}, M)^2}$$ Repeat steps 2 and 3 until $$k$$ centroids are chosen Proceed with the classic k-means algorithm sklearn \u91cc\u4f7f\u7528 k-means++ \u7b97\u6cd5\u53ea\u9700\u5728 KMeans() \u91cc\u8bbe\u7f6e init=\"k-means++\" # \u5bf9\u6bd4 k-means \u548c k-means++ import numpy as np from sklearn.utils import shuffle from sklearn.utils import check_random_state from sklearn.cluster import KMeans random_state = np.random.RandomState(0) # Number of run (with randomly generated dataset) for each strategy so as # to be able to compute an estimate of the standard deviation n_runs = 5 # k-means models can do several random inits so as to be able to trade # CPU time for convergence robustness n_init_range = np.array([1, 5, 10, 15, 20]) # Datasets generation parameters n_samples_per_center = 100 grid_size = 3 scale = 0.1 n_clusters = grid_size ** 2 def make_data(random_state, n_samples_per_center, grid_size, scale): random_state = check_random_state(random_state) centers = np.array([[i, j] for i in range(grid_size) for j in range(grid_size)]) n_clusters_true, n_features = centers.shape noise = random_state.normal( scale=scale, size=(n_samples_per_center, centers.shape[1])) X = np.concatenate([c + noise for c in centers]) y = np.concatenate([[i] * n_samples_per_center for i in range(n_clusters_true)]) return shuffle(X, y, random_state=random_state) fig = plt.figure() plots = [] legends = [] cases = [ (KMeans, 'k-means++'), (KMeans, 'random') ] for factory, init in cases: print( Evaluation of %s with %s init % (factory.__name__, init)) inertia = np.empty((len(n_init_range), n_runs)) for run_id in range(n_runs): X_data, y = make_data(run_id, n_samples_per_center, grid_size, scale) for i, n_init in enumerate(n_init_range): kmean = factory(n_clusters=n_clusters, init=init, random_state=run_id, n_init=n_init).fit(X_data) inertia[i, run_id] = kmean.inertia_ p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1)) plots.append(p[0]) legends.append( %s with %s init % (factory.__name__, init)) plt.xlabel('n_init') plt.ylabel('inertia') plt.legend(plots, legends) plt.title( Mean inertia for various k-means init across %d runs % n_runs); Evaluation of KMeans with k-means++ init Evaluation of KMeans with random init","title":"k-means++"},{"location":"w6-unsupervised-learning/6w/#implementing-k-means-in-python","text":"[ back to top ] import numpy as np from sklearn.metrics import pairwise_distances def get_initial_centroids(data, k, seed=None): '''Randomly choose k data points as initial centroids''' if seed is not None: # useful for obtaining consistent results np.random.seed(seed) n = data.shape[0] # number of data points # Pick K indices from range [0, N). rand_indices = np.random.randint(0, n, k) # Keep centroids as dense format, as many entries will be nonzero due to averaging. # As long as at least one document in a cluster contains a word, # it will carry a nonzero weight in the TF-IDF vector of the centroid. centroids = data[rand_indices,:] return centroids def smart_initialize(data, k, seed=None): '''Use k-means++ to initialize a good set of centroids''' if seed is not None: # useful for obtaining consistent results np.random.seed(seed) centroids = np.zeros((k, data.shape[1])) # Randomly choose the first centroid. # Since we have no prior knowledge, choose uniformly at random idx = np.random.randint(data.shape[0]) centroids[0] = data[idx,:] # Compute distances from the first centroid chosen to all the other data points distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten() for i in xrange(1, k): # Choose the next centroid randomly, so that the probability for each data point to be chosen # is directly proportional to its squared distance from the nearest centroid. # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible. idx = np.random.choice(data.shape[0], 1, p=distances/sum(distances)) centroids[i] = data[idx,:] # Now compute distances from the centroids to all data points distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean'),axis=1) return centroids def assign_clusters(data, centroids): # Compute distances between each data point and the set of centroids: distances_from_centroids = pairwise_distances(data, centroids) # Compute cluster assignments for each data point: cluster_assignment = np.argmin(distances_from_centroids, axis=1) return cluster_assignment def revise_centroids(data, k, cluster_assignment): new_centroids = [] for i in xrange(k): # Select all data points that belong to cluster i. Fill in the blank (RHS only) member_data_points = data[cluster_assignment == i] # Compute the mean of the data points. Fill in the blank (RHS only) centroid = member_data_points.mean(axis=0) new_centroids.append(centroid) new_centroids = np.array(new_centroids) return new_centroids def kmeans(data, k, init='kmeans++', maxiter=100, seed=None): # Initialize centroids if init == 'kmeans++': centroids = smart_initialize(data, k, seed) else: centroids = get_initial_centroids(data, k, seed) prev_cluster_assignment = None for itr in xrange(maxiter): # 1. Make cluster assignments using nearest centroids cluster_assignment = assign_clusters(data, centroids) # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster. centroids = revise_centroids(data, k, cluster_assignment) # Check for convergence: if none of the assignments changed, stop if prev_cluster_assignment is not None and \\ (prev_cluster_assignment==cluster_assignment).all(): break prev_cluster_assignment = cluster_assignment[:] return centroids, cluster_assignment centers, y_km = kmeans(X, 3, seed=0) plt.scatter(X[y_km==0,0], X[y_km==0,1], s=50, c='lightgreen', marker='s', label='cluster 1') plt.scatter(X[y_km==1,0], X[y_km==1,1], s=50, c='orange', marker='o', label='cluster 2') plt.scatter(X[y_km==2,0], X[y_km==2,1], s=50, c='lightblue', marker='v', label='cluster 3') plt.scatter(centers[:,0], centers[:,1], s=250, marker='*', c='red', label='centroids') plt.legend() plt.grid() plt.tight_layout() #plt.savefig('./figures/centroids.png', dpi=300)","title":"Implementing k-means in Python"},{"location":"w6-unsupervised-learning/6w/#using-the-elbow-method-to-find-the-optimal-number-of-clusters","text":"\u901a\u5e38\u6211\u4eec\u5e76\u4e0d\u77e5\u9053\u6570\u636e\u80fd\u5206\u6210\u51e0\u4e2a\u805a\u7c7b\uff0c\u6240\u4ee5\u80fd\u6709\u529e\u6cd5\u9009\u62e9\u5408\u9002 k \u503c\u975e\u5e38\u91cd\u8981 [ back to top ] \u5224\u65ad\u805a\u7c7b\u6548\u679c\u53ef\u7528 within-cluster SSE (Distortion)\uff0c\u8fd9\u4e2a\u53ef\u7531 KMeans() \u4e2d\u7684 inertia_ \u5c5e\u6027\u83b7\u5f97 print('Distortion: %.2f' % km.inertia_) Distortion: 72.48 The Elbow method is a \"rule-of-thumb\" approach to finding the optimal number of clusters. # elbow method \u65b9\u6cd5\u5c31\u662f\u9700\u8981\u627e\u51fa\u5f53 distortion \u53d8\u5316\u975e\u5e38\u5feb\u65f6\u7684 k \u503c\uff0c\u4e5f\u5373\u627e\u62d0\u70b9 distortions = [] for i in range(1, 11): km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=300, random_state=0) km.fit(X) distortions.append(km.inertia_) plt.plot(range(1,11), distortions , marker='o') plt.xlabel('Number of clusters') plt.ylabel('Distortion') plt.tight_layout() #plt.savefig('./figures/elbow.png', dpi=300) \u4ece\u56fe\u4e2d\u5c31\u53ef\u4ee5\u770b\u51fa, 3 \u662f\u62d0\u70b9, \u6240\u4ee5 k=3 \u662f\u6700\u597d\u7684\u9009\u62e9","title":"Using the elbow method to find the optimal number of clusters"},{"location":"w6-unsupervised-learning/6w/#quantifying-the-quality-of-clustering-via-silhouette-plots","text":"[ back to top ] \u53e6\u4e00\u79cd\u8bc4\u4ef7\u805a\u7c7b\u6548\u679c\u7684\u65b9\u6cd5\u662f silhouette analysis, \u8861\u91cf\u7684\u662f\u4e00\u4e2a\u7c7b\u522b\u4e2d\u7684\u6837\u672c\u662f\u5426\u8db3\u591f\u7d27\u51d1\u7ec4\u5408 \u8ba1\u7b97 $$x^{(i)}$$ \u7684 silhouette coefficient \u7684\u6b65\u9aa4\u4e3a: 1. Calculate the cluster cohesion $$a^{(i)}$$ as the average distance between a sample $$x^{(i)}$$ and all other points in the same cluster. 2. Calculate the cluster separation $$b^{(i)}$$ from the next closest cluster as the average distance between the sample $$x^{(i)}$$ and all samples in the nearest cluster. 3. Calculate the silhouette $$s^{(i)}$$ as the difference between cluster cohesion and separation divided by the greater of the two, as shown here: $$ s^{(i)} = \\frac{b^{(i)} - a^{(i)}}{max\\big{b^{(i)}, a^{(i)}\\big}} $$ import numpy as np from matplotlib import cm from sklearn.metrics import silhouette_samples # silhouette coefficient km = KMeans(n_clusters=3, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0) y_km = km.fit_predict(X) cluster_labels = np.unique(y_km) n_clusters = cluster_labels.shape[0] silhouette_vals = silhouette_samples(X, y_km, metric='euclidean') y_ax_lower, y_ax_upper = 0, 0 yticks = [] for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(i / float(n_clusters)) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color) yticks.append((y_ax_lower + y_ax_upper) / 2) y_ax_lower += len(c_silhouette_vals) silhouette_avg = np.mean(silhouette_vals) plt.axvline(silhouette_avg, color= red , linestyle= -- ) plt.yticks(yticks, cluster_labels + 1) plt.ylabel('Cluster') plt.xlabel('Silhouette coefficient') plt.tight_layout() # plt.savefig('./figures/silhouette.png', dpi=300) \u7ea2\u7ebf\u8868\u793a\u6240\u6709\u6570\u636e silhouette coef \u7684\u5e73\u5747\u503c\uff0c\u5b83\u53ef\u4f5c\u4e3a\u805a\u7c7b\u6a21\u578b\u7684\u4e00\u4e2a\u5ea6\u91cf\u6307\u6807 Comparison to \"bad\" clustering: km = KMeans(n_clusters=2, # \u8bbe\u5b9a\u4e3a2 init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0) y_km = km.fit_predict(X) plt.scatter(X[y_km==0,0], X[y_km==0,1], s=50, c='lightgreen', marker='s', label='cluster 1') plt.scatter(X[y_km==1,0], X[y_km==1,1], s=50, c='orange', marker='o', label='cluster 2') plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], s=250, marker='*', c='red', label='centroids') plt.legend() plt.grid() plt.tight_layout() #plt.savefig('./figures/centroids_bad.png', dpi=300) # evaluate the results cluster_labels = np.unique(y_km) n_clusters = cluster_labels.shape[0] silhouette_vals = silhouette_samples(X, y_km, metric='euclidean') y_ax_lower, y_ax_upper = 0, 0 yticks = [] for i, c in enumerate(cluster_labels): c_silhouette_vals = silhouette_vals[y_km == c] c_silhouette_vals.sort() y_ax_upper += len(c_silhouette_vals) color = cm.jet(i / float(n_clusters)) plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color) yticks.append((y_ax_lower + y_ax_upper) / 2) y_ax_lower += len(c_silhouette_vals) silhouette_avg = np.mean(silhouette_vals) plt.axvline(silhouette_avg, color= red , linestyle= -- ) plt.yticks(yticks, cluster_labels + 1) plt.ylabel('Cluster') plt.xlabel('Silhouette coefficient') plt.tight_layout() # plt.savefig('./figures/silhouette_bad.png', dpi=300)","title":"Quantifying the quality of clustering via silhouette plots"},{"location":"w6-unsupervised-learning/6w/#organizing-clusters-as-a-hierarchical-tree","text":"[ back to top ] One nice feature of hierachical clustering is that we can visualize the results as a dendrogram, a hierachical tree. Using the visualization, we can then decide how \"deep\" we want to cluster the dataset by setting a \"depth\" threshold. Or in other words, we don't need to make a decision about the number of clusters upfront. Agglomerative and divisive hierarchical clustering Furthermore, we can distinguish between 2 main approaches to hierarchical clustering: Divisive clustering and agglomerative clustering. In agglomerative clustering, we start with a single sample from our dataset and iteratively merge it with other samples to form clusters -- we can see it as a bottom-up approach for building the clustering dendrogram. In divisive clustering, however, we start with the whole dataset as one cluster, and we iteratively split it into smaller subclusters -- a top-down approach. In this notebook, we will use agglomerative clustering. Single and complete linkage Now, the next question is how we measure the similarity between samples. One approach is the familiar Euclidean distance metric that we already used via the K-Means algorithm. as a refresher, the distance between 2 m-dimensional vectors $$\\mathbf{p}$$ and $$\\mathbf{q}$$ can be computed as: \\begin{align} \\mathrm{d}(\\mathbf{q},\\mathbf{p}) = \\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 + \\cdots + (q_m-p_m)^2} \\[8pt] = \\sqrt{\\sum_{j=1}^m (q_j-p_j)^2}.\\end{align} However, that's the distance between 2 samples. Now, how do we compute the similarity between subclusters of samples in order to decide which clusters to merge when constructing the dendrogram? I.e., our goal is to iteratively merge the most similar pairs of clusters until only one big cluster remains. There are many different approaches to this, for example single and complete linkage. In single linkage, we take the pair of the most similar samples (based on the Euclidean distance, for example) in each cluster, and merge the two clusters which have the most similar 2 members into one new, bigger cluster. In complete linkage, we compare the pairs of the two most dissimilar members of each cluster with each other, and we merge the 2 clusters where the distance between its 2 most dissimilar members is smallest. # \u751f\u6210\u6570\u636e import pandas as pd import numpy as np np.random.seed(123) variables = ['X', 'Y', 'Z'] labels = ['ID_0','ID_1','ID_2','ID_3','ID_4'] X = np.random.random_sample([5,3])*10 df = pd.DataFrame(X, columns=variables, index=labels) df X Y Z ID_0 6.964692 2.861393 2.268515 ID_1 5.513148 7.194690 4.231065 ID_2 9.807642 6.848297 4.809319 ID_3 3.921175 3.431780 7.290497 ID_4 4.385722 0.596779 3.980443","title":"Organizing clusters as a hierarchical tree"},{"location":"w6-unsupervised-learning/6w/#performing-hierarchical-clustering-on-a-distance-matrix","text":"[ back to top ] # calculate the distance matrix as input for the hierarchical clustering algorithm from scipy.spatial.distance import pdist,squareform row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')), columns=labels, index=labels) row_dist ID_0 ID_1 ID_2 ID_3 ID_4 ID_0 0.000000 4.973534 5.516653 5.899885 3.835396 ID_1 4.973534 0.000000 4.347073 5.104311 6.698233 ID_2 5.516653 4.347073 0.000000 7.244262 8.316594 ID_3 5.899885 5.104311 7.244262 0.000000 4.382864 ID_4 3.835396 6.698233 8.316594 4.382864 0.000000 We can either pass a condensed distance matrix (upper triangular) from the pdist function, or we can pass the \"original\" data array and define the metric='euclidean' argument in linkage. However, we should not pass the squareform distance matrix, which would yield different distance values although the overall clustering could be the same. # 1. incorrect approach: Squareform distance matrix from scipy.cluster.hierarchy import linkage row_clusters = linkage(row_dist, method='complete', metric='euclidean') pd.DataFrame(row_clusters, columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'], index=['cluster %d' %(i+1) for i in range(row_clusters.shape[0])]) row label 1 row label 2 distance no. of items in clust. cluster 1 0.0 4.0 6.521973 2.0 cluster 2 1.0 2.0 6.729603 2.0 cluster 3 3.0 5.0 8.539247 3.0 cluster 4 6.0 7.0 12.444824 5.0 # 2. correct approach: Condensed distance matrix row_clusters = linkage(pdist(df, metric='euclidean'), method='complete') pd.DataFrame(row_clusters, columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'], index=['cluster %d' %(i+1) for i in range(row_clusters.shape[0])]) row label 1 row label 2 distance no. of items in clust. cluster 1 0.0 4.0 3.835396 2.0 cluster 2 1.0 2.0 4.347073 2.0 cluster 3 3.0 5.0 5.899885 3.0 cluster 4 6.0 7.0 8.316594 5.0 # 3. correct approach: Input sample matrix row_clusters = linkage(df.values, method='complete', metric='euclidean') pd.DataFrame(row_clusters, columns=['row label 1', 'row label 2', 'distance', 'no. of items in clust.'], index=['cluster %d' %(i+1) for i in range(row_clusters.shape[0])]) row label 1 row label 2 distance no. of items in clust. cluster 1 0.0 4.0 3.835396 2.0 cluster 2 1.0 2.0 4.347073 2.0 cluster 3 3.0 5.0 5.899885 3.0 cluster 4 6.0 7.0 8.316594 5.0 # \u53ef\u89c6\u5316\u7ed3\u679c, \u4f7f\u7528 dendrogram from scipy.cluster.hierarchy import dendrogram # make dendrogram black (part 1/2) # from scipy.cluster.hierarchy import set_link_color_palette # set_link_color_palette(['black']) row_dendr = dendrogram(row_clusters, labels=labels, # make dendrogram black (part 2/2) # color_threshold=np.inf ) plt.tight_layout() plt.ylabel('Euclidean distance'); #plt.savefig('./figures/dendrogram.png', dpi=300, bbox_inches='tight')","title":"Performing hierarchical clustering on a distance matrix"},{"location":"w6-unsupervised-learning/6w/#attaching-dendrograms-to-a-heat-map","text":"[ back to top ] # plot row dendrogram fig = plt.figure(figsize=(8, 8), facecolor='white') axd = fig.add_axes([0.09, 0.1, 0.2, 0.6]) # note: for matplotlib v1.5.1, please use orientation='right' row_dendr = dendrogram(row_clusters, orientation='left') # reorder data with respect to clustering df_rowclust = df.ix[row_dendr['leaves'][::-1]] axd.set_xticks([]) axd.set_yticks([]) # remove axes spines from dendrogram for i in axd.spines.values(): i.set_visible(False) # plot heatmap axm = fig.add_axes([0.23, 0.1, 0.6, 0.6]) # x-pos, y-pos, width, height cax = axm.matshow(df_rowclust, interpolation='nearest', cmap='hot_r') fig.colorbar(cax) axm.set_xticklabels([''] + list(df_rowclust.columns)) axm.set_yticklabels([''] + list(df_rowclust.index)); # plt.savefig('./figures/heatmap.png', dpi=300)","title":"Attaching dendrograms to a heat map"},{"location":"w6-unsupervised-learning/6w/#applying-agglomerative-clustering-via-scikit-learn","text":"[ back to top ] # \u524d\u9762\u662f\u7528 scipy, \u73b0\u5728\u7528 sklearn \u6765\u805a\u7c7b from sklearn.cluster import AgglomerativeClustering ac = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete') labels = ac.fit_predict(X) print('Cluster labels: %s' % labels) Cluster labels: [0 1 1 0 0] \u7ed3\u679c\u4e0e\u524d\u9762\u4e00\u81f4","title":"Applying agglomerative clustering via scikit-learn"},{"location":"w6-unsupervised-learning/6w/#applying-agglomerative-clustering-with-iris-dataset","text":"[ back to top ] from sklearn import datasets iris = datasets.load_iris() X = iris.data[:, [2, 3]] y = iris.target n_samples, n_features = X.shape plt.scatter(X[:, 0], X[:, 1], c=y); from scipy.cluster.hierarchy import linkage from scipy.cluster.hierarchy import dendrogram clusters = linkage(X, metric='euclidean', method='complete') dendr = dendrogram(clusters) plt.ylabel('Euclidean Distance'); from sklearn.cluster import AgglomerativeClustering ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='complete') prediction = ac.fit_predict(X) print('Cluster labels:\\n %s\\n' % prediction) Cluster labels: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] plt.scatter(X[:, 0], X[:, 1], c=prediction);","title":"Applying agglomerative clustering with iris dataset"},{"location":"w6-unsupervised-learning/6w/#locating-regions-of-high-density-via-dbscan","text":"[ back to top ] Another useful approach to clustering is Density-based Spatial Clustering of Applications with Noise (DBSCAN). In essence, we can think of DBSCAN as an algorithm that divides the dataset into subgroup based on dense regions of point. In DBSCAN, we distinguish between 3 different \"points\": Core points: A core point is a point that has at least a minimum number of other points (MinPts) in its radius epsilon. Border points: A border point is a point that is not a core point, since it doesn't have enough MinPts in its neighborhood, but lies within the radius epsilon of a core point. Noise points: All other points that are neither core points nor border points. \u7ed9\u6bcf\u4e2a\u70b9 label \u4e4b\u540e, DBSCAN \u7b97\u6cd5\u5c31\u662f\u4e0b\u9762\u4e24\u6b65: 1. Form a separate cluster for each core point or a connected group of core points (core points are connected if they are no farther away than $$\\epsilon$$ ). 2. Assign each border point to the cluster of its corresponding core point. A nice feature about DBSCAN is that we don't have to specify a number of clusters upfront. However, it requires the setting of additional hyperparameters such as the value for MinPts and the radius epsilon. # \u751f\u6210\u534a\u6708\u5f62\u6570\u636e # two visible, half-moon-shaped groups consisting of 100 sample points each from sklearn.datasets import make_moons X, y = make_moons(n_samples=200, noise=0.05, random_state=0) plt.scatter(X[:,0], X[:,1]) plt.tight_layout() #plt.savefig('./figures/moons.png', dpi=300) K-means and hierarchical clustering: # complete linkage clustering f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,3)) km = KMeans(n_clusters=2, random_state=0) y_km = km.fit_predict(X) ax1.scatter(X[y_km==0,0], X[y_km==0,1], c='lightblue', marker='o', s=40, label='cluster 1') ax1.scatter(X[y_km==1,0], X[y_km==1,1], c='red', marker='s', s=40, label='cluster 2') ax1.set_title('K-means clustering') ac = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete') y_ac = ac.fit_predict(X) ax2.scatter(X[y_ac==0,0], X[y_ac==0,1], c='lightblue', marker='o', s=40, label='cluster 1') ax2.scatter(X[y_ac==1,0], X[y_ac==1,1], c='red', marker='s', s=40, label='cluster 2') ax2.set_title('Agglomerative clustering') plt.legend() plt.tight_layout() #plt.savefig('./figures/kmeans_and_ac.png', dpi=300) \u6548\u679c\u5e76\u4e0d\u597d, \u5e76\u4e0d\u80fd\u5b8c\u5168 separated Density-based clustering: # DBSCAN from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.2, min_samples=5, metric='euclidean') y_db = db.fit_predict(X) plt.scatter(X[y_db==0,0], X[y_db==0,1], c='lightblue', marker='o', s=40, label='cluster 1') plt.scatter(X[y_db==1,0], X[y_db==1,1], c='red', marker='s', s=40, label='cluster 2') plt.legend() plt.tight_layout() #plt.savefig('./figures/moons_dbscan.png', dpi=300) DBSCAN \u80fd\u5f88\u597d\u5730\u5bf9\u534a\u6708\u6570\u636e\u8fdb\u884c\u805a\u7c7b \u53ef\u4ee5\u518d\u8bd5\u8bd5\u5706\u73af\u56fe\u5f62 from sklearn.datasets import make_circles X, y = make_circles(n_samples=500, factor=.6, noise=.05) plt.scatter(X[:, 0], X[:, 1], c=y); k-means from sklearn.cluster import KMeans km = KMeans(n_clusters=2) predict = km.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=predict); allomerative clustering from sklearn.cluster import AgglomerativeClustering ac = AgglomerativeClustering() predict = ac.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=predict); DBSCAN from sklearn.cluster import DBSCAN db = DBSCAN(eps=0.15, min_samples=9, metric='euclidean') predict = db.fit_predict(X) plt.scatter(X[:, 0], X[:, 1], c=predict);","title":"Locating regions of high density via DBSCAN"},{"location":"w6-unsupervised-learning/6w/#learning-from-labeled-and-unlabeled-data-with-label-propagation","text":"[ back to top ] \u56e0\u4e3a\u6807\u6ce8\u6210\u672c\u6bd4\u8f83\u9ad8\uff0c\u5f53\u4f60\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u53ea\u6709\u4e00\u90e8\u5206\u6570\u636e\u662f\u6709\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u4f60\u53ea\u80fd\u6254\u6389\u90a3\u4e9b\u6ca1\u6709\u6807\u6ce8\u7684 X \u3002 \u800c\u5b9e\u9645\u4e0a\uff0c\u6709\u6807\u6ce8\u7684\u6837\u672c\u548c\u65e0\u6807\u6ce8\u7684\u6837\u672c\u4e4b\u95f4\u662f\u6709\u5173\u7cfb\u7684\uff0c\u8fd9\u79cd\u5173\u7cfb\u4fe1\u606f\u4e5f\u53ef\u4ee5\u7528\u6765\u5e2e\u52a9\u5b66\u4e60\u3002\u8fd9\u5c31\u662f\u534a\u76d1\u7763\u5b66\u4e60\u6807\u7b7e\u4f20\u64ad\uff08Label Propagation\uff09\u7b97\u6cd5\u7684\u601d\u8def\u3002 \u5b83\u7684\u57fa\u672c\u903b\u8f91\u662f\u501f\u52a9\u4e8e\u8fd1\u6731\u8005\u8d64\u7684\u601d\u8def\uff0c\u4e5f\u5c31\u662f KNN \u7684\u601d\u8def\uff0c\u5982\u679c A \u548c B \u5728 X \u7a7a\u95f4\u4e0a\u5f88\u63a5\u8fd1\uff0c\u90a3\u4e48 A \u7684 y \u6807\u7b7e\u5c31\u53ef\u4ee5\u4f20\u7ed9 B\u3002 \u8fdb\u4e00\u6b65\u8fed\u4ee3\u4e0b\u53bb\uff0c\u5982\u679c C \u548c B \u4e5f\u5f88\u63a5\u8fd1\uff0cC \u7684\u6807\u7b7e\u4e5f\u5e94\u8be5\u548c B \u4e00\u6837\u3002 \u6240\u4ee5\u57fa\u672c\u8ba1\u7b97\u903b\u8f91\u5c31\u662f\u4e24\u6b65\uff0c\u7b2c\u4e00\u6b65\u662f\u8ba1\u7b97\u6837\u672c\u95f4\u7684\u8ddd\u79bb\uff0c\u6784\u5efa\u8f6c\u79fb\u77e9\u9635\uff0c\u7b2c\u4e8c\u6b65\u662f\u5c06\u8f6c\u79fb\u77e9\u9635\u548c Y \u77e9\u9635\u76f8\u4e58\uff0cY \u91cc\u9762\u5305\u62ec\u4e86\u5df2\u6807\u6ce8\u548c\u672a\u6807\u6ce8\u7684\u4e24\u90e8\u5206\uff0c \u901a\u8fc7\u76f8\u4e58\u53ef\u4ee5\u5c06\u5df2\u6807\u6ce8\u7684 Y \u8f6c\u64ad\u7ed9\u672a\u6807\u6ce8\u7684 Y\u3002 \u5177\u4f53\u8bba\u6587\u53ef\u4ee5 \u770b\u8fd9\u91cc \u3002 \u5728 Sklearn \u6a21\u5757\u4e2d\u5df2\u7ecf\u5185\u7f6e\u4e86\u8fd9\u79cd\u7b97\u6cd5\uff0c\u6587\u6863\u793a\u4f8b\u53ef\u4ee5 \u770b\u8fd9\u91cc \u3002 \u4e0b\u9762\u662f\u7528 Python \u7684 NumPy \u6a21\u5757\u5b9e\u73b0\u7684\u4e00\u4e2a toy demo\u3002 import pandas as pd import numpy as np # \u8bfb\u5165 iris \u6570\u636e from sklearn.datasets import load_iris iris = load_iris() X = iris.data y = iris.target n = len(y) # \u5207\u5206\u6570\u636e np.random.seed(42) train_index = np.random.choice(n, int(0.6*n), replace = False) test_index = np.setdiff1d(np.arange(n), train_index) # \u8ba1\u7b97\u6743\u91cd\u77e9\u9635 sigma = X.var(axis = 0) weights = np.zeros((n,n)) def weight_func(ind1, ind2, X=X, sigma=sigma): return np.exp(-np.sum((X[ind1] - X[ind2])**2/sigma)) for i in range(n): for j in range(n): weights[i,j] = weight_func(i,j) ## \u6807\u51c6\u5316\u4e3a\u8f6c\u79fb\u77e9\u9635 t = weights / weights.sum(axis=1) # y\u8f6c\u6362\u5f62\u5f0f y_m = np.zeros((n, len(np.unique(y)))) for i in range(n): y_m[i,y[i]] = 1 ## unlabel\u521d\u59cb\u5316, label\u8bb0\u4f4f y_m[test_index] = np.random.random(y_m[test_index].shape) clamp = y_m[train_index] ## \u8fed\u4ee3\u8ba1\u7b97 iter_n = 50 for _ in range(iter_n): y_m = t.dot(y_m) # LP y_m = (y_m.T / y_m.sum(axis=1)).T # normalize y_m[train_index] = clamp # clamp # \u9884\u6d4b\u51c6\u786e\u7387 predict = y_m[test_index].argmax(axis=1) np.sum(y[test_index] == predict) / float(len(predict)) 0.91666666666666663","title":"Learning from labeled and unlabeled data with label propagation"},{"location":"w6-unsupervised-learning/6w/#label-propagation-learning-a-complex-structure","text":"Example of LabelPropagation learning a complex internal structure to demonstrate \u201cmanifold learning\u201d. The outer circle should be labeled \u201cred\u201d and the inner circle \u201cblue\u201d. Because both label groups lie inside their own distinct shape, we can see that the labels propagate correctly around the circle. import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.semi_supervised import label_propagation from sklearn.datasets import make_circles # generate ring with inner box n_samples = 200 X, y = make_circles(n_samples=n_samples, shuffle=False) outer, inner = 0, 1 labels = -np.ones(n_samples) labels[0] = outer labels[-1] = inner # Learn with LabelSpreading label_spread = label_propagation.LabelSpreading(kernel='knn', alpha=1.0) label_spread.fit(X, labels) # Plot output labels output_labels = label_spread.transduction_ plt.figure(figsize=(8.5, 4)) plt.subplot(1, 2, 1) plt.scatter(X[labels == outer, 0], X[labels == outer, 1], color='navy', marker='s', lw=0, label= outer labeled , s=10) plt.scatter(X[labels == inner, 0], X[labels == inner, 1], color='c', marker='s', lw=0, label='inner labeled', s=10) plt.scatter(X[labels == -1, 0], X[labels == -1, 1], color='darkorange', marker='.', label='unlabeled') plt.legend(scatterpoints=1, shadow=False, loc='upper right') plt.title( Raw data (2 classes=outer and inner) ) plt.subplot(1, 2, 2) output_label_array = np.asarray(output_labels) outer_numbers = np.where(output_label_array == outer)[0] inner_numbers = np.where(output_label_array == inner)[0] plt.scatter(X[outer_numbers, 0], X[outer_numbers, 1], color='navy', marker='s', lw=0, s=10, label= outer learned ) plt.scatter(X[inner_numbers, 0], X[inner_numbers, 1], color='c', marker='s', lw=0, s=10, label= inner learned ) plt.legend(scatterpoints=1, shadow=False, loc='upper right') plt.title( Labels learned with Label Spreading (KNN) ) plt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92) plt.show()","title":"Label Propagation learning a complex structure"},{"location":"w6-unsupervised-learning/6w/#_1","text":"","title":"\u7ec3\u4e60\uff1a\u4f7f\u7528\u4fe1\u8d37\u6570\u636e\u96c6\u4e2d\u7684\u81ea\u53d8\u91cf\uff0c\u6765\u5bf9\u8fd9\u4e9b\u7528\u6237\u8fdb\u884c\u805a\u7c7b"},{"location":"w7-text-mining/7w/","text":"Sections Obtaining the IMDb movie review dataset Text-feature-extraction Bag-of-words model Bigrams and N-Grams Character n-grams Tfidf encoding Cleaning text data Processing documents into tokens Training a logistic regression model for sentiment classification Working with bigger data - online algorithms and out-of-core learning Model persistence word2vec Obtaining the IMDb movie review dataset [ back to top ] \u6570\u636e\u53ef\u4ece \u8fd9 \u4e0b\u8f7d \u89e3\u538b\u4e4b\u540e\uff0c\u4e0b\u9762\u4ee3\u7801\u53ef\u5c06\u6570\u636e\u8bfb\u6210 Pandas \u7684 DataFrame cd Documents/shanghai_python/data/aclImdb/ /Users/xiaokai/Documents/shanghai_python/data/aclImdb ls README imdb.vocab imdbEr.txt \u001b[34mtest\u001b[m\u001b[m/ \u001b[34mtrain\u001b[m\u001b[m/ cd train/pos/ /Users/xiaokai/Documents/shanghai_python/data/aclImdb/train/pos ls 0_9.txt 12250_10.txt 3250_9.txt 5500_10.txt 7751_7.txt 10000_8.txt 12251_9.txt 3251_7.txt 5501_10.txt 7752_8.txt 10001_10.txt 12252_10.txt 3252_9.txt 5502_10.txt 7753_10.txt 10002_7.txt 12253_9.txt 3253_8.txt 5503_10.txt 7754_9.txt 10003_8.txt 12254_10.txt 3254_9.txt 5504_10.txt 7755_9.txt 10004_8.txt 12255_10.txt 3255_10.txt 5505_10.txt 7756_10.txt 10005_7.txt 12256_9.txt 3256_9.txt 5506_10.txt 7757_9.txt 10006_7.txt 12257_8.txt 3257_9.txt 5507_7.txt 7758_7.txt 10007_7.txt 12258_10.txt 3258_10.txt 5508_10.txt 7759_10.txt 10008_7.txt 12259_7.txt 3259_7.txt 5509_10.txt 775_7.txt 10009_9.txt 1225_10.txt 325_9.txt 550_10.txt 7760_10.txt 1000_8.txt 12260_10.txt 3260_8.txt 5510_10.txt 7761_10.txt 10010_7.txt 12261_10.txt 3261_9.txt 5511_8.txt 7762_8.txt 10011_9.txt 12262_7.txt 3262_8.txt 5512_7.txt 7763_8.txt 10012_8.txt 12263_10.txt 3263_7.txt 5513_7.txt 7764_9.txt 10013_7.txt 12264_9.txt 3264_8.txt 5514_9.txt 7765_7.txt 10014_8.txt 12265_10.txt 3265_10.txt 5515_7.txt 7766_7.txt 10015_8.txt 12266_9.txt 3266_7.txt 5516_7.txt 7767_8.txt 10016_8.txt 12267_8.txt 3267_8.txt 5517_9.txt 7768_8.txt 10017_9.txt 12268_7.txt 3268_8.txt 5518_8.txt 7769_7.txt 10018_8.txt 12269_8.txt 3269_8.txt 5519_9.txt 776_7.txt 10019_8.txt 1226_10.txt 326_10.txt 551_8.txt 7770_7.txt 1001_8.txt 12270_10.txt 3270_8.txt 5520_9.txt 7771_9.txt 10020_8.txt 12271_7.txt 3271_9.txt 5521_8.txt 7772_7.txt 10021_8.txt 12272_7.txt 3272_8.txt 5522_8.txt 7773_10.txt 10022_7.txt 12273_10.txt 3273_9.txt 5523_8.txt 7774_7.txt 10023_9.txt 12274_10.txt 3274_8.txt 5524_8.txt 7775_8.txt 10024_9.txt 12275_9.txt 3275_10.txt 5525_8.txt 7776_7.txt 10025_9.txt 12276_7.txt 3276_8.txt 5526_10.txt 7777_10.txt 10026_7.txt 12277_10.txt 3277_9.txt 5527_8.txt 7778_10.txt 10027_7.txt 12278_10.txt 3278_8.txt 5528_7.txt 7779_7.txt 10028_10.txt 12279_9.txt 3279_8.txt 5529_7.txt 777_7.txt 10029_10.txt 1227_8.txt 327_8.txt 552_8.txt 7780_8.txt 1002_7.txt 12280_9.txt 3280_10.txt 5530_8.txt 7781_8.txt 10030_10.txt 12281_9.txt 3281_10.txt 5531_10.txt 7782_7.txt 10031_10.txt 12282_8.txt 3282_8.txt 5532_10.txt 7783_7.txt 10032_10.txt 12283_8.txt 3283_8.txt 5533_7.txt 7784_7.txt 10033_10.txt 12284_7.txt 3284_10.txt 5534_7.txt 7785_9.txt 10034_8.txt 12285_10.txt 3285_10.txt 5535_8.txt 7786_8.txt 10035_9.txt 12286_7.txt 3286_10.txt 5536_9.txt 7787_10.txt 10036_8.txt 12287_10.txt 3287_9.txt 5537_10.txt 7788_10.txt 10037_9.txt 12288_9.txt 3288_10.txt 5538_7.txt 7789_10.txt 10038_10.txt 12289_9.txt 3289_10.txt 5539_10.txt 778_10.txt 10039_10.txt 1228_9.txt 328_10.txt 553_7.txt 7790_9.txt 1003_10.txt 12290_10.txt 3290_8.txt 5540_10.txt 7791_10.txt 10040_10.txt 12291_10.txt 3291_8.txt 5541_9.txt 7792_10.txt 10041_10.txt 12292_10.txt 3292_10.txt 5542_9.txt 7793_10.txt 10042_10.txt 12293_10.txt 3293_10.txt 5543_7.txt 7794_7.txt 10043_10.txt 12294_8.txt 3294_9.txt 5544_7.txt 7795_9.txt 10044_9.txt 12295_9.txt 3295_10.txt 5545_8.txt 7796_10.txt 10045_10.txt 12296_8.txt 3296_10.txt 5546_7.txt 7797_10.txt 10046_9.txt 12297_7.txt 3297_10.txt 5547_8.txt 7798_10.txt 10047_10.txt 12298_8.txt 3298_10.txt 5548_7.txt 7799_8.txt 10048_10.txt 12299_7.txt 3299_10.txt 5549_8.txt 779_10.txt 10049_8.txt 1229_8.txt 329_10.txt 554_7.txt 77_7.txt 1004_7.txt 122_9.txt 32_10.txt 5550_7.txt 7800_8.txt 10050_10.txt 12300_10.txt 3300_9.txt 5551_7.txt 7801_9.txt 10051_10.txt 12301_8.txt 3301_10.txt 5552_8.txt 7802_10.txt 10052_10.txt 12302_7.txt 3302_10.txt 5553_8.txt 7803_10.txt 10053_8.txt 12303_9.txt 3303_10.txt 5554_8.txt 7804_10.txt 10054_10.txt 12304_10.txt 3304_10.txt 5555_10.txt 7805_10.txt 10055_7.txt 12305_8.txt 3305_10.txt 5556_10.txt 7806_10.txt 10056_8.txt 12306_9.txt 3306_8.txt 5557_9.txt 7807_10.txt 10057_9.txt 12307_10.txt 3307_8.txt 5558_7.txt 7808_10.txt 10058_7.txt 12308_10.txt 3308_8.txt 5559_7.txt 7809_10.txt 10059_10.txt 12309_10.txt 3309_9.txt 555_8.txt 780_7.txt 1005_10.txt 1230_10.txt 330_10.txt 5560_7.txt 7810_7.txt 10060_9.txt 12310_10.txt 3310_8.txt 5561_8.txt 7811_7.txt 10061_8.txt 12311_9.txt 3311_10.txt 5562_10.txt 7812_8.txt 10062_10.txt 12312_10.txt 3312_8.txt 5563_8.txt 7813_10.txt 10063_9.txt 12313_10.txt 3313_10.txt 5564_10.txt 7814_10.txt 10064_10.txt 12314_10.txt 3314_10.txt 5565_8.txt 7815_8.txt 10065_9.txt 12315_9.txt 3315_10.txt 5566_9.txt 7816_9.txt 10066_10.txt 12316_10.txt 3316_10.txt 5567_7.txt 7817_10.txt 10067_9.txt 12317_10.txt 3317_10.txt 5568_8.txt 7818_9.txt 10068_8.txt 12318_9.txt 3318_9.txt 5569_7.txt 7819_9.txt 10069_8.txt 12319_8.txt 3319_9.txt 556_9.txt 781_9.txt 1006_8.txt 1231_7.txt 331_10.txt 5570_10.txt 7820_8.txt 10070_9.txt 12320_8.txt 3320_8.txt 5571_10.txt 7821_8.txt 10071_9.txt 12321_10.txt 3321_7.txt 5572_10.txt 7822_8.txt 10072_9.txt 12322_10.txt 3322_8.txt 5573_10.txt 7823_7.txt 10073_10.txt 12323_10.txt 3323_9.txt 5574_10.txt 7824_10.txt 10074_9.txt 12324_10.txt 3324_7.txt 5575_8.txt 7825_9.txt 10075_9.txt 12325_9.txt 3325_7.txt 5576_9.txt 7826_8.txt 10076_9.txt 12326_10.txt 3326_8.txt 5577_8.txt 7827_10.txt 10077_10.txt 12327_9.txt 3327_10.txt 5578_10.txt 7828_10.txt 10078_8.txt 12328_10.txt 3328_7.txt 5579_8.txt 7829_7.txt 10079_8.txt 12329_10.txt 3329_7.txt 557_9.txt 782_9.txt 1007_10.txt 1232_10.txt 332_10.txt 5580_7.txt 7830_10.txt 10080_10.txt 12330_7.txt 3330_7.txt 5581_7.txt 7831_8.txt 10081_9.txt 12331_8.txt 3331_8.txt 5582_9.txt 7832_7.txt 10082_10.txt 12332_8.txt 3332_8.txt 5583_8.txt 7833_8.txt 10083_7.txt 12333_9.txt 3333_8.txt 5584_8.txt 7834_8.txt 10084_10.txt 12334_7.txt 3334_9.txt 5585_7.txt 7835_9.txt 10085_10.txt 12335_9.txt 3335_8.txt 5586_8.txt 7836_8.txt 10086_7.txt 12336_9.txt 3336_7.txt 5587_7.txt 7837_10.txt 10087_10.txt 12337_7.txt 3337_9.txt 5588_10.txt 7838_8.txt 10088_10.txt 12338_10.txt 3338_7.txt 5589_7.txt 7839_10.txt 10089_7.txt 12339_10.txt 3339_7.txt 558_10.txt 783_10.txt 1008_10.txt 1233_7.txt 333_10.txt 5590_8.txt 7840_8.txt 10090_8.txt 12340_8.txt 3340_8.txt 5591_8.txt 7841_10.txt 10091_7.txt 12341_7.txt 3341_7.txt 5592_10.txt 7842_10.txt 10092_8.txt 12342_8.txt 3342_7.txt 5593_10.txt 7843_9.txt 10093_7.txt 12343_7.txt 3343_7.txt 5594_10.txt 7844_10.txt 10094_7.txt 12344_8.txt 3344_9.txt 5595_7.txt 7845_10.txt 10095_7.txt 12345_10.txt 3345_9.txt 5596_10.txt 7846_10.txt 10096_7.txt 12346_10.txt 3346_7.txt 5597_10.txt 7847_9.txt 10097_9.txt 12347_9.txt 3347_7.txt 5598_8.txt 7848_10.txt 10098_10.txt 12348_9.txt 3348_7.txt 5599_7.txt 7849_9.txt 10099_10.txt 12349_10.txt 3349_8.txt 559_8.txt 784_10.txt 1009_8.txt 1234_10.txt 334_10.txt 55_9.txt 7850_8.txt 100_7.txt 12350_9.txt 3350_7.txt 5600_8.txt 7851_8.txt 10100_10.txt 12351_8.txt 3351_8.txt 5601_8.txt 7852_9.txt 10101_8.txt 12352_8.txt 3352_9.txt 5602_10.txt 7853_9.txt 10102_7.txt 12353_9.txt 3353_8.txt 5603_7.txt 7854_10.txt 10103_8.txt 12354_10.txt 3354_10.txt 5604_8.txt 7855_8.txt 10104_10.txt 12355_10.txt 3355_10.txt 5605_7.txt 7856_10.txt 10105_8.txt 12356_8.txt 3356_10.txt 5606_8.txt 7857_10.txt 10106_8.txt 12357_7.txt 3357_9.txt 5607_7.txt 7858_10.txt 10107_8.txt 12358_7.txt 3358_9.txt 5608_9.txt 7859_7.txt 10108_10.txt 12359_8.txt 3359_10.txt 5609_10.txt 785_9.txt 10109_10.txt 1235_10.txt 335_10.txt 560_8.txt 7860_10.txt 1010_10.txt 12360_10.txt 3360_7.txt 5610_7.txt 7861_10.txt 10110_10.txt 12361_7.txt 3361_7.txt 5611_10.txt 7862_7.txt 10111_7.txt 12362_8.txt 3362_8.txt 5612_8.txt 7863_10.txt 10112_7.txt 12363_9.txt 3363_10.txt 5613_9.txt 7864_8.txt 10113_10.txt 12364_7.txt 3364_10.txt 5614_10.txt 7865_8.txt 10114_10.txt 12365_7.txt 3365_9.txt 5615_8.txt 7866_10.txt 10115_10.txt 12366_10.txt 3366_10.txt 5616_8.txt 7867_7.txt 10116_10.txt 12367_9.txt 3367_9.txt 5617_8.txt 7868_8.txt 10117_8.txt 12368_8.txt 3368_8.txt 5618_10.txt 7869_9.txt 10118_7.txt 12369_7.txt 3369_7.txt 5619_9.txt 786_7.txt 10119_7.txt 1236_7.txt 336_10.txt 561_10.txt 7870_10.txt 1011_10.txt 12370_8.txt 3370_9.txt 5620_10.txt 7871_9.txt 10120_7.txt 12371_8.txt 3371_8.txt 5621_10.txt 7872_10.txt 10121_8.txt 12372_7.txt 3372_9.txt 5622_10.txt 7873_8.txt 10122_7.txt 12373_7.txt 3373_7.txt 5623_10.txt 7874_8.txt 10123_10.txt 12374_7.txt 3374_7.txt 5624_7.txt 7875_7.txt 10124_8.txt 12375_7.txt 3375_9.txt 5625_8.txt 7876_8.txt 10125_8.txt 12376_7.txt 3376_9.txt 5626_8.txt 7877_7.txt 10126_10.txt 12377_10.txt 3377_8.txt 5627_10.txt 7878_7.txt 10127_8.txt 12378_8.txt 3378_7.txt 5628_7.txt 7879_7.txt 10128_9.txt 12379_8.txt 3379_7.txt 5629_10.txt 787_9.txt 10129_7.txt 1237_8.txt 337_9.txt 562_8.txt 7880_8.txt 1012_10.txt 12380_7.txt 3380_7.txt 5630_8.txt 7881_9.txt 10130_10.txt 12381_8.txt 3381_10.txt 5631_8.txt 7882_7.txt 10131_10.txt 12382_8.txt 3382_10.txt 5632_8.txt 7883_10.txt 10132_9.txt 12383_7.txt 3383_8.txt 5633_8.txt 7884_7.txt 10133_7.txt 12384_8.txt 3384_8.txt 5634_8.txt 7885_9.txt 10134_7.txt 12385_7.txt 3385_9.txt 5635_7.txt 7886_10.txt 10135_7.txt 12386_8.txt 3386_9.txt 5636_10.txt 7887_7.txt 10136_7.txt 12387_10.txt 3387_10.txt 5637_10.txt 7888_8.txt 10137_7.txt 12388_8.txt 3388_7.txt 5638_10.txt 7889_10.txt 10138_8.txt 12389_10.txt 3389_7.txt 5639_7.txt 788_8.txt 10139_8.txt 1238_7.txt 338_10.txt 563_10.txt 7890_7.txt 1013_9.txt 12390_8.txt 3390_8.txt 5640_7.txt 7891_8.txt 10140_8.txt 12391_9.txt 3391_7.txt 5641_7.txt 7892_7.txt 10141_9.txt 12392_9.txt 3392_7.txt 5642_7.txt 7893_7.txt 10142_8.txt 12393_8.txt 3393_9.txt 5643_7.txt 7894_10.txt 10143_8.txt 12394_10.txt 3394_10.txt 5644_8.txt 7895_10.txt 10144_8.txt 12395_8.txt 3395_9.txt 5645_10.txt 7896_10.txt 10145_8.txt 12396_7.txt 3396_7.txt 5646_10.txt 7897_8.txt 10146_7.txt 12397_8.txt 3397_10.txt 5647_10.txt 7898_10.txt 10147_10.txt 12398_8.txt 3398_10.txt 5648_10.txt 7899_10.txt 10148_10.txt 12399_9.txt 3399_10.txt 5649_10.txt 789_7.txt 10149_9.txt 1239_8.txt 339_10.txt 564_8.txt 78_10.txt 1014_9.txt 123_10.txt 33_7.txt 5650_10.txt 7900_10.txt 10150_9.txt 12400_8.txt 3400_8.txt 5651_10.txt 7901_7.txt 10151_8.txt 12401_8.txt 3401_8.txt 5652_10.txt 7902_10.txt 10152_9.txt 12402_7.txt 3402_8.txt 5653_10.txt 7903_10.txt 10153_9.txt 12403_8.txt 3403_9.txt 5654_8.txt 7904_8.txt 10154_8.txt 12404_9.txt 3404_8.txt 5655_7.txt 7905_10.txt 10155_9.txt 12405_10.txt 3405_9.txt 5656_10.txt 7906_7.txt 10156_10.txt 12406_8.txt 3406_10.txt 5657_9.txt 7907_7.txt 10157_10.txt 12407_7.txt 3407_8.txt 5658_10.txt 7908_9.txt 10158_7.txt 12408_7.txt 3408_10.txt 5659_9.txt 7909_10.txt 10159_7.txt 12409_7.txt 3409_10.txt 565_10.txt 790_9.txt 1015_10.txt 1240_7.txt 340_10.txt 5660_10.txt 7910_10.txt 10160_7.txt 12410_8.txt 3410_9.txt 5661_10.txt 7911_10.txt 10161_9.txt 12411_7.txt 3411_9.txt 5662_10.txt 7912_8.txt 10162_9.txt 12412_9.txt 3412_8.txt 5663_10.txt 7913_10.txt 10163_8.txt 12413_9.txt 3413_10.txt 5664_8.txt 7914_10.txt 10164_7.txt 12414_10.txt 3414_8.txt 5665_9.txt 7915_8.txt 10165_7.txt 12415_8.txt 3415_9.txt 5666_10.txt 7916_8.txt 10166_7.txt 12416_10.txt 3416_10.txt 5667_10.txt 7917_8.txt 10167_7.txt 12417_10.txt 3417_10.txt 5668_10.txt 7918_10.txt 10168_8.txt 12418_10.txt 3418_10.txt 5669_10.txt 7919_10.txt 10169_7.txt 12419_10.txt 3419_10.txt 566_8.txt 791_9.txt 1016_8.txt 1241_7.txt 341_10.txt 5670_8.txt 7920_7.txt 10170_8.txt 12420_9.txt 3420_10.txt 5671_9.txt 7921_10.txt 10171_7.txt 12421_10.txt 3421_10.txt 5672_9.txt 7922_7.txt 10172_8.txt 12422_8.txt 3422_10.txt 5673_9.txt 7923_8.txt 10173_8.txt 12423_10.txt 3423_7.txt 5674_7.txt 7924_9.txt 10174_7.txt 12424_9.txt 3424_10.txt 5675_9.txt 7925_7.txt 10175_10.txt 12425_7.txt 3425_8.txt 5676_9.txt 7926_10.txt 10176_7.txt 12426_7.txt 3426_7.txt 5677_10.txt 7927_10.txt 10177_9.txt 12427_7.txt 3427_8.txt 5678_10.txt 7928_10.txt 10178_10.txt 12428_8.txt 3428_9.txt 5679_10.txt 7929_10.txt 10179_9.txt 12429_7.txt 3429_10.txt 567_10.txt 792_8.txt 1017_8.txt 1242_9.txt 342_10.txt 5680_10.txt 7930_7.txt 10180_7.txt 12430_7.txt 3430_10.txt 5681_8.txt 7931_8.txt 10181_8.txt 12431_8.txt 3431_10.txt 5682_7.txt 7932_8.txt 10182_8.txt 12432_8.txt 3432_8.txt 5683_10.txt 7933_10.txt 10183_7.txt 12433_8.txt 3433_9.txt 5684_10.txt 7934_7.txt 10184_9.txt 12434_10.txt 3434_8.txt 5685_10.txt 7935_8.txt 10185_10.txt 12435_8.txt 3435_10.txt 5686_9.txt 7936_7.txt 10186_8.txt 12436_7.txt 3436_8.txt 5687_7.txt 7937_10.txt 10187_7.txt 12437_7.txt 3437_8.txt 5688_8.txt 7938_7.txt 10188_8.txt 12438_8.txt 3438_10.txt 5689_10.txt 7939_8.txt 10189_7.txt 12439_8.txt 3439_9.txt 568_7.txt 793_9.txt 1018_8.txt 1243_9.txt 343_10.txt 5690_7.txt 7940_7.txt 10190_7.txt 12440_7.txt 3440_8.txt 5691_8.txt 7941_8.txt 10191_10.txt 12441_9.txt 3441_8.txt 5692_7.txt 7942_10.txt 10192_8.txt 12442_8.txt 3442_10.txt 5693_7.txt 7943_10.txt 10193_9.txt 12443_9.txt 3443_9.txt 5694_9.txt 7944_9.txt 10194_10.txt 12444_10.txt 3444_10.txt 5695_8.txt 7945_8.txt 10195_8.txt 12445_9.txt 3445_7.txt 5696_8.txt 7946_10.txt 10196_10.txt 12446_8.txt 3446_7.txt 5697_8.txt 7947_10.txt 10197_7.txt 12447_8.txt 3447_9.txt 5698_8.txt 7948_7.txt 10198_8.txt 12448_10.txt 3448_7.txt 5699_8.txt 7949_10.txt 10199_7.txt 12449_8.txt 3449_8.txt 569_10.txt 794_8.txt 1019_10.txt 1244_8.txt 344_8.txt 56_10.txt 7950_9.txt 101_8.txt 12450_7.txt 3450_7.txt 5700_8.txt 7951_10.txt 10200_10.txt 12451_10.txt 3451_8.txt 5701_8.txt 7952_8.txt 10201_10.txt 12452_8.txt 3452_8.txt 5702_10.txt 7953_9.txt 10202_10.txt 12453_7.txt 3453_7.txt 5703_7.txt 7954_7.txt 10203_10.txt 12454_7.txt 3454_8.txt 5704_7.txt 7955_10.txt 10204_8.txt 12455_9.txt 3455_10.txt 5705_10.txt 7956_10.txt 10205_10.txt 12456_10.txt 3456_9.txt 5706_9.txt 7957_10.txt 10206_10.txt 12457_8.txt 3457_7.txt 5707_10.txt 7958_10.txt 10207_10.txt 12458_7.txt 3458_10.txt 5708_10.txt 7959_10.txt 10208_7.txt 12459_10.txt 3459_8.txt 5709_10.txt 795_8.txt 10209_7.txt 1245_7.txt 345_7.txt 570_10.txt 7960_10.txt 1020_10.txt 12460_7.txt 3460_8.txt 5710_8.txt 7961_10.txt 10210_7.txt 12461_9.txt 3461_9.txt 5711_8.txt 7962_9.txt 10211_7.txt 12462_7.txt 3462_7.txt 5712_8.txt 7963_9.txt 10212_8.txt 12463_8.txt 3463_7.txt 5713_10.txt 7964_10.txt 10213_8.txt 12464_10.txt 3464_10.txt 5714_7.txt 7965_10.txt 10214_10.txt 12465_9.txt 3465_9.txt 5715_10.txt 7966_9.txt 10215_10.txt 12466_7.txt 3466_7.txt 5716_8.txt 7967_9.txt 10216_8.txt 12467_7.txt 3467_7.txt 5717_9.txt 7968_10.txt 10217_9.txt 12468_7.txt 3468_10.txt 5718_7.txt 7969_10.txt 10218_8.txt 12469_10.txt 3469_10.txt 5719_9.txt 796_8.txt 10219_10.txt 1246_8.txt 346_10.txt 571_10.txt 7970_10.txt 1021_10.txt 12470_10.txt 3470_8.txt 5720_10.txt 7971_10.txt 10220_7.txt 12471_7.txt 3471_8.txt 5721_10.txt 7972_10.txt 10221_8.txt 12472_10.txt 3472_10.txt 5722_8.txt 7973_10.txt 10222_9.txt 12473_10.txt 3473_9.txt 5723_7.txt 7974_7.txt 10223_10.txt 12474_9.txt 3474_10.txt 5724_10.txt 7975_8.txt 10224_10.txt 12475_10.txt 3475_9.txt 5725_9.txt 7976_7.txt 10225_9.txt 12476_10.txt 3476_10.txt 5726_7.txt 7977_8.txt 10226_10.txt 12477_10.txt 3477_10.txt 5727_9.txt 7978_8.txt 10227_10.txt 12478_9.txt 3478_8.txt 5728_10.txt 7979_9.txt 10228_8.txt 12479_10.txt 3479_9.txt 5729_9.txt 797_8.txt 10229_8.txt 1247_8.txt 347_10.txt 572_9.txt 7980_10.txt 1022_10.txt 12480_10.txt 3480_10.txt 5730_10.txt 7981_10.txt 10230_9.txt 12481_8.txt 3481_10.txt 5731_7.txt 7982_10.txt 10231_10.txt 12482_8.txt 3482_10.txt 5732_7.txt 7983_9.txt 10232_10.txt 12483_9.txt 3483_9.txt 5733_7.txt 7984_10.txt 10233_7.txt 12484_8.txt 3484_10.txt 5734_9.txt 7985_10.txt 10234_10.txt 12485_8.txt 3485_9.txt 5735_7.txt 7986_10.txt 10235_8.txt 12486_7.txt 3486_10.txt 5736_7.txt 7987_8.txt 10236_8.txt 12487_10.txt 3487_9.txt 5737_7.txt 7988_10.txt 10237_10.txt 12488_8.txt 3488_7.txt 5738_10.txt 7989_10.txt 10238_10.txt 12489_10.txt 3489_10.txt 5739_10.txt 798_10.txt 10239_10.txt 1248_8.txt 348_7.txt 573_9.txt 7990_7.txt 1023_10.txt 12490_8.txt 3490_8.txt 5740_10.txt 7991_7.txt 10240_8.txt 12491_8.txt 3491_8.txt 5741_10.txt 7992_7.txt 10241_8.txt 12492_7.txt 3492_7.txt 5742_10.txt 7993_10.txt 10242_8.txt 12493_8.txt 3493_8.txt 5743_10.txt 7994_9.txt 10243_10.txt 12494_8.txt 3494_9.txt 5744_7.txt 7995_8.txt 10244_7.txt 12495_7.txt 3495_7.txt 5745_8.txt 7996_8.txt 10245_10.txt 12496_8.txt 3496_8.txt 5746_9.txt 7997_10.txt 10246_10.txt 12497_10.txt 3497_10.txt 5747_8.txt 7998_9.txt 10247_10.txt 12498_10.txt 3498_10.txt 5748_10.txt 7999_10.txt 10248_7.txt 12499_7.txt 3499_8.txt 5749_10.txt 799_8.txt 10249_7.txt 1249_9.txt 349_10.txt 574_7.txt 79_10.txt 1024_9.txt 124_10.txt 34_8.txt 5750_8.txt 7_7.txt 10250_10.txt 1250_10.txt 3500_10.txt 5751_10.txt 8000_10.txt 10251_10.txt 1251_9.txt 3501_9.txt 5752_8.txt 8001_10.txt 10252_9.txt 1252_8.txt 3502_9.txt 5753_8.txt 8002_7.txt 10253_10.txt 1253_10.txt 3503_7.txt 5754_9.txt 8003_8.txt 10254_8.txt 1254_7.txt 3504_7.txt 5755_7.txt 8004_9.txt 10255_9.txt 1255_10.txt 3505_8.txt 5756_8.txt 8005_9.txt 10256_8.txt 1256_8.txt 3506_10.txt 5757_8.txt 8006_10.txt 10257_8.txt 1257_10.txt 3507_7.txt 5758_10.txt 8007_7.txt 10258_10.txt 1258_9.txt 3508_8.txt 5759_10.txt 8008_7.txt 10259_8.txt 1259_9.txt 3509_10.txt 575_10.txt 8009_9.txt 1025_8.txt 125_7.txt 350_9.txt 5760_8.txt 800_9.txt 10260_10.txt 1260_10.txt 3510_7.txt 5761_8.txt 8010_7.txt 10261_8.txt 1261_8.txt 3511_8.txt 5762_8.txt 8011_10.txt 10262_10.txt 1262_8.txt 3512_7.txt 5763_8.txt 8012_8.txt 10263_10.txt 1263_8.txt 3513_10.txt 5764_7.txt 8013_8.txt 10264_10.txt 1264_10.txt 3514_7.txt 5765_9.txt 8014_7.txt 10265_9.txt 1265_10.txt 3515_7.txt 5766_10.txt 8015_10.txt 10266_9.txt 1266_8.txt 3516_8.txt 5767_8.txt 8016_10.txt 10267_8.txt 1267_7.txt 3517_8.txt 5768_8.txt 8017_9.txt 10268_9.txt 1268_7.txt 3518_10.txt 5769_9.txt 8018_10.txt 10269_7.txt 1269_8.txt 3519_7.txt 576_10.txt 8019_7.txt 1026_9.txt 126_10.txt 351_10.txt 5770_10.txt 801_8.txt 10270_9.txt 1270_7.txt 3520_9.txt 5771_8.txt 8020_8.txt 10271_10.txt 1271_7.txt 3521_9.txt 5772_10.txt 8021_7.txt 10272_10.txt 1272_7.txt 3522_8.txt 5773_9.txt 8022_8.txt 10273_8.txt 1273_7.txt 3523_9.txt 5774_8.txt 8023_7.txt 10274_8.txt 1274_7.txt 3524_9.txt 5775_7.txt 8024_10.txt 10275_10.txt 1275_8.txt 3525_7.txt 5776_10.txt 8025_9.txt 10276_10.txt 1276_9.txt 3526_8.txt 5777_8.txt 8026_9.txt 10277_9.txt 1277_10.txt 3527_7.txt 5778_9.txt 8027_10.txt 10278_7.txt 1278_9.txt 3528_7.txt 5779_10.txt 8028_10.txt 10279_8.txt 1279_9.txt 3529_9.txt 577_8.txt 8029_10.txt 1027_8.txt 127_7.txt 352_10.txt 5780_10.txt 802_10.txt 10280_10.txt 1280_10.txt 3530_7.txt 5781_10.txt 8030_9.txt 10281_7.txt 1281_10.txt 3531_7.txt 5782_10.txt 8031_9.txt 10282_8.txt 1282_9.txt 3532_8.txt 5783_7.txt 8032_10.txt 10283_10.txt 1283_10.txt 3533_10.txt 5784_8.txt 8033_7.txt 10284_9.txt 1284_7.txt 3534_10.txt 5785_10.txt 8034_10.txt 10285_10.txt 1285_9.txt 3535_8.txt 5786_10.txt 8035_7.txt 10286_9.txt 1286_8.txt 3536_10.txt 5787_9.txt 8036_8.txt 10287_8.txt 1287_9.txt 3537_10.txt 5788_8.txt 8037_10.txt 10288_10.txt 1288_8.txt 3538_10.txt 5789_9.txt 8038_7.txt 10289_10.txt 1289_7.txt 3539_9.txt 578_10.txt 8039_8.txt 1028_10.txt 128_7.txt 353_9.txt 5790_7.txt 803_10.txt 10290_8.txt 1290_7.txt 3540_8.txt 5791_9.txt 8040_9.txt 10291_7.txt 1291_10.txt 3541_7.txt 5792_10.txt 8041_7.txt 10292_7.txt 1292_8.txt 3542_8.txt 5793_10.txt 8042_7.txt 10293_8.txt 1293_8.txt 3543_10.txt 5794_10.txt 8043_9.txt 10294_8.txt 1294_7.txt 3544_9.txt 5795_10.txt 8044_10.txt 10295_7.txt 1295_10.txt 3545_7.txt 5796_8.txt 8045_8.txt 10296_8.txt 1296_10.txt 3546_10.txt 5797_8.txt 8046_9.txt 10297_8.txt 1297_8.txt 3547_8.txt 5798_8.txt 8047_9.txt 10298_9.txt 1298_7.txt 3548_8.txt 5799_10.txt 8048_7.txt 10299_9.txt 1299_10.txt 3549_8.txt 579_10.txt 8049_7.txt 1029_9.txt 129_9.txt 354_9.txt 57_10.txt 804_10.txt 102_10.txt 12_9.txt 3550_10.txt 5800_10.txt 8050_10.txt 10300_10.txt 1300_10.txt 3551_8.txt 5801_10.txt 8051_7.txt 10301_8.txt 1301_10.txt 3552_10.txt 5802_10.txt 8052_9.txt 10302_9.txt 1302_10.txt 3553_8.txt 5803_10.txt 8053_8.txt 10303_7.txt 1303_10.txt 3554_10.txt 5804_10.txt 8054_8.txt 10304_7.txt 1304_7.txt 3555_7.txt 5805_9.txt 8055_8.txt 10305_8.txt 1305_10.txt 3556_10.txt 5806_7.txt 8056_8.txt 10306_8.txt 1306_7.txt 3557_9.txt 5807_8.txt 8057_10.txt 10307_8.txt 1307_10.txt 3558_8.txt 5808_10.txt 8058_8.txt 10308_8.txt 1308_9.txt 3559_10.txt 5809_10.txt 8059_10.txt 10309_7.txt 1309_10.txt 355_9.txt 580_9.txt 805_9.txt 1030_10.txt 130_9.txt 3560_8.txt 5810_9.txt 8060_9.txt 10310_9.txt 1310_7.txt 3561_9.txt 5811_9.txt 8061_10.txt 10311_9.txt 1311_10.txt 3562_8.txt 5812_9.txt 8062_8.txt 10312_10.txt 1312_9.txt 3563_8.txt 5813_8.txt 8063_10.txt 10313_7.txt 1313_9.txt 3564_10.txt 5814_8.txt 8064_10.txt 10314_8.txt 1314_7.txt 3565_10.txt 5815_10.txt 8065_9.txt 10315_8.txt 1315_10.txt 3566_9.txt 5816_10.txt 8066_9.txt 10316_8.txt 1316_9.txt 3567_8.txt 5817_10.txt 8067_8.txt 10317_7.txt 1317_8.txt 3568_7.txt 5818_7.txt 8068_9.txt 10318_7.txt 1318_8.txt 3569_8.txt 5819_10.txt 8069_8.txt 10319_7.txt 1319_10.txt 356_10.txt 581_10.txt 806_10.txt 1031_10.txt 131_10.txt 3570_10.txt 5820_7.txt 8070_8.txt 10320_7.txt 1320_10.txt 3571_7.txt 5821_7.txt 8071_9.txt 10321_10.txt 1321_10.txt 3572_10.txt 5822_10.txt 8072_10.txt 10322_7.txt 1322_8.txt 3573_7.txt 5823_10.txt 8073_8.txt 10323_10.txt 1323_10.txt 3574_10.txt 5824_10.txt 8074_10.txt 10324_9.txt 1324_7.txt 3575_10.txt 5825_9.txt 8075_8.txt 10325_10.txt 1325_9.txt 3576_9.txt 5826_10.txt 8076_7.txt 10326_10.txt 1326_10.txt 3577_7.txt 5827_10.txt 8077_7.txt 10327_7.txt 1327_9.txt 3578_7.txt 5828_9.txt 8078_10.txt 10328_8.txt 1328_10.txt 3579_8.txt 5829_10.txt 8079_8.txt 10329_8.txt 1329_8.txt 357_10.txt 582_9.txt 807_10.txt 1032_7.txt 132_9.txt 3580_10.txt 5830_8.txt 8080_8.txt 10330_8.txt 1330_9.txt 3581_9.txt 5831_8.txt 8081_9.txt 10331_10.txt 1331_7.txt 3582_8.txt 5832_9.txt 8082_8.txt 10332_8.txt 1332_8.txt 3583_10.txt 5833_10.txt 8083_7.txt 10333_8.txt 1333_10.txt 3584_10.txt 5834_9.txt 8084_7.txt 10334_8.txt 1334_8.txt 3585_9.txt 5835_10.txt 8085_7.txt 10335_8.txt 1335_10.txt 3586_10.txt 5836_8.txt 8086_9.txt 10336_8.txt 1336_7.txt 3587_10.txt 5837_7.txt 8087_8.txt 10337_9.txt 1337_7.txt 3588_8.txt 5838_8.txt 8088_7.txt 10338_9.txt 1338_7.txt 3589_8.txt 5839_7.txt 8089_10.txt 10339_7.txt 1339_9.txt 358_10.txt 583_8.txt 808_9.txt 1033_10.txt 133_10.txt 3590_8.txt 5840_7.txt 8090_9.txt 10340_9.txt 1340_10.txt 3591_9.txt 5841_7.txt 8091_9.txt 10341_7.txt 1341_10.txt 3592_10.txt 5842_8.txt 8092_8.txt 10342_7.txt 1342_10.txt 3593_8.txt 5843_10.txt 8093_7.txt 10343_7.txt 1343_9.txt 3594_7.txt 5844_8.txt 8094_9.txt 10344_7.txt 1344_9.txt 3595_10.txt 5845_7.txt 8095_8.txt 10345_7.txt 1345_9.txt 3596_8.txt 5846_10.txt 8096_10.txt 10346_9.txt 1346_9.txt 3597_10.txt 5847_10.txt 8097_7.txt 10347_9.txt 1347_10.txt 3598_10.txt 5848_7.txt 8098_7.txt 10348_8.txt 1348_10.txt 3599_8.txt 5849_8.txt 8099_7.txt 10349_10.txt 1349_8.txt 359_8.txt 584_8.txt 809_10.txt 1034_7.txt 134_10.txt 35_8.txt 5850_9.txt 80_9.txt 10350_10.txt 1350_9.txt 3600_7.txt 5851_10.txt 8100_10.txt 10351_8.txt 1351_10.txt 3601_7.txt 5852_7.txt 8101_8.txt 10352_10.txt 1352_10.txt 3602_7.txt 5853_10.txt 8102_7.txt 10353_9.txt 1353_10.txt 3603_7.txt 5854_8.txt 8103_10.txt 10354_9.txt 1354_9.txt 3604_10.txt 5855_9.txt 8104_7.txt 10355_9.txt 1355_8.txt 3605_8.txt 5856_8.txt 8105_8.txt 10356_9.txt 1356_8.txt 3606_9.txt 5857_10.txt 8106_7.txt 10357_8.txt 1357_10.txt 3607_8.txt 5858_8.txt 8107_8.txt 10358_9.txt 1358_10.txt 3608_9.txt 5859_9.txt 8108_10.txt 10359_7.txt 1359_7.txt 3609_8.txt 585_10.txt 8109_10.txt 1035_7.txt 135_7.txt 360_10.txt 5860_8.txt 810_10.txt 10360_8.txt 1360_10.txt 3610_10.txt 5861_8.txt 8110_9.txt 10361_7.txt 1361_10.txt 3611_10.txt 5862_8.txt 8111_8.txt 10362_8.txt 1362_10.txt 3612_7.txt 5863_8.txt 8112_9.txt 10363_9.txt 1363_10.txt 3613_8.txt 5864_7.txt 8113_8.txt 10364_10.txt 1364_8.txt 3614_7.txt 5865_9.txt 8114_8.txt 10365_8.txt 1365_8.txt 3615_9.txt 5866_9.txt 8115_10.txt 10366_10.txt 1366_8.txt 3616_7.txt 5867_9.txt 8116_9.txt 10367_8.txt 1367_10.txt 3617_8.txt 5868_8.txt 8117_8.txt 10368_7.txt 1368_10.txt 3618_7.txt 5869_10.txt 8118_10.txt 10369_8.txt 1369_8.txt 3619_8.txt 586_10.txt 8119_10.txt 1036_9.txt 136_10.txt 361_10.txt 5870_8.txt 811_10.txt 10370_9.txt 1370_8.txt 3620_10.txt 5871_9.txt 8120_7.txt 10371_8.txt 1371_8.txt 3621_8.txt 5872_9.txt 8121_8.txt 10372_7.txt 1372_7.txt 3622_8.txt 5873_8.txt 8122_10.txt 10373_7.txt 1373_10.txt 3623_8.txt 5874_10.txt 8123_8.txt 10374_8.txt 1374_8.txt 3624_10.txt 5875_8.txt 8124_9.txt 10375_10.txt 1375_8.txt 3625_8.txt 5876_8.txt 8125_7.txt 10376_7.txt 1376_7.txt 3626_7.txt 5877_8.txt 8126_7.txt 10377_9.txt 1377_7.txt 3627_8.txt 5878_10.txt 8127_8.txt 10378_8.txt 1378_9.txt 3628_8.txt 5879_10.txt 8128_10.txt 10379_10.txt 1379_8.txt 3629_8.txt 587_10.txt 8129_9.txt 1037_8.txt 137_7.txt 362_10.txt 5880_10.txt 812_10.txt 10380_10.txt 1380_7.txt 3630_7.txt 5881_7.txt 8130_10.txt 10381_10.txt 1381_7.txt 3631_7.txt 5882_9.txt 8131_8.txt 10382_10.txt 1382_8.txt 3632_7.txt 5883_10.txt 8132_10.txt 10383_10.txt 1383_7.txt 3633_10.txt 5884_10.txt 8133_7.txt 10384_10.txt 1384_7.txt 3634_8.txt 5885_10.txt 8134_10.txt 10385_10.txt 1385_8.txt 3635_9.txt 5886_10.txt 8135_10.txt 10386_8.txt 1386_8.txt 3636_8.txt 5887_7.txt 8136_9.txt 10387_7.txt 1387_8.txt 3637_10.txt 5888_9.txt 8137_10.txt 10388_7.txt 1388_10.txt 3638_10.txt 5889_7.txt 8138_10.txt 10389_10.txt 1389_8.txt 3639_9.txt 588_9.txt 8139_10.txt 1038_7.txt 138_7.txt 363_10.txt 5890_8.txt 813_10.txt 10390_10.txt 1390_9.txt 3640_8.txt 5891_10.txt 8140_10.txt 10391_10.txt 1391_7.txt 3641_9.txt 5892_8.txt 8141_10.txt 10392_10.txt 1392_7.txt 3642_9.txt 5893_9.txt 8142_10.txt 10393_9.txt 1393_7.txt 3643_10.txt 5894_7.txt 8143_10.txt 10394_10.txt 1394_8.txt 3644_8.txt 5895_10.txt 8144_9.txt 10395_8.txt 1395_10.txt 3645_8.txt 5896_10.txt 8145_10.txt 10396_8.txt 1396_8.txt 3646_8.txt 5897_10.txt 8146_10.txt 10397_8.txt 1397_7.txt 3647_10.txt 5898_8.txt 8147_10.txt 10398_8.txt 1398_7.txt 3648_8.txt 5899_7.txt 8148_8.txt 10399_10.txt 1399_9.txt 3649_9.txt 589_10.txt 8149_10.txt 1039_9.txt 139_10.txt 364_10.txt 58_9.txt 814_10.txt 103_7.txt 13_7.txt 3650_10.txt 5900_10.txt 8150_10.txt 10400_10.txt 1400_7.txt 3651_10.txt 5901_7.txt 8151_10.txt 10401_10.txt 1401_7.txt 3652_10.txt 5902_9.txt 8152_10.txt 10402_10.txt 1402_7.txt 3653_10.txt 5903_8.txt 8153_9.txt 10403_7.txt 1403_7.txt 3654_10.txt 5904_8.txt 8154_10.txt 10404_9.txt 1404_10.txt 3655_7.txt 5905_9.txt 8155_7.txt 10405_8.txt 1405_7.txt 3656_8.txt 5906_8.txt 8156_10.txt 10406_10.txt 1406_8.txt 3657_7.txt 5907_10.txt 8157_10.txt 10407_8.txt 1407_7.txt 3658_10.txt 5908_10.txt 8158_10.txt 10408_10.txt 1408_7.txt 3659_9.txt 5909_8.txt 8159_10.txt 10409_10.txt 1409_8.txt 365_10.txt 590_10.txt 815_7.txt 1040_10.txt 140_8.txt 3660_10.txt 5910_8.txt 8160_7.txt 10410_10.txt 1410_9.txt 3661_10.txt 5911_9.txt 8161_10.txt 10411_9.txt 1411_7.txt 3662_9.txt 5912_7.txt 8162_9.txt 10412_8.txt 1412_8.txt 3663_10.txt 5913_10.txt 8163_8.txt 10413_10.txt 1413_7.txt 3664_10.txt 5914_8.txt 8164_8.txt 10414_10.txt 1414_10.txt 3665_9.txt 5915_7.txt 8165_8.txt 10415_7.txt 1415_10.txt 3666_9.txt 5916_10.txt 8166_10.txt 10416_9.txt 1416_10.txt 3667_8.txt 5917_7.txt 8167_7.txt 10417_8.txt 1417_8.txt 3668_7.txt 5918_9.txt 8168_7.txt 10418_9.txt 1418_9.txt 3669_10.txt 5919_9.txt 8169_8.txt 10419_10.txt 1419_7.txt 366_9.txt 591_10.txt 816_10.txt 1041_9.txt 141_9.txt 3670_10.txt 5920_9.txt 8170_7.txt 10420_10.txt 1420_8.txt 3671_7.txt 5921_7.txt 8171_8.txt 10421_7.txt 1421_9.txt 3672_10.txt 5922_8.txt 8172_9.txt 10422_7.txt 1422_10.txt 3673_8.txt 5923_7.txt 8173_8.txt 10423_9.txt 1423_10.txt 3674_8.txt 5924_8.txt 8174_9.txt 10424_9.txt 1424_10.txt 3675_9.txt 5925_7.txt 8175_7.txt 10425_9.txt 1425_7.txt 3676_8.txt 5926_7.txt 8176_8.txt 10426_9.txt 1426_8.txt 3677_8.txt 5927_9.txt 8177_8.txt 10427_8.txt 1427_9.txt 3678_8.txt 5928_8.txt 8178_8.txt 10428_10.txt 1428_7.txt 3679_7.txt 5929_10.txt 8179_7.txt 10429_10.txt 1429_9.txt 367_10.txt 592_10.txt 817_10.txt 1042_10.txt 142_8.txt 3680_9.txt 5930_7.txt 8180_8.txt 10430_9.txt 1430_9.txt 3681_8.txt 5931_7.txt 8181_10.txt 10431_10.txt 1431_10.txt 3682_8.txt 5932_10.txt 8182_7.txt 10432_10.txt 1432_7.txt 3683_9.txt 5933_7.txt 8183_7.txt 10433_9.txt 1433_10.txt 3684_8.txt 5934_10.txt 8184_7.txt 10434_10.txt 1434_10.txt 3685_8.txt 5935_9.txt 8185_8.txt 10435_7.txt 1435_8.txt 3686_10.txt 5936_10.txt 8186_10.txt 10436_8.txt 1436_10.txt 3687_7.txt 5937_9.txt 8187_9.txt 10437_7.txt 1437_8.txt 3688_8.txt 5938_10.txt 8188_8.txt 10438_9.txt 1438_7.txt 3689_8.txt 5939_7.txt 8189_10.txt 10439_8.txt 1439_7.txt 368_10.txt 593_9.txt 818_10.txt 1043_10.txt 143_7.txt 3690_9.txt 5940_7.txt 8190_10.txt 10440_9.txt 1440_7.txt 3691_8.txt 5941_8.txt 8191_8.txt 10441_10.txt 1441_9.txt 3692_10.txt 5942_7.txt 8192_10.txt 10442_10.txt 1442_7.txt 3693_8.txt 5943_10.txt 8193_10.txt 10443_9.txt 1443_8.txt 3694_8.txt 5944_9.txt 8194_9.txt 10444_9.txt 1444_7.txt 3695_8.txt 5945_10.txt 8195_10.txt 10445_10.txt 1445_8.txt 3696_7.txt 5946_7.txt 8196_8.txt 10446_10.txt 1446_10.txt 3697_7.txt 5947_7.txt 8197_8.txt 10447_10.txt 1447_8.txt 3698_10.txt 5948_8.txt 8198_7.txt 10448_10.txt 1448_8.txt 3699_10.txt 5949_10.txt 8199_10.txt 10449_9.txt 1449_9.txt 369_10.txt 594_9.txt 819_10.txt 1044_8.txt 144_8.txt 36_10.txt 5950_10.txt 81_10.txt 10450_10.txt 1450_8.txt 3700_9.txt 5951_10.txt 8200_8.txt 10451_10.txt 1451_8.txt 3701_10.txt 5952_9.txt 8201_8.txt 10452_10.txt 1452_8.txt 3702_9.txt 5953_9.txt 8202_10.txt 10453_10.txt 1453_8.txt 3703_9.txt 5954_7.txt 8203_7.txt 10454_9.txt 1454_7.txt 3704_8.txt 5955_10.txt 8204_8.txt 10455_10.txt 1455_7.txt 3705_10.txt 5956_9.txt 8205_8.txt 10456_10.txt 1456_9.txt 3706_7.txt 5957_9.txt 8206_10.txt 10457_8.txt 1457_10.txt 3707_7.txt 5958_9.txt 8207_10.txt 10458_10.txt 1458_9.txt 3708_7.txt 5959_7.txt 8208_8.txt 10459_9.txt 1459_10.txt 3709_7.txt 595_9.txt 8209_8.txt 1045_8.txt 145_10.txt 370_10.txt 5960_8.txt 820_10.txt 10460_10.txt 1460_10.txt 3710_9.txt 5961_10.txt 8210_7.txt 10461_9.txt 1461_10.txt 3711_7.txt 5962_7.txt 8211_7.txt 10462_7.txt 1462_9.txt 3712_7.txt 5963_10.txt 8212_7.txt 10463_10.txt 1463_10.txt 3713_10.txt 5964_8.txt 8213_9.txt 10464_7.txt 1464_7.txt 3714_10.txt 5965_10.txt 8214_7.txt 10465_8.txt 1465_9.txt 3715_7.txt 5966_7.txt 8215_7.txt 10466_8.txt 1466_10.txt 3716_9.txt 5967_10.txt 8216_8.txt 10467_10.txt 1467_9.txt 3717_8.txt 5968_10.txt 8217_8.txt 10468_9.txt 1468_9.txt 3718_8.txt 5969_10.txt 8218_7.txt 10469_10.txt 1469_7.txt 3719_9.txt 596_7.txt 8219_7.txt 1046_10.txt 146_10.txt 371_9.txt 5970_10.txt 821_10.txt 10470_9.txt 1470_10.txt 3720_10.txt 5971_7.txt 8220_8.txt 10471_10.txt 1471_9.txt 3721_8.txt 5972_8.txt 8221_9.txt 10472_7.txt 1472_8.txt 3722_10.txt 5973_9.txt 8222_9.txt 10473_10.txt 1473_9.txt 3723_8.txt 5974_7.txt 8223_9.txt 10474_9.txt 1474_8.txt 3724_9.txt 5975_8.txt 8224_8.txt 10475_8.txt 1475_8.txt 3725_8.txt 5976_7.txt 8225_8.txt 10476_9.txt 1476_10.txt 3726_7.txt 5977_8.txt 8226_8.txt 10477_9.txt 1477_7.txt 3727_10.txt 5978_9.txt 8227_7.txt 10478_8.txt 1478_7.txt 3728_10.txt 5979_9.txt 8228_8.txt 10479_10.txt 1479_8.txt 3729_10.txt 597_7.txt 8229_10.txt 1047_8.txt 147_9.txt 372_10.txt 5980_10.txt 822_9.txt 10480_10.txt 1480_8.txt 3730_10.txt 5981_7.txt 8230_10.txt 10481_8.txt 1481_10.txt 3731_10.txt 5982_9.txt 8231_10.txt 10482_10.txt 1482_10.txt 3732_10.txt 5983_8.txt 8232_10.txt 10483_8.txt 1483_9.txt 3733_10.txt 5984_10.txt 8233_8.txt 10484_8.txt 1484_10.txt 3734_9.txt 5985_10.txt 8234_7.txt 10485_8.txt 1485_10.txt 3735_10.txt 5986_10.txt 8235_7.txt 10486_7.txt 1486_7.txt 3736_8.txt 5987_8.txt 8236_7.txt 10487_7.txt 1487_9.txt 3737_9.txt 5988_7.txt 8237_8.txt 10488_10.txt 1488_10.txt 3738_8.txt 5989_7.txt 8238_8.txt 10489_10.txt 1489_8.txt 3739_10.txt 598_9.txt 8239_10.txt 1048_8.txt 148_9.txt 373_10.txt 5990_7.txt 823_9.txt 10490_7.txt 1490_8.txt 3740_9.txt 5991_8.txt 8240_9.txt 10491_7.txt 1491_9.txt 3741_8.txt 5992_7.txt 8241_7.txt 10492_10.txt 1492_10.txt 3742_9.txt 5993_8.txt 8242_8.txt 10493_9.txt 1493_9.txt 3743_10.txt 5994_8.txt 8243_9.txt 10494_10.txt 1494_8.txt 3744_10.txt 5995_10.txt 8244_9.txt 10495_7.txt 1495_9.txt 3745_10.txt 5996_7.txt 8245_9.txt 10496_10.txt 1496_10.txt 3746_10.txt 5997_9.txt 8246_9.txt 10497_8.txt 1497_8.txt 3747_10.txt 5998_10.txt 8247_9.txt 10498_10.txt 1498_8.txt 3748_10.txt 5999_7.txt 8248_9.txt 10499_10.txt 1499_7.txt 3749_9.txt 599_10.txt 8249_10.txt 1049_7.txt 149_10.txt 374_10.txt 59_7.txt 824_8.txt 104_10.txt 14_10.txt 3750_10.txt 5_10.txt 8250_8.txt 10500_10.txt 1500_9.txt 3751_10.txt 6000_9.txt 8251_10.txt 10501_10.txt 1501_7.txt 3752_7.txt 6001_7.txt 8252_9.txt 10502_9.txt 1502_10.txt 3753_9.txt 6002_7.txt 8253_10.txt 10503_10.txt 1503_9.txt 3754_8.txt 6003_8.txt 8254_8.txt 10504_9.txt 1504_9.txt 3755_10.txt 6004_10.txt 8255_9.txt 10505_10.txt 1505_9.txt 3756_8.txt 6005_8.txt 8256_9.txt 10506_10.txt 1506_7.txt 3757_7.txt 6006_10.txt 8257_9.txt 10507_10.txt 1507_8.txt 3758_9.txt 6007_8.txt 8258_9.txt 10508_10.txt 1508_7.txt 3759_8.txt 6008_7.txt 8259_9.txt 10509_7.txt 1509_10.txt 375_9.txt 6009_10.txt 825_10.txt 1050_9.txt 150_8.txt 3760_7.txt 600_10.txt 8260_9.txt 10510_7.txt 1510_8.txt 3761_8.txt 6010_8.txt 8261_8.txt 10511_7.txt 1511_8.txt 3762_8.txt 6011_8.txt 8262_10.txt 10512_10.txt 1512_10.txt 3763_8.txt 6012_9.txt 8263_9.txt 10513_7.txt 1513_7.txt 3764_8.txt 6013_7.txt 8264_9.txt 10514_8.txt 1514_7.txt 3765_10.txt 6014_8.txt 8265_10.txt 10515_9.txt 1515_7.txt 3766_10.txt 6015_7.txt 8266_10.txt 10516_7.txt 1516_7.txt 3767_9.txt 6016_10.txt 8267_7.txt 10517_8.txt 1517_7.txt 3768_10.txt 6017_8.txt 8268_7.txt 10518_9.txt 1518_8.txt 3769_7.txt 6018_9.txt 8269_7.txt 10519_9.txt 1519_10.txt 376_10.txt 6019_8.txt 826_9.txt 1051_9.txt 151_10.txt 3770_10.txt 601_7.txt 8270_10.txt 10520_9.txt 1520_7.txt 3771_10.txt 6020_7.txt 8271_10.txt 10521_9.txt 1521_8.txt 3772_10.txt 6021_7.txt 8272_7.txt 10522_7.txt 1522_8.txt 3773_7.txt 6022_7.txt 8273_10.txt 10523_9.txt 1523_9.txt 3774_8.txt 6023_8.txt 8274_10.txt 10524_10.txt 1524_7.txt 3775_7.txt 6024_10.txt 8275_10.txt 10525_10.txt 1525_9.txt 3776_10.txt 6025_9.txt 8276_10.txt 10526_9.txt 1526_9.txt 3777_10.txt 6026_8.txt 8277_10.txt 10527_10.txt 1527_7.txt 3778_10.txt 6027_9.txt 8278_10.txt 10528_10.txt 1528_8.txt 3779_8.txt 6028_9.txt 8279_10.txt 10529_10.txt 1529_10.txt 377_7.txt 6029_7.txt 827_7.txt 1052_8.txt 152_9.txt 3780_8.txt 602_10.txt 8280_8.txt 10530_10.txt 1530_10.txt 3781_7.txt 6030_10.txt 8281_7.txt 10531_10.txt 1531_8.txt 3782_8.txt 6031_7.txt 8282_7.txt 10532_8.txt 1532_10.txt 3783_10.txt 6032_10.txt 8283_7.txt 10533_10.txt 1533_8.txt 3784_9.txt 6033_8.txt 8284_8.txt 10534_7.txt 1534_10.txt 3785_10.txt 6034_10.txt 8285_8.txt 10535_7.txt 1535_9.txt 3786_9.txt 6035_10.txt 8286_7.txt 10536_10.txt 1536_7.txt 3787_10.txt 6036_10.txt 8287_9.txt 10537_10.txt 1537_9.txt 3788_10.txt 6037_10.txt 8288_7.txt 10538_8.txt 1538_10.txt 3789_8.txt 6038_10.txt 8289_8.txt 10539_10.txt 1539_10.txt 378_8.txt 6039_10.txt 828_10.txt 1053_8.txt 153_10.txt 3790_10.txt 603_10.txt 8290_10.txt 10540_10.txt 1540_10.txt 3791_10.txt 6040_10.txt 8291_8.txt 10541_10.txt 1541_8.txt 3792_8.txt 6041_10.txt 8292_8.txt 10542_7.txt 1542_10.txt 3793_7.txt 6042_9.txt 8293_8.txt 10543_8.txt 1543_8.txt 3794_10.txt 6043_10.txt 8294_8.txt 10544_8.txt 1544_10.txt 3795_9.txt 6044_10.txt 8295_7.txt 10545_7.txt 1545_10.txt 3796_8.txt 6045_9.txt 8296_8.txt 10546_9.txt 1546_8.txt 3797_9.txt 6046_7.txt 8297_10.txt 10547_9.txt 1547_7.txt 3798_9.txt 6047_10.txt 8298_7.txt 10548_7.txt 1548_8.txt 3799_8.txt 6048_9.txt 8299_7.txt 10549_9.txt 1549_10.txt 379_10.txt 6049_8.txt 829_7.txt 1054_8.txt 154_8.txt 37_9.txt 604_8.txt 82_8.txt 10550_8.txt 1550_10.txt 3800_7.txt 6050_10.txt 8300_10.txt 10551_7.txt 1551_8.txt 3801_8.txt 6051_9.txt 8301_7.txt 10552_9.txt 1552_8.txt 3802_10.txt 6052_9.txt 8302_8.txt 10553_8.txt 1553_10.txt 3803_9.txt 6053_10.txt 8303_10.txt 10554_7.txt 1554_8.txt 3804_10.txt 6054_9.txt 8304_8.txt 10555_8.txt 1555_8.txt 3805_8.txt 6055_8.txt 8305_7.txt 10556_10.txt 1556_8.txt 3806_8.txt 6056_9.txt 8306_10.txt 10557_9.txt 1557_10.txt 3807_8.txt 6057_9.txt 8307_10.txt 10558_10.txt 1558_10.txt 3808_10.txt 6058_8.txt 8308_7.txt 10559_8.txt 1559_7.txt 3809_9.txt 6059_10.txt 8309_7.txt 1055_10.txt 155_10.txt 380_10.txt 605_8.txt 830_10.txt 10560_9.txt 1560_9.txt 3810_9.txt 6060_10.txt 8310_10.txt 10561_8.txt 1561_8.txt 3811_9.txt 6061_10.txt 8311_9.txt 10562_9.txt 1562_10.txt 3812_9.txt 6062_10.txt 8312_10.txt 10563_7.txt 1563_10.txt 3813_7.txt 6063_10.txt 8313_10.txt 10564_10.txt 1564_10.txt 3814_7.txt 6064_7.txt 8314_9.txt 10565_9.txt 1565_7.txt 3815_7.txt 6065_10.txt 8315_10.txt 10566_8.txt 1566_8.txt 3816_7.txt 6066_10.txt 8316_9.txt 10567_9.txt 1567_7.txt 3817_8.txt 6067_10.txt 8317_8.txt 10568_10.txt 1568_9.txt 3818_8.txt 6068_9.txt 8318_10.txt 10569_10.txt 1569_10.txt 3819_8.txt 6069_8.txt 8319_10.txt 1056_10.txt 156_8.txt 381_10.txt 606_10.txt 831_9.txt 10570_8.txt 1570_8.txt 3820_8.txt 6070_8.txt 8320_8.txt 10571_8.txt 1571_10.txt 3821_9.txt 6071_8.txt 8321_7.txt 10572_8.txt 1572_10.txt 3822_10.txt 6072_7.txt 8322_7.txt 10573_10.txt 1573_9.txt 3823_9.txt 6073_9.txt 8323_10.txt 10574_10.txt 1574_9.txt 3824_9.txt 6074_7.txt 8324_8.txt 10575_9.txt 1575_9.txt 3825_8.txt 6075_10.txt 8325_9.txt 10576_7.txt 1576_10.txt 3826_10.txt 6076_9.txt 8326_7.txt 10577_10.txt 1577_9.txt 3827_10.txt 6077_9.txt 8327_9.txt 10578_7.txt 1578_8.txt 3828_10.txt 6078_8.txt 8328_10.txt 10579_10.txt 1579_10.txt 3829_10.txt 6079_9.txt 8329_7.txt 1057_9.txt 157_9.txt 382_10.txt 607_10.txt 832_8.txt 10580_8.txt 1580_7.txt 3830_10.txt 6080_10.txt 8330_7.txt 10581_10.txt 1581_7.txt 3831_10.txt 6081_9.txt 8331_8.txt 10582_10.txt 1582_8.txt 3832_8.txt 6082_9.txt 8332_9.txt 10583_10.txt 1583_8.txt 3833_8.txt 6083_8.txt 8333_9.txt 10584_10.txt 1584_9.txt 3834_9.txt 6084_8.txt 8334_8.txt 10585_9.txt 1585_9.txt 3835_9.txt 6085_8.txt 8335_8.txt 10586_10.txt 1586_7.txt 3836_8.txt 6086_8.txt 8336_10.txt 10587_8.txt 1587_10.txt 3837_9.txt 6087_9.txt 8337_10.txt 10588_10.txt 1588_8.txt 3838_7.txt 6088_10.txt 8338_8.txt 10589_10.txt 1589_10.txt 3839_7.txt 6089_10.txt 8339_10.txt 1058_10.txt 158_10.txt 383_10.txt 608_8.txt 833_8.txt 10590_8.txt 1590_10.txt 3840_10.txt 6090_7.txt 8340_8.txt 10591_10.txt 1591_10.txt 3841_9.txt 6091_7.txt 8341_10.txt 10592_8.txt 1592_9.txt 3842_7.txt 6092_8.txt 8342_7.txt 10593_8.txt 1593_10.txt 3843_7.txt 6093_8.txt 8343_7.txt 10594_8.txt 1594_8.txt 3844_8.txt 6094_10.txt 8344_10.txt 10595_10.txt 1595_7.txt 3845_7.txt 6095_7.txt 8345_7.txt 10596_8.txt 1596_8.txt 3846_10.txt 6096_10.txt 8346_9.txt 10597_9.txt 1597_8.txt 3847_9.txt 6097_10.txt 8347_10.txt 10598_8.txt 1598_8.txt 3848_8.txt 6098_10.txt 8348_10.txt 10599_8.txt 1599_7.txt 3849_8.txt 6099_8.txt 8349_10.txt 1059_10.txt 159_10.txt 384_8.txt 609_9.txt 834_8.txt 105_7.txt 15_7.txt 3850_8.txt 60_8.txt 8350_9.txt 10600_9.txt 1600_7.txt 3851_10.txt 6100_10.txt 8351_9.txt 10601_10.txt 1601_8.txt 3852_8.txt 6101_8.txt 8352_9.txt 10602_10.txt 1602_9.txt 3853_9.txt 6102_10.txt 8353_10.txt 10603_10.txt 1603_9.txt 3854_10.txt 6103_10.txt 8354_8.txt 10604_7.txt 1604_9.txt 3855_8.txt 6104_10.txt 8355_7.txt 10605_7.txt 1605_10.txt 3856_10.txt 6105_10.txt 8356_10.txt 10606_10.txt 1606_7.txt 3857_9.txt 6106_9.txt 8357_10.txt 10607_10.txt 1607_9.txt 3858_10.txt 6107_7.txt 8358_9.txt 10608_10.txt 1608_9.txt 3859_8.txt 6108_8.txt 8359_8.txt 10609_10.txt 1609_9.txt 385_10.txt 6109_8.txt 835_8.txt 1060_10.txt 160_9.txt 3860_10.txt 610_9.txt 8360_9.txt 10610_8.txt 1610_10.txt 3861_10.txt 6110_8.txt 8361_8.txt 10611_8.txt 1611_10.txt 3862_9.txt 6111_10.txt 8362_7.txt 10612_7.txt 1612_10.txt 3863_7.txt 6112_7.txt 8363_8.txt 10613_8.txt 1613_10.txt 3864_7.txt 6113_8.txt 8364_10.txt 10614_7.txt 1614_10.txt 3865_8.txt 6114_7.txt 8365_8.txt 10615_8.txt 1615_8.txt 3866_7.txt 6115_10.txt 8366_10.txt 10616_7.txt 1616_9.txt 3867_10.txt 6116_8.txt 8367_10.txt 10617_8.txt 1617_8.txt 3868_10.txt 6117_10.txt 8368_10.txt 10618_8.txt 1618_10.txt 3869_9.txt 6118_10.txt 8369_8.txt 10619_8.txt 1619_10.txt 386_7.txt 6119_9.txt 836_8.txt 1061_10.txt 161_8.txt 3870_9.txt 611_10.txt 8370_10.txt 10620_10.txt 1620_10.txt 3871_8.txt 6120_9.txt 8371_10.txt 10621_10.txt 1621_10.txt 3872_9.txt 6121_10.txt 8372_10.txt 10622_10.txt 1622_8.txt 3873_9.txt 6122_8.txt 8373_9.txt 10623_8.txt 1623_10.txt 3874_9.txt 6123_7.txt 8374_7.txt 10624_7.txt 1624_10.txt 3875_9.txt 6124_7.txt 8375_8.txt 10625_7.txt 1625_7.txt 3876_8.txt 6125_9.txt 8376_10.txt 10626_7.txt 1626_10.txt 3877_8.txt 6126_10.txt 8377_8.txt 10627_10.txt 1627_10.txt 3878_10.txt 6127_8.txt 8378_10.txt 10628_7.txt 1628_10.txt 3879_8.txt 6128_7.txt 8379_10.txt 10629_10.txt 1629_10.txt 387_8.txt 6129_7.txt 837_7.txt 1062_10.txt 162_8.txt 3880_8.txt 612_10.txt 8380_9.txt 10630_8.txt 1630_8.txt 3881_9.txt 6130_7.txt 8381_8.txt 10631_8.txt 1631_8.txt 3882_10.txt 6131_10.txt 8382_7.txt 10632_10.txt 1632_7.txt 3883_10.txt 6132_10.txt 8383_7.txt 10633_9.txt 1633_9.txt 3884_8.txt 6133_7.txt 8384_7.txt 10634_10.txt 1634_7.txt 3885_10.txt 6134_7.txt 8385_10.txt 10635_8.txt 1635_7.txt 3886_10.txt 6135_8.txt 8386_9.txt 10636_8.txt 1636_10.txt 3887_10.txt 6136_10.txt 8387_8.txt 10637_10.txt 1637_10.txt 3888_10.txt 6137_7.txt 8388_7.txt 10638_8.txt 1638_8.txt 3889_10.txt 6138_8.txt 8389_8.txt 10639_7.txt 1639_10.txt 388_8.txt 6139_10.txt 838_9.txt 1063_10.txt 163_10.txt 3890_10.txt 613_10.txt 8390_8.txt 10640_8.txt 1640_10.txt 3891_10.txt 6140_9.txt 8391_8.txt 10641_7.txt 1641_10.txt 3892_10.txt 6141_9.txt 8392_8.txt 10642_8.txt 1642_10.txt 3893_10.txt 6142_9.txt 8393_8.txt 10643_8.txt 1643_10.txt 3894_10.txt 6143_7.txt 8394_10.txt 10644_8.txt 1644_10.txt 3895_8.txt 6144_7.txt 8395_8.txt 10645_8.txt 1645_9.txt 3896_7.txt 6145_8.txt 8396_8.txt 10646_8.txt 1646_9.txt 3897_10.txt 6146_7.txt 8397_10.txt 10647_8.txt 1647_10.txt 3898_10.txt 6147_7.txt 8398_8.txt 10648_8.txt 1648_10.txt 3899_9.txt 6148_10.txt 8399_10.txt 10649_7.txt 1649_8.txt 389_10.txt 6149_7.txt 839_7.txt 1064_10.txt 164_10.txt 38_10.txt 614_10.txt 83_10.txt 10650_8.txt 1650_10.txt 3900_10.txt 6150_7.txt 8400_7.txt 10651_7.txt 1651_10.txt 3901_10.txt 6151_7.txt 8401_10.txt 10652_9.txt 1652_10.txt 3902_10.txt 6152_10.txt 8402_10.txt 10653_10.txt 1653_10.txt 3903_10.txt 6153_7.txt 8403_10.txt 10654_7.txt 1654_7.txt 3904_10.txt 6154_10.txt 8404_10.txt 10655_9.txt 1655_8.txt 3905_10.txt 6155_7.txt 8405_9.txt 10656_7.txt 1656_9.txt 3906_10.txt 6156_10.txt 8406_10.txt 10657_8.txt 1657_9.txt 3907_10.txt 6157_10.txt 8407_7.txt 10658_10.txt 1658_10.txt 3908_10.txt 6158_10.txt 8408_10.txt 10659_8.txt 1659_7.txt 3909_10.txt 6159_7.txt 8409_8.txt 1065_10.txt 165_7.txt 390_10.txt 615_10.txt 840_9.txt 10660_10.txt 1660_8.txt 3910_10.txt 6160_10.txt 8410_10.txt 10661_9.txt 1661_8.txt 3911_10.txt 6161_9.txt 8411_7.txt 10662_7.txt 1662_7.txt 3912_10.txt 6162_8.txt 8412_7.txt 10663_8.txt 1663_9.txt 3913_10.txt 6163_10.txt 8413_8.txt 10664_8.txt 1664_10.txt 3914_10.txt 6164_7.txt 8414_7.txt 10665_8.txt 1665_7.txt 3915_9.txt 6165_8.txt 8415_7.txt 10666_8.txt 1666_8.txt 3916_8.txt 6166_10.txt 8416_7.txt 10667_8.txt 1667_7.txt 3917_9.txt 6167_7.txt 8417_8.txt 10668_7.txt 1668_8.txt 3918_10.txt 6168_10.txt 8418_7.txt 10669_10.txt 1669_8.txt 3919_7.txt 6169_10.txt 8419_9.txt 1066_10.txt 166_7.txt 391_8.txt 616_7.txt 841_10.txt 10670_10.txt 1670_8.txt 3920_9.txt 6170_10.txt 8420_9.txt 10671_10.txt 1671_8.txt 3921_9.txt 6171_8.txt 8421_10.txt 10672_9.txt 1672_8.txt 3922_10.txt 6172_7.txt 8422_10.txt 10673_10.txt 1673_8.txt 3923_10.txt 6173_8.txt 8423_8.txt 10674_8.txt 1674_8.txt 3924_7.txt 6174_10.txt 8424_9.txt 10675_8.txt 1675_9.txt 3925_10.txt 6175_8.txt 8425_9.txt 10676_9.txt 1676_9.txt 3926_7.txt 6176_9.txt 8426_7.txt 10677_8.txt 1677_9.txt 3927_9.txt 6177_7.txt 8427_7.txt 10678_9.txt 1678_9.txt 3928_7.txt 6178_7.txt 8428_7.txt 10679_10.txt 1679_9.txt 3929_9.txt 6179_8.txt 8429_7.txt 1067_7.txt 167_7.txt 392_9.txt 617_7.txt 842_9.txt 10680_8.txt 1680_8.txt 3930_9.txt 6180_7.txt 8430_9.txt 10681_10.txt 1681_7.txt 3931_7.txt 6181_9.txt 8431_9.txt 10682_10.txt 1682_7.txt 3932_8.txt 6182_7.txt 8432_10.txt 10683_7.txt 1683_7.txt 3933_7.txt 6183_7.txt 8433_9.txt 10684_9.txt 1684_10.txt 3934_8.txt 6184_7.txt 8434_9.txt 10685_7.txt 1685_10.txt 3935_9.txt 6185_8.txt 8435_10.txt 10686_8.txt 1686_10.txt 3936_7.txt 6186_10.txt 8436_10.txt 10687_10.txt 1687_10.txt 3937_8.txt 6187_9.txt 8437_10.txt 10688_9.txt 1688_9.txt 3938_9.txt 6188_9.txt 8438_10.txt 10689_8.txt 1689_10.txt 3939_7.txt 6189_10.txt 8439_10.txt 1068_10.txt 168_9.txt 393_8.txt 618_10.txt 843_10.txt 10690_10.txt 1690_10.txt 3940_9.txt 6190_7.txt 8440_8.txt 10691_7.txt 1691_8.txt 3941_9.txt 6191_9.txt 8441_10.txt 10692_8.txt 1692_8.txt 3942_9.txt 6192_9.txt 8442_9.txt 10693_8.txt 1693_10.txt 3943_10.txt 6193_7.txt 8443_9.txt 10694_7.txt 1694_10.txt 3944_8.txt 6194_8.txt 8444_10.txt 10695_8.txt 1695_10.txt 3945_10.txt 6195_10.txt 8445_10.txt 10696_7.txt 1696_10.txt 3946_7.txt 6196_8.txt 8446_10.txt 10697_8.txt 1697_10.txt 3947_7.txt 6197_10.txt 8447_8.txt 10698_9.txt 1698_10.txt 3948_9.txt 6198_7.txt 8448_10.txt 10699_9.txt 1699_10.txt 3949_8.txt 6199_8.txt 8449_9.txt 1069_10.txt 169_8.txt 394_8.txt 619_9.txt 844_8.txt 106_10.txt 16_7.txt 3950_7.txt 61_10.txt 8450_7.txt 10700_8.txt 1700_8.txt 3951_10.txt 6200_7.txt 8451_10.txt 10701_10.txt 1701_10.txt 3952_10.txt 6201_9.txt 8452_7.txt 10702_10.txt 1702_9.txt 3953_10.txt 6202_7.txt 8453_10.txt 10703_7.txt 1703_8.txt 3954_9.txt 6203_7.txt 8454_8.txt 10704_10.txt 1704_8.txt 3955_9.txt 6204_7.txt 8455_10.txt 10705_7.txt 1705_10.txt 3956_10.txt 6205_8.txt 8456_10.txt 10706_7.txt 1706_9.txt 3957_10.txt 6206_8.txt 8457_9.txt 10707_8.txt 1707_10.txt 3958_7.txt 6207_9.txt 8458_10.txt 10708_8.txt 1708_10.txt 3959_9.txt 6208_7.txt 8459_10.txt 10709_10.txt 1709_8.txt 395_10.txt 6209_8.txt 845_7.txt 1070_8.txt 170_10.txt 3960_10.txt 620_10.txt 8460_7.txt 10710_9.txt 1710_7.txt 3961_8.txt 6210_10.txt 8461_7.txt 10711_10.txt 1711_8.txt 3962_10.txt 6211_8.txt 8462_9.txt 10712_8.txt 1712_9.txt 3963_7.txt 6212_9.txt 8463_9.txt 10713_9.txt 1713_8.txt 3964_7.txt 6213_7.txt 8464_10.txt 10714_8.txt 1714_8.txt 3965_8.txt 6214_7.txt 8465_8.txt 10715_8.txt 1715_8.txt 3966_9.txt 6215_7.txt 8466_9.txt 10716_7.txt 1716_8.txt 3967_7.txt 6216_7.txt 8467_7.txt 10717_10.txt 1717_8.txt 3968_10.txt 6217_8.txt 8468_7.txt 10718_10.txt 1718_7.txt 3969_8.txt 6218_7.txt 8469_7.txt 10719_10.txt 1719_7.txt 396_8.txt 6219_7.txt 846_7.txt 1071_8.txt 171_8.txt 3970_7.txt 621_10.txt 8470_8.txt 10720_9.txt 1720_10.txt 3971_8.txt 6220_8.txt 8471_10.txt 10721_9.txt 1721_8.txt 3972_7.txt 6221_7.txt 8472_10.txt 10722_10.txt 1722_7.txt 3973_7.txt 6222_9.txt 8473_8.txt 10723_8.txt 1723_7.txt 3974_8.txt 6223_7.txt 8474_10.txt 10724_8.txt 1724_10.txt 3975_10.txt 6224_7.txt 8475_10.txt 10725_9.txt 1725_8.txt 3976_10.txt 6225_8.txt 8476_10.txt 10726_7.txt 1726_8.txt 3977_10.txt 6226_10.txt 8477_10.txt 10727_7.txt 1727_10.txt 3978_10.txt 6227_7.txt 8478_8.txt 10728_10.txt 1728_7.txt 3979_10.txt 6228_7.txt 8479_9.txt 10729_8.txt 1729_8.txt 397_9.txt 6229_7.txt 847_7.txt 1072_10.txt 172_10.txt 3980_10.txt 622_10.txt 8480_10.txt 10730_10.txt 1730_10.txt 3981_10.txt 6230_8.txt 8481_8.txt 10731_7.txt 1731_10.txt 3982_7.txt 6231_10.txt 8482_7.txt 10732_8.txt 1732_10.txt 3983_10.txt 6232_10.txt 8483_7.txt 10733_7.txt 1733_7.txt 3984_7.txt 6233_9.txt 8484_10.txt 10734_10.txt 1734_7.txt 3985_8.txt 6234_8.txt 8485_8.txt 10735_10.txt 1735_10.txt 3986_10.txt 6235_8.txt 8486_10.txt 10736_10.txt 1736_10.txt 3987_7.txt 6236_8.txt 8487_10.txt 10737_10.txt 1737_8.txt 3988_7.txt 6237_7.txt 8488_8.txt 10738_9.txt 1738_7.txt 3989_8.txt 6238_9.txt 8489_9.txt 10739_10.txt 1739_10.txt 398_10.txt 6239_8.txt 848_8.txt 1073_9.txt 173_7.txt 3990_8.txt 623_10.txt 8490_7.txt 10740_8.txt 1740_8.txt 3991_7.txt 6240_10.txt 8491_7.txt 10741_10.txt 1741_9.txt 3992_8.txt 6241_10.txt 8492_9.txt 10742_9.txt 1742_7.txt 3993_7.txt 6242_8.txt 8493_10.txt 10743_9.txt 1743_9.txt 3994_8.txt 6243_9.txt 8494_8.txt 10744_8.txt 1744_10.txt 3995_8.txt 6244_8.txt 8495_8.txt 10745_10.txt 1745_7.txt 3996_9.txt 6245_10.txt 8496_7.txt 10746_10.txt 1746_7.txt 3997_10.txt 6246_9.txt 8497_7.txt 10747_10.txt 1747_10.txt 3998_10.txt 6247_10.txt 8498_9.txt 10748_10.txt 1748_8.txt 3999_10.txt 6248_7.txt 8499_9.txt 10749_8.txt 1749_10.txt 399_9.txt 6249_7.txt 849_7.txt 1074_10.txt 174_7.txt 39_9.txt 624_9.txt 84_10.txt 10750_8.txt 1750_10.txt 3_10.txt 6250_10.txt 8500_7.txt 10751_10.txt 1751_8.txt 4000_10.txt 6251_7.txt 8501_8.txt 10752_10.txt 1752_8.txt 4001_8.txt 6252_10.txt 8502_9.txt 10753_10.txt 1753_9.txt 4002_8.txt 6253_8.txt 8503_8.txt 10754_10.txt 1754_10.txt 4003_9.txt 6254_10.txt 8504_8.txt 10755_10.txt 1755_10.txt 4004_9.txt 6255_8.txt 8505_8.txt 10756_8.txt 1756_7.txt 4005_10.txt 6256_8.txt 8506_8.txt 10757_10.txt 1757_8.txt 4006_8.txt 6257_10.txt 8507_10.txt 10758_8.txt 1758_10.txt 4007_9.txt 6258_8.txt 8508_7.txt 10759_9.txt 1759_8.txt 4008_9.txt 6259_8.txt 8509_7.txt 1075_10.txt 175_7.txt 4009_9.txt 625_10.txt 850_8.txt 10760_8.txt 1760_10.txt 400_10.txt 6260_10.txt 8510_7.txt 10761_10.txt 1761_9.txt 4010_10.txt 6261_8.txt 8511_10.txt 10762_10.txt 1762_7.txt 4011_8.txt 6262_9.txt 8512_7.txt 10763_8.txt 1763_9.txt 4012_8.txt 6263_7.txt 8513_9.txt 10764_9.txt 1764_10.txt 4013_7.txt 6264_7.txt 8514_7.txt 10765_10.txt 1765_8.txt 4014_10.txt 6265_7.txt 8515_8.txt 10766_7.txt 1766_10.txt 4015_7.txt 6266_8.txt 8516_9.txt 10767_10.txt 1767_8.txt 4016_10.txt 6267_10.txt 8517_8.txt 10768_7.txt 1768_9.txt 4017_7.txt 6268_9.txt 8518_10.txt 10769_10.txt 1769_8.txt 4018_7.txt 6269_10.txt 8519_10.txt 1076_8.txt 176_7.txt 4019_7.txt 626_9.txt 851_7.txt 10770_7.txt 1770_10.txt 401_10.txt 6270_8.txt 8520_8.txt 10771_10.txt 1771_10.txt 4020_10.txt 6271_9.txt 8521_10.txt 10772_10.txt 1772_10.txt 4021_7.txt 6272_7.txt 8522_10.txt 10773_9.txt 1773_9.txt 4022_8.txt 6273_9.txt 8523_9.txt 10774_8.txt 1774_10.txt 4023_9.txt 6274_8.txt 8524_10.txt 10775_8.txt 1775_9.txt 4024_9.txt 6275_7.txt 8525_10.txt 10776_8.txt 1776_10.txt 4025_9.txt 6276_7.txt 8526_10.txt 10777_9.txt 1777_10.txt 4026_9.txt 6277_7.txt 8527_8.txt 10778_8.txt 1778_10.txt 4027_10.txt 6278_7.txt 8528_7.txt 10779_10.txt 1779_10.txt 4028_10.txt 6279_10.txt 8529_9.txt 1077_8.txt 177_9.txt 4029_10.txt 627_8.txt 852_7.txt 10780_10.txt 1780_10.txt 402_10.txt 6280_7.txt 8530_9.txt 10781_10.txt 1781_10.txt 4030_9.txt 6281_8.txt 8531_7.txt 10782_7.txt 1782_7.txt 4031_10.txt 6282_7.txt 8532_7.txt 10783_10.txt 1783_8.txt 4032_9.txt 6283_7.txt 8533_8.txt 10784_10.txt 1784_10.txt 4033_9.txt 6284_7.txt 8534_10.txt 10785_10.txt 1785_9.txt 4034_9.txt 6285_7.txt 8535_7.txt 10786_10.txt 1786_7.txt 4035_10.txt 6286_8.txt 8536_9.txt 10787_10.txt 1787_10.txt 4036_7.txt 6287_10.txt 8537_10.txt 10788_10.txt 1788_7.txt 4037_7.txt 6288_7.txt 8538_10.txt 10789_10.txt 1789_7.txt 4038_10.txt 6289_10.txt 8539_8.txt 1078_8.txt 178_7.txt 4039_10.txt 628_9.txt 853_9.txt 10790_8.txt 1790_9.txt 403_8.txt 6290_7.txt 8540_9.txt 10791_9.txt 1791_8.txt 4040_8.txt 6291_10.txt 8541_10.txt 10792_9.txt 1792_10.txt 4041_7.txt 6292_10.txt 8542_9.txt 10793_10.txt 1793_9.txt 4042_7.txt 6293_10.txt 8543_8.txt 10794_10.txt 1794_7.txt 4043_7.txt 6294_10.txt 8544_9.txt 10795_7.txt 1795_8.txt 4044_9.txt 6295_7.txt 8545_9.txt 10796_9.txt 1796_8.txt 4045_10.txt 6296_7.txt 8546_10.txt 10797_8.txt 1797_9.txt 4046_8.txt 6297_10.txt 8547_10.txt 10798_8.txt 1798_9.txt 4047_10.txt 6298_9.txt 8548_10.txt 10799_7.txt 1799_7.txt 4048_7.txt 6299_8.txt 8549_8.txt 1079_7.txt 179_8.txt 4049_7.txt 629_9.txt 854_9.txt 107_10.txt 17_9.txt 404_9.txt 62_10.txt 8550_8.txt 10800_8.txt 1800_8.txt 4050_9.txt 6300_8.txt 8551_8.txt 10801_8.txt 1801_8.txt 4051_8.txt 6301_10.txt 8552_9.txt 10802_8.txt 1802_9.txt 4052_8.txt 6302_8.txt 8553_7.txt 10803_8.txt 1803_10.txt 4053_8.txt 6303_8.txt 8554_7.txt 10804_10.txt 1804_10.txt 4054_9.txt 6304_8.txt 8555_9.txt 10805_10.txt 1805_10.txt 4055_10.txt 6305_10.txt 8556_7.txt 10806_9.txt 1806_8.txt 4056_7.txt 6306_10.txt 8557_7.txt 10807_9.txt 1807_7.txt 4057_7.txt 6307_8.txt 8558_10.txt 10808_10.txt 1808_7.txt 4058_10.txt 6308_8.txt 8559_8.txt 10809_10.txt 1809_10.txt 4059_8.txt 6309_8.txt 855_9.txt 1080_9.txt 180_9.txt 405_10.txt 630_10.txt 8560_10.txt 10810_8.txt 1810_7.txt 4060_10.txt 6310_10.txt 8561_10.txt 10811_7.txt 1811_10.txt 4061_10.txt 6311_10.txt 8562_10.txt 10812_8.txt 1812_10.txt 4062_10.txt 6312_7.txt 8563_10.txt 10813_10.txt 1813_8.txt 4063_8.txt 6313_9.txt 8564_7.txt 10814_7.txt 1814_10.txt 4064_10.txt 6314_9.txt 8565_7.txt 10815_10.txt 1815_10.txt 4065_10.txt 6315_10.txt 8566_10.txt 10816_10.txt 1816_9.txt 4066_10.txt 6316_8.txt 8567_10.txt 10817_10.txt 1817_8.txt 4067_8.txt 6317_10.txt 8568_7.txt 10818_10.txt 1818_8.txt 4068_10.txt 6318_7.txt 8569_9.txt 10819_10.txt 1819_9.txt 4069_10.txt 6319_10.txt 856_7.txt 1081_10.txt 181_10.txt 406_8.txt 631_10.txt 8570_7.txt 10820_10.txt 1820_9.txt 4070_10.txt 6320_10.txt 8571_7.txt 10821_8.txt 1821_8.txt 4071_10.txt 6321_7.txt 8572_7.txt 10822_10.txt 1822_8.txt 4072_10.txt 6322_7.txt 8573_10.txt 10823_8.txt 1823_7.txt 4073_10.txt 6323_7.txt 8574_9.txt 10824_10.txt 1824_8.txt 4074_10.txt 6324_8.txt 8575_10.txt 10825_9.txt 1825_10.txt 4075_10.txt 6325_7.txt 8576_8.txt 10826_10.txt 1826_10.txt 4076_10.txt 6326_10.txt 8577_8.txt 10827_10.txt 1827_10.txt 4077_10.txt 6327_10.txt 8578_8.txt 10828_10.txt 1828_8.txt 4078_10.txt 6328_10.txt 8579_8.txt 10829_10.txt 1829_8.txt 4079_9.txt 6329_10.txt 857_8.txt 1082_10.txt 182_10.txt 407_10.txt 632_10.txt 8580_9.txt 10830_10.txt 1830_8.txt 4080_10.txt 6330_10.txt 8581_10.txt 10831_7.txt 1831_10.txt 4081_10.txt 6331_10.txt 8582_9.txt 10832_10.txt 1832_7.txt 4082_10.txt 6332_8.txt 8583_9.txt 10833_10.txt 1833_10.txt 4083_10.txt 6333_8.txt 8584_8.txt 10834_7.txt 1834_8.txt 4084_7.txt 6334_8.txt 8585_8.txt 10835_10.txt 1835_10.txt 4085_10.txt 6335_10.txt 8586_8.txt 10836_10.txt 1836_7.txt 4086_7.txt 6336_7.txt 8587_7.txt 10837_10.txt 1837_10.txt 4087_10.txt 6337_7.txt 8588_7.txt 10838_10.txt 1838_10.txt 4088_7.txt 6338_9.txt 8589_7.txt 10839_10.txt 1839_10.txt 4089_8.txt 6339_7.txt 858_8.txt 1083_10.txt 183_8.txt 408_10.txt 633_8.txt 8590_7.txt 10840_9.txt 1840_10.txt 4090_8.txt 6340_10.txt 8591_7.txt 10841_10.txt 1841_7.txt 4091_7.txt 6341_7.txt 8592_7.txt 10842_7.txt 1842_9.txt 4092_10.txt 6342_10.txt 8593_7.txt 10843_7.txt 1843_9.txt 4093_8.txt 6343_10.txt 8594_7.txt 10844_9.txt 1844_8.txt 4094_9.txt 6344_7.txt 8595_10.txt 10845_10.txt 1845_7.txt 4095_8.txt 6345_7.txt 8596_7.txt 10846_9.txt 1846_8.txt 4096_10.txt 6346_10.txt 8597_9.txt 10847_10.txt 1847_10.txt 4097_8.txt 6347_9.txt 8598_7.txt 10848_10.txt 1848_8.txt 4098_10.txt 6348_10.txt 8599_7.txt 10849_10.txt 1849_7.txt 4099_8.txt 6349_10.txt 859_9.txt 1084_9.txt 184_8.txt 409_10.txt 634_8.txt 85_10.txt 10850_10.txt 1850_10.txt 40_8.txt 6350_8.txt 8600_8.txt 10851_9.txt 1851_10.txt 4100_10.txt 6351_8.txt 8601_10.txt 10852_10.txt 1852_9.txt 4101_8.txt 6352_10.txt 8602_10.txt 10853_10.txt 1853_8.txt 4102_10.txt 6353_10.txt 8603_10.txt 10854_10.txt 1854_10.txt 4103_7.txt 6354_10.txt 8604_10.txt 10855_9.txt 1855_9.txt 4104_9.txt 6355_8.txt 8605_10.txt 10856_8.txt 1856_9.txt 4105_10.txt 6356_8.txt 8606_10.txt 10857_8.txt 1857_10.txt 4106_7.txt 6357_9.txt 8607_9.txt 10858_8.txt 1858_10.txt 4107_10.txt 6358_10.txt 8608_9.txt 10859_7.txt 1859_8.txt 4108_7.txt 6359_10.txt 8609_10.txt 1085_7.txt 185_9.txt 4109_10.txt 635_9.txt 860_8.txt 10860_7.txt 1860_9.txt 410_8.txt 6360_7.txt 8610_10.txt 10861_7.txt 1861_10.txt 4110_10.txt 6361_10.txt 8611_8.txt 10862_9.txt 1862_8.txt 4111_10.txt 6362_7.txt 8612_7.txt 10863_8.txt 1863_10.txt 4112_9.txt 6363_8.txt 8613_7.txt 10864_8.txt 1864_10.txt 4113_7.txt 6364_8.txt 8614_9.txt 10865_7.txt 1865_8.txt 4114_9.txt 6365_7.txt 8615_10.txt 10866_7.txt 1866_8.txt 4115_8.txt 6366_8.txt 8616_8.txt 10867_7.txt 1867_9.txt 4116_9.txt 6367_7.txt 8617_9.txt 10868_8.txt 1868_10.txt 4117_9.txt 6368_7.txt 8618_8.txt 10869_7.txt 1869_9.txt 4118_10.txt 6369_10.txt 8619_7.txt 1086_7.txt 186_8.txt 4119_8.txt 636_10.txt 861_7.txt 10870_8.txt 1870_10.txt 411_10.txt 6370_8.txt 8620_8.txt 10871_7.txt 1871_10.txt 4120_9.txt 6371_9.txt 8621_10.txt 10872_7.txt 1872_7.txt 4121_10.txt 6372_7.txt 8622_8.txt 10873_8.txt 1873_8.txt 4122_10.txt 6373_10.txt 8623_7.txt 10874_10.txt 1874_10.txt 4123_7.txt 6374_10.txt 8624_7.txt 10875_8.txt 1875_10.txt 4124_8.txt 6375_8.txt 8625_7.txt 10876_7.txt 1876_10.txt 4125_8.txt 6376_10.txt 8626_7.txt 10877_10.txt 1877_7.txt 4126_8.txt 6377_9.txt 8627_10.txt 10878_7.txt 1878_10.txt 4127_8.txt 6378_8.txt 8628_10.txt 10879_10.txt 1879_10.txt 4128_10.txt 6379_9.txt 8629_9.txt 1087_10.txt 187_8.txt 4129_10.txt 637_10.txt 862_7.txt 10880_8.txt 1880_10.txt 412_8.txt 6380_9.txt 8630_9.txt 10881_7.txt 1881_7.txt 4130_9.txt 6381_8.txt 8631_7.txt 10882_8.txt 1882_10.txt 4131_7.txt 6382_7.txt 8632_9.txt 10883_7.txt 1883_7.txt 4132_7.txt 6383_10.txt 8633_8.txt 10884_8.txt 1884_8.txt 4133_7.txt 6384_9.txt 8634_9.txt 10885_7.txt 1885_10.txt 4134_7.txt 6385_10.txt 8635_7.txt 10886_10.txt 1886_10.txt 4135_10.txt 6386_10.txt 8636_7.txt 10887_7.txt 1887_8.txt 4136_10.txt 6387_8.txt 8637_10.txt 10888_8.txt 1888_8.txt 4137_8.txt 6388_7.txt 8638_7.txt 10889_10.txt 1889_10.txt 4138_10.txt 6389_9.txt 8639_9.txt 1088_9.txt 188_7.txt 4139_8.txt 638_10.txt 863_10.txt 10890_9.txt 1890_10.txt 413_10.txt 6390_8.txt 8640_10.txt 10891_7.txt 1891_8.txt 4140_10.txt 6391_8.txt 8641_7.txt 10892_7.txt 1892_8.txt 4141_10.txt 6392_10.txt 8642_8.txt 10893_8.txt 1893_10.txt 4142_10.txt 6393_8.txt 8643_7.txt 10894_8.txt 1894_8.txt 4143_9.txt 6394_10.txt 8644_7.txt 10895_7.txt 1895_10.txt 4144_10.txt 6395_9.txt 8645_8.txt 10896_8.txt 1896_8.txt 4145_10.txt 6396_9.txt 8646_7.txt 10897_9.txt 1897_10.txt 4146_10.txt 6397_8.txt 8647_7.txt 10898_7.txt 1898_9.txt 4147_8.txt 6398_8.txt 8648_9.txt 10899_10.txt 1899_7.txt 4148_7.txt 6399_7.txt 8649_9.txt 1089_10.txt 189_9.txt 4149_10.txt 639_10.txt 864_9.txt 108_10.txt 18_7.txt 414_10.txt 63_10.txt 8650_10.txt 10900_8.txt 1900_8.txt 4150_10.txt 6400_9.txt 8651_9.txt 10901_10.txt 1901_7.txt 4151_10.txt 6401_8.txt 8652_8.txt 10902_10.txt 1902_10.txt 4152_10.txt 6402_8.txt 8653_10.txt 10903_10.txt 1903_10.txt 4153_10.txt 6403_8.txt 8654_9.txt 10904_10.txt 1904_7.txt 4154_10.txt 6404_8.txt 8655_9.txt 10905_8.txt 1905_10.txt 4155_10.txt 6405_7.txt 8656_9.txt 10906_10.txt 1906_10.txt 4156_10.txt 6406_9.txt 8657_9.txt 10907_9.txt 1907_7.txt 4157_8.txt 6407_10.txt 8658_8.txt 10908_10.txt 1908_9.txt 4158_10.txt 6408_10.txt 8659_7.txt 10909_10.txt 1909_10.txt 4159_9.txt 6409_7.txt 865_9.txt 1090_8.txt 190_10.txt 415_7.txt 640_10.txt 8660_7.txt 10910_10.txt 1910_9.txt 4160_9.txt 6410_10.txt 8661_8.txt 10911_7.txt 1911_10.txt 4161_8.txt 6411_10.txt 8662_8.txt 10912_7.txt 1912_9.txt 4162_10.txt 6412_9.txt 8663_7.txt 10913_7.txt 1913_10.txt 4163_9.txt 6413_7.txt 8664_8.txt 10914_8.txt 1914_7.txt 4164_10.txt 6414_8.txt 8665_8.txt 10915_9.txt 1915_10.txt 4165_8.txt 6415_8.txt 8666_7.txt 10916_7.txt 1916_8.txt 4166_9.txt 6416_7.txt 8667_7.txt 10917_8.txt 1917_9.txt 4167_10.txt 6417_7.txt 8668_9.txt 10918_8.txt 1918_9.txt 4168_7.txt 6418_10.txt 8669_7.txt 10919_10.txt 1919_8.txt 4169_7.txt 6419_10.txt 866_8.txt 1091_10.txt 191_9.txt 416_8.txt 641_8.txt 8670_8.txt 10920_10.txt 1920_10.txt 4170_8.txt 6420_7.txt 8671_7.txt 10921_10.txt 1921_9.txt 4171_7.txt 6421_8.txt 8672_8.txt 10922_10.txt 1922_9.txt 4172_10.txt 6422_7.txt 8673_7.txt 10923_9.txt 1923_10.txt 4173_10.txt 6423_7.txt 8674_9.txt 10924_10.txt 1924_10.txt 4174_9.txt 6424_10.txt 8675_8.txt 10925_9.txt 1925_9.txt 4175_8.txt 6425_9.txt 8676_8.txt 10926_9.txt 1926_10.txt 4176_10.txt 6426_8.txt 8677_9.txt 10927_10.txt 1927_8.txt 4177_9.txt 6427_10.txt 8678_9.txt 10928_7.txt 1928_10.txt 4178_10.txt 6428_8.txt 8679_8.txt 10929_10.txt 1929_10.txt 4179_10.txt 6429_7.txt 867_8.txt 1092_10.txt 192_9.txt 417_7.txt 642_10.txt 8680_9.txt 10930_8.txt 1930_10.txt 4180_10.txt 6430_10.txt 8681_8.txt 10931_7.txt 1931_8.txt 4181_9.txt 6431_8.txt 8682_7.txt 10932_7.txt 1932_10.txt 4182_10.txt 6432_9.txt 8683_9.txt 10933_10.txt 1933_8.txt 4183_10.txt 6433_7.txt 8684_10.txt 10934_10.txt 1934_9.txt 4184_8.txt 6434_7.txt 8685_9.txt 10935_7.txt 1935_10.txt 4185_8.txt 6435_7.txt 8686_8.txt 10936_8.txt 1936_10.txt 4186_7.txt 6436_10.txt 8687_9.txt 10937_9.txt 1937_10.txt 4187_7.txt 6437_9.txt 8688_8.txt 10938_10.txt 1938_9.txt 4188_8.txt 6438_10.txt 8689_7.txt 10939_8.txt 1939_8.txt 4189_8.txt 6439_10.txt 868_8.txt 1093_8.txt 193_7.txt 418_9.txt 643_10.txt 8690_8.txt 10940_10.txt 1940_10.txt 4190_10.txt 6440_8.txt 8691_7.txt 10941_7.txt 1941_9.txt 4191_10.txt 6441_9.txt 8692_10.txt 10942_9.txt 1942_10.txt 4192_10.txt 6442_9.txt 8693_10.txt 10943_10.txt 1943_10.txt 4193_8.txt 6443_9.txt 8694_10.txt 10944_8.txt 1944_8.txt 4194_10.txt 6444_8.txt 8695_9.txt 10945_9.txt 1945_8.txt 4195_9.txt 6445_10.txt 8696_10.txt 10946_7.txt 1946_9.txt 4196_9.txt 6446_10.txt 8697_7.txt 10947_8.txt 1947_8.txt 4197_8.txt 6447_8.txt 8698_10.txt 10948_10.txt 1948_10.txt 4198_7.txt 6448_10.txt 8699_10.txt 10949_8.txt 1949_8.txt 4199_7.txt 6449_10.txt 869_7.txt 1094_9.txt 194_8.txt 419_7.txt 644_9.txt 86_10.txt 10950_9.txt 1950_8.txt 41_9.txt 6450_10.txt 8700_7.txt 10951_8.txt 1951_9.txt 4200_9.txt 6451_8.txt 8701_8.txt 10952_10.txt 1952_8.txt 4201_9.txt 6452_9.txt 8702_10.txt 10953_10.txt 1953_10.txt 4202_10.txt 6453_10.txt 8703_10.txt 10954_10.txt 1954_10.txt 4203_8.txt 6454_9.txt 8704_9.txt 10955_7.txt 1955_9.txt 4204_10.txt 6455_7.txt 8705_8.txt 10956_9.txt 1956_8.txt 4205_8.txt 6456_10.txt 8706_8.txt 10957_10.txt 1957_7.txt 4206_9.txt 6457_7.txt 8707_10.txt 10958_10.txt 1958_10.txt 4207_7.txt 6458_8.txt 8708_8.txt 10959_10.txt 1959_7.txt 4208_10.txt 6459_10.txt 8709_8.txt 1095_9.txt 195_8.txt 4209_7.txt 645_10.txt 870_10.txt 10960_8.txt 1960_7.txt 420_7.txt 6460_10.txt 8710_7.txt 10961_9.txt 1961_9.txt 4210_9.txt 6461_10.txt 8711_7.txt 10962_10.txt 1962_10.txt 4211_8.txt 6462_10.txt 8712_8.txt 10963_7.txt 1963_8.txt 4212_10.txt 6463_8.txt 8713_10.txt 10964_7.txt 1964_7.txt 4213_7.txt 6464_9.txt 8714_10.txt 10965_9.txt 1965_7.txt 4214_10.txt 6465_9.txt 8715_10.txt 10966_9.txt 1966_10.txt 4215_9.txt 6466_10.txt 8716_10.txt 10967_10.txt 1967_8.txt 4216_10.txt 6467_7.txt 8717_9.txt 10968_7.txt 1968_8.txt 4217_9.txt 6468_7.txt 8718_9.txt 10969_10.txt 1969_10.txt 4218_8.txt 6469_9.txt 8719_10.txt 1096_9.txt 196_9.txt 4219_7.txt 646_9.txt 871_9.txt 10970_7.txt 1970_9.txt 421_9.txt 6470_10.txt 8720_9.txt 10971_9.txt 1971_9.txt 4220_7.txt 6471_10.txt 8721_10.txt 10972_10.txt 1972_10.txt 4221_8.txt 6472_10.txt 8722_9.txt 10973_8.txt 1973_8.txt 4222_7.txt 6473_8.txt 8723_8.txt 10974_10.txt 1974_8.txt 4223_8.txt 6474_8.txt 8724_10.txt 10975_10.txt 1975_7.txt 4224_10.txt 6475_10.txt 8725_10.txt 10976_10.txt 1976_10.txt 4225_10.txt 6476_10.txt 8726_9.txt 10977_8.txt 1977_10.txt 4226_10.txt 6477_7.txt 8727_7.txt 10978_10.txt 1978_9.txt 4227_10.txt 6478_10.txt 8728_7.txt 10979_9.txt 1979_9.txt 4228_10.txt 6479_10.txt 8729_10.txt 1097_9.txt 197_9.txt 4229_10.txt 647_10.txt 872_9.txt 10980_7.txt 1980_10.txt 422_7.txt 6480_10.txt 8730_8.txt 10981_8.txt 1981_9.txt 4230_10.txt 6481_10.txt 8731_7.txt 10982_10.txt 1982_10.txt 4231_7.txt 6482_10.txt 8732_9.txt 10983_10.txt 1983_10.txt 4232_7.txt 6483_10.txt 8733_10.txt 10984_10.txt 1984_10.txt 4233_7.txt 6484_10.txt 8734_9.txt 10985_9.txt 1985_10.txt 4234_7.txt 6485_10.txt 8735_8.txt 10986_10.txt 1986_10.txt 4235_7.txt 6486_10.txt 8736_10.txt 10987_8.txt 1987_10.txt 4236_8.txt 6487_10.txt 8737_9.txt 10988_10.txt 1988_9.txt 4237_10.txt 6488_10.txt 8738_8.txt 10989_9.txt 1989_8.txt 4238_9.txt 6489_10.txt 8739_9.txt 1098_9.txt 198_8.txt 4239_10.txt 648_7.txt 873_8.txt 10990_7.txt 1990_10.txt 423_10.txt 6490_10.txt 8740_8.txt 10991_9.txt 1991_10.txt 4240_9.txt 6491_10.txt 8741_10.txt 10992_10.txt 1992_10.txt 4241_10.txt 6492_10.txt 8742_10.txt 10993_10.txt 1993_10.txt 4242_9.txt 6493_10.txt 8743_7.txt 10994_8.txt 1994_10.txt 4243_8.txt 6494_10.txt 8744_7.txt 10995_10.txt 1995_9.txt 4244_10.txt 6495_10.txt 8745_7.txt 10996_8.txt 1996_10.txt 4245_7.txt 6496_10.txt 8746_10.txt 10997_10.txt 1997_9.txt 4246_8.txt 6497_10.txt 8747_7.txt 10998_7.txt 1998_9.txt 4247_10.txt 6498_10.txt 8748_8.txt 10999_7.txt 1999_9.txt 4248_10.txt 6499_10.txt 8749_7.txt 1099_10.txt 199_10.txt 4249_10.txt 649_10.txt 874_9.txt 109_10.txt 19_10.txt 424_8.txt 64_7.txt 8750_7.txt 10_9.txt 1_7.txt 4250_8.txt 6500_10.txt 8751_7.txt 11000_10.txt 2000_10.txt 4251_9.txt 6501_10.txt 8752_9.txt 11001_10.txt 2001_9.txt 4252_9.txt 6502_10.txt 8753_8.txt 11002_8.txt 2002_7.txt 4253_10.txt 6503_10.txt 8754_8.txt 11003_10.txt 2003_8.txt 4254_9.txt 6504_10.txt 8755_7.txt 11004_8.txt 2004_10.txt 4255_9.txt 6505_10.txt 8756_9.txt 11005_10.txt 2005_10.txt 4256_8.txt 6506_10.txt 8757_8.txt 11006_7.txt 2006_7.txt 4257_10.txt 6507_10.txt 8758_10.txt 11007_10.txt 2007_7.txt 4258_9.txt 6508_7.txt 8759_8.txt 11008_9.txt 2008_7.txt 4259_9.txt 6509_9.txt 875_10.txt 11009_7.txt 2009_10.txt 425_10.txt 650_9.txt 8760_7.txt 1100_7.txt 200_10.txt 4260_9.txt 6510_7.txt 8761_10.txt 11010_10.txt 2010_8.txt 4261_9.txt 6511_10.txt 8762_8.txt 11011_10.txt 2011_7.txt 4262_10.txt 6512_7.txt 8763_9.txt 11012_9.txt 2012_8.txt 4263_8.txt 6513_9.txt 8764_8.txt 11013_7.txt 2013_8.txt 4264_10.txt 6514_10.txt 8765_10.txt 11014_10.txt 2014_7.txt 4265_8.txt 6515_7.txt 8766_8.txt 11015_9.txt 2015_8.txt 4266_7.txt 6516_10.txt 8767_10.txt 11016_7.txt 2016_7.txt 4267_8.txt 6517_10.txt 8768_8.txt 11017_9.txt 2017_10.txt 4268_10.txt 6518_10.txt 8769_7.txt 11018_10.txt 2018_9.txt 4269_10.txt 6519_8.txt 876_7.txt 11019_10.txt 2019_10.txt 426_7.txt 651_10.txt 8770_7.txt 1101_8.txt 201_10.txt 4270_10.txt 6520_7.txt 8771_7.txt 11020_8.txt 2020_7.txt 4271_10.txt 6521_7.txt 8772_8.txt 11021_8.txt 2021_8.txt 4272_10.txt 6522_10.txt 8773_8.txt 11022_9.txt 2022_9.txt 4273_10.txt 6523_10.txt 8774_8.txt 11023_9.txt 2023_7.txt 4274_10.txt 6524_8.txt 8775_9.txt 11024_9.txt 2024_9.txt 4275_10.txt 6525_7.txt 8776_10.txt 11025_7.txt 2025_10.txt 4276_10.txt 6526_7.txt 8777_10.txt 11026_7.txt 2026_8.txt 4277_10.txt 6527_8.txt 8778_8.txt 11027_10.txt 2027_10.txt 4278_10.txt 6528_9.txt 8779_8.txt 11028_8.txt 2028_10.txt 4279_10.txt 6529_7.txt 877_8.txt 11029_7.txt 2029_8.txt 427_10.txt 652_10.txt 8780_10.txt 1102_8.txt 202_10.txt 4280_10.txt 6530_8.txt 8781_9.txt 11030_8.txt 2030_9.txt 4281_10.txt 6531_8.txt 8782_9.txt 11031_8.txt 2031_8.txt 4282_10.txt 6532_8.txt 8783_9.txt 11032_7.txt 2032_10.txt 4283_10.txt 6533_7.txt 8784_9.txt 11033_9.txt 2033_8.txt 4284_10.txt 6534_10.txt 8785_10.txt 11034_9.txt 2034_9.txt 4285_10.txt 6535_10.txt 8786_8.txt 11035_8.txt 2035_7.txt 4286_10.txt 6536_7.txt 8787_8.txt 11036_8.txt 2036_7.txt 4287_10.txt 6537_7.txt 8788_10.txt 11037_8.txt 2037_8.txt 4288_9.txt 6538_8.txt 8789_10.txt 11038_10.txt 2038_7.txt 4289_7.txt 6539_9.txt 878_7.txt 11039_8.txt 2039_9.txt 428_7.txt 653_10.txt 8790_8.txt 1103_10.txt 203_7.txt 4290_9.txt 6540_7.txt 8791_10.txt 11040_7.txt 2040_7.txt 4291_10.txt 6541_8.txt 8792_8.txt 11041_7.txt 2041_7.txt 4292_7.txt 6542_8.txt 8793_10.txt 11042_7.txt 2042_7.txt 4293_8.txt 6543_9.txt 8794_9.txt 11043_7.txt 2043_7.txt 4294_8.txt 6544_10.txt 8795_10.txt 11044_7.txt 2044_8.txt 4295_9.txt 6545_10.txt 8796_8.txt 11045_9.txt 2045_9.txt 4296_9.txt 6546_10.txt 8797_8.txt 11046_8.txt 2046_8.txt 4297_9.txt 6547_9.txt 8798_8.txt 11047_9.txt 2047_10.txt 4298_10.txt 6548_10.txt 8799_8.txt 11048_7.txt 2048_7.txt 4299_8.txt 6549_8.txt 879_8.txt 11049_8.txt 2049_7.txt 429_10.txt 654_10.txt 87_10.txt 1104_8.txt 204_10.txt 42_10.txt 6550_10.txt 8800_10.txt 11050_8.txt 2050_7.txt 4300_7.txt 6551_9.txt 8801_9.txt 11051_8.txt 2051_8.txt 4301_10.txt 6552_9.txt 8802_7.txt 11052_9.txt 2052_10.txt 4302_8.txt 6553_10.txt 8803_10.txt 11053_10.txt 2053_10.txt 4303_10.txt 6554_7.txt 8804_10.txt 11054_7.txt 2054_9.txt 4304_9.txt 6555_8.txt 8805_9.txt 11055_10.txt 2055_10.txt 4305_10.txt 6556_9.txt 8806_9.txt 11056_10.txt 2056_9.txt 4306_10.txt 6557_10.txt 8807_9.txt 11057_10.txt 2057_10.txt 4307_10.txt 6558_8.txt 8808_9.txt 11058_10.txt 2058_9.txt 4308_8.txt 6559_10.txt 8809_10.txt 11059_9.txt 2059_7.txt 4309_8.txt 655_10.txt 880_10.txt 1105_8.txt 205_8.txt 430_7.txt 6560_7.txt 8810_9.txt 11060_10.txt 2060_8.txt 4310_9.txt 6561_10.txt 8811_7.txt 11061_8.txt 2061_9.txt 4311_9.txt 6562_10.txt 8812_10.txt 11062_10.txt 2062_10.txt 4312_10.txt 6563_7.txt 8813_9.txt 11063_10.txt 2063_8.txt 4313_9.txt 6564_7.txt 8814_10.txt 11064_7.txt 2064_8.txt 4314_10.txt 6565_8.txt 8815_7.txt 11065_8.txt 2065_7.txt 4315_9.txt 6566_8.txt 8816_7.txt 11066_8.txt 2066_7.txt 4316_10.txt 6567_10.txt 8817_8.txt 11067_7.txt 2067_9.txt 4317_10.txt 6568_9.txt 8818_10.txt 11068_9.txt 2068_8.txt 4318_10.txt 6569_7.txt 8819_10.txt 11069_10.txt 2069_9.txt 4319_9.txt 656_10.txt 881_8.txt 1106_8.txt 206_10.txt 431_8.txt 6570_8.txt 8820_7.txt 11070_9.txt 2070_9.txt 4320_8.txt 6571_10.txt 8821_9.txt 11071_10.txt 2071_9.txt 4321_8.txt 6572_8.txt 8822_10.txt 11072_8.txt 2072_10.txt 4322_10.txt 6573_7.txt 8823_8.txt 11073_8.txt 2073_7.txt 4323_9.txt 6574_8.txt 8824_8.txt 11074_7.txt 2074_10.txt 4324_7.txt 6575_8.txt 8825_9.txt 11075_10.txt 2075_8.txt 4325_10.txt 6576_8.txt 8826_9.txt 11076_8.txt 2076_9.txt 4326_7.txt 6577_8.txt 8827_8.txt 11077_8.txt 2077_10.txt 4327_8.txt 6578_7.txt 8828_10.txt 11078_10.txt 2078_9.txt 4328_10.txt 6579_10.txt 8829_9.txt 11079_8.txt 2079_10.txt 4329_7.txt 657_10.txt 882_8.txt 1107_10.txt 207_8.txt 432_8.txt 6580_8.txt 8830_8.txt 11080_10.txt 2080_9.txt 4330_10.txt 6581_7.txt 8831_8.txt 11081_10.txt 2081_7.txt 4331_7.txt 6582_7.txt 8832_7.txt 11082_10.txt 2082_10.txt 4332_10.txt 6583_7.txt 8833_9.txt 11083_10.txt 2083_7.txt 4333_10.txt 6584_8.txt 8834_9.txt 11084_10.txt 2084_8.txt 4334_10.txt 6585_10.txt 8835_9.txt 11085_10.txt 2085_7.txt 4335_9.txt 6586_10.txt 8836_10.txt 11086_7.txt 2086_10.txt 4336_10.txt 6587_9.txt 8837_8.txt 11087_8.txt 2087_10.txt 4337_7.txt 6588_9.txt 8838_9.txt 11088_7.txt 2088_8.txt 4338_10.txt 6589_10.txt 8839_8.txt 11089_10.txt 2089_10.txt 4339_10.txt 658_10.txt 883_9.txt 1108_7.txt 208_9.txt 433_10.txt 6590_9.txt 8840_10.txt 11090_8.txt 2090_10.txt 4340_8.txt 6591_8.txt 8841_8.txt 11091_8.txt 2091_9.txt 4341_8.txt 6592_10.txt 8842_9.txt 11092_8.txt 2092_10.txt 4342_10.txt 6593_7.txt 8843_7.txt 11093_10.txt 2093_7.txt 4343_9.txt 6594_10.txt 8844_10.txt 11094_9.txt 2094_10.txt 4344_9.txt 6595_9.txt 8845_8.txt 11095_7.txt 2095_9.txt 4345_10.txt 6596_8.txt 8846_9.txt 11096_10.txt 2096_10.txt 4346_10.txt 6597_9.txt 8847_9.txt 11097_9.txt 2097_9.txt 4347_8.txt 6598_8.txt 8848_7.txt 11098_10.txt 2098_10.txt 4348_7.txt 6599_8.txt 8849_9.txt 11099_10.txt 2099_10.txt 4349_10.txt 659_10.txt 884_8.txt 1109_7.txt 209_8.txt 434_8.txt 65_10.txt 8850_8.txt 110_10.txt 20_9.txt 4350_10.txt 6600_9.txt 8851_7.txt 11100_10.txt 2100_7.txt 4351_10.txt 6601_9.txt 8852_9.txt 11101_10.txt 2101_7.txt 4352_10.txt 6602_7.txt 8853_7.txt 11102_10.txt 2102_10.txt 4353_9.txt 6603_8.txt 8854_10.txt 11103_10.txt 2103_7.txt 4354_8.txt 6604_7.txt 8855_10.txt 11104_10.txt 2104_7.txt 4355_10.txt 6605_7.txt 8856_8.txt 11105_10.txt 2105_8.txt 4356_8.txt 6606_9.txt 8857_10.txt 11106_10.txt 2106_10.txt 4357_10.txt 6607_9.txt 8858_10.txt 11107_10.txt 2107_7.txt 4358_10.txt 6608_7.txt 8859_10.txt 11108_10.txt 2108_10.txt 4359_10.txt 6609_8.txt 885_8.txt 11109_9.txt 2109_9.txt 435_8.txt 660_9.txt 8860_8.txt 1110_9.txt 210_10.txt 4360_10.txt 6610_8.txt 8861_9.txt 11110_9.txt 2110_9.txt 4361_10.txt 6611_7.txt 8862_8.txt 11111_9.txt 2111_7.txt 4362_7.txt 6612_8.txt 8863_8.txt 11112_7.txt 2112_9.txt 4363_8.txt 6613_7.txt 8864_10.txt 11113_9.txt 2113_10.txt 4364_7.txt 6614_9.txt 8865_10.txt 11114_10.txt 2114_10.txt 4365_9.txt 6615_8.txt 8866_9.txt 11115_10.txt 2115_10.txt 4366_9.txt 6616_7.txt 8867_10.txt 11116_10.txt 2116_10.txt 4367_9.txt 6617_8.txt 8868_10.txt 11117_10.txt 2117_10.txt 4368_10.txt 6618_8.txt 8869_7.txt 11118_10.txt 2118_9.txt 4369_9.txt 6619_9.txt 886_8.txt 11119_10.txt 2119_10.txt 436_10.txt 661_9.txt 8870_8.txt 1111_10.txt 211_9.txt 4370_10.txt 6620_10.txt 8871_10.txt 11120_10.txt 2120_8.txt 4371_8.txt 6621_8.txt 8872_8.txt 11121_10.txt 2121_10.txt 4372_10.txt 6622_10.txt 8873_8.txt 11122_10.txt 2122_7.txt 4373_10.txt 6623_7.txt 8874_8.txt 11123_10.txt 2123_10.txt 4374_10.txt 6624_9.txt 8875_9.txt 11124_10.txt 2124_10.txt 4375_9.txt 6625_7.txt 8876_10.txt 11125_10.txt 2125_10.txt 4376_9.txt 6626_7.txt 8877_9.txt 11126_10.txt 2126_10.txt 4377_10.txt 6627_7.txt 8878_9.txt 11127_9.txt 2127_9.txt 4378_9.txt 6628_7.txt 8879_7.txt 11128_10.txt 2128_8.txt 4379_8.txt 6629_10.txt 887_10.txt 11129_10.txt 2129_8.txt 437_9.txt 662_8.txt 8880_8.txt 1112_8.txt 212_9.txt 4380_8.txt 6630_10.txt 8881_8.txt 11130_9.txt 2130_10.txt 4381_10.txt 6631_7.txt 8882_10.txt 11131_9.txt 2131_9.txt 4382_8.txt 6632_7.txt 8883_8.txt 11132_10.txt 2132_9.txt 4383_9.txt 6633_10.txt 8884_9.txt 11133_10.txt 2133_10.txt 4384_10.txt 6634_10.txt 8885_10.txt 11134_9.txt 2134_10.txt 4385_9.txt 6635_10.txt 8886_10.txt 11135_9.txt 2135_8.txt 4386_9.txt 6636_8.txt 8887_9.txt 11136_10.txt 2136_9.txt 4387_9.txt 6637_8.txt 8888_7.txt 11137_7.txt 2137_8.txt 4388_9.txt 6638_10.txt 8889_8.txt 11138_10.txt 2138_9.txt 4389_10.txt 6639_10.txt 888_8.txt 11139_8.txt 2139_9.txt 438_9.txt 663_8.txt 8890_10.txt 1113_10.txt 213_9.txt 4390_8.txt 6640_8.txt 8891_10.txt 11140_9.txt 2140_10.txt 4391_8.txt 6641_8.txt 8892_10.txt 11141_10.txt 2141_10.txt 4392_9.txt 6642_10.txt 8893_9.txt 11142_8.txt 2142_8.txt 4393_7.txt 6643_7.txt 8894_10.txt 11143_10.txt 2143_9.txt 4394_9.txt 6644_8.txt 8895_10.txt 11144_8.txt 2144_8.txt 4395_10.txt 6645_8.txt 8896_10.txt 11145_8.txt 2145_7.txt 4396_10.txt 6646_10.txt 8897_10.txt 11146_8.txt 2146_9.txt 4397_7.txt 6647_8.txt 8898_8.txt 11147_9.txt 2147_10.txt 4398_9.txt 6648_7.txt 8899_8.txt 11148_9.txt 2148_10.txt 4399_9.txt 6649_8.txt 889_10.txt 11149_10.txt 2149_10.txt 439_9.txt 664_7.txt 88_9.txt 1114_8.txt 214_7.txt 43_10.txt 6650_8.txt 8900_10.txt 11150_10.txt 2150_10.txt 4400_10.txt 6651_10.txt 8901_8.txt 11151_8.txt 2151_10.txt 4401_9.txt 6652_10.txt 8902_7.txt 11152_10.txt 2152_8.txt 4402_8.txt 6653_8.txt 8903_7.txt 11153_10.txt 2153_9.txt 4403_10.txt 6654_7.txt 8904_7.txt 11154_10.txt 2154_10.txt 4404_9.txt 6655_7.txt 8905_10.txt 11155_10.txt 2155_10.txt 4405_8.txt 6656_10.txt 8906_8.txt 11156_8.txt 2156_10.txt 4406_9.txt 6657_9.txt 8907_8.txt 11157_7.txt 2157_10.txt 4407_8.txt 6658_10.txt 8908_10.txt 11158_8.txt 2158_10.txt 4408_8.txt 6659_10.txt 8909_9.txt 11159_8.txt 2159_10.txt 4409_10.txt 665_9.txt 890_9.txt 1115_9.txt 215_8.txt 440_10.txt 6660_8.txt 8910_10.txt 11160_9.txt 2160_8.txt 4410_10.txt 6661_7.txt 8911_8.txt 11161_8.txt 2161_10.txt 4411_9.txt 6662_7.txt 8912_10.txt 11162_10.txt 2162_10.txt 4412_9.txt 6663_10.txt 8913_9.txt 11163_9.txt 2163_10.txt 4413_8.txt 6664_9.txt 8914_8.txt 11164_8.txt 2164_10.txt 4414_9.txt 6665_10.txt 8915_10.txt 11165_8.txt 2165_7.txt 4415_10.txt 6666_7.txt 8916_8.txt 11166_7.txt 2166_7.txt 4416_9.txt 6667_8.txt 8917_7.txt 11167_9.txt 2167_10.txt 4417_9.txt 6668_8.txt 8918_10.txt 11168_8.txt 2168_8.txt 4418_10.txt 6669_10.txt 8919_7.txt 11169_7.txt 2169_7.txt 4419_7.txt 666_10.txt 891_10.txt 1116_9.txt 216_8.txt 441_9.txt 6670_8.txt 8920_10.txt 11170_10.txt 2170_9.txt 4420_8.txt 6671_10.txt 8921_8.txt 11171_7.txt 2171_10.txt 4421_8.txt 6672_10.txt 8922_7.txt 11172_10.txt 2172_7.txt 4422_8.txt 6673_7.txt 8923_8.txt 11173_7.txt 2173_10.txt 4423_8.txt 6674_9.txt 8924_10.txt 11174_8.txt 2174_8.txt 4424_7.txt 6675_8.txt 8925_7.txt 11175_8.txt 2175_9.txt 4425_7.txt 6676_9.txt 8926_10.txt 11176_9.txt 2176_8.txt 4426_7.txt 6677_9.txt 8927_8.txt 11177_8.txt 2177_8.txt 4427_10.txt 6678_7.txt 8928_10.txt 11178_7.txt 2178_9.txt 4428_10.txt 6679_8.txt 8929_8.txt 11179_9.txt 2179_8.txt 4429_7.txt 667_8.txt 892_10.txt 1117_10.txt 217_8.txt 442_9.txt 6680_7.txt 8930_9.txt 11180_7.txt 2180_8.txt 4430_8.txt 6681_10.txt 8931_9.txt 11181_10.txt 2181_10.txt 4431_10.txt 6682_10.txt 8932_9.txt 11182_10.txt 2182_9.txt 4432_10.txt 6683_8.txt 8933_7.txt 11183_7.txt 2183_8.txt 4433_7.txt 6684_10.txt 8934_10.txt 11184_7.txt 2184_7.txt 4434_7.txt 6685_10.txt 8935_8.txt 11185_9.txt 2185_9.txt 4435_7.txt 6686_9.txt 8936_8.txt 11186_7.txt 2186_8.txt 4436_7.txt 6687_9.txt 8937_9.txt 11187_9.txt 2187_10.txt 4437_7.txt 6688_9.txt 8938_9.txt 11188_10.txt 2188_10.txt 4438_8.txt 6689_8.txt 8939_9.txt 11189_8.txt 2189_10.txt 4439_8.txt 668_7.txt 893_8.txt 1118_10.txt 218_9.txt 443_10.txt 6690_10.txt 8940_9.txt 11190_10.txt 2190_10.txt 4440_7.txt 6691_9.txt 8941_10.txt 11191_10.txt 2191_10.txt 4441_7.txt 6692_9.txt 8942_8.txt 11192_10.txt 2192_9.txt 4442_9.txt 6693_10.txt 8943_7.txt 11193_7.txt 2193_10.txt 4443_8.txt 6694_10.txt 8944_8.txt 11194_10.txt 2194_10.txt 4444_10.txt 6695_10.txt 8945_10.txt 11195_7.txt 2195_10.txt 4445_7.txt 6696_7.txt 8946_8.txt 11196_7.txt 2196_8.txt 4446_10.txt 6697_10.txt 8947_9.txt 11197_8.txt 2197_7.txt 4447_7.txt 6698_10.txt 8948_7.txt 11198_8.txt 2198_8.txt 4448_10.txt 6699_8.txt 8949_7.txt 11199_9.txt 2199_7.txt 4449_8.txt 669_7.txt 894_9.txt 1119_10.txt 219_8.txt 444_10.txt 66_8.txt 8950_7.txt 111_10.txt 21_7.txt 4450_9.txt 6700_8.txt 8951_7.txt 11200_8.txt 2200_9.txt 4451_9.txt 6701_9.txt 8952_7.txt 11201_7.txt 2201_10.txt 4452_8.txt 6702_8.txt 8953_8.txt 11202_10.txt 2202_10.txt 4453_10.txt 6703_8.txt 8954_8.txt 11203_8.txt 2203_8.txt 4454_9.txt 6704_9.txt 8955_8.txt 11204_9.txt 2204_10.txt 4455_8.txt 6705_10.txt 8956_8.txt 11205_10.txt 2205_8.txt 4456_7.txt 6706_8.txt 8957_7.txt 11206_10.txt 2206_7.txt 4457_8.txt 6707_9.txt 8958_10.txt 11207_10.txt 2207_9.txt 4458_7.txt 6708_10.txt 8959_8.txt 11208_8.txt 2208_7.txt 4459_7.txt 6709_10.txt 895_10.txt 11209_8.txt 2209_8.txt 445_10.txt 670_7.txt 8960_10.txt 1120_7.txt 220_10.txt 4460_9.txt 6710_10.txt 8961_9.txt 11210_7.txt 2210_9.txt 4461_8.txt 6711_10.txt 8962_9.txt 11211_8.txt 2211_8.txt 4462_9.txt 6712_9.txt 8963_8.txt 11212_8.txt 2212_10.txt 4463_7.txt 6713_10.txt 8964_8.txt 11213_7.txt 2213_9.txt 4464_10.txt 6714_10.txt 8965_8.txt 11214_7.txt 2214_10.txt 4465_7.txt 6715_10.txt 8966_10.txt 11215_10.txt 2215_8.txt 4466_8.txt 6716_8.txt 8967_8.txt 11216_7.txt 2216_9.txt 4467_7.txt 6717_7.txt 8968_8.txt 11217_7.txt 2217_7.txt 4468_7.txt 6718_10.txt 8969_7.txt 11218_9.txt 2218_7.txt 4469_9.txt 6719_8.txt 896_10.txt 11219_7.txt 2219_10.txt 446_10.txt 671_7.txt 8970_10.txt 1121_7.txt 221_9.txt 4470_8.txt 6720_9.txt 8971_9.txt 11220_10.txt 2220_9.txt 4471_8.txt 6721_10.txt 8972_10.txt 11221_10.txt 2221_8.txt 4472_10.txt 6722_9.txt 8973_9.txt 11222_8.txt 2222_10.txt 4473_8.txt 6723_10.txt 8974_10.txt 11223_9.txt 2223_8.txt 4474_10.txt 6724_8.txt 8975_10.txt 11224_8.txt 2224_10.txt 4475_8.txt 6725_9.txt 8976_7.txt 11225_10.txt 2225_10.txt 4476_9.txt 6726_9.txt 8977_7.txt 11226_7.txt 2226_10.txt 4477_7.txt 6727_9.txt 8978_10.txt 11227_9.txt 2227_7.txt 4478_8.txt 6728_8.txt 8979_10.txt 11228_10.txt 2228_7.txt 4479_8.txt 6729_10.txt 897_10.txt 11229_10.txt 2229_7.txt 447_10.txt 672_9.txt 8980_10.txt 1122_7.txt 222_10.txt 4480_9.txt 6730_10.txt 8981_7.txt 11230_10.txt 2230_10.txt 4481_8.txt 6731_10.txt 8982_8.txt 11231_10.txt 2231_10.txt 4482_7.txt 6732_10.txt 8983_8.txt 11232_10.txt 2232_7.txt 4483_8.txt 6733_10.txt 8984_10.txt 11233_10.txt 2233_10.txt 4484_8.txt 6734_10.txt 8985_9.txt 11234_10.txt 2234_9.txt 4485_10.txt 6735_9.txt 8986_8.txt 11235_8.txt 2235_10.txt 4486_7.txt 6736_10.txt 8987_9.txt 11236_8.txt 2236_9.txt 4487_9.txt 6737_7.txt 8988_10.txt 11237_8.txt 2237_7.txt 4488_10.txt 6738_7.txt 8989_9.txt 11238_7.txt 2238_9.txt 4489_9.txt 6739_7.txt 898_10.txt 11239_8.txt 2239_7.txt 448_10.txt 673_8.txt 8990_10.txt 1123_10.txt 223_9.txt 4490_7.txt 6740_8.txt 8991_10.txt 11240_7.txt 2240_7.txt 4491_10.txt 6741_7.txt 8992_9.txt 11241_10.txt 2241_7.txt 4492_10.txt 6742_8.txt 8993_7.txt 11242_9.txt 2242_7.txt 4493_9.txt 6743_9.txt 8994_8.txt 11243_10.txt 2243_7.txt 4494_8.txt 6744_8.txt 8995_9.txt 11244_10.txt 2244_9.txt 4495_9.txt 6745_8.txt 8996_8.txt 11245_10.txt 2245_7.txt 4496_8.txt 6746_8.txt 8997_10.txt 11246_8.txt 2246_10.txt 4497_7.txt 6747_8.txt 8998_10.txt 11247_8.txt 2247_10.txt 4498_8.txt 6748_8.txt 8999_8.txt 11248_10.txt 2248_7.txt 4499_9.txt 6749_9.txt 899_7.txt 11249_7.txt 2249_7.txt 449_10.txt 674_10.txt 89_7.txt 1124_10.txt 224_10.txt 44_8.txt 6750_8.txt 8_7.txt 11250_9.txt 2250_8.txt 4500_10.txt 6751_8.txt 9000_10.txt 11251_10.txt 2251_8.txt 4501_7.txt 6752_7.txt 9001_8.txt 11252_9.txt 2252_10.txt 4502_7.txt 6753_7.txt 9002_9.txt 11253_9.txt 2253_8.txt 4503_7.txt 6754_8.txt 9003_10.txt 11254_8.txt 2254_8.txt 4504_9.txt 6755_7.txt 9004_7.txt 11255_10.txt 2255_8.txt 4505_8.txt 6756_10.txt 9005_8.txt 11256_10.txt 2256_8.txt 4506_10.txt 6757_8.txt 9006_7.txt 11257_7.txt 2257_7.txt 4507_7.txt 6758_10.txt 9007_10.txt 11258_8.txt 2258_7.txt 4508_7.txt 6759_7.txt 9008_10.txt 11259_10.txt 2259_9.txt 4509_10.txt 675_9.txt 9009_10.txt 1125_8.txt 225_9.txt 450_10.txt 6760_9.txt 900_10.txt 11260_7.txt 2260_10.txt 4510_7.txt 6761_9.txt 9010_7.txt 11261_9.txt 2261_10.txt 4511_9.txt 6762_9.txt 9011_9.txt 11262_7.txt 2262_10.txt 4512_9.txt 6763_8.txt 9012_7.txt 11263_10.txt 2263_10.txt 4513_8.txt 6764_9.txt 9013_7.txt 11264_10.txt 2264_9.txt 4514_10.txt 6765_9.txt 9014_10.txt 11265_10.txt 2265_7.txt 4515_10.txt 6766_8.txt 9015_8.txt 11266_10.txt 2266_8.txt 4516_10.txt 6767_8.txt 9016_10.txt 11267_10.txt 2267_8.txt 4517_9.txt 6768_8.txt 9017_8.txt 11268_8.txt 2268_10.txt 4518_9.txt 6769_9.txt 9018_8.txt 11269_8.txt 2269_10.txt 4519_9.txt 676_8.txt 9019_8.txt 1126_9.txt 226_10.txt 451_10.txt 6770_10.txt 901_8.txt 11270_10.txt 2270_9.txt 4520_7.txt 6771_7.txt 9020_7.txt 11271_7.txt 2271_8.txt 4521_10.txt 6772_10.txt 9021_10.txt 11272_10.txt 2272_7.txt 4522_9.txt 6773_10.txt 9022_10.txt 11273_9.txt 2273_9.txt 4523_10.txt 6774_10.txt 9023_10.txt 11274_9.txt 2274_9.txt 4524_9.txt 6775_8.txt 9024_10.txt 11275_10.txt 2275_7.txt 4525_8.txt 6776_7.txt 9025_9.txt 11276_10.txt 2276_7.txt 4526_10.txt 6777_8.txt 9026_9.txt 11277_10.txt 2277_10.txt 4527_9.txt 6778_8.txt 9027_7.txt 11278_8.txt 2278_10.txt 4528_8.txt 6779_8.txt 9028_10.txt 11279_10.txt 2279_10.txt 4529_8.txt 677_8.txt 9029_10.txt 1127_10.txt 227_10.txt 452_10.txt 6780_10.txt 902_9.txt 11280_8.txt 2280_7.txt 4530_7.txt 6781_10.txt 9030_10.txt 11281_8.txt 2281_9.txt 4531_8.txt 6782_9.txt 9031_10.txt 11282_7.txt 2282_10.txt 4532_7.txt 6783_7.txt 9032_8.txt 11283_8.txt 2283_9.txt 4533_8.txt 6784_8.txt 9033_9.txt 11284_8.txt 2284_10.txt 4534_9.txt 6785_8.txt 9034_10.txt 11285_7.txt 2285_10.txt 4535_7.txt 6786_10.txt 9035_10.txt 11286_10.txt 2286_10.txt 4536_7.txt 6787_9.txt 9036_10.txt 11287_8.txt 2287_8.txt 4537_8.txt 6788_9.txt 9037_8.txt 11288_8.txt 2288_9.txt 4538_7.txt 6789_8.txt 9038_8.txt 11289_10.txt 2289_9.txt 4539_9.txt 678_8.txt 9039_10.txt 1128_10.txt 228_7.txt 453_10.txt 6790_10.txt 903_8.txt 11290_10.txt 2290_10.txt 4540_7.txt 6791_10.txt 9040_9.txt 11291_10.txt 2291_7.txt 4541_10.txt 6792_10.txt 9041_8.txt 11292_10.txt 2292_8.txt 4542_9.txt 6793_10.txt 9042_10.txt 11293_7.txt 2293_7.txt 4543_7.txt 6794_10.txt 9043_7.txt 11294_10.txt 2294_8.txt 4544_10.txt 6795_9.txt 9044_9.txt 11295_9.txt 2295_7.txt 4545_10.txt 6796_8.txt 9045_9.txt 11296_8.txt 2296_9.txt 4546_10.txt 6797_8.txt 9046_8.txt 11297_7.txt 2297_7.txt 4547_8.txt 6798_9.txt 9047_10.txt 11298_8.txt 2298_7.txt 4548_7.txt 6799_9.txt 9048_8.txt 11299_8.txt 2299_7.txt 4549_7.txt 679_10.txt 9049_8.txt 1129_10.txt 229_10.txt 454_8.txt 67_10.txt 904_10.txt 112_10.txt 22_8.txt 4550_10.txt 6800_9.txt 9050_8.txt 11300_8.txt 2300_10.txt 4551_10.txt 6801_9.txt 9051_10.txt 11301_7.txt 2301_10.txt 4552_9.txt 6802_9.txt 9052_8.txt 11302_8.txt 2302_9.txt 4553_10.txt 6803_8.txt 9053_10.txt 11303_9.txt 2303_8.txt 4554_8.txt 6804_7.txt 9054_9.txt 11304_7.txt 2304_9.txt 4555_9.txt 6805_10.txt 9055_10.txt 11305_8.txt 2305_10.txt 4556_8.txt 6806_7.txt 9056_8.txt 11306_8.txt 2306_10.txt 4557_8.txt 6807_10.txt 9057_7.txt 11307_10.txt 2307_9.txt 4558_9.txt 6808_8.txt 9058_7.txt 11308_9.txt 2308_10.txt 4559_10.txt 6809_7.txt 9059_7.txt 11309_9.txt 2309_9.txt 455_10.txt 680_8.txt 905_10.txt 1130_10.txt 230_9.txt 4560_10.txt 6810_7.txt 9060_8.txt 11310_10.txt 2310_9.txt 4561_10.txt 6811_10.txt 9061_7.txt 11311_10.txt 2311_10.txt 4562_9.txt 6812_7.txt 9062_7.txt 11312_9.txt 2312_9.txt 4563_8.txt 6813_10.txt 9063_8.txt 11313_10.txt 2313_10.txt 4564_10.txt 6814_7.txt 9064_7.txt 11314_8.txt 2314_7.txt 4565_8.txt 6815_8.txt 9065_10.txt 11315_10.txt 2315_10.txt 4566_9.txt 6816_7.txt 9066_8.txt 11316_8.txt 2316_10.txt 4567_10.txt 6817_10.txt 9067_8.txt 11317_9.txt 2317_10.txt 4568_10.txt 6818_10.txt 9068_9.txt 11318_9.txt 2318_10.txt 4569_7.txt 6819_8.txt 9069_7.txt 11319_8.txt 2319_8.txt 456_10.txt 681_9.txt 906_10.txt 1131_7.txt 231_10.txt 4570_10.txt 6820_7.txt 9070_7.txt 11320_10.txt 2320_10.txt 4571_9.txt 6821_10.txt 9071_8.txt 11321_8.txt 2321_9.txt 4572_10.txt 6822_7.txt 9072_9.txt 11322_9.txt 2322_10.txt 4573_10.txt 6823_10.txt 9073_10.txt 11323_10.txt 2323_8.txt 4574_10.txt 6824_10.txt 9074_8.txt 11324_10.txt 2324_8.txt 4575_8.txt 6825_10.txt 9075_10.txt 11325_10.txt 2325_8.txt 4576_10.txt 6826_9.txt 9076_8.txt 11326_10.txt 2326_8.txt 4577_10.txt 6827_10.txt 9077_9.txt 11327_10.txt 2327_10.txt 4578_9.txt 6828_10.txt 9078_7.txt 11328_10.txt 2328_10.txt 4579_9.txt 6829_8.txt 9079_10.txt 11329_9.txt 2329_10.txt 457_10.txt 682_10.txt 907_8.txt 1132_10.txt 232_10.txt 4580_8.txt 6830_10.txt 9080_7.txt 11330_8.txt 2330_8.txt 4581_10.txt 6831_9.txt 9081_8.txt 11331_8.txt 2331_10.txt 4582_7.txt 6832_10.txt 9082_10.txt 11332_8.txt 2332_10.txt 4583_7.txt 6833_10.txt 9083_8.txt 11333_9.txt 2333_10.txt 4584_10.txt 6834_10.txt 9084_7.txt 11334_10.txt 2334_9.txt 4585_10.txt 6835_10.txt 9085_8.txt 11335_9.txt 2335_10.txt 4586_7.txt 6836_9.txt 9086_7.txt 11336_7.txt 2336_10.txt 4587_10.txt 6837_10.txt 9087_7.txt 11337_10.txt 2337_10.txt 4588_9.txt 6838_7.txt 9088_8.txt 11338_10.txt 2338_10.txt 4589_10.txt 6839_10.txt 9089_9.txt 11339_10.txt 2339_8.txt 458_10.txt 683_10.txt 908_8.txt 1133_10.txt 233_7.txt 4590_10.txt 6840_10.txt 9090_9.txt 11340_7.txt 2340_10.txt 4591_9.txt 6841_7.txt 9091_10.txt 11341_10.txt 2341_8.txt 4592_9.txt 6842_7.txt 9092_7.txt 11342_9.txt 2342_8.txt 4593_10.txt 6843_9.txt 9093_10.txt 11343_8.txt 2343_10.txt 4594_10.txt 6844_10.txt 9094_10.txt 11344_7.txt 2344_9.txt 4595_10.txt 6845_10.txt 9095_10.txt 11345_7.txt 2345_9.txt 4596_10.txt 6846_10.txt 9096_10.txt 11346_10.txt 2346_9.txt 4597_8.txt 6847_10.txt 9097_10.txt 11347_10.txt 2347_10.txt 4598_9.txt 6848_7.txt 9098_10.txt 11348_10.txt 2348_10.txt 4599_10.txt 6849_7.txt 9099_10.txt 11349_10.txt 2349_9.txt 459_10.txt 684_10.txt 909_8.txt 1134_9.txt 234_10.txt 45_10.txt 6850_7.txt 90_7.txt 11350_9.txt 2350_9.txt 4600_10.txt 6851_8.txt 9100_7.txt 11351_9.txt 2351_10.txt 4601_7.txt 6852_8.txt 9101_9.txt 11352_9.txt 2352_9.txt 4602_8.txt 6853_10.txt 9102_8.txt 11353_9.txt 2353_10.txt 4603_10.txt 6854_10.txt 9103_10.txt 11354_10.txt 2354_10.txt 4604_10.txt 6855_10.txt 9104_10.txt 11355_8.txt 2355_7.txt 4605_8.txt 6856_10.txt 9105_9.txt 11356_9.txt 2356_10.txt 4606_7.txt 6857_10.txt 9106_9.txt 11357_10.txt 2357_10.txt 4607_10.txt 6858_7.txt 9107_7.txt 11358_9.txt 2358_9.txt 4608_9.txt 6859_7.txt 9108_10.txt 11359_9.txt 2359_10.txt 4609_8.txt 685_8.txt 9109_7.txt 1135_9.txt 235_10.txt 460_9.txt 6860_8.txt 910_10.txt 11360_7.txt 2360_10.txt 4610_10.txt 6861_8.txt 9110_8.txt 11361_10.txt 2361_10.txt 4611_10.txt 6862_10.txt 9111_10.txt 11362_9.txt 2362_9.txt 4612_10.txt 6863_7.txt 9112_9.txt 11363_8.txt 2363_7.txt 4613_10.txt 6864_7.txt 9113_8.txt 11364_10.txt 2364_10.txt 4614_7.txt 6865_7.txt 9114_10.txt 11365_9.txt 2365_10.txt 4615_10.txt 6866_7.txt 9115_10.txt 11366_8.txt 2366_7.txt 4616_10.txt 6867_9.txt 9116_9.txt 11367_10.txt 2367_8.txt 4617_10.txt 6868_10.txt 9117_10.txt 11368_10.txt 2368_8.txt 4618_10.txt 6869_8.txt 9118_10.txt 11369_8.txt 2369_10.txt 4619_7.txt 686_9.txt 9119_10.txt 1136_8.txt 236_9.txt 461_8.txt 6870_8.txt 911_10.txt 11370_9.txt 2370_10.txt 4620_10.txt 6871_8.txt 9120_10.txt 11371_10.txt 2371_8.txt 4621_9.txt 6872_7.txt 9121_10.txt 11372_7.txt 2372_8.txt 4622_8.txt 6873_10.txt 9122_7.txt 11373_8.txt 2373_7.txt 4623_7.txt 6874_8.txt 9123_10.txt 11374_9.txt 2374_9.txt 4624_8.txt 6875_10.txt 9124_10.txt 11375_9.txt 2375_8.txt 4625_10.txt 6876_8.txt 9125_10.txt 11376_10.txt 2376_7.txt 4626_8.txt 6877_7.txt 9126_9.txt 11377_9.txt 2377_10.txt 4627_7.txt 6878_8.txt 9127_7.txt 11378_8.txt 2378_8.txt 4628_9.txt 6879_10.txt 9128_9.txt 11379_7.txt 2379_8.txt 4629_10.txt 687_9.txt 9129_8.txt 1137_8.txt 237_10.txt 462_8.txt 6880_7.txt 912_8.txt 11380_7.txt 2380_9.txt 4630_9.txt 6881_9.txt 9130_9.txt 11381_10.txt 2381_9.txt 4631_10.txt 6882_9.txt 9131_9.txt 11382_8.txt 2382_7.txt 4632_10.txt 6883_9.txt 9132_10.txt 11383_7.txt 2383_9.txt 4633_9.txt 6884_10.txt 9133_9.txt 11384_7.txt 2384_10.txt 4634_10.txt 6885_9.txt 9134_7.txt 11385_8.txt 2385_9.txt 4635_7.txt 6886_8.txt 9135_9.txt 11386_8.txt 2386_8.txt 4636_8.txt 6887_8.txt 9136_8.txt 11387_8.txt 2387_10.txt 4637_8.txt 6888_9.txt 9137_10.txt 11388_8.txt 2388_10.txt 4638_10.txt 6889_10.txt 9138_10.txt 11389_7.txt 2389_10.txt 4639_10.txt 688_9.txt 9139_8.txt 1138_10.txt 238_10.txt 463_7.txt 6890_8.txt 913_10.txt 11390_10.txt 2390_7.txt 4640_10.txt 6891_9.txt 9140_10.txt 11391_8.txt 2391_10.txt 4641_9.txt 6892_8.txt 9141_8.txt 11392_8.txt 2392_8.txt 4642_10.txt 6893_10.txt 9142_10.txt 11393_7.txt 2393_8.txt 4643_10.txt 6894_7.txt 9143_10.txt 11394_10.txt 2394_7.txt 4644_10.txt 6895_10.txt 9144_7.txt 11395_8.txt 2395_7.txt 4645_9.txt 6896_7.txt 9145_10.txt 11396_10.txt 2396_7.txt 4646_7.txt 6897_10.txt 9146_9.txt 11397_10.txt 2397_9.txt 4647_8.txt 6898_9.txt 9147_10.txt 11398_7.txt 2398_9.txt 4648_9.txt 6899_9.txt 9148_7.txt 11399_8.txt 2399_7.txt 4649_10.txt 689_10.txt 9149_9.txt 1139_8.txt 239_7.txt 464_10.txt 68_10.txt 914_8.txt 113_10.txt 23_7.txt 4650_7.txt 6900_8.txt 9150_8.txt 11400_10.txt 2400_7.txt 4651_7.txt 6901_10.txt 9151_10.txt 11401_9.txt 2401_8.txt 4652_9.txt 6902_7.txt 9152_8.txt 11402_10.txt 2402_7.txt 4653_10.txt 6903_9.txt 9153_8.txt 11403_9.txt 2403_10.txt 4654_7.txt 6904_8.txt 9154_7.txt 11404_10.txt 2404_8.txt 4655_7.txt 6905_7.txt 9155_10.txt 11405_8.txt 2405_10.txt 4656_8.txt 6906_8.txt 9156_10.txt 11406_9.txt 2406_10.txt 4657_10.txt 6907_8.txt 9157_10.txt 11407_7.txt 2407_10.txt 4658_8.txt 6908_7.txt 9158_10.txt 11408_7.txt 2408_8.txt 4659_8.txt 6909_9.txt 9159_10.txt 11409_9.txt 2409_9.txt 465_10.txt 690_9.txt 915_10.txt 1140_10.txt 240_10.txt 4660_7.txt 6910_9.txt 9160_8.txt 11410_7.txt 2410_9.txt 4661_10.txt 6911_10.txt 9161_10.txt 11411_8.txt 2411_9.txt 4662_8.txt 6912_10.txt 9162_10.txt 11412_10.txt 2412_10.txt 4663_8.txt 6913_9.txt 9163_10.txt 11413_10.txt 2413_10.txt 4664_8.txt 6914_8.txt 9164_10.txt 11414_9.txt 2414_10.txt 4665_10.txt 6915_9.txt 9165_10.txt 11415_8.txt 2415_10.txt 4666_8.txt 6916_8.txt 9166_9.txt 11416_10.txt 2416_10.txt 4667_10.txt 6917_9.txt 9167_7.txt 11417_7.txt 2417_9.txt 4668_10.txt 6918_10.txt 9168_7.txt 11418_7.txt 2418_8.txt 4669_10.txt 6919_9.txt 9169_8.txt 11419_10.txt 2419_10.txt 466_8.txt 691_9.txt 916_10.txt 1141_10.txt 241_8.txt 4670_9.txt 6920_8.txt 9170_9.txt 11420_9.txt 2420_10.txt 4671_10.txt 6921_9.txt 9171_9.txt 11421_10.txt 2421_9.txt 4672_10.txt 6922_8.txt 9172_10.txt 11422_8.txt 2422_9.txt 4673_9.txt 6923_10.txt 9173_10.txt 11423_8.txt 2423_7.txt 4674_7.txt 6924_7.txt 9174_9.txt 11424_8.txt 2424_7.txt 4675_9.txt 6925_9.txt 9175_10.txt 11425_8.txt 2425_7.txt 4676_9.txt 6926_10.txt 9176_10.txt 11426_10.txt 2426_9.txt 4677_9.txt 6927_10.txt 9177_10.txt 11427_10.txt 2427_10.txt 4678_10.txt 6928_9.txt 9178_10.txt 11428_7.txt 2428_8.txt 4679_10.txt 6929_8.txt 9179_10.txt 11429_7.txt 2429_8.txt 467_7.txt 692_8.txt 917_10.txt 1142_10.txt 242_8.txt 4680_10.txt 6930_8.txt 9180_9.txt 11430_8.txt 2430_10.txt 4681_7.txt 6931_8.txt 9181_10.txt 11431_10.txt 2431_8.txt 4682_9.txt 6932_7.txt 9182_10.txt 11432_9.txt 2432_7.txt 4683_7.txt 6933_9.txt 9183_10.txt 11433_8.txt 2433_8.txt 4684_8.txt 6934_9.txt 9184_10.txt 11434_7.txt 2434_8.txt 4685_7.txt 6935_8.txt 9185_9.txt 11435_10.txt 2435_8.txt 4686_10.txt 6936_7.txt 9186_8.txt 11436_9.txt 2436_10.txt 4687_8.txt 6937_8.txt 9187_10.txt 11437_9.txt 2437_10.txt 4688_10.txt 6938_9.txt 9188_10.txt 11438_10.txt 2438_9.txt 4689_10.txt 6939_9.txt 9189_10.txt 11439_8.txt 2439_8.txt 468_7.txt 693_10.txt 918_10.txt 1143_7.txt 243_10.txt 4690_9.txt 6940_10.txt 9190_7.txt 11440_10.txt 2440_10.txt 4691_10.txt 6941_9.txt 9191_10.txt 11441_10.txt 2441_10.txt 4692_10.txt 6942_10.txt 9192_7.txt 11442_10.txt 2442_10.txt 4693_7.txt 6943_9.txt 9193_10.txt 11443_10.txt 2443_9.txt 4694_10.txt 6944_9.txt 9194_9.txt 11444_10.txt 2444_8.txt 4695_9.txt 6945_10.txt 9195_10.txt 11445_7.txt 2445_10.txt 4696_10.txt 6946_10.txt 9196_10.txt 11446_10.txt 2446_10.txt 4697_7.txt 6947_10.txt 9197_7.txt 11447_7.txt 2447_10.txt 4698_10.txt 6948_10.txt 9198_8.txt 11448_10.txt 2448_8.txt 4699_8.txt 6949_10.txt 9199_8.txt 11449_10.txt 2449_7.txt 469_7.txt 694_9.txt 919_8.txt 1144_8.txt 244_10.txt 46_9.txt 6950_8.txt 91_8.txt 11450_10.txt 2450_9.txt 4700_9.txt 6951_10.txt 9200_8.txt 11451_8.txt 2451_8.txt 4701_10.txt 6952_7.txt 9201_8.txt 11452_8.txt 2452_10.txt 4702_8.txt 6953_7.txt 9202_8.txt 11453_9.txt 2453_8.txt 4703_9.txt 6954_8.txt 9203_9.txt 11454_10.txt 2454_10.txt 4704_8.txt 6955_10.txt 9204_10.txt 11455_8.txt 2455_8.txt 4705_7.txt 6956_8.txt 9205_7.txt 11456_10.txt 2456_8.txt 4706_8.txt 6957_7.txt 9206_8.txt 11457_10.txt 2457_8.txt 4707_7.txt 6958_7.txt 9207_10.txt 11458_7.txt 2458_8.txt 4708_7.txt 6959_7.txt 9208_7.txt 11459_10.txt 2459_8.txt 4709_9.txt 695_10.txt 9209_7.txt 1145_8.txt 245_9.txt 470_10.txt 6960_7.txt 920_10.txt 11460_10.txt 2460_10.txt 4710_7.txt 6961_7.txt 9210_10.txt 11461_10.txt 2461_10.txt 4711_8.txt 6962_7.txt 9211_9.txt 11462_10.txt 2462_10.txt 4712_7.txt 6963_8.txt 9212_9.txt 11463_9.txt 2463_10.txt 4713_8.txt 6964_7.txt 9213_7.txt 11464_10.txt 2464_10.txt 4714_10.txt 6965_7.txt 9214_7.txt 11465_7.txt 2465_10.txt 4715_9.txt 6966_8.txt 9215_7.txt 11466_10.txt 2466_9.txt 4716_9.txt 6967_7.txt 9216_10.txt 11467_10.txt 2467_10.txt 4717_8.txt 6968_7.txt 9217_7.txt 11468_9.txt 2468_10.txt 4718_7.txt 6969_10.txt 9218_10.txt 11469_10.txt 2469_10.txt 4719_10.txt 696_10.txt 9219_10.txt 1146_7.txt 246_7.txt 471_7.txt 6970_8.txt 921_7.txt 11470_10.txt 2470_10.txt 4720_8.txt 6971_10.txt 9220_8.txt 11471_7.txt 2471_10.txt 4721_8.txt 6972_9.txt 9221_7.txt 11472_7.txt 2472_10.txt 4722_8.txt 6973_10.txt 9222_7.txt 11473_10.txt 2473_10.txt 4723_8.txt 6974_10.txt 9223_9.txt 11474_10.txt 2474_10.txt 4724_7.txt 6975_8.txt 9224_10.txt 11475_8.txt 2475_10.txt 4725_9.txt 6976_9.txt 9225_10.txt 11476_10.txt 2476_10.txt 4726_7.txt 6977_9.txt 9226_8.txt 11477_8.txt 2477_7.txt 4727_7.txt 6978_8.txt 9227_9.txt 11478_8.txt 2478_10.txt 4728_8.txt 6979_10.txt 9228_10.txt 11479_10.txt 2479_10.txt 4729_10.txt 697_8.txt 9229_7.txt 1147_8.txt 247_10.txt 472_10.txt 6980_9.txt 922_8.txt 11480_10.txt 2480_8.txt 4730_10.txt 6981_9.txt 9230_8.txt 11481_10.txt 2481_8.txt 4731_8.txt 6982_9.txt 9231_8.txt 11482_9.txt 2482_10.txt 4732_10.txt 6983_8.txt 9232_8.txt 11483_10.txt 2483_7.txt 4733_9.txt 6984_8.txt 9233_8.txt 11484_9.txt 2484_10.txt 4734_9.txt 6985_10.txt 9234_8.txt 11485_10.txt 2485_10.txt 4735_9.txt 6986_8.txt 9235_9.txt 11486_8.txt 2486_10.txt 4736_10.txt 6987_10.txt 9236_10.txt 11487_10.txt 2487_10.txt 4737_7.txt 6988_10.txt 9237_7.txt 11488_10.txt 2488_10.txt 4738_7.txt 6989_7.txt 9238_10.txt 11489_10.txt 2489_7.txt 4739_7.txt 698_7.txt 9239_8.txt 1148_8.txt 248_10.txt 473_9.txt 6990_8.txt 923_9.txt 11490_9.txt 2490_8.txt 4740_8.txt 6991_9.txt 9240_10.txt 11491_8.txt 2491_8.txt 4741_7.txt 6992_10.txt 9241_10.txt 11492_7.txt 2492_10.txt 4742_9.txt 6993_7.txt 9242_10.txt 11493_7.txt 2493_10.txt 4743_8.txt 6994_8.txt 9243_8.txt 11494_7.txt 2494_10.txt 4744_9.txt 6995_7.txt 9244_8.txt 11495_10.txt 2495_9.txt 4745_7.txt 6996_8.txt 9245_8.txt 11496_10.txt 2496_8.txt 4746_8.txt 6997_7.txt 9246_8.txt 11497_10.txt 2497_8.txt 4747_8.txt 6998_7.txt 9247_10.txt 11498_7.txt 2498_7.txt 4748_7.txt 6999_7.txt 9248_9.txt 11499_10.txt 2499_10.txt 4749_8.txt 699_8.txt 9249_9.txt 1149_8.txt 249_10.txt 474_7.txt 69_10.txt 924_7.txt 114_10.txt 24_8.txt 4750_7.txt 6_10.txt 9250_8.txt 11500_10.txt 2500_9.txt 4751_8.txt 7000_7.txt 9251_9.txt 11501_8.txt 2501_8.txt 4752_7.txt 7001_10.txt 9252_10.txt 11502_10.txt 2502_8.txt 4753_10.txt 7002_10.txt 9253_10.txt 11503_10.txt 2503_10.txt 4754_10.txt 7003_10.txt 9254_7.txt 11504_8.txt 2504_10.txt 4755_7.txt 7004_10.txt 9255_10.txt 11505_7.txt 2505_9.txt 4756_10.txt 7005_9.txt 9256_10.txt 11506_8.txt 2506_9.txt 4757_8.txt 7006_8.txt 9257_7.txt 11507_10.txt 2507_7.txt 4758_8.txt 7007_8.txt 9258_10.txt 11508_8.txt 2508_10.txt 4759_7.txt 7008_8.txt 9259_10.txt 11509_8.txt 2509_9.txt 475_10.txt 7009_10.txt 925_10.txt 1150_10.txt 250_7.txt 4760_7.txt 700_8.txt 9260_7.txt 11510_10.txt 2510_10.txt 4761_10.txt 7010_10.txt 9261_7.txt 11511_9.txt 2511_10.txt 4762_10.txt 7011_7.txt 9262_8.txt 11512_10.txt 2512_10.txt 4763_8.txt 7012_10.txt 9263_8.txt 11513_8.txt 2513_9.txt 4764_10.txt 7013_9.txt 9264_9.txt 11514_7.txt 2514_8.txt 4765_7.txt 7014_10.txt 9265_9.txt 11515_10.txt 2515_10.txt 4766_10.txt 7015_8.txt 9266_9.txt 11516_8.txt 2516_9.txt 4767_9.txt 7016_7.txt 9267_7.txt 11517_9.txt 2517_9.txt 4768_10.txt 7017_9.txt 9268_9.txt 11518_8.txt 2518_10.txt 4769_7.txt 7018_10.txt 9269_7.txt 11519_9.txt 2519_10.txt 476_7.txt 7019_7.txt 926_7.txt 1151_9.txt 251_10.txt 4770_10.txt 701_7.txt 9270_9.txt 11520_8.txt 2520_10.txt 4771_10.txt 7020_10.txt 9271_8.txt 11521_10.txt 2521_10.txt 4772_10.txt 7021_10.txt 9272_7.txt 11522_8.txt 2522_8.txt 4773_10.txt 7022_7.txt 9273_7.txt 11523_7.txt 2523_9.txt 4774_10.txt 7023_9.txt 9274_8.txt 11524_7.txt 2524_10.txt 4775_7.txt 7024_8.txt 9275_9.txt 11525_8.txt 2525_8.txt 4776_8.txt 7025_8.txt 9276_10.txt 11526_10.txt 2526_9.txt 4777_10.txt 7026_7.txt 9277_9.txt 11527_7.txt 2527_9.txt 4778_9.txt 7027_7.txt 9278_7.txt 11528_7.txt 2528_10.txt 4779_10.txt 7028_9.txt 9279_7.txt 11529_7.txt 2529_7.txt 477_10.txt 7029_7.txt 927_10.txt 1152_10.txt 252_9.txt 4780_9.txt 702_10.txt 9280_9.txt 11530_8.txt 2530_8.txt 4781_8.txt 7030_10.txt 9281_10.txt 11531_10.txt 2531_7.txt 4782_9.txt 7031_10.txt 9282_8.txt 11532_8.txt 2532_8.txt 4783_10.txt 7032_10.txt 9283_8.txt 11533_8.txt 2533_7.txt 4784_10.txt 7033_7.txt 9284_8.txt 11534_7.txt 2534_8.txt 4785_10.txt 7034_10.txt 9285_10.txt 11535_7.txt 2535_7.txt 4786_10.txt 7035_8.txt 9286_9.txt 11536_7.txt 2536_7.txt 4787_10.txt 7036_10.txt 9287_9.txt 11537_7.txt 2537_7.txt 4788_10.txt 7037_10.txt 9288_8.txt 11538_7.txt 2538_10.txt 4789_10.txt 7038_9.txt 9289_7.txt 11539_9.txt 2539_10.txt 478_7.txt 7039_7.txt 928_10.txt 1153_10.txt 253_7.txt 4790_10.txt 703_10.txt 9290_9.txt 11540_10.txt 2540_8.txt 4791_10.txt 7040_10.txt 9291_7.txt 11541_10.txt 2541_7.txt 4792_10.txt 7041_7.txt 9292_10.txt 11542_7.txt 2542_10.txt 4793_7.txt 7042_10.txt 9293_7.txt 11543_8.txt 2543_10.txt 4794_8.txt 7043_7.txt 9294_7.txt 11544_10.txt 2544_8.txt 4795_9.txt 7044_10.txt 9295_10.txt 11545_8.txt 2545_8.txt 4796_8.txt 7045_10.txt 9296_10.txt 11546_9.txt 2546_10.txt 4797_10.txt 7046_7.txt 9297_10.txt 11547_9.txt 2547_10.txt 4798_8.txt 7047_10.txt 9298_9.txt 11548_10.txt 2548_7.txt 4799_10.txt 7048_10.txt 9299_8.txt 11549_7.txt 2549_8.txt 479_10.txt 7049_7.txt 929_10.txt 1154_10.txt 254_8.txt 47_8.txt 704_10.txt 92_9.txt 11550_7.txt 2550_9.txt 4800_7.txt 7050_7.txt 9300_10.txt 11551_7.txt 2551_9.txt 4801_7.txt 7051_10.txt 9301_10.txt 11552_9.txt 2552_10.txt 4802_8.txt 7052_8.txt 9302_10.txt 11553_9.txt 2553_7.txt 4803_8.txt 7053_7.txt 9303_8.txt 11554_8.txt 2554_7.txt 4804_7.txt 7054_9.txt 9304_9.txt 11555_10.txt 2555_10.txt 4805_8.txt 7055_8.txt 9305_10.txt 11556_7.txt 2556_8.txt 4806_8.txt 7056_7.txt 9306_9.txt 11557_8.txt 2557_10.txt 4807_10.txt 7057_9.txt 9307_10.txt 11558_10.txt 2558_10.txt 4808_8.txt 7058_8.txt 9308_7.txt 11559_10.txt 2559_9.txt 4809_10.txt 7059_10.txt 9309_9.txt 1155_10.txt 255_10.txt 480_10.txt 705_10.txt 930_7.txt 11560_9.txt 2560_7.txt 4810_9.txt 7060_10.txt 9310_10.txt 11561_10.txt 2561_7.txt 4811_8.txt 7061_7.txt 9311_10.txt 11562_9.txt 2562_10.txt 4812_8.txt 7062_7.txt 9312_10.txt 11563_10.txt 2563_10.txt 4813_9.txt 7063_8.txt 9313_9.txt 11564_9.txt 2564_10.txt 4814_10.txt 7064_8.txt 9314_10.txt 11565_9.txt 2565_9.txt 4815_10.txt 7065_7.txt 9315_7.txt 11566_10.txt 2566_8.txt 4816_8.txt 7066_8.txt 9316_7.txt 11567_8.txt 2567_9.txt 4817_10.txt 7067_7.txt 9317_8.txt 11568_7.txt 2568_10.txt 4818_9.txt 7068_8.txt 9318_9.txt 11569_8.txt 2569_10.txt 4819_7.txt 7069_8.txt 9319_8.txt 1156_9.txt 256_9.txt 481_10.txt 706_10.txt 931_10.txt 11570_9.txt 2570_10.txt 4820_7.txt 7070_9.txt 9320_10.txt 11571_9.txt 2571_7.txt 4821_10.txt 7071_9.txt 9321_8.txt 11572_8.txt 2572_10.txt 4822_9.txt 7072_9.txt 9322_8.txt 11573_10.txt 2573_10.txt 4823_7.txt 7073_10.txt 9323_10.txt 11574_10.txt 2574_7.txt 4824_7.txt 7074_7.txt 9324_7.txt 11575_10.txt 2575_7.txt 4825_8.txt 7075_10.txt 9325_10.txt 11576_7.txt 2576_8.txt 4826_7.txt 7076_10.txt 9326_10.txt 11577_8.txt 2577_10.txt 4827_9.txt 7077_10.txt 9327_8.txt 11578_10.txt 2578_10.txt 4828_7.txt 7078_9.txt 9328_7.txt 11579_10.txt 2579_8.txt 4829_8.txt 7079_9.txt 9329_8.txt 1157_10.txt 257_7.txt 482_8.txt 707_8.txt 932_10.txt 11580_10.txt 2580_10.txt 4830_9.txt 7080_10.txt 9330_8.txt 11581_10.txt 2581_8.txt 4831_8.txt 7081_8.txt 9331_9.txt 11582_10.txt 2582_8.txt 4832_8.txt 7082_8.txt 9332_10.txt 11583_10.txt 2583_10.txt 4833_10.txt 7083_10.txt 9333_8.txt 11584_8.txt 2584_7.txt 4834_8.txt 7084_10.txt 9334_9.txt 11585_10.txt 2585_7.txt 4835_10.txt 7085_9.txt 9335_10.txt 11586_9.txt 2586_10.txt 4836_8.txt 7086_10.txt 9336_9.txt 11587_10.txt 2587_9.txt 4837_10.txt 7087_9.txt 9337_10.txt 11588_10.txt 2588_7.txt 4838_10.txt 7088_10.txt 9338_8.txt 11589_10.txt 2589_7.txt 4839_10.txt 7089_9.txt 9339_10.txt 1158_9.txt 258_7.txt 483_8.txt 708_8.txt 933_10.txt 11590_10.txt 2590_7.txt 4840_7.txt 7090_10.txt 9340_8.txt 11591_10.txt 2591_7.txt 4841_7.txt 7091_9.txt 9341_7.txt 11592_10.txt 2592_10.txt 4842_10.txt 7092_8.txt 9342_8.txt 11593_10.txt 2593_10.txt 4843_7.txt 7093_10.txt 9343_8.txt 11594_10.txt 2594_9.txt 4844_8.txt 7094_10.txt 9344_8.txt 11595_10.txt 2595_9.txt 4845_9.txt 7095_9.txt 9345_9.txt 11596_10.txt 2596_10.txt 4846_7.txt 7096_10.txt 9346_10.txt 11597_10.txt 2597_8.txt 4847_9.txt 7097_9.txt 9347_7.txt 11598_8.txt 2598_10.txt 4848_7.txt 7098_9.txt 9348_7.txt 11599_8.txt 2599_10.txt 4849_10.txt 7099_10.txt 9349_8.txt 1159_10.txt 259_8.txt 484_8.txt 709_10.txt 934_9.txt 115_10.txt 25_7.txt 4850_10.txt 70_9.txt 9350_10.txt 11600_7.txt 2600_10.txt 4851_7.txt 7100_8.txt 9351_8.txt 11601_8.txt 2601_10.txt 4852_8.txt 7101_10.txt 9352_10.txt 11602_8.txt 2602_10.txt 4853_10.txt 7102_7.txt 9353_8.txt 11603_10.txt 2603_8.txt 4854_7.txt 7103_7.txt 9354_9.txt 11604_8.txt 2604_8.txt 4855_7.txt 7104_7.txt 9355_7.txt 11605_10.txt 2605_7.txt 4856_10.txt 7105_9.txt 9356_9.txt 11606_10.txt 2606_8.txt 4857_7.txt 7106_8.txt 9357_7.txt 11607_10.txt 2607_9.txt 4858_8.txt 7107_10.txt 9358_8.txt 11608_10.txt 2608_10.txt 4859_7.txt 7108_9.txt 9359_8.txt 11609_10.txt 2609_10.txt 485_8.txt 7109_10.txt 935_9.txt 1160_10.txt 260_7.txt 4860_10.txt 710_9.txt 9360_10.txt 11610_10.txt 2610_9.txt 4861_8.txt 7110_8.txt 9361_9.txt 11611_9.txt 2611_9.txt 4862_7.txt 7111_8.txt 9362_7.txt 11612_10.txt 2612_10.txt 4863_10.txt 7112_9.txt 9363_10.txt 11613_7.txt 2613_9.txt 4864_7.txt 7113_8.txt 9364_9.txt 11614_7.txt 2614_9.txt 4865_7.txt 7114_9.txt 9365_9.txt 11615_9.txt 2615_9.txt 4866_7.txt 7115_9.txt 9366_9.txt 11616_8.txt 2616_10.txt 4867_9.txt 7116_9.txt 9367_9.txt 11617_9.txt 2617_10.txt 4868_8.txt 7117_10.txt 9368_8.txt 11618_8.txt 2618_7.txt 4869_10.txt 7118_10.txt 9369_10.txt 11619_9.txt 2619_8.txt 486_9.txt 7119_10.txt 936_8.txt 1161_8.txt 261_8.txt 4870_10.txt 711_10.txt 9370_7.txt 11620_8.txt 2620_10.txt 4871_8.txt 7120_9.txt 9371_7.txt 11621_9.txt 2621_8.txt 4872_8.txt 7121_10.txt 9372_9.txt 11622_10.txt 2622_8.txt 4873_8.txt 7122_8.txt 9373_9.txt 11623_8.txt 2623_7.txt 4874_10.txt 7123_10.txt 9374_9.txt 11624_8.txt 2624_7.txt 4875_7.txt 7124_8.txt 9375_10.txt 11625_7.txt 2625_8.txt 4876_10.txt 7125_10.txt 9376_10.txt 11626_9.txt 2626_8.txt 4877_9.txt 7126_10.txt 9377_7.txt 11627_8.txt 2627_9.txt 4878_10.txt 7127_10.txt 9378_9.txt 11628_10.txt 2628_10.txt 4879_10.txt 7128_7.txt 9379_7.txt 11629_10.txt 2629_9.txt 487_8.txt 7129_10.txt 937_10.txt 1162_9.txt 262_8.txt 4880_9.txt 712_9.txt 9380_9.txt 11630_8.txt 2630_8.txt 4881_9.txt 7130_10.txt 9381_8.txt 11631_7.txt 2631_7.txt 4882_9.txt 7131_9.txt 9382_9.txt 11632_7.txt 2632_7.txt 4883_10.txt 7132_10.txt 9383_8.txt 11633_8.txt 2633_8.txt 4884_10.txt 7133_10.txt 9384_8.txt 11634_9.txt 2634_7.txt 4885_9.txt 7134_10.txt 9385_8.txt 11635_8.txt 2635_7.txt 4886_8.txt 7135_10.txt 9386_8.txt 11636_8.txt 2636_9.txt 4887_10.txt 7136_10.txt 9387_9.txt 11637_8.txt 2637_7.txt 4888_8.txt 7137_8.txt 9388_8.txt 11638_7.txt 2638_10.txt 4889_10.txt 7138_10.txt 9389_7.txt 11639_10.txt 2639_7.txt 488_9.txt 7139_10.txt 938_9.txt 1163_7.txt 263_9.txt 4890_10.txt 713_10.txt 9390_9.txt 11640_9.txt 2640_10.txt 4891_10.txt 7140_10.txt 9391_8.txt 11641_7.txt 2641_10.txt 4892_10.txt 7141_8.txt 9392_9.txt 11642_10.txt 2642_10.txt 4893_10.txt 7142_8.txt 9393_10.txt 11643_7.txt 2643_10.txt 4894_10.txt 7143_10.txt 9394_10.txt 11644_10.txt 2644_8.txt 4895_10.txt 7144_10.txt 9395_9.txt 11645_8.txt 2645_8.txt 4896_7.txt 7145_9.txt 9396_9.txt 11646_10.txt 2646_10.txt 4897_8.txt 7146_10.txt 9397_9.txt 11647_8.txt 2647_10.txt 4898_7.txt 7147_10.txt 9398_9.txt 11648_9.txt 2648_8.txt 4899_10.txt 7148_10.txt 9399_7.txt 11649_10.txt 2649_7.txt 489_7.txt 7149_10.txt 939_10.txt 1164_10.txt 264_7.txt 48_7.txt 714_10.txt 93_10.txt 11650_9.txt 2650_10.txt 4900_8.txt 7150_9.txt 9400_8.txt 11651_10.txt 2651_10.txt 4901_9.txt 7151_10.txt 9401_10.txt 11652_10.txt 2652_10.txt 4902_8.txt 7152_9.txt 9402_7.txt 11653_10.txt 2653_10.txt 4903_10.txt 7153_8.txt 9403_8.txt 11654_9.txt 2654_10.txt 4904_10.txt 7154_10.txt 9404_10.txt 11655_10.txt 2655_10.txt 4905_10.txt 7155_8.txt 9405_10.txt 11656_9.txt 2656_10.txt 4906_9.txt 7156_10.txt 9406_8.txt 11657_9.txt 2657_10.txt 4907_8.txt 7157_7.txt 9407_8.txt 11658_10.txt 2658_10.txt 4908_10.txt 7158_8.txt 9408_10.txt 11659_9.txt 2659_8.txt 4909_8.txt 7159_10.txt 9409_8.txt 1165_7.txt 265_7.txt 490_9.txt 715_10.txt 940_10.txt 11660_7.txt 2660_10.txt 4910_8.txt 7160_8.txt 9410_7.txt 11661_7.txt 2661_10.txt 4911_9.txt 7161_9.txt 9411_8.txt 11662_9.txt 2662_10.txt 4912_8.txt 7162_10.txt 9412_10.txt 11663_8.txt 2663_10.txt 4913_9.txt 7163_10.txt 9413_9.txt 11664_9.txt 2664_9.txt 4914_7.txt 7164_10.txt 9414_9.txt 11665_8.txt 2665_10.txt 4915_7.txt 7165_10.txt 9415_8.txt 11666_10.txt 2666_10.txt 4916_9.txt 7166_9.txt 9416_10.txt 11667_9.txt 2667_8.txt 4917_8.txt 7167_9.txt 9417_10.txt 11668_7.txt 2668_9.txt 4918_7.txt 7168_10.txt 9418_10.txt 11669_9.txt 2669_10.txt 4919_7.txt 7169_8.txt 9419_8.txt 1166_8.txt 266_7.txt 491_7.txt 716_10.txt 941_10.txt 11670_7.txt 2670_10.txt 4920_7.txt 7170_9.txt 9420_8.txt 11671_8.txt 2671_7.txt 4921_8.txt 7171_10.txt 9421_10.txt 11672_10.txt 2672_7.txt 4922_7.txt 7172_10.txt 9422_9.txt 11673_8.txt 2673_8.txt 4923_7.txt 7173_10.txt 9423_8.txt 11674_10.txt 2674_7.txt 4924_7.txt 7174_9.txt 9424_10.txt 11675_10.txt 2675_9.txt 4925_7.txt 7175_10.txt 9425_10.txt 11676_8.txt 2676_10.txt 4926_8.txt 7176_10.txt 9426_10.txt 11677_8.txt 2677_9.txt 4927_8.txt 7177_8.txt 9427_10.txt 11678_8.txt 2678_8.txt 4928_7.txt 7178_10.txt 9428_10.txt 11679_7.txt 2679_8.txt 4929_7.txt 7179_8.txt 9429_9.txt 1167_7.txt 267_7.txt 492_7.txt 717_7.txt 942_10.txt 11680_8.txt 2680_7.txt 4930_7.txt 7180_9.txt 9430_9.txt 11681_9.txt 2681_7.txt 4931_8.txt 7181_10.txt 9431_10.txt 11682_7.txt 2682_9.txt 4932_8.txt 7182_10.txt 9432_9.txt 11683_8.txt 2683_10.txt 4933_8.txt 7183_7.txt 9433_10.txt 11684_8.txt 2684_10.txt 4934_8.txt 7184_7.txt 9434_10.txt 11685_8.txt 2685_10.txt 4935_7.txt 7185_10.txt 9435_7.txt 11686_10.txt 2686_9.txt 4936_8.txt 7186_7.txt 9436_10.txt 11687_10.txt 2687_10.txt 4937_7.txt 7187_8.txt 9437_9.txt 11688_10.txt 2688_8.txt 4938_7.txt 7188_8.txt 9438_10.txt 11689_8.txt 2689_9.txt 4939_7.txt 7189_10.txt 9439_8.txt 1168_8.txt 268_8.txt 493_10.txt 718_10.txt 943_10.txt 11690_9.txt 2690_9.txt 4940_7.txt 7190_9.txt 9440_8.txt 11691_8.txt 2691_8.txt 4941_10.txt 7191_9.txt 9441_9.txt 11692_7.txt 2692_8.txt 4942_7.txt 7192_8.txt 9442_10.txt 11693_7.txt 2693_7.txt 4943_10.txt 7193_10.txt 9443_9.txt 11694_7.txt 2694_10.txt 4944_7.txt 7194_7.txt 9444_10.txt 11695_9.txt 2695_10.txt 4945_7.txt 7195_10.txt 9445_10.txt 11696_7.txt 2696_8.txt 4946_10.txt 7196_8.txt 9446_10.txt 11697_10.txt 2697_9.txt 4947_8.txt 7197_7.txt 9447_8.txt 11698_10.txt 2698_8.txt 4948_7.txt 7198_9.txt 9448_7.txt 11699_10.txt 2699_10.txt 4949_8.txt 7199_8.txt 9449_10.txt 1169_8.txt 269_8.txt 494_9.txt 719_10.txt 944_10.txt 116_10.txt 26_9.txt 4950_8.txt 71_10.txt 9450_10.txt 11700_10.txt 2700_10.txt 4951_8.txt 7200_9.txt 9451_8.txt 11701_9.txt 2701_9.txt 4952_7.txt 7201_8.txt 9452_9.txt 11702_10.txt 2702_9.txt 4953_8.txt 7202_8.txt 9453_8.txt 11703_9.txt 2703_9.txt 4954_10.txt 7203_9.txt 9454_10.txt 11704_10.txt 2704_10.txt 4955_9.txt 7204_8.txt 9455_9.txt 11705_7.txt 2705_10.txt 4956_10.txt 7205_10.txt 9456_8.txt 11706_10.txt 2706_9.txt 4957_8.txt 7206_9.txt 9457_9.txt 11707_10.txt 2707_10.txt 4958_10.txt 7207_7.txt 9458_10.txt 11708_8.txt 2708_7.txt 4959_7.txt 7208_7.txt 9459_8.txt 11709_10.txt 2709_8.txt 495_7.txt 7209_8.txt 945_10.txt 1170_8.txt 270_10.txt 4960_9.txt 720_7.txt 9460_8.txt 11710_10.txt 2710_7.txt 4961_10.txt 7210_8.txt 9461_7.txt 11711_10.txt 2711_7.txt 4962_7.txt 7211_10.txt 9462_9.txt 11712_10.txt 2712_10.txt 4963_10.txt 7212_9.txt 9463_10.txt 11713_10.txt 2713_8.txt 4964_10.txt 7213_10.txt 9464_8.txt 11714_10.txt 2714_10.txt 4965_9.txt 7214_7.txt 9465_7.txt 11715_7.txt 2715_8.txt 4966_10.txt 7215_7.txt 9466_7.txt 11716_7.txt 2716_10.txt 4967_10.txt 7216_10.txt 9467_10.txt 11717_8.txt 2717_10.txt 4968_10.txt 7217_7.txt 9468_7.txt 11718_10.txt 2718_9.txt 4969_7.txt 7218_9.txt 9469_10.txt 11719_7.txt 2719_10.txt 496_10.txt 7219_7.txt 946_8.txt 1171_10.txt 271_10.txt 4970_8.txt 721_10.txt 9470_8.txt 11720_8.txt 2720_7.txt 4971_7.txt 7220_7.txt 9471_7.txt 11721_7.txt 2721_9.txt 4972_9.txt 7221_8.txt 9472_8.txt 11722_8.txt 2722_8.txt 4973_9.txt 7222_8.txt 9473_10.txt 11723_7.txt 2723_7.txt 4974_10.txt 7223_10.txt 9474_10.txt 11724_8.txt 2724_9.txt 4975_8.txt 7224_8.txt 9475_9.txt 11725_10.txt 2725_8.txt 4976_10.txt 7225_10.txt 9476_9.txt 11726_9.txt 2726_9.txt 4977_7.txt 7226_8.txt 9477_9.txt 11727_7.txt 2727_9.txt 4978_7.txt 7227_7.txt 9478_8.txt 11728_9.txt 2728_10.txt 4979_10.txt 7228_9.txt 9479_10.txt 11729_10.txt 2729_10.txt 497_10.txt 7229_7.txt 947_10.txt 1172_8.txt 272_10.txt 4980_8.txt 722_7.txt 9480_10.txt 11730_10.txt 2730_9.txt 4981_10.txt 7230_8.txt 9481_9.txt 11731_10.txt 2731_10.txt 4982_7.txt 7231_8.txt 9482_7.txt 11732_7.txt 2732_8.txt 4983_7.txt 7232_8.txt 9483_10.txt 11733_10.txt 2733_10.txt 4984_8.txt 7233_7.txt 9484_10.txt 11734_10.txt 2734_9.txt 4985_10.txt 7234_8.txt 9485_10.txt 11735_10.txt 2735_10.txt 4986_8.txt 7235_8.txt 9486_10.txt 11736_9.txt 2736_9.txt 4987_10.txt 7236_8.txt 9487_8.txt 11737_8.txt 2737_9.txt 4988_7.txt 7237_10.txt 9488_10.txt 11738_9.txt 2738_10.txt 4989_8.txt 7238_9.txt 9489_9.txt 11739_8.txt 2739_9.txt 498_10.txt 7239_7.txt 948_10.txt 1173_8.txt 273_9.txt 4990_8.txt 723_8.txt 9490_10.txt 11740_10.txt 2740_8.txt 4991_10.txt 7240_8.txt 9491_10.txt 11741_10.txt 2741_10.txt 4992_7.txt 7241_8.txt 9492_7.txt 11742_9.txt 2742_9.txt 4993_10.txt 7242_7.txt 9493_8.txt 11743_7.txt 2743_9.txt 4994_9.txt 7243_10.txt 9494_10.txt 11744_9.txt 2744_10.txt 4995_9.txt 7244_10.txt 9495_8.txt 11745_10.txt 2745_10.txt 4996_9.txt 7245_10.txt 9496_10.txt 11746_9.txt 2746_10.txt 4997_9.txt 7246_10.txt 9497_8.txt 11747_8.txt 2747_9.txt 4998_9.txt 7247_10.txt 9498_8.txt 11748_8.txt 2748_8.txt 4999_9.txt 7248_9.txt 9499_7.txt 11749_10.txt 2749_10.txt 499_8.txt 7249_8.txt 949_8.txt 1174_10.txt 274_7.txt 49_10.txt 724_10.txt 94_10.txt 11750_10.txt 2750_10.txt 4_8.txt 7250_8.txt 9500_7.txt 11751_10.txt 2751_10.txt 5000_10.txt 7251_10.txt 9501_10.txt 11752_10.txt 2752_7.txt 5001_7.txt 7252_8.txt 9502_8.txt 11753_10.txt 2753_10.txt 5002_8.txt 7253_10.txt 9503_7.txt 11754_7.txt 2754_10.txt 5003_10.txt 7254_10.txt 9504_10.txt 11755_7.txt 2755_10.txt 5004_9.txt 7255_9.txt 9505_10.txt 11756_7.txt 2756_9.txt 5005_8.txt 7256_9.txt 9506_8.txt 11757_9.txt 2757_10.txt 5006_10.txt 7257_10.txt 9507_10.txt 11758_7.txt 2758_7.txt 5007_10.txt 7258_10.txt 9508_8.txt 11759_10.txt 2759_10.txt 5008_10.txt 7259_7.txt 9509_9.txt 1175_9.txt 275_10.txt 5009_9.txt 725_10.txt 950_9.txt 11760_8.txt 2760_8.txt 500_9.txt 7260_10.txt 9510_8.txt 11761_10.txt 2761_10.txt 5010_10.txt 7261_8.txt 9511_7.txt 11762_8.txt 2762_9.txt 5011_10.txt 7262_10.txt 9512_8.txt 11763_7.txt 2763_8.txt 5012_10.txt 7263_10.txt 9513_10.txt 11764_10.txt 2764_10.txt 5013_10.txt 7264_8.txt 9514_10.txt 11765_8.txt 2765_9.txt 5014_9.txt 7265_8.txt 9515_10.txt 11766_8.txt 2766_9.txt 5015_7.txt 7266_7.txt 9516_8.txt 11767_8.txt 2767_8.txt 5016_10.txt 7267_7.txt 9517_10.txt 11768_7.txt 2768_8.txt 5017_8.txt 7268_9.txt 9518_10.txt 11769_7.txt 2769_10.txt 5018_8.txt 7269_7.txt 9519_10.txt 1176_10.txt 276_10.txt 5019_10.txt 726_7.txt 951_10.txt 11770_10.txt 2770_8.txt 501_10.txt 7270_8.txt 9520_10.txt 11771_10.txt 2771_7.txt 5020_10.txt 7271_7.txt 9521_10.txt 11772_10.txt 2772_7.txt 5021_10.txt 7272_10.txt 9522_10.txt 11773_8.txt 2773_7.txt 5022_8.txt 7273_8.txt 9523_10.txt 11774_10.txt 2774_10.txt 5023_10.txt 7274_10.txt 9524_9.txt 11775_10.txt 2775_7.txt 5024_8.txt 7275_8.txt 9525_8.txt 11776_9.txt 2776_8.txt 5025_10.txt 7276_8.txt 9526_10.txt 11777_9.txt 2777_7.txt 5026_8.txt 7277_7.txt 9527_10.txt 11778_10.txt 2778_8.txt 5027_10.txt 7278_7.txt 9528_9.txt 11779_7.txt 2779_7.txt 5028_10.txt 7279_9.txt 9529_9.txt 1177_9.txt 277_8.txt 5029_8.txt 727_9.txt 952_10.txt 11780_8.txt 2780_9.txt 502_10.txt 7280_10.txt 9530_9.txt 11781_8.txt 2781_8.txt 5030_9.txt 7281_10.txt 9531_7.txt 11782_8.txt 2782_10.txt 5031_10.txt 7282_9.txt 9532_10.txt 11783_10.txt 2783_8.txt 5032_10.txt 7283_8.txt 9533_9.txt 11784_10.txt 2784_8.txt 5033_10.txt 7284_8.txt 9534_10.txt 11785_10.txt 2785_7.txt 5034_7.txt 7285_9.txt 9535_10.txt 11786_8.txt 2786_7.txt 5035_7.txt 7286_8.txt 9536_7.txt 11787_9.txt 2787_8.txt 5036_9.txt 7287_7.txt 9537_8.txt 11788_8.txt 2788_9.txt 5037_10.txt 7288_7.txt 9538_10.txt 11789_10.txt 2789_7.txt 5038_10.txt 7289_10.txt 9539_7.txt 1178_10.txt 278_9.txt 5039_10.txt 728_10.txt 953_10.txt 11790_8.txt 2790_10.txt 503_10.txt 7290_10.txt 9540_8.txt 11791_8.txt 2791_10.txt 5040_9.txt 7291_10.txt 9541_8.txt 11792_8.txt 2792_10.txt 5041_8.txt 7292_10.txt 9542_10.txt 11793_8.txt 2793_9.txt 5042_9.txt 7293_10.txt 9543_7.txt 11794_7.txt 2794_8.txt 5043_7.txt 7294_9.txt 9544_10.txt 11795_10.txt 2795_7.txt 5044_9.txt 7295_10.txt 9545_7.txt 11796_9.txt 2796_10.txt 5045_8.txt 7296_10.txt 9546_8.txt 11797_8.txt 2797_10.txt 5046_10.txt 7297_10.txt 9547_9.txt 11798_10.txt 2798_8.txt 5047_8.txt 7298_7.txt 9548_10.txt 11799_8.txt 2799_10.txt 5048_9.txt 7299_8.txt 9549_8.txt 1179_9.txt 279_9.txt 5049_9.txt 729_10.txt 954_10.txt 117_10.txt 27_10.txt 504_8.txt 72_7.txt 9550_7.txt 11800_8.txt 2800_10.txt 5050_7.txt 7300_8.txt 9551_10.txt 11801_8.txt 2801_9.txt 5051_7.txt 7301_7.txt 9552_8.txt 11802_7.txt 2802_10.txt 5052_10.txt 7302_10.txt 9553_9.txt 11803_7.txt 2803_10.txt 5053_8.txt 7303_10.txt 9554_8.txt 11804_10.txt 2804_8.txt 5054_7.txt 7304_8.txt 9555_10.txt 11805_10.txt 2805_9.txt 5055_8.txt 7305_10.txt 9556_10.txt 11806_10.txt 2806_10.txt 5056_8.txt 7306_10.txt 9557_7.txt 11807_7.txt 2807_7.txt 5057_10.txt 7307_10.txt 9558_10.txt 11808_9.txt 2808_10.txt 5058_10.txt 7308_8.txt 9559_8.txt 11809_10.txt 2809_8.txt 5059_7.txt 7309_9.txt 955_7.txt 1180_9.txt 280_8.txt 505_9.txt 730_7.txt 9560_9.txt 11810_7.txt 2810_10.txt 5060_8.txt 7310_9.txt 9561_8.txt 11811_8.txt 2811_10.txt 5061_8.txt 7311_9.txt 9562_8.txt 11812_10.txt 2812_8.txt 5062_9.txt 7312_10.txt 9563_9.txt 11813_8.txt 2813_8.txt 5063_7.txt 7313_10.txt 9564_10.txt 11814_7.txt 2814_10.txt 5064_8.txt 7314_10.txt 9565_8.txt 11815_10.txt 2815_10.txt 5065_7.txt 7315_10.txt 9566_7.txt 11816_8.txt 2816_7.txt 5066_8.txt 7316_10.txt 9567_7.txt 11817_10.txt 2817_9.txt 5067_10.txt 7317_10.txt 9568_7.txt 11818_10.txt 2818_7.txt 5068_9.txt 7318_10.txt 9569_9.txt 11819_8.txt 2819_10.txt 5069_7.txt 7319_10.txt 956_9.txt 1181_9.txt 281_10.txt 506_7.txt 731_9.txt 9570_8.txt 11820_8.txt 2820_7.txt 5070_9.txt 7320_10.txt 9571_9.txt 11821_10.txt 2821_9.txt 5071_10.txt 7321_10.txt 9572_8.txt 11822_10.txt 2822_7.txt 5072_8.txt 7322_10.txt 9573_8.txt 11823_9.txt 2823_7.txt 5073_7.txt 7323_9.txt 9574_9.txt 11824_10.txt 2824_8.txt 5074_10.txt 7324_10.txt 9575_9.txt 11825_8.txt 2825_9.txt 5075_7.txt 7325_9.txt 9576_9.txt 11826_10.txt 2826_10.txt 5076_10.txt 7326_10.txt 9577_8.txt 11827_9.txt 2827_10.txt 5077_9.txt 7327_10.txt 9578_10.txt 11828_8.txt 2828_9.txt 5078_9.txt 7328_8.txt 9579_8.txt 11829_10.txt 2829_10.txt 5079_8.txt 7329_8.txt 957_10.txt 1182_8.txt 282_9.txt 507_10.txt 732_7.txt 9580_7.txt 11830_9.txt 2830_9.txt 5080_10.txt 7330_8.txt 9581_10.txt 11831_7.txt 2831_7.txt 5081_8.txt 7331_10.txt 9582_10.txt 11832_7.txt 2832_8.txt 5082_7.txt 7332_7.txt 9583_8.txt 11833_7.txt 2833_7.txt 5083_8.txt 7333_10.txt 9584_10.txt 11834_7.txt 2834_9.txt 5084_7.txt 7334_7.txt 9585_10.txt 11835_8.txt 2835_7.txt 5085_7.txt 7335_7.txt 9586_10.txt 11836_10.txt 2836_7.txt 5086_7.txt 7336_10.txt 9587_10.txt 11837_7.txt 2837_8.txt 5087_7.txt 7337_10.txt 9588_10.txt 11838_9.txt 2838_10.txt 5088_8.txt 7338_7.txt 9589_10.txt 11839_9.txt 2839_10.txt 5089_9.txt 7339_10.txt 958_9.txt 1183_8.txt 283_8.txt 508_9.txt 733_9.txt 9590_10.txt 11840_10.txt 2840_9.txt 5090_10.txt 7340_8.txt 9591_10.txt 11841_9.txt 2841_10.txt 5091_8.txt 7341_8.txt 9592_8.txt 11842_10.txt 2842_8.txt 5092_8.txt 7342_7.txt 9593_10.txt 11843_10.txt 2843_7.txt 5093_10.txt 7343_10.txt 9594_10.txt 11844_10.txt 2844_9.txt 5094_8.txt 7344_10.txt 9595_10.txt 11845_10.txt 2845_8.txt 5095_7.txt 7345_7.txt 9596_10.txt 11846_10.txt 2846_10.txt 5096_7.txt 7346_8.txt 9597_10.txt 11847_9.txt 2847_8.txt 5097_8.txt 7347_10.txt 9598_10.txt 11848_10.txt 2848_7.txt 5098_9.txt 7348_10.txt 9599_9.txt 11849_8.txt 2849_7.txt 5099_7.txt 7349_7.txt 959_7.txt 1184_7.txt 284_10.txt 509_10.txt 734_10.txt 95_10.txt 11850_10.txt 2850_9.txt 50_10.txt 7350_8.txt 9600_8.txt 11851_10.txt 2851_10.txt 5100_10.txt 7351_10.txt 9601_10.txt 11852_10.txt 2852_7.txt 5101_7.txt 7352_10.txt 9602_10.txt 11853_7.txt 2853_8.txt 5102_7.txt 7353_7.txt 9603_10.txt 11854_7.txt 2854_10.txt 5103_8.txt 7354_7.txt 9604_10.txt 11855_7.txt 2855_8.txt 5104_10.txt 7355_10.txt 9605_10.txt 11856_10.txt 2856_10.txt 5105_8.txt 7356_10.txt 9606_10.txt 11857_10.txt 2857_9.txt 5106_8.txt 7357_10.txt 9607_9.txt 11858_9.txt 2858_10.txt 5107_10.txt 7358_10.txt 9608_10.txt 11859_10.txt 2859_10.txt 5108_10.txt 7359_10.txt 9609_7.txt 1185_8.txt 285_10.txt 5109_10.txt 735_8.txt 960_7.txt 11860_10.txt 2860_9.txt 510_8.txt 7360_10.txt 9610_10.txt 11861_10.txt 2861_10.txt 5110_10.txt 7361_10.txt 9611_8.txt 11862_10.txt 2862_10.txt 5111_10.txt 7362_10.txt 9612_8.txt 11863_10.txt 2863_8.txt 5112_9.txt 7363_9.txt 9613_10.txt 11864_8.txt 2864_9.txt 5113_10.txt 7364_7.txt 9614_10.txt 11865_10.txt 2865_10.txt 5114_10.txt 7365_7.txt 9615_9.txt 11866_8.txt 2866_9.txt 5115_10.txt 7366_7.txt 9616_8.txt 11867_8.txt 2867_10.txt 5116_10.txt 7367_7.txt 9617_10.txt 11868_10.txt 2868_7.txt 5117_10.txt 7368_8.txt 9618_8.txt 11869_10.txt 2869_10.txt 5118_10.txt 7369_7.txt 9619_10.txt 1186_8.txt 286_10.txt 5119_10.txt 736_10.txt 961_9.txt 11870_10.txt 2870_10.txt 511_10.txt 7370_7.txt 9620_7.txt 11871_7.txt 2871_10.txt 5120_10.txt 7371_7.txt 9621_8.txt 11872_10.txt 2872_10.txt 5121_10.txt 7372_9.txt 9622_10.txt 11873_8.txt 2873_10.txt 5122_9.txt 7373_8.txt 9623_7.txt 11874_10.txt 2874_10.txt 5123_8.txt 7374_10.txt 9624_10.txt 11875_7.txt 2875_10.txt 5124_10.txt 7375_7.txt 9625_8.txt 11876_10.txt 2876_10.txt 5125_7.txt 7376_8.txt 9626_8.txt 11877_10.txt 2877_9.txt 5126_7.txt 7377_9.txt 9627_10.txt 11878_10.txt 2878_8.txt 5127_10.txt 7378_8.txt 9628_8.txt 11879_10.txt 2879_9.txt 5128_9.txt 7379_9.txt 9629_8.txt 1187_10.txt 287_9.txt 5129_9.txt 737_8.txt 962_8.txt 11880_10.txt 2880_8.txt 512_10.txt 7380_9.txt 9630_10.txt 11881_9.txt 2881_8.txt 5130_10.txt 7381_8.txt 9631_10.txt 11882_8.txt 2882_9.txt 5131_10.txt 7382_8.txt 9632_8.txt 11883_8.txt 2883_9.txt 5132_8.txt 7383_9.txt 9633_7.txt 11884_8.txt 2884_7.txt 5133_10.txt 7384_10.txt 9634_8.txt 11885_7.txt 2885_9.txt 5134_10.txt 7385_10.txt 9635_7.txt 11886_10.txt 2886_8.txt 5135_10.txt 7386_7.txt 9636_7.txt 11887_8.txt 2887_9.txt 5136_10.txt 7387_7.txt 9637_8.txt 11888_9.txt 2888_10.txt 5137_10.txt 7388_10.txt 9638_8.txt 11889_7.txt 2889_10.txt 5138_10.txt 7389_10.txt 9639_10.txt 1188_8.txt 288_10.txt 5139_9.txt 738_8.txt 963_8.txt 11890_7.txt 2890_7.txt 513_10.txt 7390_10.txt 9640_9.txt 11891_9.txt 2891_8.txt 5140_10.txt 7391_9.txt 9641_9.txt 11892_9.txt 2892_10.txt 5141_9.txt 7392_8.txt 9642_10.txt 11893_10.txt 2893_10.txt 5142_10.txt 7393_7.txt 9643_10.txt 11894_10.txt 2894_10.txt 5143_8.txt 7394_9.txt 9644_10.txt 11895_8.txt 2895_9.txt 5144_10.txt 7395_8.txt 9645_8.txt 11896_9.txt 2896_9.txt 5145_10.txt 7396_9.txt 9646_7.txt 11897_9.txt 2897_10.txt 5146_9.txt 7397_8.txt 9647_10.txt 11898_8.txt 2898_9.txt 5147_8.txt 7398_10.txt 9648_8.txt 11899_7.txt 2899_10.txt 5148_7.txt 7399_10.txt 9649_9.txt 1189_9.txt 289_10.txt 5149_8.txt 739_7.txt 964_8.txt 118_8.txt 28_10.txt 514_10.txt 73_7.txt 9650_10.txt 11900_8.txt 2900_9.txt 5150_7.txt 7400_9.txt 9651_9.txt 11901_9.txt 2901_7.txt 5151_8.txt 7401_9.txt 9652_7.txt 11902_9.txt 2902_9.txt 5152_9.txt 7402_8.txt 9653_9.txt 11903_10.txt 2903_9.txt 5153_8.txt 7403_8.txt 9654_10.txt 11904_10.txt 2904_9.txt 5154_8.txt 7404_9.txt 9655_10.txt 11905_8.txt 2905_7.txt 5155_9.txt 7405_10.txt 9656_10.txt 11906_7.txt 2906_9.txt 5156_9.txt 7406_9.txt 9657_10.txt 11907_7.txt 2907_7.txt 5157_7.txt 7407_10.txt 9658_7.txt 11908_8.txt 2908_8.txt 5158_10.txt 7408_8.txt 9659_8.txt 11909_7.txt 2909_10.txt 5159_10.txt 7409_10.txt 965_10.txt 1190_7.txt 290_9.txt 515_10.txt 740_7.txt 9660_9.txt 11910_10.txt 2910_7.txt 5160_8.txt 7410_7.txt 9661_8.txt 11911_10.txt 2911_10.txt 5161_8.txt 7411_10.txt 9662_10.txt 11912_10.txt 2912_9.txt 5162_9.txt 7412_9.txt 9663_10.txt 11913_10.txt 2913_7.txt 5163_7.txt 7413_9.txt 9664_10.txt 11914_7.txt 2914_8.txt 5164_10.txt 7414_8.txt 9665_10.txt 11915_10.txt 2915_7.txt 5165_9.txt 7415_8.txt 9666_10.txt 11916_7.txt 2916_9.txt 5166_10.txt 7416_9.txt 9667_9.txt 11917_8.txt 2917_7.txt 5167_7.txt 7417_10.txt 9668_9.txt 11918_10.txt 2918_9.txt 5168_10.txt 7418_8.txt 9669_9.txt 11919_7.txt 2919_10.txt 5169_7.txt 7419_10.txt 966_7.txt 1191_9.txt 291_10.txt 516_10.txt 741_7.txt 9670_9.txt 11920_9.txt 2920_8.txt 5170_9.txt 7420_7.txt 9671_10.txt 11921_10.txt 2921_10.txt 5171_10.txt 7421_7.txt 9672_7.txt 11922_10.txt 2922_10.txt 5172_9.txt 7422_10.txt 9673_7.txt 11923_10.txt 2923_10.txt 5173_9.txt 7423_9.txt 9674_9.txt 11924_10.txt 2924_10.txt 5174_10.txt 7424_10.txt 9675_8.txt 11925_7.txt 2925_10.txt 5175_10.txt 7425_9.txt 9676_10.txt 11926_10.txt 2926_8.txt 5176_10.txt 7426_10.txt 9677_9.txt 11927_10.txt 2927_10.txt 5177_8.txt 7427_9.txt 9678_8.txt 11928_8.txt 2928_10.txt 5178_10.txt 7428_10.txt 9679_10.txt 11929_9.txt 2929_10.txt 5179_7.txt 7429_9.txt 967_7.txt 1192_8.txt 292_10.txt 517_10.txt 742_9.txt 9680_8.txt 11930_10.txt 2930_10.txt 5180_9.txt 7430_10.txt 9681_9.txt 11931_9.txt 2931_10.txt 5181_10.txt 7431_10.txt 9682_10.txt 11932_7.txt 2932_10.txt 5182_8.txt 7432_10.txt 9683_9.txt 11933_8.txt 2933_10.txt 5183_9.txt 7433_10.txt 9684_10.txt 11934_8.txt 2934_10.txt 5184_9.txt 7434_7.txt 9685_10.txt 11935_7.txt 2935_10.txt 5185_10.txt 7435_9.txt 9686_7.txt 11936_7.txt 2936_10.txt 5186_10.txt 7436_10.txt 9687_9.txt 11937_8.txt 2937_10.txt 5187_7.txt 7437_10.txt 9688_9.txt 11938_7.txt 2938_10.txt 5188_8.txt 7438_10.txt 9689_9.txt 11939_10.txt 2939_10.txt 5189_10.txt 7439_9.txt 968_10.txt 1193_9.txt 293_7.txt 518_10.txt 743_7.txt 9690_10.txt 11940_7.txt 2940_10.txt 5190_8.txt 7440_7.txt 9691_10.txt 11941_8.txt 2941_10.txt 5191_8.txt 7441_7.txt 9692_10.txt 11942_8.txt 2942_10.txt 5192_7.txt 7442_7.txt 9693_8.txt 11943_7.txt 2943_10.txt 5193_9.txt 7443_7.txt 9694_9.txt 11944_10.txt 2944_10.txt 5194_10.txt 7444_10.txt 9695_10.txt 11945_9.txt 2945_10.txt 5195_7.txt 7445_7.txt 9696_8.txt 11946_9.txt 2946_10.txt 5196_9.txt 7446_8.txt 9697_8.txt 11947_9.txt 2947_10.txt 5197_10.txt 7447_8.txt 9698_10.txt 11948_10.txt 2948_10.txt 5198_7.txt 7448_8.txt 9699_10.txt 11949_8.txt 2949_10.txt 5199_7.txt 7449_10.txt 969_7.txt 1194_7.txt 294_10.txt 519_10.txt 744_7.txt 96_10.txt 11950_8.txt 2950_10.txt 51_10.txt 7450_9.txt 9700_10.txt 11951_8.txt 2951_8.txt 5200_10.txt 7451_9.txt 9701_10.txt 11952_8.txt 2952_8.txt 5201_10.txt 7452_10.txt 9702_10.txt 11953_8.txt 2953_8.txt 5202_8.txt 7453_10.txt 9703_9.txt 11954_9.txt 2954_10.txt 5203_7.txt 7454_10.txt 9704_10.txt 11955_7.txt 2955_10.txt 5204_8.txt 7455_8.txt 9705_10.txt 11956_9.txt 2956_8.txt 5205_7.txt 7456_10.txt 9706_10.txt 11957_8.txt 2957_10.txt 5206_7.txt 7457_9.txt 9707_10.txt 11958_9.txt 2958_10.txt 5207_7.txt 7458_10.txt 9708_10.txt 11959_8.txt 2959_10.txt 5208_7.txt 7459_10.txt 9709_10.txt 1195_8.txt 295_10.txt 5209_8.txt 745_10.txt 970_10.txt 11960_8.txt 2960_10.txt 520_8.txt 7460_10.txt 9710_10.txt 11961_8.txt 2961_10.txt 5210_10.txt 7461_10.txt 9711_9.txt 11962_7.txt 2962_7.txt 5211_8.txt 7462_10.txt 9712_10.txt 11963_10.txt 2963_8.txt 5212_7.txt 7463_10.txt 9713_10.txt 11964_7.txt 2964_8.txt 5213_8.txt 7464_10.txt 9714_10.txt 11965_8.txt 2965_9.txt 5214_7.txt 7465_10.txt 9715_7.txt 11966_10.txt 2966_10.txt 5215_10.txt 7466_10.txt 9716_10.txt 11967_8.txt 2967_9.txt 5216_8.txt 7467_10.txt 9717_8.txt 11968_10.txt 2968_10.txt 5217_8.txt 7468_10.txt 9718_7.txt 11969_10.txt 2969_10.txt 5218_7.txt 7469_10.txt 9719_7.txt 1196_8.txt 296_10.txt 5219_7.txt 746_10.txt 971_8.txt 11970_10.txt 2970_10.txt 521_10.txt 7470_10.txt 9720_8.txt 11971_10.txt 2971_8.txt 5220_8.txt 7471_10.txt 9721_7.txt 11972_10.txt 2972_7.txt 5221_7.txt 7472_9.txt 9722_7.txt 11973_10.txt 2973_9.txt 5222_8.txt 7473_9.txt 9723_9.txt 11974_10.txt 2974_8.txt 5223_7.txt 7474_9.txt 9724_7.txt 11975_7.txt 2975_7.txt 5224_10.txt 7475_7.txt 9725_7.txt 11976_10.txt 2976_10.txt 5225_9.txt 7476_8.txt 9726_7.txt 11977_8.txt 2977_10.txt 5226_10.txt 7477_9.txt 9727_7.txt 11978_10.txt 2978_10.txt 5227_10.txt 7478_10.txt 9728_7.txt 11979_10.txt 2979_10.txt 5228_8.txt 7479_8.txt 9729_9.txt 1197_10.txt 297_10.txt 5229_10.txt 747_10.txt 972_9.txt 11980_10.txt 2980_7.txt 522_8.txt 7480_10.txt 9730_8.txt 11981_9.txt 2981_10.txt 5230_10.txt 7481_10.txt 9731_8.txt 11982_7.txt 2982_8.txt 5231_10.txt 7482_10.txt 9732_10.txt 11983_10.txt 2983_7.txt 5232_8.txt 7483_10.txt 9733_8.txt 11984_10.txt 2984_8.txt 5233_10.txt 7484_10.txt 9734_7.txt 11985_10.txt 2985_8.txt 5234_10.txt 7485_8.txt 9735_8.txt 11986_8.txt 2986_8.txt 5235_8.txt 7486_8.txt 9736_8.txt 11987_7.txt 2987_10.txt 5236_10.txt 7487_8.txt 9737_9.txt 11988_8.txt 2988_7.txt 5237_10.txt 7488_7.txt 9738_8.txt 11989_10.txt 2989_10.txt 5238_7.txt 7489_7.txt 9739_7.txt 1198_10.txt 298_8.txt 5239_7.txt 748_9.txt 973_9.txt 11990_10.txt 2990_10.txt 523_10.txt 7490_8.txt 9740_7.txt 11991_10.txt 2991_7.txt 5240_7.txt 7491_7.txt 9741_8.txt 11992_10.txt 2992_7.txt 5241_7.txt 7492_7.txt 9742_10.txt 11993_9.txt 2993_10.txt 5242_7.txt 7493_7.txt 9743_8.txt 11994_10.txt 2994_8.txt 5243_9.txt 7494_10.txt 9744_7.txt 11995_10.txt 2995_7.txt 5244_7.txt 7495_7.txt 9745_7.txt 11996_10.txt 2996_7.txt 5245_8.txt 7496_8.txt 9746_9.txt 11997_10.txt 2997_7.txt 5246_10.txt 7497_10.txt 9747_10.txt 11998_9.txt 2998_7.txt 5247_10.txt 7498_10.txt 9748_10.txt 11999_10.txt 2999_7.txt 5248_8.txt 7499_7.txt 9749_9.txt 1199_10.txt 299_10.txt 5249_9.txt 749_10.txt 974_10.txt 119_10.txt 29_10.txt 524_10.txt 74_8.txt 9750_10.txt 11_9.txt 2_9.txt 5250_10.txt 7500_8.txt 9751_7.txt 12000_8.txt 3000_8.txt 5251_7.txt 7501_8.txt 9752_10.txt 12001_8.txt 3001_10.txt 5252_9.txt 7502_7.txt 9753_10.txt 12002_10.txt 3002_8.txt 5253_10.txt 7503_7.txt 9754_10.txt 12003_10.txt 3003_9.txt 5254_9.txt 7504_7.txt 9755_7.txt 12004_10.txt 3004_10.txt 5255_10.txt 7505_9.txt 9756_10.txt 12005_10.txt 3005_8.txt 5256_10.txt 7506_7.txt 9757_8.txt 12006_10.txt 3006_9.txt 5257_10.txt 7507_8.txt 9758_10.txt 12007_10.txt 3007_7.txt 5258_10.txt 7508_7.txt 9759_10.txt 12008_8.txt 3008_7.txt 5259_9.txt 7509_8.txt 975_9.txt 12009_10.txt 3009_8.txt 525_10.txt 750_8.txt 9760_8.txt 1200_10.txt 300_9.txt 5260_10.txt 7510_8.txt 9761_10.txt 12010_10.txt 3010_8.txt 5261_10.txt 7511_9.txt 9762_8.txt 12011_10.txt 3011_7.txt 5262_7.txt 7512_8.txt 9763_8.txt 12012_10.txt 3012_8.txt 5263_8.txt 7513_9.txt 9764_9.txt 12013_10.txt 3013_8.txt 5264_7.txt 7514_8.txt 9765_10.txt 12014_10.txt 3014_8.txt 5265_9.txt 7515_8.txt 9766_9.txt 12015_10.txt 3015_9.txt 5266_10.txt 7516_7.txt 9767_7.txt 12016_7.txt 3016_10.txt 5267_7.txt 7517_7.txt 9768_10.txt 12017_7.txt 3017_7.txt 5268_10.txt 7518_9.txt 9769_8.txt 12018_7.txt 3018_10.txt 5269_10.txt 7519_9.txt 976_8.txt 12019_8.txt 3019_8.txt 526_10.txt 751_9.txt 9770_10.txt 1201_8.txt 301_10.txt 5270_7.txt 7520_8.txt 9771_10.txt 12020_7.txt 3020_9.txt 5271_7.txt 7521_7.txt 9772_10.txt 12021_9.txt 3021_8.txt 5272_7.txt 7522_8.txt 9773_7.txt 12022_10.txt 3022_7.txt 5273_7.txt 7523_7.txt 9774_8.txt 12023_9.txt 3023_7.txt 5274_7.txt 7524_9.txt 9775_8.txt 12024_7.txt 3024_9.txt 5275_10.txt 7525_7.txt 9776_8.txt 12025_7.txt 3025_7.txt 5276_10.txt 7526_8.txt 9777_8.txt 12026_8.txt 3026_7.txt 5277_10.txt 7527_10.txt 9778_8.txt 12027_10.txt 3027_8.txt 5278_7.txt 7528_10.txt 9779_9.txt 12028_10.txt 3028_7.txt 5279_8.txt 7529_10.txt 977_8.txt 12029_10.txt 3029_8.txt 527_9.txt 752_7.txt 9780_10.txt 1202_9.txt 302_10.txt 5280_8.txt 7530_9.txt 9781_8.txt 12030_9.txt 3030_9.txt 5281_10.txt 7531_8.txt 9782_10.txt 12031_7.txt 3031_8.txt 5282_10.txt 7532_7.txt 9783_9.txt 12032_8.txt 3032_7.txt 5283_7.txt 7533_10.txt 9784_8.txt 12033_8.txt 3033_8.txt 5284_9.txt 7534_10.txt 9785_7.txt 12034_10.txt 3034_7.txt 5285_7.txt 7535_10.txt 9786_8.txt 12035_8.txt 3035_9.txt 5286_10.txt 7536_10.txt 9787_7.txt 12036_7.txt 3036_9.txt 5287_8.txt 7537_10.txt 9788_9.txt 12037_10.txt 3037_7.txt 5288_7.txt 7538_7.txt 9789_10.txt 12038_9.txt 3038_8.txt 5289_10.txt 7539_10.txt 978_9.txt 12039_10.txt 3039_8.txt 528_9.txt 753_8.txt 9790_9.txt 1203_8.txt 303_10.txt 5290_10.txt 7540_9.txt 9791_9.txt 12040_7.txt 3040_8.txt 5291_8.txt 7541_10.txt 9792_8.txt 12041_9.txt 3041_7.txt 5292_7.txt 7542_10.txt 9793_7.txt 12042_10.txt 3042_8.txt 5293_8.txt 7543_8.txt 9794_7.txt 12043_10.txt 3043_8.txt 5294_8.txt 7544_7.txt 9795_7.txt 12044_7.txt 3044_7.txt 5295_8.txt 7545_8.txt 9796_9.txt 12045_10.txt 3045_10.txt 5296_10.txt 7546_7.txt 9797_7.txt 12046_10.txt 3046_10.txt 5297_8.txt 7547_10.txt 9798_7.txt 12047_10.txt 3047_7.txt 5298_8.txt 7548_8.txt 9799_7.txt 12048_10.txt 3048_10.txt 5299_8.txt 7549_7.txt 979_8.txt 12049_10.txt 3049_9.txt 529_10.txt 754_9.txt 97_9.txt 1204_10.txt 304_10.txt 52_10.txt 7550_9.txt 9800_9.txt 12050_10.txt 3050_9.txt 5300_8.txt 7551_10.txt 9801_10.txt 12051_10.txt 3051_9.txt 5301_8.txt 7552_10.txt 9802_10.txt 12052_9.txt 3052_10.txt 5302_9.txt 7553_10.txt 9803_7.txt 12053_9.txt 3053_8.txt 5303_10.txt 7554_10.txt 9804_10.txt 12054_10.txt 3054_7.txt 5304_10.txt 7555_7.txt 9805_10.txt 12055_10.txt 3055_9.txt 5305_7.txt 7556_8.txt 9806_8.txt 12056_10.txt 3056_8.txt 5306_7.txt 7557_9.txt 9807_10.txt 12057_10.txt 3057_8.txt 5307_7.txt 7558_8.txt 9808_9.txt 12058_9.txt 3058_9.txt 5308_10.txt 7559_10.txt 9809_7.txt 12059_9.txt 3059_8.txt 5309_7.txt 755_10.txt 980_7.txt 1205_8.txt 305_8.txt 530_10.txt 7560_9.txt 9810_7.txt 12060_9.txt 3060_10.txt 5310_10.txt 7561_9.txt 9811_7.txt 12061_10.txt 3061_10.txt 5311_7.txt 7562_10.txt 9812_7.txt 12062_10.txt 3062_10.txt 5312_7.txt 7563_10.txt 9813_8.txt 12063_10.txt 3063_10.txt 5313_7.txt 7564_10.txt 9814_10.txt 12064_10.txt 3064_8.txt 5314_10.txt 7565_10.txt 9815_7.txt 12065_7.txt 3065_10.txt 5315_9.txt 7566_10.txt 9816_10.txt 12066_8.txt 3066_10.txt 5316_7.txt 7567_10.txt 9817_8.txt 12067_9.txt 3067_9.txt 5317_7.txt 7568_10.txt 9818_7.txt 12068_9.txt 3068_9.txt 5318_7.txt 7569_10.txt 9819_10.txt 12069_10.txt 3069_9.txt 5319_8.txt 756_10.txt 981_7.txt 1206_10.txt 306_10.txt 531_10.txt 7570_10.txt 9820_10.txt 12070_7.txt 3070_7.txt 5320_10.txt 7571_8.txt 9821_8.txt 12071_8.txt 3071_9.txt 5321_10.txt 7572_10.txt 9822_10.txt 12072_10.txt 3072_8.txt 5322_10.txt 7573_9.txt 9823_7.txt 12073_9.txt 3073_8.txt 5323_7.txt 7574_10.txt 9824_10.txt 12074_10.txt 3074_8.txt 5324_10.txt 7575_10.txt 9825_10.txt 12075_7.txt 3075_9.txt 5325_7.txt 7576_10.txt 9826_10.txt 12076_8.txt 3076_8.txt 5326_10.txt 7577_8.txt 9827_10.txt 12077_8.txt 3077_10.txt 5327_10.txt 7578_8.txt 9828_10.txt 12078_8.txt 3078_10.txt 5328_8.txt 7579_8.txt 9829_7.txt 12079_7.txt 3079_10.txt 5329_9.txt 757_8.txt 982_8.txt 1207_10.txt 307_8.txt 532_9.txt 7580_7.txt 9830_7.txt 12080_8.txt 3080_10.txt 5330_7.txt 7581_8.txt 9831_8.txt 12081_8.txt 3081_9.txt 5331_10.txt 7582_8.txt 9832_10.txt 12082_8.txt 3082_10.txt 5332_7.txt 7583_7.txt 9833_8.txt 12083_8.txt 3083_10.txt 5333_10.txt 7584_7.txt 9834_10.txt 12084_8.txt 3084_10.txt 5334_7.txt 7585_8.txt 9835_9.txt 12085_8.txt 3085_10.txt 5335_10.txt 7586_10.txt 9836_9.txt 12086_10.txt 3086_10.txt 5336_8.txt 7587_9.txt 9837_7.txt 12087_10.txt 3087_10.txt 5337_9.txt 7588_8.txt 9838_7.txt 12088_9.txt 3088_8.txt 5338_10.txt 7589_8.txt 9839_7.txt 12089_10.txt 3089_9.txt 5339_8.txt 758_9.txt 983_7.txt 1208_9.txt 308_8.txt 533_10.txt 7590_10.txt 9840_10.txt 12090_9.txt 3090_10.txt 5340_8.txt 7591_8.txt 9841_7.txt 12091_8.txt 3091_10.txt 5341_10.txt 7592_9.txt 9842_7.txt 12092_7.txt 3092_10.txt 5342_10.txt 7593_7.txt 9843_7.txt 12093_8.txt 3093_10.txt 5343_8.txt 7594_8.txt 9844_8.txt 12094_7.txt 3094_10.txt 5344_7.txt 7595_10.txt 9845_8.txt 12095_8.txt 3095_10.txt 5345_9.txt 7596_9.txt 9846_7.txt 12096_8.txt 3096_10.txt 5346_7.txt 7597_9.txt 9847_7.txt 12097_7.txt 3097_9.txt 5347_7.txt 7598_10.txt 9848_7.txt 12098_8.txt 3098_10.txt 5348_8.txt 7599_10.txt 9849_9.txt 12099_9.txt 3099_10.txt 5349_7.txt 759_10.txt 984_7.txt 1209_10.txt 309_9.txt 534_10.txt 75_8.txt 9850_7.txt 120_8.txt 30_7.txt 5350_9.txt 7600_10.txt 9851_7.txt 12100_8.txt 3100_10.txt 5351_7.txt 7601_10.txt 9852_7.txt 12101_7.txt 3101_9.txt 5352_7.txt 7602_10.txt 9853_8.txt 12102_8.txt 3102_9.txt 5353_7.txt 7603_10.txt 9854_7.txt 12103_7.txt 3103_7.txt 5354_10.txt 7604_9.txt 9855_7.txt 12104_7.txt 3104_10.txt 5355_8.txt 7605_8.txt 9856_8.txt 12105_10.txt 3105_8.txt 5356_8.txt 7606_9.txt 9857_7.txt 12106_10.txt 3106_8.txt 5357_8.txt 7607_8.txt 9858_7.txt 12107_10.txt 3107_8.txt 5358_10.txt 7608_10.txt 9859_10.txt 12108_9.txt 3108_8.txt 5359_10.txt 7609_10.txt 985_7.txt 12109_10.txt 3109_10.txt 535_10.txt 760_7.txt 9860_7.txt 1210_8.txt 310_7.txt 5360_10.txt 7610_9.txt 9861_10.txt 12110_8.txt 3110_8.txt 5361_10.txt 7611_9.txt 9862_9.txt 12111_7.txt 3111_7.txt 5362_8.txt 7612_9.txt 9863_10.txt 12112_10.txt 3112_7.txt 5363_9.txt 7613_9.txt 9864_8.txt 12113_8.txt 3113_9.txt 5364_10.txt 7614_10.txt 9865_8.txt 12114_7.txt 3114_9.txt 5365_10.txt 7615_10.txt 9866_7.txt 12115_10.txt 3115_8.txt 5366_10.txt 7616_10.txt 9867_10.txt 12116_9.txt 3116_10.txt 5367_9.txt 7617_9.txt 9868_9.txt 12117_7.txt 3117_8.txt 5368_9.txt 7618_10.txt 9869_10.txt 12118_7.txt 3118_9.txt 5369_9.txt 7619_10.txt 986_10.txt 12119_7.txt 3119_8.txt 536_10.txt 761_10.txt 9870_9.txt 1211_7.txt 311_9.txt 5370_7.txt 7620_10.txt 9871_7.txt 12120_10.txt 3120_8.txt 5371_8.txt 7621_10.txt 9872_10.txt 12121_7.txt 3121_10.txt 5372_7.txt 7622_10.txt 9873_7.txt 12122_7.txt 3122_8.txt 5373_7.txt 7623_9.txt 9874_8.txt 12123_8.txt 3123_10.txt 5374_8.txt 7624_7.txt 9875_7.txt 12124_8.txt 3124_9.txt 5375_8.txt 7625_10.txt 9876_8.txt 12125_8.txt 3125_10.txt 5376_7.txt 7626_9.txt 9877_8.txt 12126_8.txt 3126_10.txt 5377_7.txt 7627_10.txt 9878_7.txt 12127_8.txt 3127_9.txt 5378_7.txt 7628_8.txt 9879_8.txt 12128_8.txt 3128_9.txt 5379_7.txt 7629_10.txt 987_8.txt 12129_7.txt 3129_10.txt 537_10.txt 762_9.txt 9880_10.txt 1212_8.txt 312_10.txt 5380_7.txt 7630_8.txt 9881_8.txt 12130_7.txt 3130_9.txt 5381_7.txt 7631_10.txt 9882_8.txt 12131_7.txt 3131_7.txt 5382_7.txt 7632_10.txt 9883_8.txt 12132_10.txt 3132_9.txt 5383_10.txt 7633_8.txt 9884_10.txt 12133_7.txt 3133_9.txt 5384_8.txt 7634_7.txt 9885_10.txt 12134_8.txt 3134_8.txt 5385_7.txt 7635_10.txt 9886_10.txt 12135_10.txt 3135_9.txt 5386_7.txt 7636_8.txt 9887_9.txt 12136_9.txt 3136_8.txt 5387_8.txt 7637_7.txt 9888_10.txt 12137_10.txt 3137_8.txt 5388_8.txt 7638_7.txt 9889_9.txt 12138_7.txt 3138_9.txt 5389_7.txt 7639_8.txt 988_8.txt 12139_10.txt 3139_10.txt 538_10.txt 763_10.txt 9890_8.txt 1213_10.txt 313_10.txt 5390_9.txt 7640_10.txt 9891_10.txt 12140_8.txt 3140_7.txt 5391_9.txt 7641_10.txt 9892_8.txt 12141_8.txt 3141_10.txt 5392_10.txt 7642_9.txt 9893_7.txt 12142_9.txt 3142_8.txt 5393_10.txt 7643_7.txt 9894_8.txt 12143_8.txt 3143_10.txt 5394_10.txt 7644_7.txt 9895_8.txt 12144_8.txt 3144_10.txt 5395_10.txt 7645_7.txt 9896_8.txt 12145_10.txt 3145_10.txt 5396_9.txt 7646_8.txt 9897_10.txt 12146_7.txt 3146_10.txt 5397_9.txt 7647_7.txt 9898_10.txt 12147_10.txt 3147_10.txt 5398_8.txt 7648_7.txt 9899_7.txt 12148_10.txt 3148_9.txt 5399_8.txt 7649_9.txt 989_9.txt 12149_10.txt 3149_8.txt 539_10.txt 764_10.txt 98_10.txt 1214_10.txt 314_10.txt 53_10.txt 7650_9.txt 9900_10.txt 12150_7.txt 3150_10.txt 5400_10.txt 7651_8.txt 9901_8.txt 12151_10.txt 3151_9.txt 5401_9.txt 7652_8.txt 9902_7.txt 12152_9.txt 3152_9.txt 5402_9.txt 7653_10.txt 9903_9.txt 12153_10.txt 3153_10.txt 5403_10.txt 7654_10.txt 9904_8.txt 12154_7.txt 3154_10.txt 5404_10.txt 7655_10.txt 9905_10.txt 12155_7.txt 3155_10.txt 5405_10.txt 7656_10.txt 9906_10.txt 12156_8.txt 3156_9.txt 5406_7.txt 7657_10.txt 9907_7.txt 12157_9.txt 3157_10.txt 5407_7.txt 7658_10.txt 9908_8.txt 12158_7.txt 3158_8.txt 5408_7.txt 7659_10.txt 9909_7.txt 12159_7.txt 3159_9.txt 5409_10.txt 765_9.txt 990_9.txt 1215_10.txt 315_10.txt 540_8.txt 7660_10.txt 9910_10.txt 12160_10.txt 3160_7.txt 5410_7.txt 7661_10.txt 9911_10.txt 12161_7.txt 3161_10.txt 5411_10.txt 7662_10.txt 9912_10.txt 12162_8.txt 3162_8.txt 5412_9.txt 7663_7.txt 9913_10.txt 12163_8.txt 3163_10.txt 5413_10.txt 7664_10.txt 9914_7.txt 12164_9.txt 3164_10.txt 5414_10.txt 7665_7.txt 9915_8.txt 12165_10.txt 3165_10.txt 5415_10.txt 7666_7.txt 9916_7.txt 12166_10.txt 3166_9.txt 5416_9.txt 7667_7.txt 9917_10.txt 12167_10.txt 3167_7.txt 5417_10.txt 7668_10.txt 9918_10.txt 12168_10.txt 3168_7.txt 5418_10.txt 7669_8.txt 9919_9.txt 12169_7.txt 3169_8.txt 5419_10.txt 766_10.txt 991_7.txt 1216_10.txt 316_10.txt 541_7.txt 7670_8.txt 9920_7.txt 12170_8.txt 3170_9.txt 5420_7.txt 7671_10.txt 9921_8.txt 12171_7.txt 3171_9.txt 5421_7.txt 7672_10.txt 9922_10.txt 12172_7.txt 3172_9.txt 5422_9.txt 7673_7.txt 9923_7.txt 12173_8.txt 3173_8.txt 5423_9.txt 7674_10.txt 9924_9.txt 12174_7.txt 3174_9.txt 5424_10.txt 7675_8.txt 9925_7.txt 12175_7.txt 3175_8.txt 5425_10.txt 7676_9.txt 9926_7.txt 12176_8.txt 3176_9.txt 5426_10.txt 7677_10.txt 9927_9.txt 12177_7.txt 3177_9.txt 5427_10.txt 7678_10.txt 9928_7.txt 12178_7.txt 3178_8.txt 5428_10.txt 7679_10.txt 9929_7.txt 12179_8.txt 3179_8.txt 5429_10.txt 767_9.txt 992_7.txt 1217_10.txt 317_10.txt 542_9.txt 7680_8.txt 9930_8.txt 12180_10.txt 3180_8.txt 5430_10.txt 7681_9.txt 9931_9.txt 12181_8.txt 3181_10.txt 5431_10.txt 7682_8.txt 9932_8.txt 12182_8.txt 3182_8.txt 5432_7.txt 7683_10.txt 9933_8.txt 12183_10.txt 3183_9.txt 5433_10.txt 7684_7.txt 9934_8.txt 12184_10.txt 3184_8.txt 5434_9.txt 7685_10.txt 9935_7.txt 12185_9.txt 3185_10.txt 5435_10.txt 7686_10.txt 9936_10.txt 12186_9.txt 3186_8.txt 5436_7.txt 7687_8.txt 9937_10.txt 12187_9.txt 3187_7.txt 5437_10.txt 7688_10.txt 9938_9.txt 12188_10.txt 3188_8.txt 5438_10.txt 7689_10.txt 9939_10.txt 12189_7.txt 3189_8.txt 5439_10.txt 768_9.txt 993_8.txt 1218_8.txt 318_10.txt 543_10.txt 7690_9.txt 9940_9.txt 12190_7.txt 3190_7.txt 5440_10.txt 7691_9.txt 9941_7.txt 12191_7.txt 3191_8.txt 5441_10.txt 7692_7.txt 9942_7.txt 12192_7.txt 3192_10.txt 5442_8.txt 7693_9.txt 9943_7.txt 12193_10.txt 3193_9.txt 5443_8.txt 7694_10.txt 9944_9.txt 12194_10.txt 3194_9.txt 5444_8.txt 7695_9.txt 9945_8.txt 12195_8.txt 3195_10.txt 5445_10.txt 7696_8.txt 9946_9.txt 12196_10.txt 3196_10.txt 5446_9.txt 7697_9.txt 9947_10.txt 12197_9.txt 3197_10.txt 5447_10.txt 7698_10.txt 9948_8.txt 12198_10.txt 3198_10.txt 5448_10.txt 7699_10.txt 9949_9.txt 12199_9.txt 3199_10.txt 5449_8.txt 769_8.txt 994_7.txt 1219_10.txt 319_9.txt 544_8.txt 76_7.txt 9950_8.txt 121_10.txt 31_8.txt 5450_10.txt 7700_10.txt 9951_7.txt 12200_8.txt 3200_10.txt 5451_8.txt 7701_10.txt 9952_8.txt 12201_8.txt 3201_10.txt 5452_8.txt 7702_10.txt 9953_10.txt 12202_9.txt 3202_10.txt 5453_8.txt 7703_8.txt 9954_8.txt 12203_10.txt 3203_10.txt 5454_7.txt 7704_10.txt 9955_9.txt 12204_8.txt 3204_10.txt 5455_7.txt 7705_7.txt 9956_9.txt 12205_8.txt 3205_8.txt 5456_10.txt 7706_7.txt 9957_7.txt 12206_9.txt 3206_8.txt 5457_8.txt 7707_9.txt 9958_7.txt 12207_8.txt 3207_7.txt 5458_10.txt 7708_10.txt 9959_7.txt 12208_8.txt 3208_7.txt 5459_8.txt 7709_7.txt 995_9.txt 12209_7.txt 3209_8.txt 545_10.txt 770_10.txt 9960_7.txt 1220_9.txt 320_8.txt 5460_8.txt 7710_8.txt 9961_9.txt 12210_9.txt 3210_8.txt 5461_7.txt 7711_10.txt 9962_9.txt 12211_10.txt 3211_7.txt 5462_8.txt 7712_10.txt 9963_10.txt 12212_10.txt 3212_7.txt 5463_9.txt 7713_10.txt 9964_10.txt 12213_10.txt 3213_10.txt 5464_7.txt 7714_10.txt 9965_10.txt 12214_10.txt 3214_10.txt 5465_8.txt 7715_8.txt 9966_10.txt 12215_10.txt 3215_10.txt 5466_8.txt 7716_9.txt 9967_10.txt 12216_8.txt 3216_8.txt 5467_9.txt 7717_10.txt 9968_9.txt 12217_10.txt 3217_8.txt 5468_7.txt 7718_10.txt 9969_10.txt 12218_7.txt 3218_9.txt 5469_7.txt 7719_9.txt 996_9.txt 12219_10.txt 3219_10.txt 546_10.txt 771_7.txt 9970_10.txt 1221_7.txt 321_10.txt 5470_10.txt 7720_8.txt 9971_10.txt 12220_8.txt 3220_10.txt 5471_8.txt 7721_7.txt 9972_10.txt 12221_8.txt 3221_10.txt 5472_7.txt 7722_10.txt 9973_8.txt 12222_10.txt 3222_7.txt 5473_8.txt 7723_10.txt 9974_8.txt 12223_10.txt 3223_8.txt 5474_9.txt 7724_10.txt 9975_10.txt 12224_9.txt 3224_7.txt 5475_8.txt 7725_10.txt 9976_7.txt 12225_7.txt 3225_10.txt 5476_8.txt 7726_10.txt 9977_7.txt 12226_8.txt 3226_10.txt 5477_7.txt 7727_9.txt 9978_8.txt 12227_7.txt 3227_7.txt 5478_9.txt 7728_7.txt 9979_7.txt 12228_8.txt 3228_7.txt 5479_10.txt 7729_7.txt 997_7.txt 12229_9.txt 3229_7.txt 547_10.txt 772_10.txt 9980_8.txt 1222_10.txt 322_10.txt 5480_10.txt 7730_7.txt 9981_7.txt 12230_9.txt 3230_8.txt 5481_10.txt 7731_9.txt 9982_9.txt 12231_10.txt 3231_10.txt 5482_10.txt 7732_8.txt 9983_7.txt 12232_8.txt 3232_9.txt 5483_10.txt 7733_9.txt 9984_9.txt 12233_8.txt 3233_10.txt 5484_10.txt 7734_10.txt 9985_9.txt 12234_8.txt 3234_9.txt 5485_10.txt 7735_10.txt 9986_9.txt 12235_7.txt 3235_9.txt 5486_10.txt 7736_10.txt 9987_9.txt 12236_10.txt 3236_7.txt 5487_10.txt 7737_10.txt 9988_8.txt 12237_10.txt 3237_8.txt 5488_10.txt 7738_8.txt 9989_9.txt 12238_8.txt 3238_9.txt 5489_10.txt 7739_10.txt 998_7.txt 12239_10.txt 3239_10.txt 548_7.txt 773_7.txt 9990_8.txt 1223_7.txt 323_10.txt 5490_9.txt 7740_10.txt 9991_10.txt 12240_8.txt 3240_10.txt 5491_10.txt 7741_9.txt 9992_10.txt 12241_10.txt 3241_8.txt 5492_10.txt 7742_8.txt 9993_10.txt 12242_10.txt 3242_8.txt 5493_10.txt 7743_8.txt 9994_10.txt 12243_8.txt 3243_8.txt 5494_10.txt 7744_10.txt 9995_10.txt 12244_7.txt 3244_10.txt 5495_8.txt 7745_8.txt 9996_9.txt 12245_10.txt 3245_10.txt 5496_9.txt 7746_10.txt 9997_7.txt 12246_9.txt 3246_9.txt 5497_9.txt 7747_10.txt 9998_9.txt 12247_8.txt 3247_10.txt 5498_7.txt 7748_9.txt 9999_8.txt 12248_7.txt 3248_10.txt 5499_10.txt 7749_8.txt 999_10.txt 12249_8.txt 3249_9.txt 549_9.txt 774_8.txt 99_8.txt 1224_9.txt 324_8.txt 54_10.txt 7750_8.txt 9_7.txt cat 0_9.txt Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't! # pip install pyprind import pyprind # module for Python Progress Indicator import pandas as pd import os labels = {'pos':1, 'neg':0} # 1 = positive and 0 = negative pbar = pyprind.ProgBar(50000) df = pd.DataFrame() for s in ('test', 'train'): for l in ('pos', 'neg'): path ='data/aclImdb/%s/%s' % (s, l) for file in os.listdir(path): with open(os.path.join(path, file), 'r') as infile: txt = infile.read() df = df.append([[txt, labels[l]]], ignore_index=True) pbar.update() df.columns = ['review', 'sentiment'] 0% 100% [##############################] | ETA: 00:00:00 Total time elapsed: 00:01:41 df.head(3) review sentiment 0 I went and saw this movie last night after bei... 1 1 Actor turned director Bill Paxton follows up h... 1 2 As a recreational golfer with some knowledge o... 1 Shuffling the DataFrame: import numpy as np np.random.seed(0) df = df.reindex(np.random.permutation(df.index)) df.head(3) review sentiment 11841 In 1974, the teenager Martha Moxley (Maggie Gr... 1 19602 OK... so... I really like Kris Kristofferson a... 0 45519 ***SPOILER*** Do not read this, if you think a... 0 Optional: Saving the assembled data as CSV file: df.to_csv('./data/movie_data.csv', index=False, encoding='utf-8') import pandas as pd df = pd.read_csv('./data/movie_data.csv', encoding='utf-8') df.head(3) review sentiment 0 In 1974, the teenager Martha Moxley (Maggie Gr... 1 1 OK... so... I really like Kris Kristofferson a... 0 2 ***SPOILER*** Do not read this, if you think a... 0 Text feature extraction [ back to top ] Bag-of-words model [ back to top ] Free text with variables length is very far from the fixed length numeric representation that we need to do machine learning with scikit-learn. However, there is an easy and effective way to go from text data to a numeric representation using the so-called bag-of-words model , which provides a data structure that is compatible with the machine learning aglorithms in scikit-learn. import numpy as np from sklearn.feature_extraction.text import CountVectorizer count = CountVectorizer() docs = np.array([ 'The sun is shining', 'The weather is sweet', 'The sun is shining, the weather is sweet, and one and one is two']) bag = count.fit_transform(docs) count.vocabulary_ {u'and': 0, u'is': 1, u'one': 2, u'shining': 3, u'sun': 4, u'sweet': 5, u'the': 6, u'two': 7, u'weather': 8} count.get_feature_names() [u'and', u'is', u'one', u'shining', u'sun', u'sweet', u'the', u'two', u'weather'] As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words that are mapped to integer indices. Next let us print the feature vectors that we just created: bag.toarray() # \u6bcf\u884c\u5bf9\u5e94\u7740\u4e00\u4e2a document\uff0c \u6bcf\u5217\u5bf9\u5e94\u4e00\u4e2a word\uff0c \u503c\u662f word \u5bf9\u5e94\u7684\u8ba1\u6570 array([[0, 1, 0, 1, 1, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 1], [2, 3, 2, 1, 1, 1, 2, 1, 1]]) count.inverse_transform(bag) [array([u'the', u'sun', u'is', u'shining'], dtype=' U7'), array([u'the', u'is', u'weather', u'sweet'], dtype=' U7'), array([u'the', u'sun', u'is', u'shining', u'weather', u'sweet', u'and', u'one', u'two'], dtype=' U7')] Bigrams and N-Grams [ back to top ] In last section, we used the so-called 1-gram (unigram) tokenization: Each token represents a single element with regard to the splittling criterion. Entirely discarding word order is not always a good idea, as composite phrases often have specific meaning, and modifiers like \"not\" can invert the meaning of words. A simple way to include some word order are n-grams, which don't only look at a single token, but at all pairs of neighborhing tokens. For example, in 2-gram (bigram) tokenization, we would group words together with an overlap of one word; in 3-gram (trigram) splits we would create an overlap two words, and so forth: original text: \"this is how you get ants\" 1-gram: \"this\", \"is\", \"how\", \"you\", \"get\", \"ants\" 2-gram: \"this is\", \"is how\", \"how you\", \"you get\", \"get ants\" 3-gram: \"this is how\", \"is how you\", \"how you get\", \"you get ants\" Which \"n\" we choose for \"n-gram\" tokenization to obtain the optimal performance in our predictive model depends on the learning algorithm, dataset, and task. Or in other words, we have consider \"n\" in \"n-grams\" as a tuning parameters. The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. While a 1-gram representation is used by default, we could switch to a 2-gram representation by initializing a new CountVectorizer instance with ngram_range=(2,2) bigram_vectorizer = CountVectorizer(ngram_range=(2,2)) bigram_vectorizer.fit_transform(docs).toarray() array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) bigram_vectorizer.vocabulary_ {u'and one': 0, u'is shining': 1, u'is sweet': 2, u'is two': 3, u'one and': 4, u'one is': 5, u'shining the': 6, u'sun is': 7, u'sweet and': 8, u'the sun': 9, u'the weather': 10, u'weather is': 11} Character n-grams [ back to top ] Sometimes it is also helpful not only to look at words, but to consider single characters instead. That is particularly useful if we have very noisy data and want to identify the language, or if we want to predict something about a single word. We can simply look at characters instead of words by setting analyzer=\"char\" . X = ['Some say the world will end in fire,', 'Some say in ice.'] char_vectorizer = CountVectorizer(analyzer= char ) char_vectorizer.fit(X) CountVectorizer(analyzer='char', binary=False, decode_error=u'strict', dtype= type 'numpy.int64' , encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0, max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None, stop_words=None, strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None) print(char_vectorizer.get_feature_names()) [u' ', u',', u'.', u'a', u'c', u'd', u'e', u'f', u'h', u'i', u'l', u'm', u'n', u'o', u'r', u's', u't', u'w', u'y'] Tfidf encoding [ back to top ] np.set_printoptions(precision=2) When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweight those frequently occurring words in the feature vectors. The tf-idf can be de ned as the product of the term frequency and the inverse document frequency: $$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$ Here the tf(t, d) is the term frequency that we introduced in the previous section, and the inverse document frequency idf(t, d) can be calculated as: $$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$ where $$n_d$$ is the total number of documents, and df(d, t) is the number of documents d that contain the term t . Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight. Scikit-learn implements yet another transformer, the TfidfTransformer , that takes the raw term frequencies from CountVectorizer as input and transforms them into tf-idfs: from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True) print(tfidf.fit_transform(count.fit_transform(docs)).toarray()) [[ 0. 0.43 0. 0.56 0.56 0. 0.43 0. 0. ] [ 0. 0.43 0. 0. 0. 0.56 0.43 0. 0.56] [ 0.5 0.45 0.5 0.19 0.19 0.19 0.3 0.25 0.19]] As we saw in the previous subsection, the word \"is\" (column 2) had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, we see that the word \"is\" is now associated with a relatively small tf-idf (0.45) in document 3 since it is also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information. However, if we'd manually calculated the tf-idfs of the individual terms in our feature vectors, we'd have noticed that the TfidfTransformer calculates the tf-idfs slightly differently compared to the standard textbook equations that we see earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are: $$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$ The tf-idf equation that was implemented in scikit-learn is as follows: $$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$ While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the TfidfTransformer normalizes the tf-idfs directly. By default ( norm='l2' ), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector v by its L2-norm: $$v_{\\text{norm}} = \\frac{v}{||v|| 2} = \\frac{v}{\\sqrt{v {1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$ To make sure that we understand how TfidfTransformer works, let us walk through an example and calculate the tf-idf of the word is in the 3rd document. The word is has a term frequency of 3 (tf = 3) in document 3, and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows: $$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$ Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency: $$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$ tf_is = 3 n_docs = 3 idf_is = np.log((n_docs+1) / (3+1)) tfidf_is = tf_is * (idf_is + 1) print('tf-idf of term is = %.2f' % tfidf_is) tf-idf of term \"is\" = 3.00 If we repeated these calculations for all terms in the 3rd document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. However, we notice that the values in this feature vector are different from the values that we obtained from the TfidfTransformer that we used previously. The step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows: $$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$ $$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$ $$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$ tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True) raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1] raw_tfidf array([ 3.39, 3. , 3.39, 1.29, 1.29, 1.29, 2. , 1.69, 1.29]) l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2)) l2_tfidf array([ 0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19]) As we can see, the results match the results returned by scikit-learn's TfidfTransformer (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset. Cleaning text data [ back to top ] df.loc[0, 'review'][-50:] # last 50 characters from the first document u'is seven. br / br / Title (Brazil): Not Available' text contains html markup tags, we need clean them. we will now remove all punctuation marks but only keep emoticon characters such as \":)\" import re # Python \u4e2d\u5904\u7406\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u6a21\u5757 def preprocessor(text): text = re.sub(r' [^ ]* ', '', text) # \u53bb\u9664 HTML \u6807\u7b7e emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) text = re.sub(r'\\W+', ' ', text.lower()) + \\ ' '.join(emoticons).replace('-', '') # \u5c06\u8868\u60c5\u7b26\u53f7\u62fc\u63a5\u5728\u6b63\u6587\u540e\u9762 return text preprocessor(df.loc[0, 'review'][-50:]) u'is seven title brazil not available' preprocessor( /a This :) is :( a test :-)! ) 'this is a test :) :( :)' df['review'] = df['review'].apply(preprocessor) Processing documents into tokens [ back to top ] split the text corpora into individual elements. tokenize \u662f\u628a\u957f\u6587\u672c\u5207\u6210\u4e00\u7cfb\u5217\u5355\u8bcd\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u4ee5 whitespace \u5207\u5206. word stemming \u662f\u5c06\u8bcd\u8f6c\u4e3a\u6700\u539f\u59cb\u7684\u5f62\u5f0f, root form, (\u4f8b\u5982 running - run), \u4e00\u79cd\u7b97\u6cd5\u662f Porter stemmer algorithm \u4ee5\u4e0b\u9700\u8981\u4f7f\u7528 nltk, \u9700\u8981\u5148\u5b89\u88c5: pip install nltk from nltk.stem.porter import PorterStemmer porter = PorterStemmer() def tokenizer(text): return text.split() # \u9ed8\u8ba4\u4ee5\u7a7a\u683c\u5207\u5206 def tokenizer_porter(text): return [porter.stem(word) for word in text.split()] tokenizer('runners like running and thus they run') ['runners', 'like', 'running', 'and', 'thus', 'they', 'run'] tokenizer_porter('runners like running and thus they run') [u'runner', u'like', u'run', u'and', u'thu', u'they', u'run'] Stop-words (\u505c\u8bcd) \u662f \u6700\u5e38\u89c1\u7684\u4e00\u4e9b\u5355\u8bcd, \u5b83\u4eec\u7684\u5b9e\u9645\u610f\u4e49\u5e76\u4e0d\u662f\u5f88\u5927, \u5927\u591a\u662f\u8d77\u8f85\u52a9\u4f5c\u7528\u7684, \u4f46\u662f\u5b83\u4eec\u7684\u9891\u6b21\u975e\u5e38\u9ad8, \u6240\u4ee5\u9700\u8981\u53bb\u9664, \u4f8b\u5982 is, and, has import nltk nltk.download('stopwords') # \u4e0b\u8f7d\u505c\u8bcd [nltk_data] Downloading package stopwords to /Users/alan/nltk_data... [nltk_data] Package stopwords is already up-to-date! True from nltk.corpus import stopwords stop = stopwords.words('english') [w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop] [u'runner', u'like', u'run', u'run', u'lot'] Training a logistic regression model for sentiment classification [ back to top ] Strip HTML and punctuation to speed up the GridSearch later: # 25,000 documents for training and 25,000 documents for testing, \u9700\u8981\u5927\u7ea640\u5206\u949f # \u6240\u4ee5\u5148\u4f7f\u75285000 documents X_train = df.loc[:5000, 'review'].values y_train = df.loc[:5000, 'sentiment'].values X_test = df.loc[5000:, 'review'].values y_test = df.loc[5000:, 'sentiment'].values from sklearn.grid_search import GridSearchCV from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import TfidfVectorizer # TfidfVectorizer \u7b49\u540c\u4e8e CountVectorizer + TfidfTransformer tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None) # grig search param_grid = [{'vect__ngram_range': [(1,1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, {'vect__ngram_range': [(1,1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'vect__use_idf':[False], 'vect__norm':[None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, ] # \u5148\u8f6c\u5316\u4e3a tfidf matrix lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))]) gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1) # \u6570\u636e\u91cf\u51cf\u5c11\u4e3a5000 \u540e\u9700\u8981\u65f6\u95f4\u5927\u7ea6\u4e3a8\u5206\u949f gs_lr_tfidf.fit(X_train, y_train) Fitting 5 folds for each of 48 candidates, totalling 240 fits [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 1.4min [Parallel(n_jobs=-1)]: Done 184 tasks | elapsed: 8.0min [Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 10.5min finished GridSearchCV(cv=5, error_score='raise', estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict', dtype= type 'numpy.int64' , encoding=u'utf-8', input=u'content', lowercase=False, max_df=1.0, max_features=None, min_df=1, ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=Tru...nalty='l2', random_state=0, solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]), fit_params={}, iid=True, n_jobs=-1, param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__tokenizer': [ function tokenizer at 0x130dca410 , function tokenizer_porter at 0x130dca488 ], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'y...x130dca488 ], 'vect__use_idf': [False], 'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2']}], pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1) print('Best parameter set: %s ' % gs_lr_tfidf.best_params_) print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_) Best parameter set: {'vect__ngram_range': (1, 1), 'vect__tokenizer': function tokenizer at 0x130dca410 , 'clf__penalty': 'l2', 'clf__C': 10.0, 'vect__stop_words': [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']} CV Accuracy: 0.862 clf = gs_lr_tfidf.best_estimator_ print('Test Accuracy: %.3f' % clf.score(X_test, y_test)) Test Accuracy: 0.873 Working with bigger data - online algorithms and out-of-core learning [ back to top ] Out-of-Core learning is the task of training a machine learning model on a dataset that does not fit into memory or RAM. This requires the following conditions: a feature extraction layer with fixed output dimensionality knowing the list of all classes in advance (in this case we only have positive and negative tweets) a machine learning algorithm that supports incremental learning (the partial_fit method in scikit-learn). import numpy as np import re from nltk.corpus import stopwords stop = stopwords.words('english') def tokenizer(text): text = re.sub(' [^ ]* ', '', text) emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()) text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '') tokenized = [w for w in text.split() if w not in stop] return tokenized def stream_docs(path): # \u751f\u6210\u5668 # generator funciton, reads in and returns one document at a time with open(path, 'r') as csv: next(csv) # skip header for line in csv: text, label = line[:-3], int(line[-2]) yield text, label # To verify that our stream_docs function works correctly gen = stream_docs(path='./data/movie_data.csv') next(gen) ('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder. br / br / \"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven. br / br / Title (Brazil): Not Available\"', 1) def get_minibatch(doc_stream, size): ''' take a document stream from the stream_docs function and return a particular number of documents ''' docs, y = [], [] try: for _ in range(size): text, label = next(doc_stream) docs.append(text) y.append(label) except StopIteration: return None, None return docs, y from sklearn.feature_extraction.text import HashingVectorizer # makes use of the Hashing trick from sklearn.linear_model import SGDClassifier # train a logistic regression model using small minibatches of documents vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer) clf = SGDClassifier(loss='log', random_state=1, n_iter=1) doc_stream = stream_docs(path='./data/movie_data.csv') # iterated over 45 minibatches of documents # where each minibatch consists of 1,000 documents each import pyprind pbar = pyprind.ProgBar(45) classes = np.array([0, 1]) for _ in range(45): X_train, y_train = get_minibatch(doc_stream, size=1000) if not X_train: break X_train = vect.transform(X_train) clf.partial_fit(X_train, y_train, classes=classes) pbar.update() 0% 100% [##############################] | ETA: 00:00:00 Total time elapsed: 00:00:40 #use the last 5,000 documents to evaluate the performance X_test, y_test = get_minibatch(doc_stream, size=5000) X_test = vect.transform(X_test) print('Accuracy: %.3f' % clf.score(X_test, y_test)) Accuracy: 0.867 \u867d\u7136\u51c6\u786e\u7387\u7565\u4f4e\u4e8e\u524d\u9762\uff0c\u4f46\u8bad\u7ec3\u901f\u5ea6\u5feb\u4e86\u5f88\u591a\uff0c\u800c\u4e14\u4f7f\u7528\u7684\u5185\u5b58\u66f4\u5c11 # use the last 5,000 documents to update our model # \u53ef\u4ee5\u4f7f\u7528 partial_fit \u7ee7\u7eed\u8bad\u7ec3 clf = clf.partial_fit(X_test, y_test) Model persistence \u8bad\u7ec3\u6a21\u578b\u662f expensive \u5e76\u4e14\u8017\u8d39\u65f6\u95f4\u7684, \u6211\u4eec\u4e0d\u5e0c\u671b\u5728\u5e94\u7528\u4e2d\u6bcf\u6b21\u90fd\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b, \u6240\u4ee5\u6211\u4eec\u9700\u8981\u4fdd\u5b58\u6a21\u578b, \u5e76\u4e14\u80fd\u8fdb\u884c\u65b0\u7684\u9884\u6d4b\u4ee5\u53ca\u66f4\u65b0\u3002 \u53ef\u4ee5\u7528\u5230 pickle \u6a21\u5757\u6765\u50a8\u5b58\u6a21\u578b, \u5c06 python object \u50a8\u5b58\u4e3abyte code, \u53ef\u4ee5\u8bfb\u53d6\u4e5f\u53ef\u4ee5\u5199\u5165 [ back to top ] After we trained the logistic regression model as shown above, we can save the classifier along with the stop words, Porter Stemmer, and HashingVectorizer as serialized objects to our local disk so that we can use the fitted classifier in our web application later. import pickle import os # created a movieclassifier directory # created a pkl_objects subdirectory to save the serialized Python objects to our local drive dest = os.path.join('movieclassifier', 'pkl_objects') if not os.path.exists(dest): os.makedirs(dest) # \u5199\u5165 pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=2) pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=2) Next, we save the HashingVectorizer as in a separate file so that we can import it later. %%writefile movieclassifier/vectorizer.py from sklearn.feature_extraction.text import HashingVectorizer import re import os import pickle cur_dir = os.path.dirname(__file__) stop = pickle.load(open( os.path.join(cur_dir, 'pkl_objects', 'stopwords.pkl'), 'rb')) def tokenizer(text): text = re.sub(' [^ ]* ', '', text) emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()) text = re.sub('[\\W]+', ' ', text.lower()) \\ + ' '.join(emoticons).replace('-', '') tokenized = [w for w in text.split() if w not in stop] return tokenized vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer) Overwriting movieclassifier/vectorizer.py After executing the preceeding code cells, we can now restart the IPython notebook kernel to check if the objects were serialized correctly. First, change the current Python directory to movieclassifer : import os os.chdir('movieclassifier') import pickle import re import os from vectorizer import vect clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb')) import numpy as np label = {0:'negative', 1:'positive'} example = ['I love this movie'] X = vect.transform(example) print('Prediction: %s\\nProbability: %.2f%%' %\\ (label[clf.predict(X)[0]], np.max(clf.predict_proba(X))*100)) Prediction: positive Probability: 82.53% word2vec [ back to top ] word2vec \u662f Mikolov et al. \u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u65b9\u6cd5\u3002\u5b83\u6709 Continuous Bag-of-Words model (CBOW) and the Skip-Gram model \u4e24\u79cd\u53d8\u5f0f\uff0c\u524d\u8005\u662f\u7528\u4e00\u4e2a\u8bcd\u5e8f\u5217\u7a97\u53e3\u4e2d\u7684\u5176\u4ed6\u8bcd\u6765\u9884\u6d4b\u4e2d\u5fc3\u8bcd\uff0c\u540e\u8005\u5219\u662f\u7528\u4e2d\u5fc3\u8bcd\u6765\u9884\u6d4b\u5176\u4ed6\u8bcd\u3002\u5728\u5b9e\u9645\u4f7f\u7528 word2vec \u65f6\uff0c\u4e00\u822c\u4f7f\u7528 Skip-Gram model \u7ed3\u5408 Negative Sampling \u8fdb\u884c\u8bad\u7ec3\u3002 # gensim \u5e93\u4e2d\u5305\u542b\u4e86 word2vec \u6a21\u5757 from gensim.models.word2vec import Word2Vec \u8bad\u7ec3\u8bcd\u5411\u91cf\uff0c gensim \u8bad\u7ec3\u8bcd\u5411\u91cf\u65f6\u53ef\u4ee5\u5582\u5165\u4e00\u4e2a iterator class MySentence: def __init__(self, data): self.data = data def __iter__(self): for line in self.data: yield line.lower().split() train_corpus = MySentence(df['review']) # \u6216\u8005 train_corpus = [s.split() for s in df['review']] model = Word2Vec(train_corpus, size=200, # \u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6 iter=20, # \u6570\u636e\u5728\u8bad\u7ec3\u4e2d\u7528\u5230\u7684\u6b21\u6570, \u5373 epoch \u6570 workers=4) # \u8c03\u7528\u7684\u8fdb\u7a0b\u6570 \u7b80\u5355\u67e5\u770b\u8bcd\u5411\u91cf\u7684\u7ed3\u679c model.most_similar('good') [(u'decent', 0.7367605566978455), (u'bad', 0.7155213356018066), (u'great', 0.7067341804504395), (u'nice', 0.6252745389938354), (u'cool', 0.5865251421928406), (u'passable', 0.5830472707748413), (u'funny', 0.5790721774101257), (u'fine', 0.5767843127250671), (u'lousy', 0.5747220516204834), (u'terrible', 0.5679782629013062)] \u83b7\u5f97\u8bcd\u5bf9\u5e94\u7684\u8bcd\u5411\u91cf model['good'] array([-1.80578566, 2.59199047, 1.00538623, 0.00376823, -1.63011074, 1.19826996, -0.05748612, 0.34942579, -3.64253092, 0.77269369, -0.12568648, 0.63311213, -0.29951221, 1.00349665, 0.57139468, 0.26391807, -2.15211248, 1.37436974, 0.72486091, 1.39909697, 1.39214003, -0.55878437, 1.28609359, 1.4021709 , -0.61972415, 1.40909946, 1.23801959, 0.12476239, -2.10969472, -0.16599979, 1.07056391, -0.9283669 , -1.24553549, -1.00224996, 0.85817921, -2.98209238, 2.52684212, -0.84701031, -1.30793285, -3.52102351, 2.51759624, 1.55442834, -0.81624299, 0.80133152, 0.76279849, -1.61152196, 0.84180117, 1.95387816, 0.39838204, -2.50793004, -1.3055681 , 2.4016645 , 2.7259481 , 1.65427065, 2.08329916, -0.67130673, -2.77032781, -0.86488032, -0.83239168, 0.70329827, 0.1054525 , -0.48466596, 1.93771338, 0.57588094, -0.16703451, 2.06361747, 0.90677142, 1.05519104, 1.84931016, 0.77389133, 2.08386183, -2.16565847, 1.20824552, 0.05443871, 3.20145893, -2.12376046, -0.46623436, 1.19470978, -0.60041159, -2.01585841, -1.14179373, 0.25336841, -2.7429111 , 0.19020027, 0.77245057, 1.07972777, 3.45747972, -1.17416704, 0.14808762, -0.04948059, 2.25656533, -1.87219381, -2.38383865, -2.15413165, -0.18855318, -1.33702457, -0.72171617, -0.28952792, 0.9959411 , 2.43245673, -0.94892198, 1.32803464, 3.27445745, 0.44902089, 1.76025999, -0.15520753, -0.42482886, 0.22728981, -0.93469167, 0.87159711, 1.20421445, 1.03957427, 0.56455994, 0.19218144, -1.84992611, 1.75870144, -2.13160086, 0.99597716, -1.92741001, 1.47426069, -2.23345065, -0.38542071, -0.33888757, -0.50440568, -1.29633522, 1.55562842, 1.08055615, 1.2215786 , -1.75132465, -0.11741698, 0.77109945, -0.68743432, -0.31800482, 0.23696584, 0.77463788, -1.75956166, 1.77490389, 0.07634074, 2.27028704, 0.66218233, 3.3049078 , 3.30950427, -1.33130598, -0.92713892, 0.15286015, 0.287049 , 0.42833641, -1.46425474, 0.9658314 , 0.40959406, 0.47199422, -0.95312113, -3.23242712, -0.08304592, -1.2138108 , 0.30118775, -0.96748334, -0.68403792, -2.61612749, -0.97091049, 0.03540362, -1.22498131, 0.23248808, 2.79292464, 1.39140284, 0.35824764, 0.53603202, 2.08097696, 1.61062503, -1.45209515, -0.87486774, 0.6024124 , 1.00253165, -0.25757971, 3.5850575 , 0.37494364, 1.26284862, -1.56030715, -2.38174224, -2.99708772, 0.75272441, 0.95936358, 0.24002089, 1.7718761 , 1.28533518, -2.32952619, 2.59281707, -3.69943428, 0.34991279, -0.52845728, 1.35607052, 0.67054856, -1.21081388, 1.81272793, 0.64519709, -0.4952068 , 2.65413833, 1.53806341, -2.95830393, 2.38203692], dtype=float32) \u5b58\u50a8/\u8bfb\u53d6\u6a21\u578b model.save('data/imdb.d2v') model = Word2Vec.load('data/imdb.d2v') \u5229\u7528\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\u6765\u8fdb\u884c\u60c5\u611f\u5206\u6790 # \u7528 word vec \u7684\u5747\u503c\u4f5c\u4e3a doc vec def get_doc_vec(sentence, model): scores = [model[word] for word in sentence.split() if word in model] # \u5982\u679c\u8bcd\u9891\u5c0f\u4e8e min_count, word2vec \u4e0d\u4f1a\u628a\u8fd9\u4e2a\u8bcd\u653e\u5165 vocab \u91cc return np.mean(scores, axis=0) X_word2vec_train = np.array([get_doc_vec(sentence, model) for sentence in X_train]) X_word2vec_test = np.array([get_doc_vec(sentence, model) for sentence in X_test]) from sklearn.grid_search import GridSearchCV from sklearn.linear_model import LogisticRegression lr = LogisticRegression(random_state=0) param_grid = [{'penalty': ['l1', 'l2'], 'C': [1.0, 10.0, 100.0]}] gs = GridSearchCV(lr, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1) gs.fit(X_word2vec_train, y_train); Fitting 5 folds for each of 6 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 14.5s finished gs.best_params_ {'C': 1.0, 'penalty': 'l2'} gs.best_score_ 0.85642871425714862 clf = gs.best_estimator_ print('Test Accuracy: %.3f' % clf.score(X_word2vec_test, y_test)) Test Accuracy: 0.869 \u4e2d\u6587\u65b0\u95fb\u5206\u7c7b from os import path import os import re import pandas as pd import numpy as np rootdir = 'data/SogouC.reduced/Reduced' dirs = os.listdir(rootdir) dirs = [path.join(rootdir,f) for f in dirs if f.startswith('C')] dirs ['data/SogouC.reduced/Reduced/C000008', 'data/SogouC.reduced/Reduced/C000010', 'data/SogouC.reduced/Reduced/C000013', 'data/SogouC.reduced/Reduced/C000014', 'data/SogouC.reduced/Reduced/C000016', 'data/SogouC.reduced/Reduced/C000020', 'data/SogouC.reduced/Reduced/C000022', 'data/SogouC.reduced/Reduced/C000023', 'data/SogouC.reduced/Reduced/C000024'] def load_txt(x): with open(x) as f: res = [t.decode('gbk','ignore') for t in f] return ''.join(res) text_t = {} for i, d in enumerate(dirs): files = os.listdir(d) files = [path.join(d, x) for x in files if x.endswith('txt') and not x.startswith('.')] text_t[i] = [load_txt(f) for f in files] print(text_t[0][0][:100]) \u3000\u3000\u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77 \u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f\u6210\u4e3a\u5e02\u503c100\u4ebf\u7f8e\u5143\u7684\u4e0a\u5e02\u516c\u53f8\u3002 \u3000\u3000\u8fd9\u5bb6\u516c\u53f8\u53eb\u505a\u5feb\u94b1\uff0c\u8bf4\u8fd9\u53e5\u8bdd\u7684\u662f\u5feb\u94b1\u7684CEO\u5173\u56fd\u5149\u3002\u4ed6\u4e4b\u524d\u66fe\u4efb\u7f51\u6613\u7684\u9ad8\u7ea7\u526f flen = [len(t) for t in text_t.values()] labels = np.repeat(text_t.keys(),flen) # flatter nested list import itertools merged = list(itertools.chain.from_iterable(text_t.values())) df = pd.DataFrame({'label': labels, 'txt': merged}) df.head() label txt 0 0 \u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77\\r\\n\u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f... 1 0 \u8bc1\u5238\u901a\uff1a\u767e\u8054\u80a1\u4efd\u672a\u67655\u5e74\u6709\u80fd\u529b\u4fdd\u6301\u9ad8\u901f\u589e\u957f\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2... 2 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 3 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 4 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... # cut word import jieba jieba.enable_parallel(4) def cutword_1(x): words = jieba.cut(x) return ' '.join(words) Building prefix dict from the default dictionary ... Loading model from cache /var/folders/y8/z6ws1f2907vbb33mcp8c63300000gn/T/jieba.cache Loading model cost 1.119 seconds. Prefix dict has been built succesfully. df['seg_word'] = df.txt.map(cutword_1) df.head() label txt seg_word 0 0 \u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77\\r\\n\u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f... \u672c\u62a5\u8bb0\u8005 \u9648\u96ea\u9891 \u5b9e\u4e60 \u8bb0\u8005 \u5510\u7fd4 \u53d1\u81ea \u4e0a\u6d77 \\r\\n \u3000 \u3000 \u4e00\u5bb6 \u521a\u521a \u6210\u7acb ... 1 0 \u8bc1\u5238\u901a\uff1a\u767e\u8054\u80a1\u4efd\u672a\u67655\u5e74\u6709\u80fd\u529b\u4fdd\u6301\u9ad8\u901f\u589e\u957f\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2... \u8bc1\u5238 \u901a \uff1a \u767e\u8054 \u80a1\u4efd \u672a\u6765 5 \u5e74 \u6709 \u80fd\u529b \u4fdd\u6301\u9ad8\u901f \u589e\u957f \\r\\n ... 2 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 3 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 4 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... from cPickle import dump,load dump(df, open('data/tmdf.pickle', 'wb')) # df = load(open('df.pickle','rb')) from sklearn.feature_extraction.text import TfidfVectorizer vect = TfidfVectorizer(ngram_range=(1,1), min_df = 2, max_features = 10000) xvec = vect.fit_transform(df.seg_word) xvec.shape (17910, 10000) y = df.label from sklearn.cross_validation import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import MultinomialNB from sklearn.ensemble import RandomForestClassifier train_X, test_X, train_y, test_y = train_test_split(xvec, y , train_size=0.7, random_state=1) clf = MultinomialNB() clf.fit(train_X, train_y) MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) from sklearn import metrics pre = clf.predict(test_X) print metrics.classification_report(test_y, pre) precision recall f1-score support 0 0.90 0.88 0.89 577 1 0.89 0.81 0.85 603 2 0.88 0.82 0.85 619 3 0.98 0.97 0.98 584 4 0.86 0.88 0.87 570 5 0.88 0.79 0.83 600 6 0.77 0.90 0.83 600 7 0.76 0.83 0.80 615 8 0.92 0.93 0.93 605 avg / total 0.87 0.87 0.87 5373 # word2vec txt = df.seg_word.values txtlist = [] for sent in txt: temp = [w for w in sent.split()] txtlist.append(temp) num_features = 100 min_word_count = 10 num_workers = 4 context = 5 epoch = 20 sample = 1e-5 from gensim.models import word2vec model = word2vec.Word2Vec(txtlist, workers = num_workers, sample = sample, size = num_features, min_count=min_word_count, window = context, iter = epoch) model.syn0.shape (57675, 100) for w in model.most_similar(u'\u4e92\u8054\u7f51'): print w[0], w[1] \u7f51\u7edc 0.787674069405 \u95e8\u6237\u7f51\u7ad9 0.747487425804 \u641c\u7d22\u5f15\u64ce 0.744884610176 \u65e0\u7ebf 0.732329308987 B2B 0.713720798492 \u7f51\u7edc\u5e7f\u544a 0.712735056877 \u817e\u8baf 0.702631175518 MSN 0.701346695423 \u5927\u65d7\u7f51 0.69608104229 Google 0.68867880106 #model.save('sogo_wv') #model = word2vec.Word2Vec.load('sogo_wv') # \u5c06\u8bcd\u5411\u91cf\u5e73\u5747\u5316\u4e3a\u6587\u6863\u5411\u91cf def sentvec_1(sent,m=num_features,model=model): res = np.zeros(m) words = sent.split() num = 0 for w in words: if w in model.index2word: res += model[w] num += 1.0 if num == 0: return np.zeros(m) else: return res/num n = df.shape[0] sent_matrix = np.zeros([n,num_features],float) for i ,sent in enumerate(df.seg_word.values): sent_matrix[i,:] = sentvec_1(sent) sent_matrix.shape (17910, 100) from sklearn.ensemble import GradientBoostingClassifier from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(sent_matrix, y , train_size=0.7, random_state=1) clf = GradientBoostingClassifier() clf.fit(train_X, train_y) from sklearn import metrics pre = clf.predict(test_X) print metrics.classification_report(test_y, pre) precision recall f1-score support 0 0.91 0.85 0.88 577 1 0.84 0.82 0.83 603 2 0.86 0.88 0.87 619 3 0.98 0.97 0.98 584 4 0.84 0.86 0.85 570 5 0.84 0.80 0.82 600 6 0.86 0.87 0.87 600 7 0.77 0.83 0.80 615 8 0.93 0.93 0.93 605 avg / total 0.87 0.87 0.87 5373","title":"7w"},{"location":"w7-text-mining/7w/#sections","text":"Obtaining the IMDb movie review dataset Text-feature-extraction Bag-of-words model Bigrams and N-Grams Character n-grams Tfidf encoding Cleaning text data Processing documents into tokens Training a logistic regression model for sentiment classification Working with bigger data - online algorithms and out-of-core learning Model persistence word2vec","title":"Sections"},{"location":"w7-text-mining/7w/#obtaining-the-imdb-movie-review-dataset","text":"[ back to top ] \u6570\u636e\u53ef\u4ece \u8fd9 \u4e0b\u8f7d \u89e3\u538b\u4e4b\u540e\uff0c\u4e0b\u9762\u4ee3\u7801\u53ef\u5c06\u6570\u636e\u8bfb\u6210 Pandas \u7684 DataFrame cd Documents/shanghai_python/data/aclImdb/ /Users/xiaokai/Documents/shanghai_python/data/aclImdb ls README imdb.vocab imdbEr.txt \u001b[34mtest\u001b[m\u001b[m/ \u001b[34mtrain\u001b[m\u001b[m/ cd train/pos/ /Users/xiaokai/Documents/shanghai_python/data/aclImdb/train/pos ls 0_9.txt 12250_10.txt 3250_9.txt 5500_10.txt 7751_7.txt 10000_8.txt 12251_9.txt 3251_7.txt 5501_10.txt 7752_8.txt 10001_10.txt 12252_10.txt 3252_9.txt 5502_10.txt 7753_10.txt 10002_7.txt 12253_9.txt 3253_8.txt 5503_10.txt 7754_9.txt 10003_8.txt 12254_10.txt 3254_9.txt 5504_10.txt 7755_9.txt 10004_8.txt 12255_10.txt 3255_10.txt 5505_10.txt 7756_10.txt 10005_7.txt 12256_9.txt 3256_9.txt 5506_10.txt 7757_9.txt 10006_7.txt 12257_8.txt 3257_9.txt 5507_7.txt 7758_7.txt 10007_7.txt 12258_10.txt 3258_10.txt 5508_10.txt 7759_10.txt 10008_7.txt 12259_7.txt 3259_7.txt 5509_10.txt 775_7.txt 10009_9.txt 1225_10.txt 325_9.txt 550_10.txt 7760_10.txt 1000_8.txt 12260_10.txt 3260_8.txt 5510_10.txt 7761_10.txt 10010_7.txt 12261_10.txt 3261_9.txt 5511_8.txt 7762_8.txt 10011_9.txt 12262_7.txt 3262_8.txt 5512_7.txt 7763_8.txt 10012_8.txt 12263_10.txt 3263_7.txt 5513_7.txt 7764_9.txt 10013_7.txt 12264_9.txt 3264_8.txt 5514_9.txt 7765_7.txt 10014_8.txt 12265_10.txt 3265_10.txt 5515_7.txt 7766_7.txt 10015_8.txt 12266_9.txt 3266_7.txt 5516_7.txt 7767_8.txt 10016_8.txt 12267_8.txt 3267_8.txt 5517_9.txt 7768_8.txt 10017_9.txt 12268_7.txt 3268_8.txt 5518_8.txt 7769_7.txt 10018_8.txt 12269_8.txt 3269_8.txt 5519_9.txt 776_7.txt 10019_8.txt 1226_10.txt 326_10.txt 551_8.txt 7770_7.txt 1001_8.txt 12270_10.txt 3270_8.txt 5520_9.txt 7771_9.txt 10020_8.txt 12271_7.txt 3271_9.txt 5521_8.txt 7772_7.txt 10021_8.txt 12272_7.txt 3272_8.txt 5522_8.txt 7773_10.txt 10022_7.txt 12273_10.txt 3273_9.txt 5523_8.txt 7774_7.txt 10023_9.txt 12274_10.txt 3274_8.txt 5524_8.txt 7775_8.txt 10024_9.txt 12275_9.txt 3275_10.txt 5525_8.txt 7776_7.txt 10025_9.txt 12276_7.txt 3276_8.txt 5526_10.txt 7777_10.txt 10026_7.txt 12277_10.txt 3277_9.txt 5527_8.txt 7778_10.txt 10027_7.txt 12278_10.txt 3278_8.txt 5528_7.txt 7779_7.txt 10028_10.txt 12279_9.txt 3279_8.txt 5529_7.txt 777_7.txt 10029_10.txt 1227_8.txt 327_8.txt 552_8.txt 7780_8.txt 1002_7.txt 12280_9.txt 3280_10.txt 5530_8.txt 7781_8.txt 10030_10.txt 12281_9.txt 3281_10.txt 5531_10.txt 7782_7.txt 10031_10.txt 12282_8.txt 3282_8.txt 5532_10.txt 7783_7.txt 10032_10.txt 12283_8.txt 3283_8.txt 5533_7.txt 7784_7.txt 10033_10.txt 12284_7.txt 3284_10.txt 5534_7.txt 7785_9.txt 10034_8.txt 12285_10.txt 3285_10.txt 5535_8.txt 7786_8.txt 10035_9.txt 12286_7.txt 3286_10.txt 5536_9.txt 7787_10.txt 10036_8.txt 12287_10.txt 3287_9.txt 5537_10.txt 7788_10.txt 10037_9.txt 12288_9.txt 3288_10.txt 5538_7.txt 7789_10.txt 10038_10.txt 12289_9.txt 3289_10.txt 5539_10.txt 778_10.txt 10039_10.txt 1228_9.txt 328_10.txt 553_7.txt 7790_9.txt 1003_10.txt 12290_10.txt 3290_8.txt 5540_10.txt 7791_10.txt 10040_10.txt 12291_10.txt 3291_8.txt 5541_9.txt 7792_10.txt 10041_10.txt 12292_10.txt 3292_10.txt 5542_9.txt 7793_10.txt 10042_10.txt 12293_10.txt 3293_10.txt 5543_7.txt 7794_7.txt 10043_10.txt 12294_8.txt 3294_9.txt 5544_7.txt 7795_9.txt 10044_9.txt 12295_9.txt 3295_10.txt 5545_8.txt 7796_10.txt 10045_10.txt 12296_8.txt 3296_10.txt 5546_7.txt 7797_10.txt 10046_9.txt 12297_7.txt 3297_10.txt 5547_8.txt 7798_10.txt 10047_10.txt 12298_8.txt 3298_10.txt 5548_7.txt 7799_8.txt 10048_10.txt 12299_7.txt 3299_10.txt 5549_8.txt 779_10.txt 10049_8.txt 1229_8.txt 329_10.txt 554_7.txt 77_7.txt 1004_7.txt 122_9.txt 32_10.txt 5550_7.txt 7800_8.txt 10050_10.txt 12300_10.txt 3300_9.txt 5551_7.txt 7801_9.txt 10051_10.txt 12301_8.txt 3301_10.txt 5552_8.txt 7802_10.txt 10052_10.txt 12302_7.txt 3302_10.txt 5553_8.txt 7803_10.txt 10053_8.txt 12303_9.txt 3303_10.txt 5554_8.txt 7804_10.txt 10054_10.txt 12304_10.txt 3304_10.txt 5555_10.txt 7805_10.txt 10055_7.txt 12305_8.txt 3305_10.txt 5556_10.txt 7806_10.txt 10056_8.txt 12306_9.txt 3306_8.txt 5557_9.txt 7807_10.txt 10057_9.txt 12307_10.txt 3307_8.txt 5558_7.txt 7808_10.txt 10058_7.txt 12308_10.txt 3308_8.txt 5559_7.txt 7809_10.txt 10059_10.txt 12309_10.txt 3309_9.txt 555_8.txt 780_7.txt 1005_10.txt 1230_10.txt 330_10.txt 5560_7.txt 7810_7.txt 10060_9.txt 12310_10.txt 3310_8.txt 5561_8.txt 7811_7.txt 10061_8.txt 12311_9.txt 3311_10.txt 5562_10.txt 7812_8.txt 10062_10.txt 12312_10.txt 3312_8.txt 5563_8.txt 7813_10.txt 10063_9.txt 12313_10.txt 3313_10.txt 5564_10.txt 7814_10.txt 10064_10.txt 12314_10.txt 3314_10.txt 5565_8.txt 7815_8.txt 10065_9.txt 12315_9.txt 3315_10.txt 5566_9.txt 7816_9.txt 10066_10.txt 12316_10.txt 3316_10.txt 5567_7.txt 7817_10.txt 10067_9.txt 12317_10.txt 3317_10.txt 5568_8.txt 7818_9.txt 10068_8.txt 12318_9.txt 3318_9.txt 5569_7.txt 7819_9.txt 10069_8.txt 12319_8.txt 3319_9.txt 556_9.txt 781_9.txt 1006_8.txt 1231_7.txt 331_10.txt 5570_10.txt 7820_8.txt 10070_9.txt 12320_8.txt 3320_8.txt 5571_10.txt 7821_8.txt 10071_9.txt 12321_10.txt 3321_7.txt 5572_10.txt 7822_8.txt 10072_9.txt 12322_10.txt 3322_8.txt 5573_10.txt 7823_7.txt 10073_10.txt 12323_10.txt 3323_9.txt 5574_10.txt 7824_10.txt 10074_9.txt 12324_10.txt 3324_7.txt 5575_8.txt 7825_9.txt 10075_9.txt 12325_9.txt 3325_7.txt 5576_9.txt 7826_8.txt 10076_9.txt 12326_10.txt 3326_8.txt 5577_8.txt 7827_10.txt 10077_10.txt 12327_9.txt 3327_10.txt 5578_10.txt 7828_10.txt 10078_8.txt 12328_10.txt 3328_7.txt 5579_8.txt 7829_7.txt 10079_8.txt 12329_10.txt 3329_7.txt 557_9.txt 782_9.txt 1007_10.txt 1232_10.txt 332_10.txt 5580_7.txt 7830_10.txt 10080_10.txt 12330_7.txt 3330_7.txt 5581_7.txt 7831_8.txt 10081_9.txt 12331_8.txt 3331_8.txt 5582_9.txt 7832_7.txt 10082_10.txt 12332_8.txt 3332_8.txt 5583_8.txt 7833_8.txt 10083_7.txt 12333_9.txt 3333_8.txt 5584_8.txt 7834_8.txt 10084_10.txt 12334_7.txt 3334_9.txt 5585_7.txt 7835_9.txt 10085_10.txt 12335_9.txt 3335_8.txt 5586_8.txt 7836_8.txt 10086_7.txt 12336_9.txt 3336_7.txt 5587_7.txt 7837_10.txt 10087_10.txt 12337_7.txt 3337_9.txt 5588_10.txt 7838_8.txt 10088_10.txt 12338_10.txt 3338_7.txt 5589_7.txt 7839_10.txt 10089_7.txt 12339_10.txt 3339_7.txt 558_10.txt 783_10.txt 1008_10.txt 1233_7.txt 333_10.txt 5590_8.txt 7840_8.txt 10090_8.txt 12340_8.txt 3340_8.txt 5591_8.txt 7841_10.txt 10091_7.txt 12341_7.txt 3341_7.txt 5592_10.txt 7842_10.txt 10092_8.txt 12342_8.txt 3342_7.txt 5593_10.txt 7843_9.txt 10093_7.txt 12343_7.txt 3343_7.txt 5594_10.txt 7844_10.txt 10094_7.txt 12344_8.txt 3344_9.txt 5595_7.txt 7845_10.txt 10095_7.txt 12345_10.txt 3345_9.txt 5596_10.txt 7846_10.txt 10096_7.txt 12346_10.txt 3346_7.txt 5597_10.txt 7847_9.txt 10097_9.txt 12347_9.txt 3347_7.txt 5598_8.txt 7848_10.txt 10098_10.txt 12348_9.txt 3348_7.txt 5599_7.txt 7849_9.txt 10099_10.txt 12349_10.txt 3349_8.txt 559_8.txt 784_10.txt 1009_8.txt 1234_10.txt 334_10.txt 55_9.txt 7850_8.txt 100_7.txt 12350_9.txt 3350_7.txt 5600_8.txt 7851_8.txt 10100_10.txt 12351_8.txt 3351_8.txt 5601_8.txt 7852_9.txt 10101_8.txt 12352_8.txt 3352_9.txt 5602_10.txt 7853_9.txt 10102_7.txt 12353_9.txt 3353_8.txt 5603_7.txt 7854_10.txt 10103_8.txt 12354_10.txt 3354_10.txt 5604_8.txt 7855_8.txt 10104_10.txt 12355_10.txt 3355_10.txt 5605_7.txt 7856_10.txt 10105_8.txt 12356_8.txt 3356_10.txt 5606_8.txt 7857_10.txt 10106_8.txt 12357_7.txt 3357_9.txt 5607_7.txt 7858_10.txt 10107_8.txt 12358_7.txt 3358_9.txt 5608_9.txt 7859_7.txt 10108_10.txt 12359_8.txt 3359_10.txt 5609_10.txt 785_9.txt 10109_10.txt 1235_10.txt 335_10.txt 560_8.txt 7860_10.txt 1010_10.txt 12360_10.txt 3360_7.txt 5610_7.txt 7861_10.txt 10110_10.txt 12361_7.txt 3361_7.txt 5611_10.txt 7862_7.txt 10111_7.txt 12362_8.txt 3362_8.txt 5612_8.txt 7863_10.txt 10112_7.txt 12363_9.txt 3363_10.txt 5613_9.txt 7864_8.txt 10113_10.txt 12364_7.txt 3364_10.txt 5614_10.txt 7865_8.txt 10114_10.txt 12365_7.txt 3365_9.txt 5615_8.txt 7866_10.txt 10115_10.txt 12366_10.txt 3366_10.txt 5616_8.txt 7867_7.txt 10116_10.txt 12367_9.txt 3367_9.txt 5617_8.txt 7868_8.txt 10117_8.txt 12368_8.txt 3368_8.txt 5618_10.txt 7869_9.txt 10118_7.txt 12369_7.txt 3369_7.txt 5619_9.txt 786_7.txt 10119_7.txt 1236_7.txt 336_10.txt 561_10.txt 7870_10.txt 1011_10.txt 12370_8.txt 3370_9.txt 5620_10.txt 7871_9.txt 10120_7.txt 12371_8.txt 3371_8.txt 5621_10.txt 7872_10.txt 10121_8.txt 12372_7.txt 3372_9.txt 5622_10.txt 7873_8.txt 10122_7.txt 12373_7.txt 3373_7.txt 5623_10.txt 7874_8.txt 10123_10.txt 12374_7.txt 3374_7.txt 5624_7.txt 7875_7.txt 10124_8.txt 12375_7.txt 3375_9.txt 5625_8.txt 7876_8.txt 10125_8.txt 12376_7.txt 3376_9.txt 5626_8.txt 7877_7.txt 10126_10.txt 12377_10.txt 3377_8.txt 5627_10.txt 7878_7.txt 10127_8.txt 12378_8.txt 3378_7.txt 5628_7.txt 7879_7.txt 10128_9.txt 12379_8.txt 3379_7.txt 5629_10.txt 787_9.txt 10129_7.txt 1237_8.txt 337_9.txt 562_8.txt 7880_8.txt 1012_10.txt 12380_7.txt 3380_7.txt 5630_8.txt 7881_9.txt 10130_10.txt 12381_8.txt 3381_10.txt 5631_8.txt 7882_7.txt 10131_10.txt 12382_8.txt 3382_10.txt 5632_8.txt 7883_10.txt 10132_9.txt 12383_7.txt 3383_8.txt 5633_8.txt 7884_7.txt 10133_7.txt 12384_8.txt 3384_8.txt 5634_8.txt 7885_9.txt 10134_7.txt 12385_7.txt 3385_9.txt 5635_7.txt 7886_10.txt 10135_7.txt 12386_8.txt 3386_9.txt 5636_10.txt 7887_7.txt 10136_7.txt 12387_10.txt 3387_10.txt 5637_10.txt 7888_8.txt 10137_7.txt 12388_8.txt 3388_7.txt 5638_10.txt 7889_10.txt 10138_8.txt 12389_10.txt 3389_7.txt 5639_7.txt 788_8.txt 10139_8.txt 1238_7.txt 338_10.txt 563_10.txt 7890_7.txt 1013_9.txt 12390_8.txt 3390_8.txt 5640_7.txt 7891_8.txt 10140_8.txt 12391_9.txt 3391_7.txt 5641_7.txt 7892_7.txt 10141_9.txt 12392_9.txt 3392_7.txt 5642_7.txt 7893_7.txt 10142_8.txt 12393_8.txt 3393_9.txt 5643_7.txt 7894_10.txt 10143_8.txt 12394_10.txt 3394_10.txt 5644_8.txt 7895_10.txt 10144_8.txt 12395_8.txt 3395_9.txt 5645_10.txt 7896_10.txt 10145_8.txt 12396_7.txt 3396_7.txt 5646_10.txt 7897_8.txt 10146_7.txt 12397_8.txt 3397_10.txt 5647_10.txt 7898_10.txt 10147_10.txt 12398_8.txt 3398_10.txt 5648_10.txt 7899_10.txt 10148_10.txt 12399_9.txt 3399_10.txt 5649_10.txt 789_7.txt 10149_9.txt 1239_8.txt 339_10.txt 564_8.txt 78_10.txt 1014_9.txt 123_10.txt 33_7.txt 5650_10.txt 7900_10.txt 10150_9.txt 12400_8.txt 3400_8.txt 5651_10.txt 7901_7.txt 10151_8.txt 12401_8.txt 3401_8.txt 5652_10.txt 7902_10.txt 10152_9.txt 12402_7.txt 3402_8.txt 5653_10.txt 7903_10.txt 10153_9.txt 12403_8.txt 3403_9.txt 5654_8.txt 7904_8.txt 10154_8.txt 12404_9.txt 3404_8.txt 5655_7.txt 7905_10.txt 10155_9.txt 12405_10.txt 3405_9.txt 5656_10.txt 7906_7.txt 10156_10.txt 12406_8.txt 3406_10.txt 5657_9.txt 7907_7.txt 10157_10.txt 12407_7.txt 3407_8.txt 5658_10.txt 7908_9.txt 10158_7.txt 12408_7.txt 3408_10.txt 5659_9.txt 7909_10.txt 10159_7.txt 12409_7.txt 3409_10.txt 565_10.txt 790_9.txt 1015_10.txt 1240_7.txt 340_10.txt 5660_10.txt 7910_10.txt 10160_7.txt 12410_8.txt 3410_9.txt 5661_10.txt 7911_10.txt 10161_9.txt 12411_7.txt 3411_9.txt 5662_10.txt 7912_8.txt 10162_9.txt 12412_9.txt 3412_8.txt 5663_10.txt 7913_10.txt 10163_8.txt 12413_9.txt 3413_10.txt 5664_8.txt 7914_10.txt 10164_7.txt 12414_10.txt 3414_8.txt 5665_9.txt 7915_8.txt 10165_7.txt 12415_8.txt 3415_9.txt 5666_10.txt 7916_8.txt 10166_7.txt 12416_10.txt 3416_10.txt 5667_10.txt 7917_8.txt 10167_7.txt 12417_10.txt 3417_10.txt 5668_10.txt 7918_10.txt 10168_8.txt 12418_10.txt 3418_10.txt 5669_10.txt 7919_10.txt 10169_7.txt 12419_10.txt 3419_10.txt 566_8.txt 791_9.txt 1016_8.txt 1241_7.txt 341_10.txt 5670_8.txt 7920_7.txt 10170_8.txt 12420_9.txt 3420_10.txt 5671_9.txt 7921_10.txt 10171_7.txt 12421_10.txt 3421_10.txt 5672_9.txt 7922_7.txt 10172_8.txt 12422_8.txt 3422_10.txt 5673_9.txt 7923_8.txt 10173_8.txt 12423_10.txt 3423_7.txt 5674_7.txt 7924_9.txt 10174_7.txt 12424_9.txt 3424_10.txt 5675_9.txt 7925_7.txt 10175_10.txt 12425_7.txt 3425_8.txt 5676_9.txt 7926_10.txt 10176_7.txt 12426_7.txt 3426_7.txt 5677_10.txt 7927_10.txt 10177_9.txt 12427_7.txt 3427_8.txt 5678_10.txt 7928_10.txt 10178_10.txt 12428_8.txt 3428_9.txt 5679_10.txt 7929_10.txt 10179_9.txt 12429_7.txt 3429_10.txt 567_10.txt 792_8.txt 1017_8.txt 1242_9.txt 342_10.txt 5680_10.txt 7930_7.txt 10180_7.txt 12430_7.txt 3430_10.txt 5681_8.txt 7931_8.txt 10181_8.txt 12431_8.txt 3431_10.txt 5682_7.txt 7932_8.txt 10182_8.txt 12432_8.txt 3432_8.txt 5683_10.txt 7933_10.txt 10183_7.txt 12433_8.txt 3433_9.txt 5684_10.txt 7934_7.txt 10184_9.txt 12434_10.txt 3434_8.txt 5685_10.txt 7935_8.txt 10185_10.txt 12435_8.txt 3435_10.txt 5686_9.txt 7936_7.txt 10186_8.txt 12436_7.txt 3436_8.txt 5687_7.txt 7937_10.txt 10187_7.txt 12437_7.txt 3437_8.txt 5688_8.txt 7938_7.txt 10188_8.txt 12438_8.txt 3438_10.txt 5689_10.txt 7939_8.txt 10189_7.txt 12439_8.txt 3439_9.txt 568_7.txt 793_9.txt 1018_8.txt 1243_9.txt 343_10.txt 5690_7.txt 7940_7.txt 10190_7.txt 12440_7.txt 3440_8.txt 5691_8.txt 7941_8.txt 10191_10.txt 12441_9.txt 3441_8.txt 5692_7.txt 7942_10.txt 10192_8.txt 12442_8.txt 3442_10.txt 5693_7.txt 7943_10.txt 10193_9.txt 12443_9.txt 3443_9.txt 5694_9.txt 7944_9.txt 10194_10.txt 12444_10.txt 3444_10.txt 5695_8.txt 7945_8.txt 10195_8.txt 12445_9.txt 3445_7.txt 5696_8.txt 7946_10.txt 10196_10.txt 12446_8.txt 3446_7.txt 5697_8.txt 7947_10.txt 10197_7.txt 12447_8.txt 3447_9.txt 5698_8.txt 7948_7.txt 10198_8.txt 12448_10.txt 3448_7.txt 5699_8.txt 7949_10.txt 10199_7.txt 12449_8.txt 3449_8.txt 569_10.txt 794_8.txt 1019_10.txt 1244_8.txt 344_8.txt 56_10.txt 7950_9.txt 101_8.txt 12450_7.txt 3450_7.txt 5700_8.txt 7951_10.txt 10200_10.txt 12451_10.txt 3451_8.txt 5701_8.txt 7952_8.txt 10201_10.txt 12452_8.txt 3452_8.txt 5702_10.txt 7953_9.txt 10202_10.txt 12453_7.txt 3453_7.txt 5703_7.txt 7954_7.txt 10203_10.txt 12454_7.txt 3454_8.txt 5704_7.txt 7955_10.txt 10204_8.txt 12455_9.txt 3455_10.txt 5705_10.txt 7956_10.txt 10205_10.txt 12456_10.txt 3456_9.txt 5706_9.txt 7957_10.txt 10206_10.txt 12457_8.txt 3457_7.txt 5707_10.txt 7958_10.txt 10207_10.txt 12458_7.txt 3458_10.txt 5708_10.txt 7959_10.txt 10208_7.txt 12459_10.txt 3459_8.txt 5709_10.txt 795_8.txt 10209_7.txt 1245_7.txt 345_7.txt 570_10.txt 7960_10.txt 1020_10.txt 12460_7.txt 3460_8.txt 5710_8.txt 7961_10.txt 10210_7.txt 12461_9.txt 3461_9.txt 5711_8.txt 7962_9.txt 10211_7.txt 12462_7.txt 3462_7.txt 5712_8.txt 7963_9.txt 10212_8.txt 12463_8.txt 3463_7.txt 5713_10.txt 7964_10.txt 10213_8.txt 12464_10.txt 3464_10.txt 5714_7.txt 7965_10.txt 10214_10.txt 12465_9.txt 3465_9.txt 5715_10.txt 7966_9.txt 10215_10.txt 12466_7.txt 3466_7.txt 5716_8.txt 7967_9.txt 10216_8.txt 12467_7.txt 3467_7.txt 5717_9.txt 7968_10.txt 10217_9.txt 12468_7.txt 3468_10.txt 5718_7.txt 7969_10.txt 10218_8.txt 12469_10.txt 3469_10.txt 5719_9.txt 796_8.txt 10219_10.txt 1246_8.txt 346_10.txt 571_10.txt 7970_10.txt 1021_10.txt 12470_10.txt 3470_8.txt 5720_10.txt 7971_10.txt 10220_7.txt 12471_7.txt 3471_8.txt 5721_10.txt 7972_10.txt 10221_8.txt 12472_10.txt 3472_10.txt 5722_8.txt 7973_10.txt 10222_9.txt 12473_10.txt 3473_9.txt 5723_7.txt 7974_7.txt 10223_10.txt 12474_9.txt 3474_10.txt 5724_10.txt 7975_8.txt 10224_10.txt 12475_10.txt 3475_9.txt 5725_9.txt 7976_7.txt 10225_9.txt 12476_10.txt 3476_10.txt 5726_7.txt 7977_8.txt 10226_10.txt 12477_10.txt 3477_10.txt 5727_9.txt 7978_8.txt 10227_10.txt 12478_9.txt 3478_8.txt 5728_10.txt 7979_9.txt 10228_8.txt 12479_10.txt 3479_9.txt 5729_9.txt 797_8.txt 10229_8.txt 1247_8.txt 347_10.txt 572_9.txt 7980_10.txt 1022_10.txt 12480_10.txt 3480_10.txt 5730_10.txt 7981_10.txt 10230_9.txt 12481_8.txt 3481_10.txt 5731_7.txt 7982_10.txt 10231_10.txt 12482_8.txt 3482_10.txt 5732_7.txt 7983_9.txt 10232_10.txt 12483_9.txt 3483_9.txt 5733_7.txt 7984_10.txt 10233_7.txt 12484_8.txt 3484_10.txt 5734_9.txt 7985_10.txt 10234_10.txt 12485_8.txt 3485_9.txt 5735_7.txt 7986_10.txt 10235_8.txt 12486_7.txt 3486_10.txt 5736_7.txt 7987_8.txt 10236_8.txt 12487_10.txt 3487_9.txt 5737_7.txt 7988_10.txt 10237_10.txt 12488_8.txt 3488_7.txt 5738_10.txt 7989_10.txt 10238_10.txt 12489_10.txt 3489_10.txt 5739_10.txt 798_10.txt 10239_10.txt 1248_8.txt 348_7.txt 573_9.txt 7990_7.txt 1023_10.txt 12490_8.txt 3490_8.txt 5740_10.txt 7991_7.txt 10240_8.txt 12491_8.txt 3491_8.txt 5741_10.txt 7992_7.txt 10241_8.txt 12492_7.txt 3492_7.txt 5742_10.txt 7993_10.txt 10242_8.txt 12493_8.txt 3493_8.txt 5743_10.txt 7994_9.txt 10243_10.txt 12494_8.txt 3494_9.txt 5744_7.txt 7995_8.txt 10244_7.txt 12495_7.txt 3495_7.txt 5745_8.txt 7996_8.txt 10245_10.txt 12496_8.txt 3496_8.txt 5746_9.txt 7997_10.txt 10246_10.txt 12497_10.txt 3497_10.txt 5747_8.txt 7998_9.txt 10247_10.txt 12498_10.txt 3498_10.txt 5748_10.txt 7999_10.txt 10248_7.txt 12499_7.txt 3499_8.txt 5749_10.txt 799_8.txt 10249_7.txt 1249_9.txt 349_10.txt 574_7.txt 79_10.txt 1024_9.txt 124_10.txt 34_8.txt 5750_8.txt 7_7.txt 10250_10.txt 1250_10.txt 3500_10.txt 5751_10.txt 8000_10.txt 10251_10.txt 1251_9.txt 3501_9.txt 5752_8.txt 8001_10.txt 10252_9.txt 1252_8.txt 3502_9.txt 5753_8.txt 8002_7.txt 10253_10.txt 1253_10.txt 3503_7.txt 5754_9.txt 8003_8.txt 10254_8.txt 1254_7.txt 3504_7.txt 5755_7.txt 8004_9.txt 10255_9.txt 1255_10.txt 3505_8.txt 5756_8.txt 8005_9.txt 10256_8.txt 1256_8.txt 3506_10.txt 5757_8.txt 8006_10.txt 10257_8.txt 1257_10.txt 3507_7.txt 5758_10.txt 8007_7.txt 10258_10.txt 1258_9.txt 3508_8.txt 5759_10.txt 8008_7.txt 10259_8.txt 1259_9.txt 3509_10.txt 575_10.txt 8009_9.txt 1025_8.txt 125_7.txt 350_9.txt 5760_8.txt 800_9.txt 10260_10.txt 1260_10.txt 3510_7.txt 5761_8.txt 8010_7.txt 10261_8.txt 1261_8.txt 3511_8.txt 5762_8.txt 8011_10.txt 10262_10.txt 1262_8.txt 3512_7.txt 5763_8.txt 8012_8.txt 10263_10.txt 1263_8.txt 3513_10.txt 5764_7.txt 8013_8.txt 10264_10.txt 1264_10.txt 3514_7.txt 5765_9.txt 8014_7.txt 10265_9.txt 1265_10.txt 3515_7.txt 5766_10.txt 8015_10.txt 10266_9.txt 1266_8.txt 3516_8.txt 5767_8.txt 8016_10.txt 10267_8.txt 1267_7.txt 3517_8.txt 5768_8.txt 8017_9.txt 10268_9.txt 1268_7.txt 3518_10.txt 5769_9.txt 8018_10.txt 10269_7.txt 1269_8.txt 3519_7.txt 576_10.txt 8019_7.txt 1026_9.txt 126_10.txt 351_10.txt 5770_10.txt 801_8.txt 10270_9.txt 1270_7.txt 3520_9.txt 5771_8.txt 8020_8.txt 10271_10.txt 1271_7.txt 3521_9.txt 5772_10.txt 8021_7.txt 10272_10.txt 1272_7.txt 3522_8.txt 5773_9.txt 8022_8.txt 10273_8.txt 1273_7.txt 3523_9.txt 5774_8.txt 8023_7.txt 10274_8.txt 1274_7.txt 3524_9.txt 5775_7.txt 8024_10.txt 10275_10.txt 1275_8.txt 3525_7.txt 5776_10.txt 8025_9.txt 10276_10.txt 1276_9.txt 3526_8.txt 5777_8.txt 8026_9.txt 10277_9.txt 1277_10.txt 3527_7.txt 5778_9.txt 8027_10.txt 10278_7.txt 1278_9.txt 3528_7.txt 5779_10.txt 8028_10.txt 10279_8.txt 1279_9.txt 3529_9.txt 577_8.txt 8029_10.txt 1027_8.txt 127_7.txt 352_10.txt 5780_10.txt 802_10.txt 10280_10.txt 1280_10.txt 3530_7.txt 5781_10.txt 8030_9.txt 10281_7.txt 1281_10.txt 3531_7.txt 5782_10.txt 8031_9.txt 10282_8.txt 1282_9.txt 3532_8.txt 5783_7.txt 8032_10.txt 10283_10.txt 1283_10.txt 3533_10.txt 5784_8.txt 8033_7.txt 10284_9.txt 1284_7.txt 3534_10.txt 5785_10.txt 8034_10.txt 10285_10.txt 1285_9.txt 3535_8.txt 5786_10.txt 8035_7.txt 10286_9.txt 1286_8.txt 3536_10.txt 5787_9.txt 8036_8.txt 10287_8.txt 1287_9.txt 3537_10.txt 5788_8.txt 8037_10.txt 10288_10.txt 1288_8.txt 3538_10.txt 5789_9.txt 8038_7.txt 10289_10.txt 1289_7.txt 3539_9.txt 578_10.txt 8039_8.txt 1028_10.txt 128_7.txt 353_9.txt 5790_7.txt 803_10.txt 10290_8.txt 1290_7.txt 3540_8.txt 5791_9.txt 8040_9.txt 10291_7.txt 1291_10.txt 3541_7.txt 5792_10.txt 8041_7.txt 10292_7.txt 1292_8.txt 3542_8.txt 5793_10.txt 8042_7.txt 10293_8.txt 1293_8.txt 3543_10.txt 5794_10.txt 8043_9.txt 10294_8.txt 1294_7.txt 3544_9.txt 5795_10.txt 8044_10.txt 10295_7.txt 1295_10.txt 3545_7.txt 5796_8.txt 8045_8.txt 10296_8.txt 1296_10.txt 3546_10.txt 5797_8.txt 8046_9.txt 10297_8.txt 1297_8.txt 3547_8.txt 5798_8.txt 8047_9.txt 10298_9.txt 1298_7.txt 3548_8.txt 5799_10.txt 8048_7.txt 10299_9.txt 1299_10.txt 3549_8.txt 579_10.txt 8049_7.txt 1029_9.txt 129_9.txt 354_9.txt 57_10.txt 804_10.txt 102_10.txt 12_9.txt 3550_10.txt 5800_10.txt 8050_10.txt 10300_10.txt 1300_10.txt 3551_8.txt 5801_10.txt 8051_7.txt 10301_8.txt 1301_10.txt 3552_10.txt 5802_10.txt 8052_9.txt 10302_9.txt 1302_10.txt 3553_8.txt 5803_10.txt 8053_8.txt 10303_7.txt 1303_10.txt 3554_10.txt 5804_10.txt 8054_8.txt 10304_7.txt 1304_7.txt 3555_7.txt 5805_9.txt 8055_8.txt 10305_8.txt 1305_10.txt 3556_10.txt 5806_7.txt 8056_8.txt 10306_8.txt 1306_7.txt 3557_9.txt 5807_8.txt 8057_10.txt 10307_8.txt 1307_10.txt 3558_8.txt 5808_10.txt 8058_8.txt 10308_8.txt 1308_9.txt 3559_10.txt 5809_10.txt 8059_10.txt 10309_7.txt 1309_10.txt 355_9.txt 580_9.txt 805_9.txt 1030_10.txt 130_9.txt 3560_8.txt 5810_9.txt 8060_9.txt 10310_9.txt 1310_7.txt 3561_9.txt 5811_9.txt 8061_10.txt 10311_9.txt 1311_10.txt 3562_8.txt 5812_9.txt 8062_8.txt 10312_10.txt 1312_9.txt 3563_8.txt 5813_8.txt 8063_10.txt 10313_7.txt 1313_9.txt 3564_10.txt 5814_8.txt 8064_10.txt 10314_8.txt 1314_7.txt 3565_10.txt 5815_10.txt 8065_9.txt 10315_8.txt 1315_10.txt 3566_9.txt 5816_10.txt 8066_9.txt 10316_8.txt 1316_9.txt 3567_8.txt 5817_10.txt 8067_8.txt 10317_7.txt 1317_8.txt 3568_7.txt 5818_7.txt 8068_9.txt 10318_7.txt 1318_8.txt 3569_8.txt 5819_10.txt 8069_8.txt 10319_7.txt 1319_10.txt 356_10.txt 581_10.txt 806_10.txt 1031_10.txt 131_10.txt 3570_10.txt 5820_7.txt 8070_8.txt 10320_7.txt 1320_10.txt 3571_7.txt 5821_7.txt 8071_9.txt 10321_10.txt 1321_10.txt 3572_10.txt 5822_10.txt 8072_10.txt 10322_7.txt 1322_8.txt 3573_7.txt 5823_10.txt 8073_8.txt 10323_10.txt 1323_10.txt 3574_10.txt 5824_10.txt 8074_10.txt 10324_9.txt 1324_7.txt 3575_10.txt 5825_9.txt 8075_8.txt 10325_10.txt 1325_9.txt 3576_9.txt 5826_10.txt 8076_7.txt 10326_10.txt 1326_10.txt 3577_7.txt 5827_10.txt 8077_7.txt 10327_7.txt 1327_9.txt 3578_7.txt 5828_9.txt 8078_10.txt 10328_8.txt 1328_10.txt 3579_8.txt 5829_10.txt 8079_8.txt 10329_8.txt 1329_8.txt 357_10.txt 582_9.txt 807_10.txt 1032_7.txt 132_9.txt 3580_10.txt 5830_8.txt 8080_8.txt 10330_8.txt 1330_9.txt 3581_9.txt 5831_8.txt 8081_9.txt 10331_10.txt 1331_7.txt 3582_8.txt 5832_9.txt 8082_8.txt 10332_8.txt 1332_8.txt 3583_10.txt 5833_10.txt 8083_7.txt 10333_8.txt 1333_10.txt 3584_10.txt 5834_9.txt 8084_7.txt 10334_8.txt 1334_8.txt 3585_9.txt 5835_10.txt 8085_7.txt 10335_8.txt 1335_10.txt 3586_10.txt 5836_8.txt 8086_9.txt 10336_8.txt 1336_7.txt 3587_10.txt 5837_7.txt 8087_8.txt 10337_9.txt 1337_7.txt 3588_8.txt 5838_8.txt 8088_7.txt 10338_9.txt 1338_7.txt 3589_8.txt 5839_7.txt 8089_10.txt 10339_7.txt 1339_9.txt 358_10.txt 583_8.txt 808_9.txt 1033_10.txt 133_10.txt 3590_8.txt 5840_7.txt 8090_9.txt 10340_9.txt 1340_10.txt 3591_9.txt 5841_7.txt 8091_9.txt 10341_7.txt 1341_10.txt 3592_10.txt 5842_8.txt 8092_8.txt 10342_7.txt 1342_10.txt 3593_8.txt 5843_10.txt 8093_7.txt 10343_7.txt 1343_9.txt 3594_7.txt 5844_8.txt 8094_9.txt 10344_7.txt 1344_9.txt 3595_10.txt 5845_7.txt 8095_8.txt 10345_7.txt 1345_9.txt 3596_8.txt 5846_10.txt 8096_10.txt 10346_9.txt 1346_9.txt 3597_10.txt 5847_10.txt 8097_7.txt 10347_9.txt 1347_10.txt 3598_10.txt 5848_7.txt 8098_7.txt 10348_8.txt 1348_10.txt 3599_8.txt 5849_8.txt 8099_7.txt 10349_10.txt 1349_8.txt 359_8.txt 584_8.txt 809_10.txt 1034_7.txt 134_10.txt 35_8.txt 5850_9.txt 80_9.txt 10350_10.txt 1350_9.txt 3600_7.txt 5851_10.txt 8100_10.txt 10351_8.txt 1351_10.txt 3601_7.txt 5852_7.txt 8101_8.txt 10352_10.txt 1352_10.txt 3602_7.txt 5853_10.txt 8102_7.txt 10353_9.txt 1353_10.txt 3603_7.txt 5854_8.txt 8103_10.txt 10354_9.txt 1354_9.txt 3604_10.txt 5855_9.txt 8104_7.txt 10355_9.txt 1355_8.txt 3605_8.txt 5856_8.txt 8105_8.txt 10356_9.txt 1356_8.txt 3606_9.txt 5857_10.txt 8106_7.txt 10357_8.txt 1357_10.txt 3607_8.txt 5858_8.txt 8107_8.txt 10358_9.txt 1358_10.txt 3608_9.txt 5859_9.txt 8108_10.txt 10359_7.txt 1359_7.txt 3609_8.txt 585_10.txt 8109_10.txt 1035_7.txt 135_7.txt 360_10.txt 5860_8.txt 810_10.txt 10360_8.txt 1360_10.txt 3610_10.txt 5861_8.txt 8110_9.txt 10361_7.txt 1361_10.txt 3611_10.txt 5862_8.txt 8111_8.txt 10362_8.txt 1362_10.txt 3612_7.txt 5863_8.txt 8112_9.txt 10363_9.txt 1363_10.txt 3613_8.txt 5864_7.txt 8113_8.txt 10364_10.txt 1364_8.txt 3614_7.txt 5865_9.txt 8114_8.txt 10365_8.txt 1365_8.txt 3615_9.txt 5866_9.txt 8115_10.txt 10366_10.txt 1366_8.txt 3616_7.txt 5867_9.txt 8116_9.txt 10367_8.txt 1367_10.txt 3617_8.txt 5868_8.txt 8117_8.txt 10368_7.txt 1368_10.txt 3618_7.txt 5869_10.txt 8118_10.txt 10369_8.txt 1369_8.txt 3619_8.txt 586_10.txt 8119_10.txt 1036_9.txt 136_10.txt 361_10.txt 5870_8.txt 811_10.txt 10370_9.txt 1370_8.txt 3620_10.txt 5871_9.txt 8120_7.txt 10371_8.txt 1371_8.txt 3621_8.txt 5872_9.txt 8121_8.txt 10372_7.txt 1372_7.txt 3622_8.txt 5873_8.txt 8122_10.txt 10373_7.txt 1373_10.txt 3623_8.txt 5874_10.txt 8123_8.txt 10374_8.txt 1374_8.txt 3624_10.txt 5875_8.txt 8124_9.txt 10375_10.txt 1375_8.txt 3625_8.txt 5876_8.txt 8125_7.txt 10376_7.txt 1376_7.txt 3626_7.txt 5877_8.txt 8126_7.txt 10377_9.txt 1377_7.txt 3627_8.txt 5878_10.txt 8127_8.txt 10378_8.txt 1378_9.txt 3628_8.txt 5879_10.txt 8128_10.txt 10379_10.txt 1379_8.txt 3629_8.txt 587_10.txt 8129_9.txt 1037_8.txt 137_7.txt 362_10.txt 5880_10.txt 812_10.txt 10380_10.txt 1380_7.txt 3630_7.txt 5881_7.txt 8130_10.txt 10381_10.txt 1381_7.txt 3631_7.txt 5882_9.txt 8131_8.txt 10382_10.txt 1382_8.txt 3632_7.txt 5883_10.txt 8132_10.txt 10383_10.txt 1383_7.txt 3633_10.txt 5884_10.txt 8133_7.txt 10384_10.txt 1384_7.txt 3634_8.txt 5885_10.txt 8134_10.txt 10385_10.txt 1385_8.txt 3635_9.txt 5886_10.txt 8135_10.txt 10386_8.txt 1386_8.txt 3636_8.txt 5887_7.txt 8136_9.txt 10387_7.txt 1387_8.txt 3637_10.txt 5888_9.txt 8137_10.txt 10388_7.txt 1388_10.txt 3638_10.txt 5889_7.txt 8138_10.txt 10389_10.txt 1389_8.txt 3639_9.txt 588_9.txt 8139_10.txt 1038_7.txt 138_7.txt 363_10.txt 5890_8.txt 813_10.txt 10390_10.txt 1390_9.txt 3640_8.txt 5891_10.txt 8140_10.txt 10391_10.txt 1391_7.txt 3641_9.txt 5892_8.txt 8141_10.txt 10392_10.txt 1392_7.txt 3642_9.txt 5893_9.txt 8142_10.txt 10393_9.txt 1393_7.txt 3643_10.txt 5894_7.txt 8143_10.txt 10394_10.txt 1394_8.txt 3644_8.txt 5895_10.txt 8144_9.txt 10395_8.txt 1395_10.txt 3645_8.txt 5896_10.txt 8145_10.txt 10396_8.txt 1396_8.txt 3646_8.txt 5897_10.txt 8146_10.txt 10397_8.txt 1397_7.txt 3647_10.txt 5898_8.txt 8147_10.txt 10398_8.txt 1398_7.txt 3648_8.txt 5899_7.txt 8148_8.txt 10399_10.txt 1399_9.txt 3649_9.txt 589_10.txt 8149_10.txt 1039_9.txt 139_10.txt 364_10.txt 58_9.txt 814_10.txt 103_7.txt 13_7.txt 3650_10.txt 5900_10.txt 8150_10.txt 10400_10.txt 1400_7.txt 3651_10.txt 5901_7.txt 8151_10.txt 10401_10.txt 1401_7.txt 3652_10.txt 5902_9.txt 8152_10.txt 10402_10.txt 1402_7.txt 3653_10.txt 5903_8.txt 8153_9.txt 10403_7.txt 1403_7.txt 3654_10.txt 5904_8.txt 8154_10.txt 10404_9.txt 1404_10.txt 3655_7.txt 5905_9.txt 8155_7.txt 10405_8.txt 1405_7.txt 3656_8.txt 5906_8.txt 8156_10.txt 10406_10.txt 1406_8.txt 3657_7.txt 5907_10.txt 8157_10.txt 10407_8.txt 1407_7.txt 3658_10.txt 5908_10.txt 8158_10.txt 10408_10.txt 1408_7.txt 3659_9.txt 5909_8.txt 8159_10.txt 10409_10.txt 1409_8.txt 365_10.txt 590_10.txt 815_7.txt 1040_10.txt 140_8.txt 3660_10.txt 5910_8.txt 8160_7.txt 10410_10.txt 1410_9.txt 3661_10.txt 5911_9.txt 8161_10.txt 10411_9.txt 1411_7.txt 3662_9.txt 5912_7.txt 8162_9.txt 10412_8.txt 1412_8.txt 3663_10.txt 5913_10.txt 8163_8.txt 10413_10.txt 1413_7.txt 3664_10.txt 5914_8.txt 8164_8.txt 10414_10.txt 1414_10.txt 3665_9.txt 5915_7.txt 8165_8.txt 10415_7.txt 1415_10.txt 3666_9.txt 5916_10.txt 8166_10.txt 10416_9.txt 1416_10.txt 3667_8.txt 5917_7.txt 8167_7.txt 10417_8.txt 1417_8.txt 3668_7.txt 5918_9.txt 8168_7.txt 10418_9.txt 1418_9.txt 3669_10.txt 5919_9.txt 8169_8.txt 10419_10.txt 1419_7.txt 366_9.txt 591_10.txt 816_10.txt 1041_9.txt 141_9.txt 3670_10.txt 5920_9.txt 8170_7.txt 10420_10.txt 1420_8.txt 3671_7.txt 5921_7.txt 8171_8.txt 10421_7.txt 1421_9.txt 3672_10.txt 5922_8.txt 8172_9.txt 10422_7.txt 1422_10.txt 3673_8.txt 5923_7.txt 8173_8.txt 10423_9.txt 1423_10.txt 3674_8.txt 5924_8.txt 8174_9.txt 10424_9.txt 1424_10.txt 3675_9.txt 5925_7.txt 8175_7.txt 10425_9.txt 1425_7.txt 3676_8.txt 5926_7.txt 8176_8.txt 10426_9.txt 1426_8.txt 3677_8.txt 5927_9.txt 8177_8.txt 10427_8.txt 1427_9.txt 3678_8.txt 5928_8.txt 8178_8.txt 10428_10.txt 1428_7.txt 3679_7.txt 5929_10.txt 8179_7.txt 10429_10.txt 1429_9.txt 367_10.txt 592_10.txt 817_10.txt 1042_10.txt 142_8.txt 3680_9.txt 5930_7.txt 8180_8.txt 10430_9.txt 1430_9.txt 3681_8.txt 5931_7.txt 8181_10.txt 10431_10.txt 1431_10.txt 3682_8.txt 5932_10.txt 8182_7.txt 10432_10.txt 1432_7.txt 3683_9.txt 5933_7.txt 8183_7.txt 10433_9.txt 1433_10.txt 3684_8.txt 5934_10.txt 8184_7.txt 10434_10.txt 1434_10.txt 3685_8.txt 5935_9.txt 8185_8.txt 10435_7.txt 1435_8.txt 3686_10.txt 5936_10.txt 8186_10.txt 10436_8.txt 1436_10.txt 3687_7.txt 5937_9.txt 8187_9.txt 10437_7.txt 1437_8.txt 3688_8.txt 5938_10.txt 8188_8.txt 10438_9.txt 1438_7.txt 3689_8.txt 5939_7.txt 8189_10.txt 10439_8.txt 1439_7.txt 368_10.txt 593_9.txt 818_10.txt 1043_10.txt 143_7.txt 3690_9.txt 5940_7.txt 8190_10.txt 10440_9.txt 1440_7.txt 3691_8.txt 5941_8.txt 8191_8.txt 10441_10.txt 1441_9.txt 3692_10.txt 5942_7.txt 8192_10.txt 10442_10.txt 1442_7.txt 3693_8.txt 5943_10.txt 8193_10.txt 10443_9.txt 1443_8.txt 3694_8.txt 5944_9.txt 8194_9.txt 10444_9.txt 1444_7.txt 3695_8.txt 5945_10.txt 8195_10.txt 10445_10.txt 1445_8.txt 3696_7.txt 5946_7.txt 8196_8.txt 10446_10.txt 1446_10.txt 3697_7.txt 5947_7.txt 8197_8.txt 10447_10.txt 1447_8.txt 3698_10.txt 5948_8.txt 8198_7.txt 10448_10.txt 1448_8.txt 3699_10.txt 5949_10.txt 8199_10.txt 10449_9.txt 1449_9.txt 369_10.txt 594_9.txt 819_10.txt 1044_8.txt 144_8.txt 36_10.txt 5950_10.txt 81_10.txt 10450_10.txt 1450_8.txt 3700_9.txt 5951_10.txt 8200_8.txt 10451_10.txt 1451_8.txt 3701_10.txt 5952_9.txt 8201_8.txt 10452_10.txt 1452_8.txt 3702_9.txt 5953_9.txt 8202_10.txt 10453_10.txt 1453_8.txt 3703_9.txt 5954_7.txt 8203_7.txt 10454_9.txt 1454_7.txt 3704_8.txt 5955_10.txt 8204_8.txt 10455_10.txt 1455_7.txt 3705_10.txt 5956_9.txt 8205_8.txt 10456_10.txt 1456_9.txt 3706_7.txt 5957_9.txt 8206_10.txt 10457_8.txt 1457_10.txt 3707_7.txt 5958_9.txt 8207_10.txt 10458_10.txt 1458_9.txt 3708_7.txt 5959_7.txt 8208_8.txt 10459_9.txt 1459_10.txt 3709_7.txt 595_9.txt 8209_8.txt 1045_8.txt 145_10.txt 370_10.txt 5960_8.txt 820_10.txt 10460_10.txt 1460_10.txt 3710_9.txt 5961_10.txt 8210_7.txt 10461_9.txt 1461_10.txt 3711_7.txt 5962_7.txt 8211_7.txt 10462_7.txt 1462_9.txt 3712_7.txt 5963_10.txt 8212_7.txt 10463_10.txt 1463_10.txt 3713_10.txt 5964_8.txt 8213_9.txt 10464_7.txt 1464_7.txt 3714_10.txt 5965_10.txt 8214_7.txt 10465_8.txt 1465_9.txt 3715_7.txt 5966_7.txt 8215_7.txt 10466_8.txt 1466_10.txt 3716_9.txt 5967_10.txt 8216_8.txt 10467_10.txt 1467_9.txt 3717_8.txt 5968_10.txt 8217_8.txt 10468_9.txt 1468_9.txt 3718_8.txt 5969_10.txt 8218_7.txt 10469_10.txt 1469_7.txt 3719_9.txt 596_7.txt 8219_7.txt 1046_10.txt 146_10.txt 371_9.txt 5970_10.txt 821_10.txt 10470_9.txt 1470_10.txt 3720_10.txt 5971_7.txt 8220_8.txt 10471_10.txt 1471_9.txt 3721_8.txt 5972_8.txt 8221_9.txt 10472_7.txt 1472_8.txt 3722_10.txt 5973_9.txt 8222_9.txt 10473_10.txt 1473_9.txt 3723_8.txt 5974_7.txt 8223_9.txt 10474_9.txt 1474_8.txt 3724_9.txt 5975_8.txt 8224_8.txt 10475_8.txt 1475_8.txt 3725_8.txt 5976_7.txt 8225_8.txt 10476_9.txt 1476_10.txt 3726_7.txt 5977_8.txt 8226_8.txt 10477_9.txt 1477_7.txt 3727_10.txt 5978_9.txt 8227_7.txt 10478_8.txt 1478_7.txt 3728_10.txt 5979_9.txt 8228_8.txt 10479_10.txt 1479_8.txt 3729_10.txt 597_7.txt 8229_10.txt 1047_8.txt 147_9.txt 372_10.txt 5980_10.txt 822_9.txt 10480_10.txt 1480_8.txt 3730_10.txt 5981_7.txt 8230_10.txt 10481_8.txt 1481_10.txt 3731_10.txt 5982_9.txt 8231_10.txt 10482_10.txt 1482_10.txt 3732_10.txt 5983_8.txt 8232_10.txt 10483_8.txt 1483_9.txt 3733_10.txt 5984_10.txt 8233_8.txt 10484_8.txt 1484_10.txt 3734_9.txt 5985_10.txt 8234_7.txt 10485_8.txt 1485_10.txt 3735_10.txt 5986_10.txt 8235_7.txt 10486_7.txt 1486_7.txt 3736_8.txt 5987_8.txt 8236_7.txt 10487_7.txt 1487_9.txt 3737_9.txt 5988_7.txt 8237_8.txt 10488_10.txt 1488_10.txt 3738_8.txt 5989_7.txt 8238_8.txt 10489_10.txt 1489_8.txt 3739_10.txt 598_9.txt 8239_10.txt 1048_8.txt 148_9.txt 373_10.txt 5990_7.txt 823_9.txt 10490_7.txt 1490_8.txt 3740_9.txt 5991_8.txt 8240_9.txt 10491_7.txt 1491_9.txt 3741_8.txt 5992_7.txt 8241_7.txt 10492_10.txt 1492_10.txt 3742_9.txt 5993_8.txt 8242_8.txt 10493_9.txt 1493_9.txt 3743_10.txt 5994_8.txt 8243_9.txt 10494_10.txt 1494_8.txt 3744_10.txt 5995_10.txt 8244_9.txt 10495_7.txt 1495_9.txt 3745_10.txt 5996_7.txt 8245_9.txt 10496_10.txt 1496_10.txt 3746_10.txt 5997_9.txt 8246_9.txt 10497_8.txt 1497_8.txt 3747_10.txt 5998_10.txt 8247_9.txt 10498_10.txt 1498_8.txt 3748_10.txt 5999_7.txt 8248_9.txt 10499_10.txt 1499_7.txt 3749_9.txt 599_10.txt 8249_10.txt 1049_7.txt 149_10.txt 374_10.txt 59_7.txt 824_8.txt 104_10.txt 14_10.txt 3750_10.txt 5_10.txt 8250_8.txt 10500_10.txt 1500_9.txt 3751_10.txt 6000_9.txt 8251_10.txt 10501_10.txt 1501_7.txt 3752_7.txt 6001_7.txt 8252_9.txt 10502_9.txt 1502_10.txt 3753_9.txt 6002_7.txt 8253_10.txt 10503_10.txt 1503_9.txt 3754_8.txt 6003_8.txt 8254_8.txt 10504_9.txt 1504_9.txt 3755_10.txt 6004_10.txt 8255_9.txt 10505_10.txt 1505_9.txt 3756_8.txt 6005_8.txt 8256_9.txt 10506_10.txt 1506_7.txt 3757_7.txt 6006_10.txt 8257_9.txt 10507_10.txt 1507_8.txt 3758_9.txt 6007_8.txt 8258_9.txt 10508_10.txt 1508_7.txt 3759_8.txt 6008_7.txt 8259_9.txt 10509_7.txt 1509_10.txt 375_9.txt 6009_10.txt 825_10.txt 1050_9.txt 150_8.txt 3760_7.txt 600_10.txt 8260_9.txt 10510_7.txt 1510_8.txt 3761_8.txt 6010_8.txt 8261_8.txt 10511_7.txt 1511_8.txt 3762_8.txt 6011_8.txt 8262_10.txt 10512_10.txt 1512_10.txt 3763_8.txt 6012_9.txt 8263_9.txt 10513_7.txt 1513_7.txt 3764_8.txt 6013_7.txt 8264_9.txt 10514_8.txt 1514_7.txt 3765_10.txt 6014_8.txt 8265_10.txt 10515_9.txt 1515_7.txt 3766_10.txt 6015_7.txt 8266_10.txt 10516_7.txt 1516_7.txt 3767_9.txt 6016_10.txt 8267_7.txt 10517_8.txt 1517_7.txt 3768_10.txt 6017_8.txt 8268_7.txt 10518_9.txt 1518_8.txt 3769_7.txt 6018_9.txt 8269_7.txt 10519_9.txt 1519_10.txt 376_10.txt 6019_8.txt 826_9.txt 1051_9.txt 151_10.txt 3770_10.txt 601_7.txt 8270_10.txt 10520_9.txt 1520_7.txt 3771_10.txt 6020_7.txt 8271_10.txt 10521_9.txt 1521_8.txt 3772_10.txt 6021_7.txt 8272_7.txt 10522_7.txt 1522_8.txt 3773_7.txt 6022_7.txt 8273_10.txt 10523_9.txt 1523_9.txt 3774_8.txt 6023_8.txt 8274_10.txt 10524_10.txt 1524_7.txt 3775_7.txt 6024_10.txt 8275_10.txt 10525_10.txt 1525_9.txt 3776_10.txt 6025_9.txt 8276_10.txt 10526_9.txt 1526_9.txt 3777_10.txt 6026_8.txt 8277_10.txt 10527_10.txt 1527_7.txt 3778_10.txt 6027_9.txt 8278_10.txt 10528_10.txt 1528_8.txt 3779_8.txt 6028_9.txt 8279_10.txt 10529_10.txt 1529_10.txt 377_7.txt 6029_7.txt 827_7.txt 1052_8.txt 152_9.txt 3780_8.txt 602_10.txt 8280_8.txt 10530_10.txt 1530_10.txt 3781_7.txt 6030_10.txt 8281_7.txt 10531_10.txt 1531_8.txt 3782_8.txt 6031_7.txt 8282_7.txt 10532_8.txt 1532_10.txt 3783_10.txt 6032_10.txt 8283_7.txt 10533_10.txt 1533_8.txt 3784_9.txt 6033_8.txt 8284_8.txt 10534_7.txt 1534_10.txt 3785_10.txt 6034_10.txt 8285_8.txt 10535_7.txt 1535_9.txt 3786_9.txt 6035_10.txt 8286_7.txt 10536_10.txt 1536_7.txt 3787_10.txt 6036_10.txt 8287_9.txt 10537_10.txt 1537_9.txt 3788_10.txt 6037_10.txt 8288_7.txt 10538_8.txt 1538_10.txt 3789_8.txt 6038_10.txt 8289_8.txt 10539_10.txt 1539_10.txt 378_8.txt 6039_10.txt 828_10.txt 1053_8.txt 153_10.txt 3790_10.txt 603_10.txt 8290_10.txt 10540_10.txt 1540_10.txt 3791_10.txt 6040_10.txt 8291_8.txt 10541_10.txt 1541_8.txt 3792_8.txt 6041_10.txt 8292_8.txt 10542_7.txt 1542_10.txt 3793_7.txt 6042_9.txt 8293_8.txt 10543_8.txt 1543_8.txt 3794_10.txt 6043_10.txt 8294_8.txt 10544_8.txt 1544_10.txt 3795_9.txt 6044_10.txt 8295_7.txt 10545_7.txt 1545_10.txt 3796_8.txt 6045_9.txt 8296_8.txt 10546_9.txt 1546_8.txt 3797_9.txt 6046_7.txt 8297_10.txt 10547_9.txt 1547_7.txt 3798_9.txt 6047_10.txt 8298_7.txt 10548_7.txt 1548_8.txt 3799_8.txt 6048_9.txt 8299_7.txt 10549_9.txt 1549_10.txt 379_10.txt 6049_8.txt 829_7.txt 1054_8.txt 154_8.txt 37_9.txt 604_8.txt 82_8.txt 10550_8.txt 1550_10.txt 3800_7.txt 6050_10.txt 8300_10.txt 10551_7.txt 1551_8.txt 3801_8.txt 6051_9.txt 8301_7.txt 10552_9.txt 1552_8.txt 3802_10.txt 6052_9.txt 8302_8.txt 10553_8.txt 1553_10.txt 3803_9.txt 6053_10.txt 8303_10.txt 10554_7.txt 1554_8.txt 3804_10.txt 6054_9.txt 8304_8.txt 10555_8.txt 1555_8.txt 3805_8.txt 6055_8.txt 8305_7.txt 10556_10.txt 1556_8.txt 3806_8.txt 6056_9.txt 8306_10.txt 10557_9.txt 1557_10.txt 3807_8.txt 6057_9.txt 8307_10.txt 10558_10.txt 1558_10.txt 3808_10.txt 6058_8.txt 8308_7.txt 10559_8.txt 1559_7.txt 3809_9.txt 6059_10.txt 8309_7.txt 1055_10.txt 155_10.txt 380_10.txt 605_8.txt 830_10.txt 10560_9.txt 1560_9.txt 3810_9.txt 6060_10.txt 8310_10.txt 10561_8.txt 1561_8.txt 3811_9.txt 6061_10.txt 8311_9.txt 10562_9.txt 1562_10.txt 3812_9.txt 6062_10.txt 8312_10.txt 10563_7.txt 1563_10.txt 3813_7.txt 6063_10.txt 8313_10.txt 10564_10.txt 1564_10.txt 3814_7.txt 6064_7.txt 8314_9.txt 10565_9.txt 1565_7.txt 3815_7.txt 6065_10.txt 8315_10.txt 10566_8.txt 1566_8.txt 3816_7.txt 6066_10.txt 8316_9.txt 10567_9.txt 1567_7.txt 3817_8.txt 6067_10.txt 8317_8.txt 10568_10.txt 1568_9.txt 3818_8.txt 6068_9.txt 8318_10.txt 10569_10.txt 1569_10.txt 3819_8.txt 6069_8.txt 8319_10.txt 1056_10.txt 156_8.txt 381_10.txt 606_10.txt 831_9.txt 10570_8.txt 1570_8.txt 3820_8.txt 6070_8.txt 8320_8.txt 10571_8.txt 1571_10.txt 3821_9.txt 6071_8.txt 8321_7.txt 10572_8.txt 1572_10.txt 3822_10.txt 6072_7.txt 8322_7.txt 10573_10.txt 1573_9.txt 3823_9.txt 6073_9.txt 8323_10.txt 10574_10.txt 1574_9.txt 3824_9.txt 6074_7.txt 8324_8.txt 10575_9.txt 1575_9.txt 3825_8.txt 6075_10.txt 8325_9.txt 10576_7.txt 1576_10.txt 3826_10.txt 6076_9.txt 8326_7.txt 10577_10.txt 1577_9.txt 3827_10.txt 6077_9.txt 8327_9.txt 10578_7.txt 1578_8.txt 3828_10.txt 6078_8.txt 8328_10.txt 10579_10.txt 1579_10.txt 3829_10.txt 6079_9.txt 8329_7.txt 1057_9.txt 157_9.txt 382_10.txt 607_10.txt 832_8.txt 10580_8.txt 1580_7.txt 3830_10.txt 6080_10.txt 8330_7.txt 10581_10.txt 1581_7.txt 3831_10.txt 6081_9.txt 8331_8.txt 10582_10.txt 1582_8.txt 3832_8.txt 6082_9.txt 8332_9.txt 10583_10.txt 1583_8.txt 3833_8.txt 6083_8.txt 8333_9.txt 10584_10.txt 1584_9.txt 3834_9.txt 6084_8.txt 8334_8.txt 10585_9.txt 1585_9.txt 3835_9.txt 6085_8.txt 8335_8.txt 10586_10.txt 1586_7.txt 3836_8.txt 6086_8.txt 8336_10.txt 10587_8.txt 1587_10.txt 3837_9.txt 6087_9.txt 8337_10.txt 10588_10.txt 1588_8.txt 3838_7.txt 6088_10.txt 8338_8.txt 10589_10.txt 1589_10.txt 3839_7.txt 6089_10.txt 8339_10.txt 1058_10.txt 158_10.txt 383_10.txt 608_8.txt 833_8.txt 10590_8.txt 1590_10.txt 3840_10.txt 6090_7.txt 8340_8.txt 10591_10.txt 1591_10.txt 3841_9.txt 6091_7.txt 8341_10.txt 10592_8.txt 1592_9.txt 3842_7.txt 6092_8.txt 8342_7.txt 10593_8.txt 1593_10.txt 3843_7.txt 6093_8.txt 8343_7.txt 10594_8.txt 1594_8.txt 3844_8.txt 6094_10.txt 8344_10.txt 10595_10.txt 1595_7.txt 3845_7.txt 6095_7.txt 8345_7.txt 10596_8.txt 1596_8.txt 3846_10.txt 6096_10.txt 8346_9.txt 10597_9.txt 1597_8.txt 3847_9.txt 6097_10.txt 8347_10.txt 10598_8.txt 1598_8.txt 3848_8.txt 6098_10.txt 8348_10.txt 10599_8.txt 1599_7.txt 3849_8.txt 6099_8.txt 8349_10.txt 1059_10.txt 159_10.txt 384_8.txt 609_9.txt 834_8.txt 105_7.txt 15_7.txt 3850_8.txt 60_8.txt 8350_9.txt 10600_9.txt 1600_7.txt 3851_10.txt 6100_10.txt 8351_9.txt 10601_10.txt 1601_8.txt 3852_8.txt 6101_8.txt 8352_9.txt 10602_10.txt 1602_9.txt 3853_9.txt 6102_10.txt 8353_10.txt 10603_10.txt 1603_9.txt 3854_10.txt 6103_10.txt 8354_8.txt 10604_7.txt 1604_9.txt 3855_8.txt 6104_10.txt 8355_7.txt 10605_7.txt 1605_10.txt 3856_10.txt 6105_10.txt 8356_10.txt 10606_10.txt 1606_7.txt 3857_9.txt 6106_9.txt 8357_10.txt 10607_10.txt 1607_9.txt 3858_10.txt 6107_7.txt 8358_9.txt 10608_10.txt 1608_9.txt 3859_8.txt 6108_8.txt 8359_8.txt 10609_10.txt 1609_9.txt 385_10.txt 6109_8.txt 835_8.txt 1060_10.txt 160_9.txt 3860_10.txt 610_9.txt 8360_9.txt 10610_8.txt 1610_10.txt 3861_10.txt 6110_8.txt 8361_8.txt 10611_8.txt 1611_10.txt 3862_9.txt 6111_10.txt 8362_7.txt 10612_7.txt 1612_10.txt 3863_7.txt 6112_7.txt 8363_8.txt 10613_8.txt 1613_10.txt 3864_7.txt 6113_8.txt 8364_10.txt 10614_7.txt 1614_10.txt 3865_8.txt 6114_7.txt 8365_8.txt 10615_8.txt 1615_8.txt 3866_7.txt 6115_10.txt 8366_10.txt 10616_7.txt 1616_9.txt 3867_10.txt 6116_8.txt 8367_10.txt 10617_8.txt 1617_8.txt 3868_10.txt 6117_10.txt 8368_10.txt 10618_8.txt 1618_10.txt 3869_9.txt 6118_10.txt 8369_8.txt 10619_8.txt 1619_10.txt 386_7.txt 6119_9.txt 836_8.txt 1061_10.txt 161_8.txt 3870_9.txt 611_10.txt 8370_10.txt 10620_10.txt 1620_10.txt 3871_8.txt 6120_9.txt 8371_10.txt 10621_10.txt 1621_10.txt 3872_9.txt 6121_10.txt 8372_10.txt 10622_10.txt 1622_8.txt 3873_9.txt 6122_8.txt 8373_9.txt 10623_8.txt 1623_10.txt 3874_9.txt 6123_7.txt 8374_7.txt 10624_7.txt 1624_10.txt 3875_9.txt 6124_7.txt 8375_8.txt 10625_7.txt 1625_7.txt 3876_8.txt 6125_9.txt 8376_10.txt 10626_7.txt 1626_10.txt 3877_8.txt 6126_10.txt 8377_8.txt 10627_10.txt 1627_10.txt 3878_10.txt 6127_8.txt 8378_10.txt 10628_7.txt 1628_10.txt 3879_8.txt 6128_7.txt 8379_10.txt 10629_10.txt 1629_10.txt 387_8.txt 6129_7.txt 837_7.txt 1062_10.txt 162_8.txt 3880_8.txt 612_10.txt 8380_9.txt 10630_8.txt 1630_8.txt 3881_9.txt 6130_7.txt 8381_8.txt 10631_8.txt 1631_8.txt 3882_10.txt 6131_10.txt 8382_7.txt 10632_10.txt 1632_7.txt 3883_10.txt 6132_10.txt 8383_7.txt 10633_9.txt 1633_9.txt 3884_8.txt 6133_7.txt 8384_7.txt 10634_10.txt 1634_7.txt 3885_10.txt 6134_7.txt 8385_10.txt 10635_8.txt 1635_7.txt 3886_10.txt 6135_8.txt 8386_9.txt 10636_8.txt 1636_10.txt 3887_10.txt 6136_10.txt 8387_8.txt 10637_10.txt 1637_10.txt 3888_10.txt 6137_7.txt 8388_7.txt 10638_8.txt 1638_8.txt 3889_10.txt 6138_8.txt 8389_8.txt 10639_7.txt 1639_10.txt 388_8.txt 6139_10.txt 838_9.txt 1063_10.txt 163_10.txt 3890_10.txt 613_10.txt 8390_8.txt 10640_8.txt 1640_10.txt 3891_10.txt 6140_9.txt 8391_8.txt 10641_7.txt 1641_10.txt 3892_10.txt 6141_9.txt 8392_8.txt 10642_8.txt 1642_10.txt 3893_10.txt 6142_9.txt 8393_8.txt 10643_8.txt 1643_10.txt 3894_10.txt 6143_7.txt 8394_10.txt 10644_8.txt 1644_10.txt 3895_8.txt 6144_7.txt 8395_8.txt 10645_8.txt 1645_9.txt 3896_7.txt 6145_8.txt 8396_8.txt 10646_8.txt 1646_9.txt 3897_10.txt 6146_7.txt 8397_10.txt 10647_8.txt 1647_10.txt 3898_10.txt 6147_7.txt 8398_8.txt 10648_8.txt 1648_10.txt 3899_9.txt 6148_10.txt 8399_10.txt 10649_7.txt 1649_8.txt 389_10.txt 6149_7.txt 839_7.txt 1064_10.txt 164_10.txt 38_10.txt 614_10.txt 83_10.txt 10650_8.txt 1650_10.txt 3900_10.txt 6150_7.txt 8400_7.txt 10651_7.txt 1651_10.txt 3901_10.txt 6151_7.txt 8401_10.txt 10652_9.txt 1652_10.txt 3902_10.txt 6152_10.txt 8402_10.txt 10653_10.txt 1653_10.txt 3903_10.txt 6153_7.txt 8403_10.txt 10654_7.txt 1654_7.txt 3904_10.txt 6154_10.txt 8404_10.txt 10655_9.txt 1655_8.txt 3905_10.txt 6155_7.txt 8405_9.txt 10656_7.txt 1656_9.txt 3906_10.txt 6156_10.txt 8406_10.txt 10657_8.txt 1657_9.txt 3907_10.txt 6157_10.txt 8407_7.txt 10658_10.txt 1658_10.txt 3908_10.txt 6158_10.txt 8408_10.txt 10659_8.txt 1659_7.txt 3909_10.txt 6159_7.txt 8409_8.txt 1065_10.txt 165_7.txt 390_10.txt 615_10.txt 840_9.txt 10660_10.txt 1660_8.txt 3910_10.txt 6160_10.txt 8410_10.txt 10661_9.txt 1661_8.txt 3911_10.txt 6161_9.txt 8411_7.txt 10662_7.txt 1662_7.txt 3912_10.txt 6162_8.txt 8412_7.txt 10663_8.txt 1663_9.txt 3913_10.txt 6163_10.txt 8413_8.txt 10664_8.txt 1664_10.txt 3914_10.txt 6164_7.txt 8414_7.txt 10665_8.txt 1665_7.txt 3915_9.txt 6165_8.txt 8415_7.txt 10666_8.txt 1666_8.txt 3916_8.txt 6166_10.txt 8416_7.txt 10667_8.txt 1667_7.txt 3917_9.txt 6167_7.txt 8417_8.txt 10668_7.txt 1668_8.txt 3918_10.txt 6168_10.txt 8418_7.txt 10669_10.txt 1669_8.txt 3919_7.txt 6169_10.txt 8419_9.txt 1066_10.txt 166_7.txt 391_8.txt 616_7.txt 841_10.txt 10670_10.txt 1670_8.txt 3920_9.txt 6170_10.txt 8420_9.txt 10671_10.txt 1671_8.txt 3921_9.txt 6171_8.txt 8421_10.txt 10672_9.txt 1672_8.txt 3922_10.txt 6172_7.txt 8422_10.txt 10673_10.txt 1673_8.txt 3923_10.txt 6173_8.txt 8423_8.txt 10674_8.txt 1674_8.txt 3924_7.txt 6174_10.txt 8424_9.txt 10675_8.txt 1675_9.txt 3925_10.txt 6175_8.txt 8425_9.txt 10676_9.txt 1676_9.txt 3926_7.txt 6176_9.txt 8426_7.txt 10677_8.txt 1677_9.txt 3927_9.txt 6177_7.txt 8427_7.txt 10678_9.txt 1678_9.txt 3928_7.txt 6178_7.txt 8428_7.txt 10679_10.txt 1679_9.txt 3929_9.txt 6179_8.txt 8429_7.txt 1067_7.txt 167_7.txt 392_9.txt 617_7.txt 842_9.txt 10680_8.txt 1680_8.txt 3930_9.txt 6180_7.txt 8430_9.txt 10681_10.txt 1681_7.txt 3931_7.txt 6181_9.txt 8431_9.txt 10682_10.txt 1682_7.txt 3932_8.txt 6182_7.txt 8432_10.txt 10683_7.txt 1683_7.txt 3933_7.txt 6183_7.txt 8433_9.txt 10684_9.txt 1684_10.txt 3934_8.txt 6184_7.txt 8434_9.txt 10685_7.txt 1685_10.txt 3935_9.txt 6185_8.txt 8435_10.txt 10686_8.txt 1686_10.txt 3936_7.txt 6186_10.txt 8436_10.txt 10687_10.txt 1687_10.txt 3937_8.txt 6187_9.txt 8437_10.txt 10688_9.txt 1688_9.txt 3938_9.txt 6188_9.txt 8438_10.txt 10689_8.txt 1689_10.txt 3939_7.txt 6189_10.txt 8439_10.txt 1068_10.txt 168_9.txt 393_8.txt 618_10.txt 843_10.txt 10690_10.txt 1690_10.txt 3940_9.txt 6190_7.txt 8440_8.txt 10691_7.txt 1691_8.txt 3941_9.txt 6191_9.txt 8441_10.txt 10692_8.txt 1692_8.txt 3942_9.txt 6192_9.txt 8442_9.txt 10693_8.txt 1693_10.txt 3943_10.txt 6193_7.txt 8443_9.txt 10694_7.txt 1694_10.txt 3944_8.txt 6194_8.txt 8444_10.txt 10695_8.txt 1695_10.txt 3945_10.txt 6195_10.txt 8445_10.txt 10696_7.txt 1696_10.txt 3946_7.txt 6196_8.txt 8446_10.txt 10697_8.txt 1697_10.txt 3947_7.txt 6197_10.txt 8447_8.txt 10698_9.txt 1698_10.txt 3948_9.txt 6198_7.txt 8448_10.txt 10699_9.txt 1699_10.txt 3949_8.txt 6199_8.txt 8449_9.txt 1069_10.txt 169_8.txt 394_8.txt 619_9.txt 844_8.txt 106_10.txt 16_7.txt 3950_7.txt 61_10.txt 8450_7.txt 10700_8.txt 1700_8.txt 3951_10.txt 6200_7.txt 8451_10.txt 10701_10.txt 1701_10.txt 3952_10.txt 6201_9.txt 8452_7.txt 10702_10.txt 1702_9.txt 3953_10.txt 6202_7.txt 8453_10.txt 10703_7.txt 1703_8.txt 3954_9.txt 6203_7.txt 8454_8.txt 10704_10.txt 1704_8.txt 3955_9.txt 6204_7.txt 8455_10.txt 10705_7.txt 1705_10.txt 3956_10.txt 6205_8.txt 8456_10.txt 10706_7.txt 1706_9.txt 3957_10.txt 6206_8.txt 8457_9.txt 10707_8.txt 1707_10.txt 3958_7.txt 6207_9.txt 8458_10.txt 10708_8.txt 1708_10.txt 3959_9.txt 6208_7.txt 8459_10.txt 10709_10.txt 1709_8.txt 395_10.txt 6209_8.txt 845_7.txt 1070_8.txt 170_10.txt 3960_10.txt 620_10.txt 8460_7.txt 10710_9.txt 1710_7.txt 3961_8.txt 6210_10.txt 8461_7.txt 10711_10.txt 1711_8.txt 3962_10.txt 6211_8.txt 8462_9.txt 10712_8.txt 1712_9.txt 3963_7.txt 6212_9.txt 8463_9.txt 10713_9.txt 1713_8.txt 3964_7.txt 6213_7.txt 8464_10.txt 10714_8.txt 1714_8.txt 3965_8.txt 6214_7.txt 8465_8.txt 10715_8.txt 1715_8.txt 3966_9.txt 6215_7.txt 8466_9.txt 10716_7.txt 1716_8.txt 3967_7.txt 6216_7.txt 8467_7.txt 10717_10.txt 1717_8.txt 3968_10.txt 6217_8.txt 8468_7.txt 10718_10.txt 1718_7.txt 3969_8.txt 6218_7.txt 8469_7.txt 10719_10.txt 1719_7.txt 396_8.txt 6219_7.txt 846_7.txt 1071_8.txt 171_8.txt 3970_7.txt 621_10.txt 8470_8.txt 10720_9.txt 1720_10.txt 3971_8.txt 6220_8.txt 8471_10.txt 10721_9.txt 1721_8.txt 3972_7.txt 6221_7.txt 8472_10.txt 10722_10.txt 1722_7.txt 3973_7.txt 6222_9.txt 8473_8.txt 10723_8.txt 1723_7.txt 3974_8.txt 6223_7.txt 8474_10.txt 10724_8.txt 1724_10.txt 3975_10.txt 6224_7.txt 8475_10.txt 10725_9.txt 1725_8.txt 3976_10.txt 6225_8.txt 8476_10.txt 10726_7.txt 1726_8.txt 3977_10.txt 6226_10.txt 8477_10.txt 10727_7.txt 1727_10.txt 3978_10.txt 6227_7.txt 8478_8.txt 10728_10.txt 1728_7.txt 3979_10.txt 6228_7.txt 8479_9.txt 10729_8.txt 1729_8.txt 397_9.txt 6229_7.txt 847_7.txt 1072_10.txt 172_10.txt 3980_10.txt 622_10.txt 8480_10.txt 10730_10.txt 1730_10.txt 3981_10.txt 6230_8.txt 8481_8.txt 10731_7.txt 1731_10.txt 3982_7.txt 6231_10.txt 8482_7.txt 10732_8.txt 1732_10.txt 3983_10.txt 6232_10.txt 8483_7.txt 10733_7.txt 1733_7.txt 3984_7.txt 6233_9.txt 8484_10.txt 10734_10.txt 1734_7.txt 3985_8.txt 6234_8.txt 8485_8.txt 10735_10.txt 1735_10.txt 3986_10.txt 6235_8.txt 8486_10.txt 10736_10.txt 1736_10.txt 3987_7.txt 6236_8.txt 8487_10.txt 10737_10.txt 1737_8.txt 3988_7.txt 6237_7.txt 8488_8.txt 10738_9.txt 1738_7.txt 3989_8.txt 6238_9.txt 8489_9.txt 10739_10.txt 1739_10.txt 398_10.txt 6239_8.txt 848_8.txt 1073_9.txt 173_7.txt 3990_8.txt 623_10.txt 8490_7.txt 10740_8.txt 1740_8.txt 3991_7.txt 6240_10.txt 8491_7.txt 10741_10.txt 1741_9.txt 3992_8.txt 6241_10.txt 8492_9.txt 10742_9.txt 1742_7.txt 3993_7.txt 6242_8.txt 8493_10.txt 10743_9.txt 1743_9.txt 3994_8.txt 6243_9.txt 8494_8.txt 10744_8.txt 1744_10.txt 3995_8.txt 6244_8.txt 8495_8.txt 10745_10.txt 1745_7.txt 3996_9.txt 6245_10.txt 8496_7.txt 10746_10.txt 1746_7.txt 3997_10.txt 6246_9.txt 8497_7.txt 10747_10.txt 1747_10.txt 3998_10.txt 6247_10.txt 8498_9.txt 10748_10.txt 1748_8.txt 3999_10.txt 6248_7.txt 8499_9.txt 10749_8.txt 1749_10.txt 399_9.txt 6249_7.txt 849_7.txt 1074_10.txt 174_7.txt 39_9.txt 624_9.txt 84_10.txt 10750_8.txt 1750_10.txt 3_10.txt 6250_10.txt 8500_7.txt 10751_10.txt 1751_8.txt 4000_10.txt 6251_7.txt 8501_8.txt 10752_10.txt 1752_8.txt 4001_8.txt 6252_10.txt 8502_9.txt 10753_10.txt 1753_9.txt 4002_8.txt 6253_8.txt 8503_8.txt 10754_10.txt 1754_10.txt 4003_9.txt 6254_10.txt 8504_8.txt 10755_10.txt 1755_10.txt 4004_9.txt 6255_8.txt 8505_8.txt 10756_8.txt 1756_7.txt 4005_10.txt 6256_8.txt 8506_8.txt 10757_10.txt 1757_8.txt 4006_8.txt 6257_10.txt 8507_10.txt 10758_8.txt 1758_10.txt 4007_9.txt 6258_8.txt 8508_7.txt 10759_9.txt 1759_8.txt 4008_9.txt 6259_8.txt 8509_7.txt 1075_10.txt 175_7.txt 4009_9.txt 625_10.txt 850_8.txt 10760_8.txt 1760_10.txt 400_10.txt 6260_10.txt 8510_7.txt 10761_10.txt 1761_9.txt 4010_10.txt 6261_8.txt 8511_10.txt 10762_10.txt 1762_7.txt 4011_8.txt 6262_9.txt 8512_7.txt 10763_8.txt 1763_9.txt 4012_8.txt 6263_7.txt 8513_9.txt 10764_9.txt 1764_10.txt 4013_7.txt 6264_7.txt 8514_7.txt 10765_10.txt 1765_8.txt 4014_10.txt 6265_7.txt 8515_8.txt 10766_7.txt 1766_10.txt 4015_7.txt 6266_8.txt 8516_9.txt 10767_10.txt 1767_8.txt 4016_10.txt 6267_10.txt 8517_8.txt 10768_7.txt 1768_9.txt 4017_7.txt 6268_9.txt 8518_10.txt 10769_10.txt 1769_8.txt 4018_7.txt 6269_10.txt 8519_10.txt 1076_8.txt 176_7.txt 4019_7.txt 626_9.txt 851_7.txt 10770_7.txt 1770_10.txt 401_10.txt 6270_8.txt 8520_8.txt 10771_10.txt 1771_10.txt 4020_10.txt 6271_9.txt 8521_10.txt 10772_10.txt 1772_10.txt 4021_7.txt 6272_7.txt 8522_10.txt 10773_9.txt 1773_9.txt 4022_8.txt 6273_9.txt 8523_9.txt 10774_8.txt 1774_10.txt 4023_9.txt 6274_8.txt 8524_10.txt 10775_8.txt 1775_9.txt 4024_9.txt 6275_7.txt 8525_10.txt 10776_8.txt 1776_10.txt 4025_9.txt 6276_7.txt 8526_10.txt 10777_9.txt 1777_10.txt 4026_9.txt 6277_7.txt 8527_8.txt 10778_8.txt 1778_10.txt 4027_10.txt 6278_7.txt 8528_7.txt 10779_10.txt 1779_10.txt 4028_10.txt 6279_10.txt 8529_9.txt 1077_8.txt 177_9.txt 4029_10.txt 627_8.txt 852_7.txt 10780_10.txt 1780_10.txt 402_10.txt 6280_7.txt 8530_9.txt 10781_10.txt 1781_10.txt 4030_9.txt 6281_8.txt 8531_7.txt 10782_7.txt 1782_7.txt 4031_10.txt 6282_7.txt 8532_7.txt 10783_10.txt 1783_8.txt 4032_9.txt 6283_7.txt 8533_8.txt 10784_10.txt 1784_10.txt 4033_9.txt 6284_7.txt 8534_10.txt 10785_10.txt 1785_9.txt 4034_9.txt 6285_7.txt 8535_7.txt 10786_10.txt 1786_7.txt 4035_10.txt 6286_8.txt 8536_9.txt 10787_10.txt 1787_10.txt 4036_7.txt 6287_10.txt 8537_10.txt 10788_10.txt 1788_7.txt 4037_7.txt 6288_7.txt 8538_10.txt 10789_10.txt 1789_7.txt 4038_10.txt 6289_10.txt 8539_8.txt 1078_8.txt 178_7.txt 4039_10.txt 628_9.txt 853_9.txt 10790_8.txt 1790_9.txt 403_8.txt 6290_7.txt 8540_9.txt 10791_9.txt 1791_8.txt 4040_8.txt 6291_10.txt 8541_10.txt 10792_9.txt 1792_10.txt 4041_7.txt 6292_10.txt 8542_9.txt 10793_10.txt 1793_9.txt 4042_7.txt 6293_10.txt 8543_8.txt 10794_10.txt 1794_7.txt 4043_7.txt 6294_10.txt 8544_9.txt 10795_7.txt 1795_8.txt 4044_9.txt 6295_7.txt 8545_9.txt 10796_9.txt 1796_8.txt 4045_10.txt 6296_7.txt 8546_10.txt 10797_8.txt 1797_9.txt 4046_8.txt 6297_10.txt 8547_10.txt 10798_8.txt 1798_9.txt 4047_10.txt 6298_9.txt 8548_10.txt 10799_7.txt 1799_7.txt 4048_7.txt 6299_8.txt 8549_8.txt 1079_7.txt 179_8.txt 4049_7.txt 629_9.txt 854_9.txt 107_10.txt 17_9.txt 404_9.txt 62_10.txt 8550_8.txt 10800_8.txt 1800_8.txt 4050_9.txt 6300_8.txt 8551_8.txt 10801_8.txt 1801_8.txt 4051_8.txt 6301_10.txt 8552_9.txt 10802_8.txt 1802_9.txt 4052_8.txt 6302_8.txt 8553_7.txt 10803_8.txt 1803_10.txt 4053_8.txt 6303_8.txt 8554_7.txt 10804_10.txt 1804_10.txt 4054_9.txt 6304_8.txt 8555_9.txt 10805_10.txt 1805_10.txt 4055_10.txt 6305_10.txt 8556_7.txt 10806_9.txt 1806_8.txt 4056_7.txt 6306_10.txt 8557_7.txt 10807_9.txt 1807_7.txt 4057_7.txt 6307_8.txt 8558_10.txt 10808_10.txt 1808_7.txt 4058_10.txt 6308_8.txt 8559_8.txt 10809_10.txt 1809_10.txt 4059_8.txt 6309_8.txt 855_9.txt 1080_9.txt 180_9.txt 405_10.txt 630_10.txt 8560_10.txt 10810_8.txt 1810_7.txt 4060_10.txt 6310_10.txt 8561_10.txt 10811_7.txt 1811_10.txt 4061_10.txt 6311_10.txt 8562_10.txt 10812_8.txt 1812_10.txt 4062_10.txt 6312_7.txt 8563_10.txt 10813_10.txt 1813_8.txt 4063_8.txt 6313_9.txt 8564_7.txt 10814_7.txt 1814_10.txt 4064_10.txt 6314_9.txt 8565_7.txt 10815_10.txt 1815_10.txt 4065_10.txt 6315_10.txt 8566_10.txt 10816_10.txt 1816_9.txt 4066_10.txt 6316_8.txt 8567_10.txt 10817_10.txt 1817_8.txt 4067_8.txt 6317_10.txt 8568_7.txt 10818_10.txt 1818_8.txt 4068_10.txt 6318_7.txt 8569_9.txt 10819_10.txt 1819_9.txt 4069_10.txt 6319_10.txt 856_7.txt 1081_10.txt 181_10.txt 406_8.txt 631_10.txt 8570_7.txt 10820_10.txt 1820_9.txt 4070_10.txt 6320_10.txt 8571_7.txt 10821_8.txt 1821_8.txt 4071_10.txt 6321_7.txt 8572_7.txt 10822_10.txt 1822_8.txt 4072_10.txt 6322_7.txt 8573_10.txt 10823_8.txt 1823_7.txt 4073_10.txt 6323_7.txt 8574_9.txt 10824_10.txt 1824_8.txt 4074_10.txt 6324_8.txt 8575_10.txt 10825_9.txt 1825_10.txt 4075_10.txt 6325_7.txt 8576_8.txt 10826_10.txt 1826_10.txt 4076_10.txt 6326_10.txt 8577_8.txt 10827_10.txt 1827_10.txt 4077_10.txt 6327_10.txt 8578_8.txt 10828_10.txt 1828_8.txt 4078_10.txt 6328_10.txt 8579_8.txt 10829_10.txt 1829_8.txt 4079_9.txt 6329_10.txt 857_8.txt 1082_10.txt 182_10.txt 407_10.txt 632_10.txt 8580_9.txt 10830_10.txt 1830_8.txt 4080_10.txt 6330_10.txt 8581_10.txt 10831_7.txt 1831_10.txt 4081_10.txt 6331_10.txt 8582_9.txt 10832_10.txt 1832_7.txt 4082_10.txt 6332_8.txt 8583_9.txt 10833_10.txt 1833_10.txt 4083_10.txt 6333_8.txt 8584_8.txt 10834_7.txt 1834_8.txt 4084_7.txt 6334_8.txt 8585_8.txt 10835_10.txt 1835_10.txt 4085_10.txt 6335_10.txt 8586_8.txt 10836_10.txt 1836_7.txt 4086_7.txt 6336_7.txt 8587_7.txt 10837_10.txt 1837_10.txt 4087_10.txt 6337_7.txt 8588_7.txt 10838_10.txt 1838_10.txt 4088_7.txt 6338_9.txt 8589_7.txt 10839_10.txt 1839_10.txt 4089_8.txt 6339_7.txt 858_8.txt 1083_10.txt 183_8.txt 408_10.txt 633_8.txt 8590_7.txt 10840_9.txt 1840_10.txt 4090_8.txt 6340_10.txt 8591_7.txt 10841_10.txt 1841_7.txt 4091_7.txt 6341_7.txt 8592_7.txt 10842_7.txt 1842_9.txt 4092_10.txt 6342_10.txt 8593_7.txt 10843_7.txt 1843_9.txt 4093_8.txt 6343_10.txt 8594_7.txt 10844_9.txt 1844_8.txt 4094_9.txt 6344_7.txt 8595_10.txt 10845_10.txt 1845_7.txt 4095_8.txt 6345_7.txt 8596_7.txt 10846_9.txt 1846_8.txt 4096_10.txt 6346_10.txt 8597_9.txt 10847_10.txt 1847_10.txt 4097_8.txt 6347_9.txt 8598_7.txt 10848_10.txt 1848_8.txt 4098_10.txt 6348_10.txt 8599_7.txt 10849_10.txt 1849_7.txt 4099_8.txt 6349_10.txt 859_9.txt 1084_9.txt 184_8.txt 409_10.txt 634_8.txt 85_10.txt 10850_10.txt 1850_10.txt 40_8.txt 6350_8.txt 8600_8.txt 10851_9.txt 1851_10.txt 4100_10.txt 6351_8.txt 8601_10.txt 10852_10.txt 1852_9.txt 4101_8.txt 6352_10.txt 8602_10.txt 10853_10.txt 1853_8.txt 4102_10.txt 6353_10.txt 8603_10.txt 10854_10.txt 1854_10.txt 4103_7.txt 6354_10.txt 8604_10.txt 10855_9.txt 1855_9.txt 4104_9.txt 6355_8.txt 8605_10.txt 10856_8.txt 1856_9.txt 4105_10.txt 6356_8.txt 8606_10.txt 10857_8.txt 1857_10.txt 4106_7.txt 6357_9.txt 8607_9.txt 10858_8.txt 1858_10.txt 4107_10.txt 6358_10.txt 8608_9.txt 10859_7.txt 1859_8.txt 4108_7.txt 6359_10.txt 8609_10.txt 1085_7.txt 185_9.txt 4109_10.txt 635_9.txt 860_8.txt 10860_7.txt 1860_9.txt 410_8.txt 6360_7.txt 8610_10.txt 10861_7.txt 1861_10.txt 4110_10.txt 6361_10.txt 8611_8.txt 10862_9.txt 1862_8.txt 4111_10.txt 6362_7.txt 8612_7.txt 10863_8.txt 1863_10.txt 4112_9.txt 6363_8.txt 8613_7.txt 10864_8.txt 1864_10.txt 4113_7.txt 6364_8.txt 8614_9.txt 10865_7.txt 1865_8.txt 4114_9.txt 6365_7.txt 8615_10.txt 10866_7.txt 1866_8.txt 4115_8.txt 6366_8.txt 8616_8.txt 10867_7.txt 1867_9.txt 4116_9.txt 6367_7.txt 8617_9.txt 10868_8.txt 1868_10.txt 4117_9.txt 6368_7.txt 8618_8.txt 10869_7.txt 1869_9.txt 4118_10.txt 6369_10.txt 8619_7.txt 1086_7.txt 186_8.txt 4119_8.txt 636_10.txt 861_7.txt 10870_8.txt 1870_10.txt 411_10.txt 6370_8.txt 8620_8.txt 10871_7.txt 1871_10.txt 4120_9.txt 6371_9.txt 8621_10.txt 10872_7.txt 1872_7.txt 4121_10.txt 6372_7.txt 8622_8.txt 10873_8.txt 1873_8.txt 4122_10.txt 6373_10.txt 8623_7.txt 10874_10.txt 1874_10.txt 4123_7.txt 6374_10.txt 8624_7.txt 10875_8.txt 1875_10.txt 4124_8.txt 6375_8.txt 8625_7.txt 10876_7.txt 1876_10.txt 4125_8.txt 6376_10.txt 8626_7.txt 10877_10.txt 1877_7.txt 4126_8.txt 6377_9.txt 8627_10.txt 10878_7.txt 1878_10.txt 4127_8.txt 6378_8.txt 8628_10.txt 10879_10.txt 1879_10.txt 4128_10.txt 6379_9.txt 8629_9.txt 1087_10.txt 187_8.txt 4129_10.txt 637_10.txt 862_7.txt 10880_8.txt 1880_10.txt 412_8.txt 6380_9.txt 8630_9.txt 10881_7.txt 1881_7.txt 4130_9.txt 6381_8.txt 8631_7.txt 10882_8.txt 1882_10.txt 4131_7.txt 6382_7.txt 8632_9.txt 10883_7.txt 1883_7.txt 4132_7.txt 6383_10.txt 8633_8.txt 10884_8.txt 1884_8.txt 4133_7.txt 6384_9.txt 8634_9.txt 10885_7.txt 1885_10.txt 4134_7.txt 6385_10.txt 8635_7.txt 10886_10.txt 1886_10.txt 4135_10.txt 6386_10.txt 8636_7.txt 10887_7.txt 1887_8.txt 4136_10.txt 6387_8.txt 8637_10.txt 10888_8.txt 1888_8.txt 4137_8.txt 6388_7.txt 8638_7.txt 10889_10.txt 1889_10.txt 4138_10.txt 6389_9.txt 8639_9.txt 1088_9.txt 188_7.txt 4139_8.txt 638_10.txt 863_10.txt 10890_9.txt 1890_10.txt 413_10.txt 6390_8.txt 8640_10.txt 10891_7.txt 1891_8.txt 4140_10.txt 6391_8.txt 8641_7.txt 10892_7.txt 1892_8.txt 4141_10.txt 6392_10.txt 8642_8.txt 10893_8.txt 1893_10.txt 4142_10.txt 6393_8.txt 8643_7.txt 10894_8.txt 1894_8.txt 4143_9.txt 6394_10.txt 8644_7.txt 10895_7.txt 1895_10.txt 4144_10.txt 6395_9.txt 8645_8.txt 10896_8.txt 1896_8.txt 4145_10.txt 6396_9.txt 8646_7.txt 10897_9.txt 1897_10.txt 4146_10.txt 6397_8.txt 8647_7.txt 10898_7.txt 1898_9.txt 4147_8.txt 6398_8.txt 8648_9.txt 10899_10.txt 1899_7.txt 4148_7.txt 6399_7.txt 8649_9.txt 1089_10.txt 189_9.txt 4149_10.txt 639_10.txt 864_9.txt 108_10.txt 18_7.txt 414_10.txt 63_10.txt 8650_10.txt 10900_8.txt 1900_8.txt 4150_10.txt 6400_9.txt 8651_9.txt 10901_10.txt 1901_7.txt 4151_10.txt 6401_8.txt 8652_8.txt 10902_10.txt 1902_10.txt 4152_10.txt 6402_8.txt 8653_10.txt 10903_10.txt 1903_10.txt 4153_10.txt 6403_8.txt 8654_9.txt 10904_10.txt 1904_7.txt 4154_10.txt 6404_8.txt 8655_9.txt 10905_8.txt 1905_10.txt 4155_10.txt 6405_7.txt 8656_9.txt 10906_10.txt 1906_10.txt 4156_10.txt 6406_9.txt 8657_9.txt 10907_9.txt 1907_7.txt 4157_8.txt 6407_10.txt 8658_8.txt 10908_10.txt 1908_9.txt 4158_10.txt 6408_10.txt 8659_7.txt 10909_10.txt 1909_10.txt 4159_9.txt 6409_7.txt 865_9.txt 1090_8.txt 190_10.txt 415_7.txt 640_10.txt 8660_7.txt 10910_10.txt 1910_9.txt 4160_9.txt 6410_10.txt 8661_8.txt 10911_7.txt 1911_10.txt 4161_8.txt 6411_10.txt 8662_8.txt 10912_7.txt 1912_9.txt 4162_10.txt 6412_9.txt 8663_7.txt 10913_7.txt 1913_10.txt 4163_9.txt 6413_7.txt 8664_8.txt 10914_8.txt 1914_7.txt 4164_10.txt 6414_8.txt 8665_8.txt 10915_9.txt 1915_10.txt 4165_8.txt 6415_8.txt 8666_7.txt 10916_7.txt 1916_8.txt 4166_9.txt 6416_7.txt 8667_7.txt 10917_8.txt 1917_9.txt 4167_10.txt 6417_7.txt 8668_9.txt 10918_8.txt 1918_9.txt 4168_7.txt 6418_10.txt 8669_7.txt 10919_10.txt 1919_8.txt 4169_7.txt 6419_10.txt 866_8.txt 1091_10.txt 191_9.txt 416_8.txt 641_8.txt 8670_8.txt 10920_10.txt 1920_10.txt 4170_8.txt 6420_7.txt 8671_7.txt 10921_10.txt 1921_9.txt 4171_7.txt 6421_8.txt 8672_8.txt 10922_10.txt 1922_9.txt 4172_10.txt 6422_7.txt 8673_7.txt 10923_9.txt 1923_10.txt 4173_10.txt 6423_7.txt 8674_9.txt 10924_10.txt 1924_10.txt 4174_9.txt 6424_10.txt 8675_8.txt 10925_9.txt 1925_9.txt 4175_8.txt 6425_9.txt 8676_8.txt 10926_9.txt 1926_10.txt 4176_10.txt 6426_8.txt 8677_9.txt 10927_10.txt 1927_8.txt 4177_9.txt 6427_10.txt 8678_9.txt 10928_7.txt 1928_10.txt 4178_10.txt 6428_8.txt 8679_8.txt 10929_10.txt 1929_10.txt 4179_10.txt 6429_7.txt 867_8.txt 1092_10.txt 192_9.txt 417_7.txt 642_10.txt 8680_9.txt 10930_8.txt 1930_10.txt 4180_10.txt 6430_10.txt 8681_8.txt 10931_7.txt 1931_8.txt 4181_9.txt 6431_8.txt 8682_7.txt 10932_7.txt 1932_10.txt 4182_10.txt 6432_9.txt 8683_9.txt 10933_10.txt 1933_8.txt 4183_10.txt 6433_7.txt 8684_10.txt 10934_10.txt 1934_9.txt 4184_8.txt 6434_7.txt 8685_9.txt 10935_7.txt 1935_10.txt 4185_8.txt 6435_7.txt 8686_8.txt 10936_8.txt 1936_10.txt 4186_7.txt 6436_10.txt 8687_9.txt 10937_9.txt 1937_10.txt 4187_7.txt 6437_9.txt 8688_8.txt 10938_10.txt 1938_9.txt 4188_8.txt 6438_10.txt 8689_7.txt 10939_8.txt 1939_8.txt 4189_8.txt 6439_10.txt 868_8.txt 1093_8.txt 193_7.txt 418_9.txt 643_10.txt 8690_8.txt 10940_10.txt 1940_10.txt 4190_10.txt 6440_8.txt 8691_7.txt 10941_7.txt 1941_9.txt 4191_10.txt 6441_9.txt 8692_10.txt 10942_9.txt 1942_10.txt 4192_10.txt 6442_9.txt 8693_10.txt 10943_10.txt 1943_10.txt 4193_8.txt 6443_9.txt 8694_10.txt 10944_8.txt 1944_8.txt 4194_10.txt 6444_8.txt 8695_9.txt 10945_9.txt 1945_8.txt 4195_9.txt 6445_10.txt 8696_10.txt 10946_7.txt 1946_9.txt 4196_9.txt 6446_10.txt 8697_7.txt 10947_8.txt 1947_8.txt 4197_8.txt 6447_8.txt 8698_10.txt 10948_10.txt 1948_10.txt 4198_7.txt 6448_10.txt 8699_10.txt 10949_8.txt 1949_8.txt 4199_7.txt 6449_10.txt 869_7.txt 1094_9.txt 194_8.txt 419_7.txt 644_9.txt 86_10.txt 10950_9.txt 1950_8.txt 41_9.txt 6450_10.txt 8700_7.txt 10951_8.txt 1951_9.txt 4200_9.txt 6451_8.txt 8701_8.txt 10952_10.txt 1952_8.txt 4201_9.txt 6452_9.txt 8702_10.txt 10953_10.txt 1953_10.txt 4202_10.txt 6453_10.txt 8703_10.txt 10954_10.txt 1954_10.txt 4203_8.txt 6454_9.txt 8704_9.txt 10955_7.txt 1955_9.txt 4204_10.txt 6455_7.txt 8705_8.txt 10956_9.txt 1956_8.txt 4205_8.txt 6456_10.txt 8706_8.txt 10957_10.txt 1957_7.txt 4206_9.txt 6457_7.txt 8707_10.txt 10958_10.txt 1958_10.txt 4207_7.txt 6458_8.txt 8708_8.txt 10959_10.txt 1959_7.txt 4208_10.txt 6459_10.txt 8709_8.txt 1095_9.txt 195_8.txt 4209_7.txt 645_10.txt 870_10.txt 10960_8.txt 1960_7.txt 420_7.txt 6460_10.txt 8710_7.txt 10961_9.txt 1961_9.txt 4210_9.txt 6461_10.txt 8711_7.txt 10962_10.txt 1962_10.txt 4211_8.txt 6462_10.txt 8712_8.txt 10963_7.txt 1963_8.txt 4212_10.txt 6463_8.txt 8713_10.txt 10964_7.txt 1964_7.txt 4213_7.txt 6464_9.txt 8714_10.txt 10965_9.txt 1965_7.txt 4214_10.txt 6465_9.txt 8715_10.txt 10966_9.txt 1966_10.txt 4215_9.txt 6466_10.txt 8716_10.txt 10967_10.txt 1967_8.txt 4216_10.txt 6467_7.txt 8717_9.txt 10968_7.txt 1968_8.txt 4217_9.txt 6468_7.txt 8718_9.txt 10969_10.txt 1969_10.txt 4218_8.txt 6469_9.txt 8719_10.txt 1096_9.txt 196_9.txt 4219_7.txt 646_9.txt 871_9.txt 10970_7.txt 1970_9.txt 421_9.txt 6470_10.txt 8720_9.txt 10971_9.txt 1971_9.txt 4220_7.txt 6471_10.txt 8721_10.txt 10972_10.txt 1972_10.txt 4221_8.txt 6472_10.txt 8722_9.txt 10973_8.txt 1973_8.txt 4222_7.txt 6473_8.txt 8723_8.txt 10974_10.txt 1974_8.txt 4223_8.txt 6474_8.txt 8724_10.txt 10975_10.txt 1975_7.txt 4224_10.txt 6475_10.txt 8725_10.txt 10976_10.txt 1976_10.txt 4225_10.txt 6476_10.txt 8726_9.txt 10977_8.txt 1977_10.txt 4226_10.txt 6477_7.txt 8727_7.txt 10978_10.txt 1978_9.txt 4227_10.txt 6478_10.txt 8728_7.txt 10979_9.txt 1979_9.txt 4228_10.txt 6479_10.txt 8729_10.txt 1097_9.txt 197_9.txt 4229_10.txt 647_10.txt 872_9.txt 10980_7.txt 1980_10.txt 422_7.txt 6480_10.txt 8730_8.txt 10981_8.txt 1981_9.txt 4230_10.txt 6481_10.txt 8731_7.txt 10982_10.txt 1982_10.txt 4231_7.txt 6482_10.txt 8732_9.txt 10983_10.txt 1983_10.txt 4232_7.txt 6483_10.txt 8733_10.txt 10984_10.txt 1984_10.txt 4233_7.txt 6484_10.txt 8734_9.txt 10985_9.txt 1985_10.txt 4234_7.txt 6485_10.txt 8735_8.txt 10986_10.txt 1986_10.txt 4235_7.txt 6486_10.txt 8736_10.txt 10987_8.txt 1987_10.txt 4236_8.txt 6487_10.txt 8737_9.txt 10988_10.txt 1988_9.txt 4237_10.txt 6488_10.txt 8738_8.txt 10989_9.txt 1989_8.txt 4238_9.txt 6489_10.txt 8739_9.txt 1098_9.txt 198_8.txt 4239_10.txt 648_7.txt 873_8.txt 10990_7.txt 1990_10.txt 423_10.txt 6490_10.txt 8740_8.txt 10991_9.txt 1991_10.txt 4240_9.txt 6491_10.txt 8741_10.txt 10992_10.txt 1992_10.txt 4241_10.txt 6492_10.txt 8742_10.txt 10993_10.txt 1993_10.txt 4242_9.txt 6493_10.txt 8743_7.txt 10994_8.txt 1994_10.txt 4243_8.txt 6494_10.txt 8744_7.txt 10995_10.txt 1995_9.txt 4244_10.txt 6495_10.txt 8745_7.txt 10996_8.txt 1996_10.txt 4245_7.txt 6496_10.txt 8746_10.txt 10997_10.txt 1997_9.txt 4246_8.txt 6497_10.txt 8747_7.txt 10998_7.txt 1998_9.txt 4247_10.txt 6498_10.txt 8748_8.txt 10999_7.txt 1999_9.txt 4248_10.txt 6499_10.txt 8749_7.txt 1099_10.txt 199_10.txt 4249_10.txt 649_10.txt 874_9.txt 109_10.txt 19_10.txt 424_8.txt 64_7.txt 8750_7.txt 10_9.txt 1_7.txt 4250_8.txt 6500_10.txt 8751_7.txt 11000_10.txt 2000_10.txt 4251_9.txt 6501_10.txt 8752_9.txt 11001_10.txt 2001_9.txt 4252_9.txt 6502_10.txt 8753_8.txt 11002_8.txt 2002_7.txt 4253_10.txt 6503_10.txt 8754_8.txt 11003_10.txt 2003_8.txt 4254_9.txt 6504_10.txt 8755_7.txt 11004_8.txt 2004_10.txt 4255_9.txt 6505_10.txt 8756_9.txt 11005_10.txt 2005_10.txt 4256_8.txt 6506_10.txt 8757_8.txt 11006_7.txt 2006_7.txt 4257_10.txt 6507_10.txt 8758_10.txt 11007_10.txt 2007_7.txt 4258_9.txt 6508_7.txt 8759_8.txt 11008_9.txt 2008_7.txt 4259_9.txt 6509_9.txt 875_10.txt 11009_7.txt 2009_10.txt 425_10.txt 650_9.txt 8760_7.txt 1100_7.txt 200_10.txt 4260_9.txt 6510_7.txt 8761_10.txt 11010_10.txt 2010_8.txt 4261_9.txt 6511_10.txt 8762_8.txt 11011_10.txt 2011_7.txt 4262_10.txt 6512_7.txt 8763_9.txt 11012_9.txt 2012_8.txt 4263_8.txt 6513_9.txt 8764_8.txt 11013_7.txt 2013_8.txt 4264_10.txt 6514_10.txt 8765_10.txt 11014_10.txt 2014_7.txt 4265_8.txt 6515_7.txt 8766_8.txt 11015_9.txt 2015_8.txt 4266_7.txt 6516_10.txt 8767_10.txt 11016_7.txt 2016_7.txt 4267_8.txt 6517_10.txt 8768_8.txt 11017_9.txt 2017_10.txt 4268_10.txt 6518_10.txt 8769_7.txt 11018_10.txt 2018_9.txt 4269_10.txt 6519_8.txt 876_7.txt 11019_10.txt 2019_10.txt 426_7.txt 651_10.txt 8770_7.txt 1101_8.txt 201_10.txt 4270_10.txt 6520_7.txt 8771_7.txt 11020_8.txt 2020_7.txt 4271_10.txt 6521_7.txt 8772_8.txt 11021_8.txt 2021_8.txt 4272_10.txt 6522_10.txt 8773_8.txt 11022_9.txt 2022_9.txt 4273_10.txt 6523_10.txt 8774_8.txt 11023_9.txt 2023_7.txt 4274_10.txt 6524_8.txt 8775_9.txt 11024_9.txt 2024_9.txt 4275_10.txt 6525_7.txt 8776_10.txt 11025_7.txt 2025_10.txt 4276_10.txt 6526_7.txt 8777_10.txt 11026_7.txt 2026_8.txt 4277_10.txt 6527_8.txt 8778_8.txt 11027_10.txt 2027_10.txt 4278_10.txt 6528_9.txt 8779_8.txt 11028_8.txt 2028_10.txt 4279_10.txt 6529_7.txt 877_8.txt 11029_7.txt 2029_8.txt 427_10.txt 652_10.txt 8780_10.txt 1102_8.txt 202_10.txt 4280_10.txt 6530_8.txt 8781_9.txt 11030_8.txt 2030_9.txt 4281_10.txt 6531_8.txt 8782_9.txt 11031_8.txt 2031_8.txt 4282_10.txt 6532_8.txt 8783_9.txt 11032_7.txt 2032_10.txt 4283_10.txt 6533_7.txt 8784_9.txt 11033_9.txt 2033_8.txt 4284_10.txt 6534_10.txt 8785_10.txt 11034_9.txt 2034_9.txt 4285_10.txt 6535_10.txt 8786_8.txt 11035_8.txt 2035_7.txt 4286_10.txt 6536_7.txt 8787_8.txt 11036_8.txt 2036_7.txt 4287_10.txt 6537_7.txt 8788_10.txt 11037_8.txt 2037_8.txt 4288_9.txt 6538_8.txt 8789_10.txt 11038_10.txt 2038_7.txt 4289_7.txt 6539_9.txt 878_7.txt 11039_8.txt 2039_9.txt 428_7.txt 653_10.txt 8790_8.txt 1103_10.txt 203_7.txt 4290_9.txt 6540_7.txt 8791_10.txt 11040_7.txt 2040_7.txt 4291_10.txt 6541_8.txt 8792_8.txt 11041_7.txt 2041_7.txt 4292_7.txt 6542_8.txt 8793_10.txt 11042_7.txt 2042_7.txt 4293_8.txt 6543_9.txt 8794_9.txt 11043_7.txt 2043_7.txt 4294_8.txt 6544_10.txt 8795_10.txt 11044_7.txt 2044_8.txt 4295_9.txt 6545_10.txt 8796_8.txt 11045_9.txt 2045_9.txt 4296_9.txt 6546_10.txt 8797_8.txt 11046_8.txt 2046_8.txt 4297_9.txt 6547_9.txt 8798_8.txt 11047_9.txt 2047_10.txt 4298_10.txt 6548_10.txt 8799_8.txt 11048_7.txt 2048_7.txt 4299_8.txt 6549_8.txt 879_8.txt 11049_8.txt 2049_7.txt 429_10.txt 654_10.txt 87_10.txt 1104_8.txt 204_10.txt 42_10.txt 6550_10.txt 8800_10.txt 11050_8.txt 2050_7.txt 4300_7.txt 6551_9.txt 8801_9.txt 11051_8.txt 2051_8.txt 4301_10.txt 6552_9.txt 8802_7.txt 11052_9.txt 2052_10.txt 4302_8.txt 6553_10.txt 8803_10.txt 11053_10.txt 2053_10.txt 4303_10.txt 6554_7.txt 8804_10.txt 11054_7.txt 2054_9.txt 4304_9.txt 6555_8.txt 8805_9.txt 11055_10.txt 2055_10.txt 4305_10.txt 6556_9.txt 8806_9.txt 11056_10.txt 2056_9.txt 4306_10.txt 6557_10.txt 8807_9.txt 11057_10.txt 2057_10.txt 4307_10.txt 6558_8.txt 8808_9.txt 11058_10.txt 2058_9.txt 4308_8.txt 6559_10.txt 8809_10.txt 11059_9.txt 2059_7.txt 4309_8.txt 655_10.txt 880_10.txt 1105_8.txt 205_8.txt 430_7.txt 6560_7.txt 8810_9.txt 11060_10.txt 2060_8.txt 4310_9.txt 6561_10.txt 8811_7.txt 11061_8.txt 2061_9.txt 4311_9.txt 6562_10.txt 8812_10.txt 11062_10.txt 2062_10.txt 4312_10.txt 6563_7.txt 8813_9.txt 11063_10.txt 2063_8.txt 4313_9.txt 6564_7.txt 8814_10.txt 11064_7.txt 2064_8.txt 4314_10.txt 6565_8.txt 8815_7.txt 11065_8.txt 2065_7.txt 4315_9.txt 6566_8.txt 8816_7.txt 11066_8.txt 2066_7.txt 4316_10.txt 6567_10.txt 8817_8.txt 11067_7.txt 2067_9.txt 4317_10.txt 6568_9.txt 8818_10.txt 11068_9.txt 2068_8.txt 4318_10.txt 6569_7.txt 8819_10.txt 11069_10.txt 2069_9.txt 4319_9.txt 656_10.txt 881_8.txt 1106_8.txt 206_10.txt 431_8.txt 6570_8.txt 8820_7.txt 11070_9.txt 2070_9.txt 4320_8.txt 6571_10.txt 8821_9.txt 11071_10.txt 2071_9.txt 4321_8.txt 6572_8.txt 8822_10.txt 11072_8.txt 2072_10.txt 4322_10.txt 6573_7.txt 8823_8.txt 11073_8.txt 2073_7.txt 4323_9.txt 6574_8.txt 8824_8.txt 11074_7.txt 2074_10.txt 4324_7.txt 6575_8.txt 8825_9.txt 11075_10.txt 2075_8.txt 4325_10.txt 6576_8.txt 8826_9.txt 11076_8.txt 2076_9.txt 4326_7.txt 6577_8.txt 8827_8.txt 11077_8.txt 2077_10.txt 4327_8.txt 6578_7.txt 8828_10.txt 11078_10.txt 2078_9.txt 4328_10.txt 6579_10.txt 8829_9.txt 11079_8.txt 2079_10.txt 4329_7.txt 657_10.txt 882_8.txt 1107_10.txt 207_8.txt 432_8.txt 6580_8.txt 8830_8.txt 11080_10.txt 2080_9.txt 4330_10.txt 6581_7.txt 8831_8.txt 11081_10.txt 2081_7.txt 4331_7.txt 6582_7.txt 8832_7.txt 11082_10.txt 2082_10.txt 4332_10.txt 6583_7.txt 8833_9.txt 11083_10.txt 2083_7.txt 4333_10.txt 6584_8.txt 8834_9.txt 11084_10.txt 2084_8.txt 4334_10.txt 6585_10.txt 8835_9.txt 11085_10.txt 2085_7.txt 4335_9.txt 6586_10.txt 8836_10.txt 11086_7.txt 2086_10.txt 4336_10.txt 6587_9.txt 8837_8.txt 11087_8.txt 2087_10.txt 4337_7.txt 6588_9.txt 8838_9.txt 11088_7.txt 2088_8.txt 4338_10.txt 6589_10.txt 8839_8.txt 11089_10.txt 2089_10.txt 4339_10.txt 658_10.txt 883_9.txt 1108_7.txt 208_9.txt 433_10.txt 6590_9.txt 8840_10.txt 11090_8.txt 2090_10.txt 4340_8.txt 6591_8.txt 8841_8.txt 11091_8.txt 2091_9.txt 4341_8.txt 6592_10.txt 8842_9.txt 11092_8.txt 2092_10.txt 4342_10.txt 6593_7.txt 8843_7.txt 11093_10.txt 2093_7.txt 4343_9.txt 6594_10.txt 8844_10.txt 11094_9.txt 2094_10.txt 4344_9.txt 6595_9.txt 8845_8.txt 11095_7.txt 2095_9.txt 4345_10.txt 6596_8.txt 8846_9.txt 11096_10.txt 2096_10.txt 4346_10.txt 6597_9.txt 8847_9.txt 11097_9.txt 2097_9.txt 4347_8.txt 6598_8.txt 8848_7.txt 11098_10.txt 2098_10.txt 4348_7.txt 6599_8.txt 8849_9.txt 11099_10.txt 2099_10.txt 4349_10.txt 659_10.txt 884_8.txt 1109_7.txt 209_8.txt 434_8.txt 65_10.txt 8850_8.txt 110_10.txt 20_9.txt 4350_10.txt 6600_9.txt 8851_7.txt 11100_10.txt 2100_7.txt 4351_10.txt 6601_9.txt 8852_9.txt 11101_10.txt 2101_7.txt 4352_10.txt 6602_7.txt 8853_7.txt 11102_10.txt 2102_10.txt 4353_9.txt 6603_8.txt 8854_10.txt 11103_10.txt 2103_7.txt 4354_8.txt 6604_7.txt 8855_10.txt 11104_10.txt 2104_7.txt 4355_10.txt 6605_7.txt 8856_8.txt 11105_10.txt 2105_8.txt 4356_8.txt 6606_9.txt 8857_10.txt 11106_10.txt 2106_10.txt 4357_10.txt 6607_9.txt 8858_10.txt 11107_10.txt 2107_7.txt 4358_10.txt 6608_7.txt 8859_10.txt 11108_10.txt 2108_10.txt 4359_10.txt 6609_8.txt 885_8.txt 11109_9.txt 2109_9.txt 435_8.txt 660_9.txt 8860_8.txt 1110_9.txt 210_10.txt 4360_10.txt 6610_8.txt 8861_9.txt 11110_9.txt 2110_9.txt 4361_10.txt 6611_7.txt 8862_8.txt 11111_9.txt 2111_7.txt 4362_7.txt 6612_8.txt 8863_8.txt 11112_7.txt 2112_9.txt 4363_8.txt 6613_7.txt 8864_10.txt 11113_9.txt 2113_10.txt 4364_7.txt 6614_9.txt 8865_10.txt 11114_10.txt 2114_10.txt 4365_9.txt 6615_8.txt 8866_9.txt 11115_10.txt 2115_10.txt 4366_9.txt 6616_7.txt 8867_10.txt 11116_10.txt 2116_10.txt 4367_9.txt 6617_8.txt 8868_10.txt 11117_10.txt 2117_10.txt 4368_10.txt 6618_8.txt 8869_7.txt 11118_10.txt 2118_9.txt 4369_9.txt 6619_9.txt 886_8.txt 11119_10.txt 2119_10.txt 436_10.txt 661_9.txt 8870_8.txt 1111_10.txt 211_9.txt 4370_10.txt 6620_10.txt 8871_10.txt 11120_10.txt 2120_8.txt 4371_8.txt 6621_8.txt 8872_8.txt 11121_10.txt 2121_10.txt 4372_10.txt 6622_10.txt 8873_8.txt 11122_10.txt 2122_7.txt 4373_10.txt 6623_7.txt 8874_8.txt 11123_10.txt 2123_10.txt 4374_10.txt 6624_9.txt 8875_9.txt 11124_10.txt 2124_10.txt 4375_9.txt 6625_7.txt 8876_10.txt 11125_10.txt 2125_10.txt 4376_9.txt 6626_7.txt 8877_9.txt 11126_10.txt 2126_10.txt 4377_10.txt 6627_7.txt 8878_9.txt 11127_9.txt 2127_9.txt 4378_9.txt 6628_7.txt 8879_7.txt 11128_10.txt 2128_8.txt 4379_8.txt 6629_10.txt 887_10.txt 11129_10.txt 2129_8.txt 437_9.txt 662_8.txt 8880_8.txt 1112_8.txt 212_9.txt 4380_8.txt 6630_10.txt 8881_8.txt 11130_9.txt 2130_10.txt 4381_10.txt 6631_7.txt 8882_10.txt 11131_9.txt 2131_9.txt 4382_8.txt 6632_7.txt 8883_8.txt 11132_10.txt 2132_9.txt 4383_9.txt 6633_10.txt 8884_9.txt 11133_10.txt 2133_10.txt 4384_10.txt 6634_10.txt 8885_10.txt 11134_9.txt 2134_10.txt 4385_9.txt 6635_10.txt 8886_10.txt 11135_9.txt 2135_8.txt 4386_9.txt 6636_8.txt 8887_9.txt 11136_10.txt 2136_9.txt 4387_9.txt 6637_8.txt 8888_7.txt 11137_7.txt 2137_8.txt 4388_9.txt 6638_10.txt 8889_8.txt 11138_10.txt 2138_9.txt 4389_10.txt 6639_10.txt 888_8.txt 11139_8.txt 2139_9.txt 438_9.txt 663_8.txt 8890_10.txt 1113_10.txt 213_9.txt 4390_8.txt 6640_8.txt 8891_10.txt 11140_9.txt 2140_10.txt 4391_8.txt 6641_8.txt 8892_10.txt 11141_10.txt 2141_10.txt 4392_9.txt 6642_10.txt 8893_9.txt 11142_8.txt 2142_8.txt 4393_7.txt 6643_7.txt 8894_10.txt 11143_10.txt 2143_9.txt 4394_9.txt 6644_8.txt 8895_10.txt 11144_8.txt 2144_8.txt 4395_10.txt 6645_8.txt 8896_10.txt 11145_8.txt 2145_7.txt 4396_10.txt 6646_10.txt 8897_10.txt 11146_8.txt 2146_9.txt 4397_7.txt 6647_8.txt 8898_8.txt 11147_9.txt 2147_10.txt 4398_9.txt 6648_7.txt 8899_8.txt 11148_9.txt 2148_10.txt 4399_9.txt 6649_8.txt 889_10.txt 11149_10.txt 2149_10.txt 439_9.txt 664_7.txt 88_9.txt 1114_8.txt 214_7.txt 43_10.txt 6650_8.txt 8900_10.txt 11150_10.txt 2150_10.txt 4400_10.txt 6651_10.txt 8901_8.txt 11151_8.txt 2151_10.txt 4401_9.txt 6652_10.txt 8902_7.txt 11152_10.txt 2152_8.txt 4402_8.txt 6653_8.txt 8903_7.txt 11153_10.txt 2153_9.txt 4403_10.txt 6654_7.txt 8904_7.txt 11154_10.txt 2154_10.txt 4404_9.txt 6655_7.txt 8905_10.txt 11155_10.txt 2155_10.txt 4405_8.txt 6656_10.txt 8906_8.txt 11156_8.txt 2156_10.txt 4406_9.txt 6657_9.txt 8907_8.txt 11157_7.txt 2157_10.txt 4407_8.txt 6658_10.txt 8908_10.txt 11158_8.txt 2158_10.txt 4408_8.txt 6659_10.txt 8909_9.txt 11159_8.txt 2159_10.txt 4409_10.txt 665_9.txt 890_9.txt 1115_9.txt 215_8.txt 440_10.txt 6660_8.txt 8910_10.txt 11160_9.txt 2160_8.txt 4410_10.txt 6661_7.txt 8911_8.txt 11161_8.txt 2161_10.txt 4411_9.txt 6662_7.txt 8912_10.txt 11162_10.txt 2162_10.txt 4412_9.txt 6663_10.txt 8913_9.txt 11163_9.txt 2163_10.txt 4413_8.txt 6664_9.txt 8914_8.txt 11164_8.txt 2164_10.txt 4414_9.txt 6665_10.txt 8915_10.txt 11165_8.txt 2165_7.txt 4415_10.txt 6666_7.txt 8916_8.txt 11166_7.txt 2166_7.txt 4416_9.txt 6667_8.txt 8917_7.txt 11167_9.txt 2167_10.txt 4417_9.txt 6668_8.txt 8918_10.txt 11168_8.txt 2168_8.txt 4418_10.txt 6669_10.txt 8919_7.txt 11169_7.txt 2169_7.txt 4419_7.txt 666_10.txt 891_10.txt 1116_9.txt 216_8.txt 441_9.txt 6670_8.txt 8920_10.txt 11170_10.txt 2170_9.txt 4420_8.txt 6671_10.txt 8921_8.txt 11171_7.txt 2171_10.txt 4421_8.txt 6672_10.txt 8922_7.txt 11172_10.txt 2172_7.txt 4422_8.txt 6673_7.txt 8923_8.txt 11173_7.txt 2173_10.txt 4423_8.txt 6674_9.txt 8924_10.txt 11174_8.txt 2174_8.txt 4424_7.txt 6675_8.txt 8925_7.txt 11175_8.txt 2175_9.txt 4425_7.txt 6676_9.txt 8926_10.txt 11176_9.txt 2176_8.txt 4426_7.txt 6677_9.txt 8927_8.txt 11177_8.txt 2177_8.txt 4427_10.txt 6678_7.txt 8928_10.txt 11178_7.txt 2178_9.txt 4428_10.txt 6679_8.txt 8929_8.txt 11179_9.txt 2179_8.txt 4429_7.txt 667_8.txt 892_10.txt 1117_10.txt 217_8.txt 442_9.txt 6680_7.txt 8930_9.txt 11180_7.txt 2180_8.txt 4430_8.txt 6681_10.txt 8931_9.txt 11181_10.txt 2181_10.txt 4431_10.txt 6682_10.txt 8932_9.txt 11182_10.txt 2182_9.txt 4432_10.txt 6683_8.txt 8933_7.txt 11183_7.txt 2183_8.txt 4433_7.txt 6684_10.txt 8934_10.txt 11184_7.txt 2184_7.txt 4434_7.txt 6685_10.txt 8935_8.txt 11185_9.txt 2185_9.txt 4435_7.txt 6686_9.txt 8936_8.txt 11186_7.txt 2186_8.txt 4436_7.txt 6687_9.txt 8937_9.txt 11187_9.txt 2187_10.txt 4437_7.txt 6688_9.txt 8938_9.txt 11188_10.txt 2188_10.txt 4438_8.txt 6689_8.txt 8939_9.txt 11189_8.txt 2189_10.txt 4439_8.txt 668_7.txt 893_8.txt 1118_10.txt 218_9.txt 443_10.txt 6690_10.txt 8940_9.txt 11190_10.txt 2190_10.txt 4440_7.txt 6691_9.txt 8941_10.txt 11191_10.txt 2191_10.txt 4441_7.txt 6692_9.txt 8942_8.txt 11192_10.txt 2192_9.txt 4442_9.txt 6693_10.txt 8943_7.txt 11193_7.txt 2193_10.txt 4443_8.txt 6694_10.txt 8944_8.txt 11194_10.txt 2194_10.txt 4444_10.txt 6695_10.txt 8945_10.txt 11195_7.txt 2195_10.txt 4445_7.txt 6696_7.txt 8946_8.txt 11196_7.txt 2196_8.txt 4446_10.txt 6697_10.txt 8947_9.txt 11197_8.txt 2197_7.txt 4447_7.txt 6698_10.txt 8948_7.txt 11198_8.txt 2198_8.txt 4448_10.txt 6699_8.txt 8949_7.txt 11199_9.txt 2199_7.txt 4449_8.txt 669_7.txt 894_9.txt 1119_10.txt 219_8.txt 444_10.txt 66_8.txt 8950_7.txt 111_10.txt 21_7.txt 4450_9.txt 6700_8.txt 8951_7.txt 11200_8.txt 2200_9.txt 4451_9.txt 6701_9.txt 8952_7.txt 11201_7.txt 2201_10.txt 4452_8.txt 6702_8.txt 8953_8.txt 11202_10.txt 2202_10.txt 4453_10.txt 6703_8.txt 8954_8.txt 11203_8.txt 2203_8.txt 4454_9.txt 6704_9.txt 8955_8.txt 11204_9.txt 2204_10.txt 4455_8.txt 6705_10.txt 8956_8.txt 11205_10.txt 2205_8.txt 4456_7.txt 6706_8.txt 8957_7.txt 11206_10.txt 2206_7.txt 4457_8.txt 6707_9.txt 8958_10.txt 11207_10.txt 2207_9.txt 4458_7.txt 6708_10.txt 8959_8.txt 11208_8.txt 2208_7.txt 4459_7.txt 6709_10.txt 895_10.txt 11209_8.txt 2209_8.txt 445_10.txt 670_7.txt 8960_10.txt 1120_7.txt 220_10.txt 4460_9.txt 6710_10.txt 8961_9.txt 11210_7.txt 2210_9.txt 4461_8.txt 6711_10.txt 8962_9.txt 11211_8.txt 2211_8.txt 4462_9.txt 6712_9.txt 8963_8.txt 11212_8.txt 2212_10.txt 4463_7.txt 6713_10.txt 8964_8.txt 11213_7.txt 2213_9.txt 4464_10.txt 6714_10.txt 8965_8.txt 11214_7.txt 2214_10.txt 4465_7.txt 6715_10.txt 8966_10.txt 11215_10.txt 2215_8.txt 4466_8.txt 6716_8.txt 8967_8.txt 11216_7.txt 2216_9.txt 4467_7.txt 6717_7.txt 8968_8.txt 11217_7.txt 2217_7.txt 4468_7.txt 6718_10.txt 8969_7.txt 11218_9.txt 2218_7.txt 4469_9.txt 6719_8.txt 896_10.txt 11219_7.txt 2219_10.txt 446_10.txt 671_7.txt 8970_10.txt 1121_7.txt 221_9.txt 4470_8.txt 6720_9.txt 8971_9.txt 11220_10.txt 2220_9.txt 4471_8.txt 6721_10.txt 8972_10.txt 11221_10.txt 2221_8.txt 4472_10.txt 6722_9.txt 8973_9.txt 11222_8.txt 2222_10.txt 4473_8.txt 6723_10.txt 8974_10.txt 11223_9.txt 2223_8.txt 4474_10.txt 6724_8.txt 8975_10.txt 11224_8.txt 2224_10.txt 4475_8.txt 6725_9.txt 8976_7.txt 11225_10.txt 2225_10.txt 4476_9.txt 6726_9.txt 8977_7.txt 11226_7.txt 2226_10.txt 4477_7.txt 6727_9.txt 8978_10.txt 11227_9.txt 2227_7.txt 4478_8.txt 6728_8.txt 8979_10.txt 11228_10.txt 2228_7.txt 4479_8.txt 6729_10.txt 897_10.txt 11229_10.txt 2229_7.txt 447_10.txt 672_9.txt 8980_10.txt 1122_7.txt 222_10.txt 4480_9.txt 6730_10.txt 8981_7.txt 11230_10.txt 2230_10.txt 4481_8.txt 6731_10.txt 8982_8.txt 11231_10.txt 2231_10.txt 4482_7.txt 6732_10.txt 8983_8.txt 11232_10.txt 2232_7.txt 4483_8.txt 6733_10.txt 8984_10.txt 11233_10.txt 2233_10.txt 4484_8.txt 6734_10.txt 8985_9.txt 11234_10.txt 2234_9.txt 4485_10.txt 6735_9.txt 8986_8.txt 11235_8.txt 2235_10.txt 4486_7.txt 6736_10.txt 8987_9.txt 11236_8.txt 2236_9.txt 4487_9.txt 6737_7.txt 8988_10.txt 11237_8.txt 2237_7.txt 4488_10.txt 6738_7.txt 8989_9.txt 11238_7.txt 2238_9.txt 4489_9.txt 6739_7.txt 898_10.txt 11239_8.txt 2239_7.txt 448_10.txt 673_8.txt 8990_10.txt 1123_10.txt 223_9.txt 4490_7.txt 6740_8.txt 8991_10.txt 11240_7.txt 2240_7.txt 4491_10.txt 6741_7.txt 8992_9.txt 11241_10.txt 2241_7.txt 4492_10.txt 6742_8.txt 8993_7.txt 11242_9.txt 2242_7.txt 4493_9.txt 6743_9.txt 8994_8.txt 11243_10.txt 2243_7.txt 4494_8.txt 6744_8.txt 8995_9.txt 11244_10.txt 2244_9.txt 4495_9.txt 6745_8.txt 8996_8.txt 11245_10.txt 2245_7.txt 4496_8.txt 6746_8.txt 8997_10.txt 11246_8.txt 2246_10.txt 4497_7.txt 6747_8.txt 8998_10.txt 11247_8.txt 2247_10.txt 4498_8.txt 6748_8.txt 8999_8.txt 11248_10.txt 2248_7.txt 4499_9.txt 6749_9.txt 899_7.txt 11249_7.txt 2249_7.txt 449_10.txt 674_10.txt 89_7.txt 1124_10.txt 224_10.txt 44_8.txt 6750_8.txt 8_7.txt 11250_9.txt 2250_8.txt 4500_10.txt 6751_8.txt 9000_10.txt 11251_10.txt 2251_8.txt 4501_7.txt 6752_7.txt 9001_8.txt 11252_9.txt 2252_10.txt 4502_7.txt 6753_7.txt 9002_9.txt 11253_9.txt 2253_8.txt 4503_7.txt 6754_8.txt 9003_10.txt 11254_8.txt 2254_8.txt 4504_9.txt 6755_7.txt 9004_7.txt 11255_10.txt 2255_8.txt 4505_8.txt 6756_10.txt 9005_8.txt 11256_10.txt 2256_8.txt 4506_10.txt 6757_8.txt 9006_7.txt 11257_7.txt 2257_7.txt 4507_7.txt 6758_10.txt 9007_10.txt 11258_8.txt 2258_7.txt 4508_7.txt 6759_7.txt 9008_10.txt 11259_10.txt 2259_9.txt 4509_10.txt 675_9.txt 9009_10.txt 1125_8.txt 225_9.txt 450_10.txt 6760_9.txt 900_10.txt 11260_7.txt 2260_10.txt 4510_7.txt 6761_9.txt 9010_7.txt 11261_9.txt 2261_10.txt 4511_9.txt 6762_9.txt 9011_9.txt 11262_7.txt 2262_10.txt 4512_9.txt 6763_8.txt 9012_7.txt 11263_10.txt 2263_10.txt 4513_8.txt 6764_9.txt 9013_7.txt 11264_10.txt 2264_9.txt 4514_10.txt 6765_9.txt 9014_10.txt 11265_10.txt 2265_7.txt 4515_10.txt 6766_8.txt 9015_8.txt 11266_10.txt 2266_8.txt 4516_10.txt 6767_8.txt 9016_10.txt 11267_10.txt 2267_8.txt 4517_9.txt 6768_8.txt 9017_8.txt 11268_8.txt 2268_10.txt 4518_9.txt 6769_9.txt 9018_8.txt 11269_8.txt 2269_10.txt 4519_9.txt 676_8.txt 9019_8.txt 1126_9.txt 226_10.txt 451_10.txt 6770_10.txt 901_8.txt 11270_10.txt 2270_9.txt 4520_7.txt 6771_7.txt 9020_7.txt 11271_7.txt 2271_8.txt 4521_10.txt 6772_10.txt 9021_10.txt 11272_10.txt 2272_7.txt 4522_9.txt 6773_10.txt 9022_10.txt 11273_9.txt 2273_9.txt 4523_10.txt 6774_10.txt 9023_10.txt 11274_9.txt 2274_9.txt 4524_9.txt 6775_8.txt 9024_10.txt 11275_10.txt 2275_7.txt 4525_8.txt 6776_7.txt 9025_9.txt 11276_10.txt 2276_7.txt 4526_10.txt 6777_8.txt 9026_9.txt 11277_10.txt 2277_10.txt 4527_9.txt 6778_8.txt 9027_7.txt 11278_8.txt 2278_10.txt 4528_8.txt 6779_8.txt 9028_10.txt 11279_10.txt 2279_10.txt 4529_8.txt 677_8.txt 9029_10.txt 1127_10.txt 227_10.txt 452_10.txt 6780_10.txt 902_9.txt 11280_8.txt 2280_7.txt 4530_7.txt 6781_10.txt 9030_10.txt 11281_8.txt 2281_9.txt 4531_8.txt 6782_9.txt 9031_10.txt 11282_7.txt 2282_10.txt 4532_7.txt 6783_7.txt 9032_8.txt 11283_8.txt 2283_9.txt 4533_8.txt 6784_8.txt 9033_9.txt 11284_8.txt 2284_10.txt 4534_9.txt 6785_8.txt 9034_10.txt 11285_7.txt 2285_10.txt 4535_7.txt 6786_10.txt 9035_10.txt 11286_10.txt 2286_10.txt 4536_7.txt 6787_9.txt 9036_10.txt 11287_8.txt 2287_8.txt 4537_8.txt 6788_9.txt 9037_8.txt 11288_8.txt 2288_9.txt 4538_7.txt 6789_8.txt 9038_8.txt 11289_10.txt 2289_9.txt 4539_9.txt 678_8.txt 9039_10.txt 1128_10.txt 228_7.txt 453_10.txt 6790_10.txt 903_8.txt 11290_10.txt 2290_10.txt 4540_7.txt 6791_10.txt 9040_9.txt 11291_10.txt 2291_7.txt 4541_10.txt 6792_10.txt 9041_8.txt 11292_10.txt 2292_8.txt 4542_9.txt 6793_10.txt 9042_10.txt 11293_7.txt 2293_7.txt 4543_7.txt 6794_10.txt 9043_7.txt 11294_10.txt 2294_8.txt 4544_10.txt 6795_9.txt 9044_9.txt 11295_9.txt 2295_7.txt 4545_10.txt 6796_8.txt 9045_9.txt 11296_8.txt 2296_9.txt 4546_10.txt 6797_8.txt 9046_8.txt 11297_7.txt 2297_7.txt 4547_8.txt 6798_9.txt 9047_10.txt 11298_8.txt 2298_7.txt 4548_7.txt 6799_9.txt 9048_8.txt 11299_8.txt 2299_7.txt 4549_7.txt 679_10.txt 9049_8.txt 1129_10.txt 229_10.txt 454_8.txt 67_10.txt 904_10.txt 112_10.txt 22_8.txt 4550_10.txt 6800_9.txt 9050_8.txt 11300_8.txt 2300_10.txt 4551_10.txt 6801_9.txt 9051_10.txt 11301_7.txt 2301_10.txt 4552_9.txt 6802_9.txt 9052_8.txt 11302_8.txt 2302_9.txt 4553_10.txt 6803_8.txt 9053_10.txt 11303_9.txt 2303_8.txt 4554_8.txt 6804_7.txt 9054_9.txt 11304_7.txt 2304_9.txt 4555_9.txt 6805_10.txt 9055_10.txt 11305_8.txt 2305_10.txt 4556_8.txt 6806_7.txt 9056_8.txt 11306_8.txt 2306_10.txt 4557_8.txt 6807_10.txt 9057_7.txt 11307_10.txt 2307_9.txt 4558_9.txt 6808_8.txt 9058_7.txt 11308_9.txt 2308_10.txt 4559_10.txt 6809_7.txt 9059_7.txt 11309_9.txt 2309_9.txt 455_10.txt 680_8.txt 905_10.txt 1130_10.txt 230_9.txt 4560_10.txt 6810_7.txt 9060_8.txt 11310_10.txt 2310_9.txt 4561_10.txt 6811_10.txt 9061_7.txt 11311_10.txt 2311_10.txt 4562_9.txt 6812_7.txt 9062_7.txt 11312_9.txt 2312_9.txt 4563_8.txt 6813_10.txt 9063_8.txt 11313_10.txt 2313_10.txt 4564_10.txt 6814_7.txt 9064_7.txt 11314_8.txt 2314_7.txt 4565_8.txt 6815_8.txt 9065_10.txt 11315_10.txt 2315_10.txt 4566_9.txt 6816_7.txt 9066_8.txt 11316_8.txt 2316_10.txt 4567_10.txt 6817_10.txt 9067_8.txt 11317_9.txt 2317_10.txt 4568_10.txt 6818_10.txt 9068_9.txt 11318_9.txt 2318_10.txt 4569_7.txt 6819_8.txt 9069_7.txt 11319_8.txt 2319_8.txt 456_10.txt 681_9.txt 906_10.txt 1131_7.txt 231_10.txt 4570_10.txt 6820_7.txt 9070_7.txt 11320_10.txt 2320_10.txt 4571_9.txt 6821_10.txt 9071_8.txt 11321_8.txt 2321_9.txt 4572_10.txt 6822_7.txt 9072_9.txt 11322_9.txt 2322_10.txt 4573_10.txt 6823_10.txt 9073_10.txt 11323_10.txt 2323_8.txt 4574_10.txt 6824_10.txt 9074_8.txt 11324_10.txt 2324_8.txt 4575_8.txt 6825_10.txt 9075_10.txt 11325_10.txt 2325_8.txt 4576_10.txt 6826_9.txt 9076_8.txt 11326_10.txt 2326_8.txt 4577_10.txt 6827_10.txt 9077_9.txt 11327_10.txt 2327_10.txt 4578_9.txt 6828_10.txt 9078_7.txt 11328_10.txt 2328_10.txt 4579_9.txt 6829_8.txt 9079_10.txt 11329_9.txt 2329_10.txt 457_10.txt 682_10.txt 907_8.txt 1132_10.txt 232_10.txt 4580_8.txt 6830_10.txt 9080_7.txt 11330_8.txt 2330_8.txt 4581_10.txt 6831_9.txt 9081_8.txt 11331_8.txt 2331_10.txt 4582_7.txt 6832_10.txt 9082_10.txt 11332_8.txt 2332_10.txt 4583_7.txt 6833_10.txt 9083_8.txt 11333_9.txt 2333_10.txt 4584_10.txt 6834_10.txt 9084_7.txt 11334_10.txt 2334_9.txt 4585_10.txt 6835_10.txt 9085_8.txt 11335_9.txt 2335_10.txt 4586_7.txt 6836_9.txt 9086_7.txt 11336_7.txt 2336_10.txt 4587_10.txt 6837_10.txt 9087_7.txt 11337_10.txt 2337_10.txt 4588_9.txt 6838_7.txt 9088_8.txt 11338_10.txt 2338_10.txt 4589_10.txt 6839_10.txt 9089_9.txt 11339_10.txt 2339_8.txt 458_10.txt 683_10.txt 908_8.txt 1133_10.txt 233_7.txt 4590_10.txt 6840_10.txt 9090_9.txt 11340_7.txt 2340_10.txt 4591_9.txt 6841_7.txt 9091_10.txt 11341_10.txt 2341_8.txt 4592_9.txt 6842_7.txt 9092_7.txt 11342_9.txt 2342_8.txt 4593_10.txt 6843_9.txt 9093_10.txt 11343_8.txt 2343_10.txt 4594_10.txt 6844_10.txt 9094_10.txt 11344_7.txt 2344_9.txt 4595_10.txt 6845_10.txt 9095_10.txt 11345_7.txt 2345_9.txt 4596_10.txt 6846_10.txt 9096_10.txt 11346_10.txt 2346_9.txt 4597_8.txt 6847_10.txt 9097_10.txt 11347_10.txt 2347_10.txt 4598_9.txt 6848_7.txt 9098_10.txt 11348_10.txt 2348_10.txt 4599_10.txt 6849_7.txt 9099_10.txt 11349_10.txt 2349_9.txt 459_10.txt 684_10.txt 909_8.txt 1134_9.txt 234_10.txt 45_10.txt 6850_7.txt 90_7.txt 11350_9.txt 2350_9.txt 4600_10.txt 6851_8.txt 9100_7.txt 11351_9.txt 2351_10.txt 4601_7.txt 6852_8.txt 9101_9.txt 11352_9.txt 2352_9.txt 4602_8.txt 6853_10.txt 9102_8.txt 11353_9.txt 2353_10.txt 4603_10.txt 6854_10.txt 9103_10.txt 11354_10.txt 2354_10.txt 4604_10.txt 6855_10.txt 9104_10.txt 11355_8.txt 2355_7.txt 4605_8.txt 6856_10.txt 9105_9.txt 11356_9.txt 2356_10.txt 4606_7.txt 6857_10.txt 9106_9.txt 11357_10.txt 2357_10.txt 4607_10.txt 6858_7.txt 9107_7.txt 11358_9.txt 2358_9.txt 4608_9.txt 6859_7.txt 9108_10.txt 11359_9.txt 2359_10.txt 4609_8.txt 685_8.txt 9109_7.txt 1135_9.txt 235_10.txt 460_9.txt 6860_8.txt 910_10.txt 11360_7.txt 2360_10.txt 4610_10.txt 6861_8.txt 9110_8.txt 11361_10.txt 2361_10.txt 4611_10.txt 6862_10.txt 9111_10.txt 11362_9.txt 2362_9.txt 4612_10.txt 6863_7.txt 9112_9.txt 11363_8.txt 2363_7.txt 4613_10.txt 6864_7.txt 9113_8.txt 11364_10.txt 2364_10.txt 4614_7.txt 6865_7.txt 9114_10.txt 11365_9.txt 2365_10.txt 4615_10.txt 6866_7.txt 9115_10.txt 11366_8.txt 2366_7.txt 4616_10.txt 6867_9.txt 9116_9.txt 11367_10.txt 2367_8.txt 4617_10.txt 6868_10.txt 9117_10.txt 11368_10.txt 2368_8.txt 4618_10.txt 6869_8.txt 9118_10.txt 11369_8.txt 2369_10.txt 4619_7.txt 686_9.txt 9119_10.txt 1136_8.txt 236_9.txt 461_8.txt 6870_8.txt 911_10.txt 11370_9.txt 2370_10.txt 4620_10.txt 6871_8.txt 9120_10.txt 11371_10.txt 2371_8.txt 4621_9.txt 6872_7.txt 9121_10.txt 11372_7.txt 2372_8.txt 4622_8.txt 6873_10.txt 9122_7.txt 11373_8.txt 2373_7.txt 4623_7.txt 6874_8.txt 9123_10.txt 11374_9.txt 2374_9.txt 4624_8.txt 6875_10.txt 9124_10.txt 11375_9.txt 2375_8.txt 4625_10.txt 6876_8.txt 9125_10.txt 11376_10.txt 2376_7.txt 4626_8.txt 6877_7.txt 9126_9.txt 11377_9.txt 2377_10.txt 4627_7.txt 6878_8.txt 9127_7.txt 11378_8.txt 2378_8.txt 4628_9.txt 6879_10.txt 9128_9.txt 11379_7.txt 2379_8.txt 4629_10.txt 687_9.txt 9129_8.txt 1137_8.txt 237_10.txt 462_8.txt 6880_7.txt 912_8.txt 11380_7.txt 2380_9.txt 4630_9.txt 6881_9.txt 9130_9.txt 11381_10.txt 2381_9.txt 4631_10.txt 6882_9.txt 9131_9.txt 11382_8.txt 2382_7.txt 4632_10.txt 6883_9.txt 9132_10.txt 11383_7.txt 2383_9.txt 4633_9.txt 6884_10.txt 9133_9.txt 11384_7.txt 2384_10.txt 4634_10.txt 6885_9.txt 9134_7.txt 11385_8.txt 2385_9.txt 4635_7.txt 6886_8.txt 9135_9.txt 11386_8.txt 2386_8.txt 4636_8.txt 6887_8.txt 9136_8.txt 11387_8.txt 2387_10.txt 4637_8.txt 6888_9.txt 9137_10.txt 11388_8.txt 2388_10.txt 4638_10.txt 6889_10.txt 9138_10.txt 11389_7.txt 2389_10.txt 4639_10.txt 688_9.txt 9139_8.txt 1138_10.txt 238_10.txt 463_7.txt 6890_8.txt 913_10.txt 11390_10.txt 2390_7.txt 4640_10.txt 6891_9.txt 9140_10.txt 11391_8.txt 2391_10.txt 4641_9.txt 6892_8.txt 9141_8.txt 11392_8.txt 2392_8.txt 4642_10.txt 6893_10.txt 9142_10.txt 11393_7.txt 2393_8.txt 4643_10.txt 6894_7.txt 9143_10.txt 11394_10.txt 2394_7.txt 4644_10.txt 6895_10.txt 9144_7.txt 11395_8.txt 2395_7.txt 4645_9.txt 6896_7.txt 9145_10.txt 11396_10.txt 2396_7.txt 4646_7.txt 6897_10.txt 9146_9.txt 11397_10.txt 2397_9.txt 4647_8.txt 6898_9.txt 9147_10.txt 11398_7.txt 2398_9.txt 4648_9.txt 6899_9.txt 9148_7.txt 11399_8.txt 2399_7.txt 4649_10.txt 689_10.txt 9149_9.txt 1139_8.txt 239_7.txt 464_10.txt 68_10.txt 914_8.txt 113_10.txt 23_7.txt 4650_7.txt 6900_8.txt 9150_8.txt 11400_10.txt 2400_7.txt 4651_7.txt 6901_10.txt 9151_10.txt 11401_9.txt 2401_8.txt 4652_9.txt 6902_7.txt 9152_8.txt 11402_10.txt 2402_7.txt 4653_10.txt 6903_9.txt 9153_8.txt 11403_9.txt 2403_10.txt 4654_7.txt 6904_8.txt 9154_7.txt 11404_10.txt 2404_8.txt 4655_7.txt 6905_7.txt 9155_10.txt 11405_8.txt 2405_10.txt 4656_8.txt 6906_8.txt 9156_10.txt 11406_9.txt 2406_10.txt 4657_10.txt 6907_8.txt 9157_10.txt 11407_7.txt 2407_10.txt 4658_8.txt 6908_7.txt 9158_10.txt 11408_7.txt 2408_8.txt 4659_8.txt 6909_9.txt 9159_10.txt 11409_9.txt 2409_9.txt 465_10.txt 690_9.txt 915_10.txt 1140_10.txt 240_10.txt 4660_7.txt 6910_9.txt 9160_8.txt 11410_7.txt 2410_9.txt 4661_10.txt 6911_10.txt 9161_10.txt 11411_8.txt 2411_9.txt 4662_8.txt 6912_10.txt 9162_10.txt 11412_10.txt 2412_10.txt 4663_8.txt 6913_9.txt 9163_10.txt 11413_10.txt 2413_10.txt 4664_8.txt 6914_8.txt 9164_10.txt 11414_9.txt 2414_10.txt 4665_10.txt 6915_9.txt 9165_10.txt 11415_8.txt 2415_10.txt 4666_8.txt 6916_8.txt 9166_9.txt 11416_10.txt 2416_10.txt 4667_10.txt 6917_9.txt 9167_7.txt 11417_7.txt 2417_9.txt 4668_10.txt 6918_10.txt 9168_7.txt 11418_7.txt 2418_8.txt 4669_10.txt 6919_9.txt 9169_8.txt 11419_10.txt 2419_10.txt 466_8.txt 691_9.txt 916_10.txt 1141_10.txt 241_8.txt 4670_9.txt 6920_8.txt 9170_9.txt 11420_9.txt 2420_10.txt 4671_10.txt 6921_9.txt 9171_9.txt 11421_10.txt 2421_9.txt 4672_10.txt 6922_8.txt 9172_10.txt 11422_8.txt 2422_9.txt 4673_9.txt 6923_10.txt 9173_10.txt 11423_8.txt 2423_7.txt 4674_7.txt 6924_7.txt 9174_9.txt 11424_8.txt 2424_7.txt 4675_9.txt 6925_9.txt 9175_10.txt 11425_8.txt 2425_7.txt 4676_9.txt 6926_10.txt 9176_10.txt 11426_10.txt 2426_9.txt 4677_9.txt 6927_10.txt 9177_10.txt 11427_10.txt 2427_10.txt 4678_10.txt 6928_9.txt 9178_10.txt 11428_7.txt 2428_8.txt 4679_10.txt 6929_8.txt 9179_10.txt 11429_7.txt 2429_8.txt 467_7.txt 692_8.txt 917_10.txt 1142_10.txt 242_8.txt 4680_10.txt 6930_8.txt 9180_9.txt 11430_8.txt 2430_10.txt 4681_7.txt 6931_8.txt 9181_10.txt 11431_10.txt 2431_8.txt 4682_9.txt 6932_7.txt 9182_10.txt 11432_9.txt 2432_7.txt 4683_7.txt 6933_9.txt 9183_10.txt 11433_8.txt 2433_8.txt 4684_8.txt 6934_9.txt 9184_10.txt 11434_7.txt 2434_8.txt 4685_7.txt 6935_8.txt 9185_9.txt 11435_10.txt 2435_8.txt 4686_10.txt 6936_7.txt 9186_8.txt 11436_9.txt 2436_10.txt 4687_8.txt 6937_8.txt 9187_10.txt 11437_9.txt 2437_10.txt 4688_10.txt 6938_9.txt 9188_10.txt 11438_10.txt 2438_9.txt 4689_10.txt 6939_9.txt 9189_10.txt 11439_8.txt 2439_8.txt 468_7.txt 693_10.txt 918_10.txt 1143_7.txt 243_10.txt 4690_9.txt 6940_10.txt 9190_7.txt 11440_10.txt 2440_10.txt 4691_10.txt 6941_9.txt 9191_10.txt 11441_10.txt 2441_10.txt 4692_10.txt 6942_10.txt 9192_7.txt 11442_10.txt 2442_10.txt 4693_7.txt 6943_9.txt 9193_10.txt 11443_10.txt 2443_9.txt 4694_10.txt 6944_9.txt 9194_9.txt 11444_10.txt 2444_8.txt 4695_9.txt 6945_10.txt 9195_10.txt 11445_7.txt 2445_10.txt 4696_10.txt 6946_10.txt 9196_10.txt 11446_10.txt 2446_10.txt 4697_7.txt 6947_10.txt 9197_7.txt 11447_7.txt 2447_10.txt 4698_10.txt 6948_10.txt 9198_8.txt 11448_10.txt 2448_8.txt 4699_8.txt 6949_10.txt 9199_8.txt 11449_10.txt 2449_7.txt 469_7.txt 694_9.txt 919_8.txt 1144_8.txt 244_10.txt 46_9.txt 6950_8.txt 91_8.txt 11450_10.txt 2450_9.txt 4700_9.txt 6951_10.txt 9200_8.txt 11451_8.txt 2451_8.txt 4701_10.txt 6952_7.txt 9201_8.txt 11452_8.txt 2452_10.txt 4702_8.txt 6953_7.txt 9202_8.txt 11453_9.txt 2453_8.txt 4703_9.txt 6954_8.txt 9203_9.txt 11454_10.txt 2454_10.txt 4704_8.txt 6955_10.txt 9204_10.txt 11455_8.txt 2455_8.txt 4705_7.txt 6956_8.txt 9205_7.txt 11456_10.txt 2456_8.txt 4706_8.txt 6957_7.txt 9206_8.txt 11457_10.txt 2457_8.txt 4707_7.txt 6958_7.txt 9207_10.txt 11458_7.txt 2458_8.txt 4708_7.txt 6959_7.txt 9208_7.txt 11459_10.txt 2459_8.txt 4709_9.txt 695_10.txt 9209_7.txt 1145_8.txt 245_9.txt 470_10.txt 6960_7.txt 920_10.txt 11460_10.txt 2460_10.txt 4710_7.txt 6961_7.txt 9210_10.txt 11461_10.txt 2461_10.txt 4711_8.txt 6962_7.txt 9211_9.txt 11462_10.txt 2462_10.txt 4712_7.txt 6963_8.txt 9212_9.txt 11463_9.txt 2463_10.txt 4713_8.txt 6964_7.txt 9213_7.txt 11464_10.txt 2464_10.txt 4714_10.txt 6965_7.txt 9214_7.txt 11465_7.txt 2465_10.txt 4715_9.txt 6966_8.txt 9215_7.txt 11466_10.txt 2466_9.txt 4716_9.txt 6967_7.txt 9216_10.txt 11467_10.txt 2467_10.txt 4717_8.txt 6968_7.txt 9217_7.txt 11468_9.txt 2468_10.txt 4718_7.txt 6969_10.txt 9218_10.txt 11469_10.txt 2469_10.txt 4719_10.txt 696_10.txt 9219_10.txt 1146_7.txt 246_7.txt 471_7.txt 6970_8.txt 921_7.txt 11470_10.txt 2470_10.txt 4720_8.txt 6971_10.txt 9220_8.txt 11471_7.txt 2471_10.txt 4721_8.txt 6972_9.txt 9221_7.txt 11472_7.txt 2472_10.txt 4722_8.txt 6973_10.txt 9222_7.txt 11473_10.txt 2473_10.txt 4723_8.txt 6974_10.txt 9223_9.txt 11474_10.txt 2474_10.txt 4724_7.txt 6975_8.txt 9224_10.txt 11475_8.txt 2475_10.txt 4725_9.txt 6976_9.txt 9225_10.txt 11476_10.txt 2476_10.txt 4726_7.txt 6977_9.txt 9226_8.txt 11477_8.txt 2477_7.txt 4727_7.txt 6978_8.txt 9227_9.txt 11478_8.txt 2478_10.txt 4728_8.txt 6979_10.txt 9228_10.txt 11479_10.txt 2479_10.txt 4729_10.txt 697_8.txt 9229_7.txt 1147_8.txt 247_10.txt 472_10.txt 6980_9.txt 922_8.txt 11480_10.txt 2480_8.txt 4730_10.txt 6981_9.txt 9230_8.txt 11481_10.txt 2481_8.txt 4731_8.txt 6982_9.txt 9231_8.txt 11482_9.txt 2482_10.txt 4732_10.txt 6983_8.txt 9232_8.txt 11483_10.txt 2483_7.txt 4733_9.txt 6984_8.txt 9233_8.txt 11484_9.txt 2484_10.txt 4734_9.txt 6985_10.txt 9234_8.txt 11485_10.txt 2485_10.txt 4735_9.txt 6986_8.txt 9235_9.txt 11486_8.txt 2486_10.txt 4736_10.txt 6987_10.txt 9236_10.txt 11487_10.txt 2487_10.txt 4737_7.txt 6988_10.txt 9237_7.txt 11488_10.txt 2488_10.txt 4738_7.txt 6989_7.txt 9238_10.txt 11489_10.txt 2489_7.txt 4739_7.txt 698_7.txt 9239_8.txt 1148_8.txt 248_10.txt 473_9.txt 6990_8.txt 923_9.txt 11490_9.txt 2490_8.txt 4740_8.txt 6991_9.txt 9240_10.txt 11491_8.txt 2491_8.txt 4741_7.txt 6992_10.txt 9241_10.txt 11492_7.txt 2492_10.txt 4742_9.txt 6993_7.txt 9242_10.txt 11493_7.txt 2493_10.txt 4743_8.txt 6994_8.txt 9243_8.txt 11494_7.txt 2494_10.txt 4744_9.txt 6995_7.txt 9244_8.txt 11495_10.txt 2495_9.txt 4745_7.txt 6996_8.txt 9245_8.txt 11496_10.txt 2496_8.txt 4746_8.txt 6997_7.txt 9246_8.txt 11497_10.txt 2497_8.txt 4747_8.txt 6998_7.txt 9247_10.txt 11498_7.txt 2498_7.txt 4748_7.txt 6999_7.txt 9248_9.txt 11499_10.txt 2499_10.txt 4749_8.txt 699_8.txt 9249_9.txt 1149_8.txt 249_10.txt 474_7.txt 69_10.txt 924_7.txt 114_10.txt 24_8.txt 4750_7.txt 6_10.txt 9250_8.txt 11500_10.txt 2500_9.txt 4751_8.txt 7000_7.txt 9251_9.txt 11501_8.txt 2501_8.txt 4752_7.txt 7001_10.txt 9252_10.txt 11502_10.txt 2502_8.txt 4753_10.txt 7002_10.txt 9253_10.txt 11503_10.txt 2503_10.txt 4754_10.txt 7003_10.txt 9254_7.txt 11504_8.txt 2504_10.txt 4755_7.txt 7004_10.txt 9255_10.txt 11505_7.txt 2505_9.txt 4756_10.txt 7005_9.txt 9256_10.txt 11506_8.txt 2506_9.txt 4757_8.txt 7006_8.txt 9257_7.txt 11507_10.txt 2507_7.txt 4758_8.txt 7007_8.txt 9258_10.txt 11508_8.txt 2508_10.txt 4759_7.txt 7008_8.txt 9259_10.txt 11509_8.txt 2509_9.txt 475_10.txt 7009_10.txt 925_10.txt 1150_10.txt 250_7.txt 4760_7.txt 700_8.txt 9260_7.txt 11510_10.txt 2510_10.txt 4761_10.txt 7010_10.txt 9261_7.txt 11511_9.txt 2511_10.txt 4762_10.txt 7011_7.txt 9262_8.txt 11512_10.txt 2512_10.txt 4763_8.txt 7012_10.txt 9263_8.txt 11513_8.txt 2513_9.txt 4764_10.txt 7013_9.txt 9264_9.txt 11514_7.txt 2514_8.txt 4765_7.txt 7014_10.txt 9265_9.txt 11515_10.txt 2515_10.txt 4766_10.txt 7015_8.txt 9266_9.txt 11516_8.txt 2516_9.txt 4767_9.txt 7016_7.txt 9267_7.txt 11517_9.txt 2517_9.txt 4768_10.txt 7017_9.txt 9268_9.txt 11518_8.txt 2518_10.txt 4769_7.txt 7018_10.txt 9269_7.txt 11519_9.txt 2519_10.txt 476_7.txt 7019_7.txt 926_7.txt 1151_9.txt 251_10.txt 4770_10.txt 701_7.txt 9270_9.txt 11520_8.txt 2520_10.txt 4771_10.txt 7020_10.txt 9271_8.txt 11521_10.txt 2521_10.txt 4772_10.txt 7021_10.txt 9272_7.txt 11522_8.txt 2522_8.txt 4773_10.txt 7022_7.txt 9273_7.txt 11523_7.txt 2523_9.txt 4774_10.txt 7023_9.txt 9274_8.txt 11524_7.txt 2524_10.txt 4775_7.txt 7024_8.txt 9275_9.txt 11525_8.txt 2525_8.txt 4776_8.txt 7025_8.txt 9276_10.txt 11526_10.txt 2526_9.txt 4777_10.txt 7026_7.txt 9277_9.txt 11527_7.txt 2527_9.txt 4778_9.txt 7027_7.txt 9278_7.txt 11528_7.txt 2528_10.txt 4779_10.txt 7028_9.txt 9279_7.txt 11529_7.txt 2529_7.txt 477_10.txt 7029_7.txt 927_10.txt 1152_10.txt 252_9.txt 4780_9.txt 702_10.txt 9280_9.txt 11530_8.txt 2530_8.txt 4781_8.txt 7030_10.txt 9281_10.txt 11531_10.txt 2531_7.txt 4782_9.txt 7031_10.txt 9282_8.txt 11532_8.txt 2532_8.txt 4783_10.txt 7032_10.txt 9283_8.txt 11533_8.txt 2533_7.txt 4784_10.txt 7033_7.txt 9284_8.txt 11534_7.txt 2534_8.txt 4785_10.txt 7034_10.txt 9285_10.txt 11535_7.txt 2535_7.txt 4786_10.txt 7035_8.txt 9286_9.txt 11536_7.txt 2536_7.txt 4787_10.txt 7036_10.txt 9287_9.txt 11537_7.txt 2537_7.txt 4788_10.txt 7037_10.txt 9288_8.txt 11538_7.txt 2538_10.txt 4789_10.txt 7038_9.txt 9289_7.txt 11539_9.txt 2539_10.txt 478_7.txt 7039_7.txt 928_10.txt 1153_10.txt 253_7.txt 4790_10.txt 703_10.txt 9290_9.txt 11540_10.txt 2540_8.txt 4791_10.txt 7040_10.txt 9291_7.txt 11541_10.txt 2541_7.txt 4792_10.txt 7041_7.txt 9292_10.txt 11542_7.txt 2542_10.txt 4793_7.txt 7042_10.txt 9293_7.txt 11543_8.txt 2543_10.txt 4794_8.txt 7043_7.txt 9294_7.txt 11544_10.txt 2544_8.txt 4795_9.txt 7044_10.txt 9295_10.txt 11545_8.txt 2545_8.txt 4796_8.txt 7045_10.txt 9296_10.txt 11546_9.txt 2546_10.txt 4797_10.txt 7046_7.txt 9297_10.txt 11547_9.txt 2547_10.txt 4798_8.txt 7047_10.txt 9298_9.txt 11548_10.txt 2548_7.txt 4799_10.txt 7048_10.txt 9299_8.txt 11549_7.txt 2549_8.txt 479_10.txt 7049_7.txt 929_10.txt 1154_10.txt 254_8.txt 47_8.txt 704_10.txt 92_9.txt 11550_7.txt 2550_9.txt 4800_7.txt 7050_7.txt 9300_10.txt 11551_7.txt 2551_9.txt 4801_7.txt 7051_10.txt 9301_10.txt 11552_9.txt 2552_10.txt 4802_8.txt 7052_8.txt 9302_10.txt 11553_9.txt 2553_7.txt 4803_8.txt 7053_7.txt 9303_8.txt 11554_8.txt 2554_7.txt 4804_7.txt 7054_9.txt 9304_9.txt 11555_10.txt 2555_10.txt 4805_8.txt 7055_8.txt 9305_10.txt 11556_7.txt 2556_8.txt 4806_8.txt 7056_7.txt 9306_9.txt 11557_8.txt 2557_10.txt 4807_10.txt 7057_9.txt 9307_10.txt 11558_10.txt 2558_10.txt 4808_8.txt 7058_8.txt 9308_7.txt 11559_10.txt 2559_9.txt 4809_10.txt 7059_10.txt 9309_9.txt 1155_10.txt 255_10.txt 480_10.txt 705_10.txt 930_7.txt 11560_9.txt 2560_7.txt 4810_9.txt 7060_10.txt 9310_10.txt 11561_10.txt 2561_7.txt 4811_8.txt 7061_7.txt 9311_10.txt 11562_9.txt 2562_10.txt 4812_8.txt 7062_7.txt 9312_10.txt 11563_10.txt 2563_10.txt 4813_9.txt 7063_8.txt 9313_9.txt 11564_9.txt 2564_10.txt 4814_10.txt 7064_8.txt 9314_10.txt 11565_9.txt 2565_9.txt 4815_10.txt 7065_7.txt 9315_7.txt 11566_10.txt 2566_8.txt 4816_8.txt 7066_8.txt 9316_7.txt 11567_8.txt 2567_9.txt 4817_10.txt 7067_7.txt 9317_8.txt 11568_7.txt 2568_10.txt 4818_9.txt 7068_8.txt 9318_9.txt 11569_8.txt 2569_10.txt 4819_7.txt 7069_8.txt 9319_8.txt 1156_9.txt 256_9.txt 481_10.txt 706_10.txt 931_10.txt 11570_9.txt 2570_10.txt 4820_7.txt 7070_9.txt 9320_10.txt 11571_9.txt 2571_7.txt 4821_10.txt 7071_9.txt 9321_8.txt 11572_8.txt 2572_10.txt 4822_9.txt 7072_9.txt 9322_8.txt 11573_10.txt 2573_10.txt 4823_7.txt 7073_10.txt 9323_10.txt 11574_10.txt 2574_7.txt 4824_7.txt 7074_7.txt 9324_7.txt 11575_10.txt 2575_7.txt 4825_8.txt 7075_10.txt 9325_10.txt 11576_7.txt 2576_8.txt 4826_7.txt 7076_10.txt 9326_10.txt 11577_8.txt 2577_10.txt 4827_9.txt 7077_10.txt 9327_8.txt 11578_10.txt 2578_10.txt 4828_7.txt 7078_9.txt 9328_7.txt 11579_10.txt 2579_8.txt 4829_8.txt 7079_9.txt 9329_8.txt 1157_10.txt 257_7.txt 482_8.txt 707_8.txt 932_10.txt 11580_10.txt 2580_10.txt 4830_9.txt 7080_10.txt 9330_8.txt 11581_10.txt 2581_8.txt 4831_8.txt 7081_8.txt 9331_9.txt 11582_10.txt 2582_8.txt 4832_8.txt 7082_8.txt 9332_10.txt 11583_10.txt 2583_10.txt 4833_10.txt 7083_10.txt 9333_8.txt 11584_8.txt 2584_7.txt 4834_8.txt 7084_10.txt 9334_9.txt 11585_10.txt 2585_7.txt 4835_10.txt 7085_9.txt 9335_10.txt 11586_9.txt 2586_10.txt 4836_8.txt 7086_10.txt 9336_9.txt 11587_10.txt 2587_9.txt 4837_10.txt 7087_9.txt 9337_10.txt 11588_10.txt 2588_7.txt 4838_10.txt 7088_10.txt 9338_8.txt 11589_10.txt 2589_7.txt 4839_10.txt 7089_9.txt 9339_10.txt 1158_9.txt 258_7.txt 483_8.txt 708_8.txt 933_10.txt 11590_10.txt 2590_7.txt 4840_7.txt 7090_10.txt 9340_8.txt 11591_10.txt 2591_7.txt 4841_7.txt 7091_9.txt 9341_7.txt 11592_10.txt 2592_10.txt 4842_10.txt 7092_8.txt 9342_8.txt 11593_10.txt 2593_10.txt 4843_7.txt 7093_10.txt 9343_8.txt 11594_10.txt 2594_9.txt 4844_8.txt 7094_10.txt 9344_8.txt 11595_10.txt 2595_9.txt 4845_9.txt 7095_9.txt 9345_9.txt 11596_10.txt 2596_10.txt 4846_7.txt 7096_10.txt 9346_10.txt 11597_10.txt 2597_8.txt 4847_9.txt 7097_9.txt 9347_7.txt 11598_8.txt 2598_10.txt 4848_7.txt 7098_9.txt 9348_7.txt 11599_8.txt 2599_10.txt 4849_10.txt 7099_10.txt 9349_8.txt 1159_10.txt 259_8.txt 484_8.txt 709_10.txt 934_9.txt 115_10.txt 25_7.txt 4850_10.txt 70_9.txt 9350_10.txt 11600_7.txt 2600_10.txt 4851_7.txt 7100_8.txt 9351_8.txt 11601_8.txt 2601_10.txt 4852_8.txt 7101_10.txt 9352_10.txt 11602_8.txt 2602_10.txt 4853_10.txt 7102_7.txt 9353_8.txt 11603_10.txt 2603_8.txt 4854_7.txt 7103_7.txt 9354_9.txt 11604_8.txt 2604_8.txt 4855_7.txt 7104_7.txt 9355_7.txt 11605_10.txt 2605_7.txt 4856_10.txt 7105_9.txt 9356_9.txt 11606_10.txt 2606_8.txt 4857_7.txt 7106_8.txt 9357_7.txt 11607_10.txt 2607_9.txt 4858_8.txt 7107_10.txt 9358_8.txt 11608_10.txt 2608_10.txt 4859_7.txt 7108_9.txt 9359_8.txt 11609_10.txt 2609_10.txt 485_8.txt 7109_10.txt 935_9.txt 1160_10.txt 260_7.txt 4860_10.txt 710_9.txt 9360_10.txt 11610_10.txt 2610_9.txt 4861_8.txt 7110_8.txt 9361_9.txt 11611_9.txt 2611_9.txt 4862_7.txt 7111_8.txt 9362_7.txt 11612_10.txt 2612_10.txt 4863_10.txt 7112_9.txt 9363_10.txt 11613_7.txt 2613_9.txt 4864_7.txt 7113_8.txt 9364_9.txt 11614_7.txt 2614_9.txt 4865_7.txt 7114_9.txt 9365_9.txt 11615_9.txt 2615_9.txt 4866_7.txt 7115_9.txt 9366_9.txt 11616_8.txt 2616_10.txt 4867_9.txt 7116_9.txt 9367_9.txt 11617_9.txt 2617_10.txt 4868_8.txt 7117_10.txt 9368_8.txt 11618_8.txt 2618_7.txt 4869_10.txt 7118_10.txt 9369_10.txt 11619_9.txt 2619_8.txt 486_9.txt 7119_10.txt 936_8.txt 1161_8.txt 261_8.txt 4870_10.txt 711_10.txt 9370_7.txt 11620_8.txt 2620_10.txt 4871_8.txt 7120_9.txt 9371_7.txt 11621_9.txt 2621_8.txt 4872_8.txt 7121_10.txt 9372_9.txt 11622_10.txt 2622_8.txt 4873_8.txt 7122_8.txt 9373_9.txt 11623_8.txt 2623_7.txt 4874_10.txt 7123_10.txt 9374_9.txt 11624_8.txt 2624_7.txt 4875_7.txt 7124_8.txt 9375_10.txt 11625_7.txt 2625_8.txt 4876_10.txt 7125_10.txt 9376_10.txt 11626_9.txt 2626_8.txt 4877_9.txt 7126_10.txt 9377_7.txt 11627_8.txt 2627_9.txt 4878_10.txt 7127_10.txt 9378_9.txt 11628_10.txt 2628_10.txt 4879_10.txt 7128_7.txt 9379_7.txt 11629_10.txt 2629_9.txt 487_8.txt 7129_10.txt 937_10.txt 1162_9.txt 262_8.txt 4880_9.txt 712_9.txt 9380_9.txt 11630_8.txt 2630_8.txt 4881_9.txt 7130_10.txt 9381_8.txt 11631_7.txt 2631_7.txt 4882_9.txt 7131_9.txt 9382_9.txt 11632_7.txt 2632_7.txt 4883_10.txt 7132_10.txt 9383_8.txt 11633_8.txt 2633_8.txt 4884_10.txt 7133_10.txt 9384_8.txt 11634_9.txt 2634_7.txt 4885_9.txt 7134_10.txt 9385_8.txt 11635_8.txt 2635_7.txt 4886_8.txt 7135_10.txt 9386_8.txt 11636_8.txt 2636_9.txt 4887_10.txt 7136_10.txt 9387_9.txt 11637_8.txt 2637_7.txt 4888_8.txt 7137_8.txt 9388_8.txt 11638_7.txt 2638_10.txt 4889_10.txt 7138_10.txt 9389_7.txt 11639_10.txt 2639_7.txt 488_9.txt 7139_10.txt 938_9.txt 1163_7.txt 263_9.txt 4890_10.txt 713_10.txt 9390_9.txt 11640_9.txt 2640_10.txt 4891_10.txt 7140_10.txt 9391_8.txt 11641_7.txt 2641_10.txt 4892_10.txt 7141_8.txt 9392_9.txt 11642_10.txt 2642_10.txt 4893_10.txt 7142_8.txt 9393_10.txt 11643_7.txt 2643_10.txt 4894_10.txt 7143_10.txt 9394_10.txt 11644_10.txt 2644_8.txt 4895_10.txt 7144_10.txt 9395_9.txt 11645_8.txt 2645_8.txt 4896_7.txt 7145_9.txt 9396_9.txt 11646_10.txt 2646_10.txt 4897_8.txt 7146_10.txt 9397_9.txt 11647_8.txt 2647_10.txt 4898_7.txt 7147_10.txt 9398_9.txt 11648_9.txt 2648_8.txt 4899_10.txt 7148_10.txt 9399_7.txt 11649_10.txt 2649_7.txt 489_7.txt 7149_10.txt 939_10.txt 1164_10.txt 264_7.txt 48_7.txt 714_10.txt 93_10.txt 11650_9.txt 2650_10.txt 4900_8.txt 7150_9.txt 9400_8.txt 11651_10.txt 2651_10.txt 4901_9.txt 7151_10.txt 9401_10.txt 11652_10.txt 2652_10.txt 4902_8.txt 7152_9.txt 9402_7.txt 11653_10.txt 2653_10.txt 4903_10.txt 7153_8.txt 9403_8.txt 11654_9.txt 2654_10.txt 4904_10.txt 7154_10.txt 9404_10.txt 11655_10.txt 2655_10.txt 4905_10.txt 7155_8.txt 9405_10.txt 11656_9.txt 2656_10.txt 4906_9.txt 7156_10.txt 9406_8.txt 11657_9.txt 2657_10.txt 4907_8.txt 7157_7.txt 9407_8.txt 11658_10.txt 2658_10.txt 4908_10.txt 7158_8.txt 9408_10.txt 11659_9.txt 2659_8.txt 4909_8.txt 7159_10.txt 9409_8.txt 1165_7.txt 265_7.txt 490_9.txt 715_10.txt 940_10.txt 11660_7.txt 2660_10.txt 4910_8.txt 7160_8.txt 9410_7.txt 11661_7.txt 2661_10.txt 4911_9.txt 7161_9.txt 9411_8.txt 11662_9.txt 2662_10.txt 4912_8.txt 7162_10.txt 9412_10.txt 11663_8.txt 2663_10.txt 4913_9.txt 7163_10.txt 9413_9.txt 11664_9.txt 2664_9.txt 4914_7.txt 7164_10.txt 9414_9.txt 11665_8.txt 2665_10.txt 4915_7.txt 7165_10.txt 9415_8.txt 11666_10.txt 2666_10.txt 4916_9.txt 7166_9.txt 9416_10.txt 11667_9.txt 2667_8.txt 4917_8.txt 7167_9.txt 9417_10.txt 11668_7.txt 2668_9.txt 4918_7.txt 7168_10.txt 9418_10.txt 11669_9.txt 2669_10.txt 4919_7.txt 7169_8.txt 9419_8.txt 1166_8.txt 266_7.txt 491_7.txt 716_10.txt 941_10.txt 11670_7.txt 2670_10.txt 4920_7.txt 7170_9.txt 9420_8.txt 11671_8.txt 2671_7.txt 4921_8.txt 7171_10.txt 9421_10.txt 11672_10.txt 2672_7.txt 4922_7.txt 7172_10.txt 9422_9.txt 11673_8.txt 2673_8.txt 4923_7.txt 7173_10.txt 9423_8.txt 11674_10.txt 2674_7.txt 4924_7.txt 7174_9.txt 9424_10.txt 11675_10.txt 2675_9.txt 4925_7.txt 7175_10.txt 9425_10.txt 11676_8.txt 2676_10.txt 4926_8.txt 7176_10.txt 9426_10.txt 11677_8.txt 2677_9.txt 4927_8.txt 7177_8.txt 9427_10.txt 11678_8.txt 2678_8.txt 4928_7.txt 7178_10.txt 9428_10.txt 11679_7.txt 2679_8.txt 4929_7.txt 7179_8.txt 9429_9.txt 1167_7.txt 267_7.txt 492_7.txt 717_7.txt 942_10.txt 11680_8.txt 2680_7.txt 4930_7.txt 7180_9.txt 9430_9.txt 11681_9.txt 2681_7.txt 4931_8.txt 7181_10.txt 9431_10.txt 11682_7.txt 2682_9.txt 4932_8.txt 7182_10.txt 9432_9.txt 11683_8.txt 2683_10.txt 4933_8.txt 7183_7.txt 9433_10.txt 11684_8.txt 2684_10.txt 4934_8.txt 7184_7.txt 9434_10.txt 11685_8.txt 2685_10.txt 4935_7.txt 7185_10.txt 9435_7.txt 11686_10.txt 2686_9.txt 4936_8.txt 7186_7.txt 9436_10.txt 11687_10.txt 2687_10.txt 4937_7.txt 7187_8.txt 9437_9.txt 11688_10.txt 2688_8.txt 4938_7.txt 7188_8.txt 9438_10.txt 11689_8.txt 2689_9.txt 4939_7.txt 7189_10.txt 9439_8.txt 1168_8.txt 268_8.txt 493_10.txt 718_10.txt 943_10.txt 11690_9.txt 2690_9.txt 4940_7.txt 7190_9.txt 9440_8.txt 11691_8.txt 2691_8.txt 4941_10.txt 7191_9.txt 9441_9.txt 11692_7.txt 2692_8.txt 4942_7.txt 7192_8.txt 9442_10.txt 11693_7.txt 2693_7.txt 4943_10.txt 7193_10.txt 9443_9.txt 11694_7.txt 2694_10.txt 4944_7.txt 7194_7.txt 9444_10.txt 11695_9.txt 2695_10.txt 4945_7.txt 7195_10.txt 9445_10.txt 11696_7.txt 2696_8.txt 4946_10.txt 7196_8.txt 9446_10.txt 11697_10.txt 2697_9.txt 4947_8.txt 7197_7.txt 9447_8.txt 11698_10.txt 2698_8.txt 4948_7.txt 7198_9.txt 9448_7.txt 11699_10.txt 2699_10.txt 4949_8.txt 7199_8.txt 9449_10.txt 1169_8.txt 269_8.txt 494_9.txt 719_10.txt 944_10.txt 116_10.txt 26_9.txt 4950_8.txt 71_10.txt 9450_10.txt 11700_10.txt 2700_10.txt 4951_8.txt 7200_9.txt 9451_8.txt 11701_9.txt 2701_9.txt 4952_7.txt 7201_8.txt 9452_9.txt 11702_10.txt 2702_9.txt 4953_8.txt 7202_8.txt 9453_8.txt 11703_9.txt 2703_9.txt 4954_10.txt 7203_9.txt 9454_10.txt 11704_10.txt 2704_10.txt 4955_9.txt 7204_8.txt 9455_9.txt 11705_7.txt 2705_10.txt 4956_10.txt 7205_10.txt 9456_8.txt 11706_10.txt 2706_9.txt 4957_8.txt 7206_9.txt 9457_9.txt 11707_10.txt 2707_10.txt 4958_10.txt 7207_7.txt 9458_10.txt 11708_8.txt 2708_7.txt 4959_7.txt 7208_7.txt 9459_8.txt 11709_10.txt 2709_8.txt 495_7.txt 7209_8.txt 945_10.txt 1170_8.txt 270_10.txt 4960_9.txt 720_7.txt 9460_8.txt 11710_10.txt 2710_7.txt 4961_10.txt 7210_8.txt 9461_7.txt 11711_10.txt 2711_7.txt 4962_7.txt 7211_10.txt 9462_9.txt 11712_10.txt 2712_10.txt 4963_10.txt 7212_9.txt 9463_10.txt 11713_10.txt 2713_8.txt 4964_10.txt 7213_10.txt 9464_8.txt 11714_10.txt 2714_10.txt 4965_9.txt 7214_7.txt 9465_7.txt 11715_7.txt 2715_8.txt 4966_10.txt 7215_7.txt 9466_7.txt 11716_7.txt 2716_10.txt 4967_10.txt 7216_10.txt 9467_10.txt 11717_8.txt 2717_10.txt 4968_10.txt 7217_7.txt 9468_7.txt 11718_10.txt 2718_9.txt 4969_7.txt 7218_9.txt 9469_10.txt 11719_7.txt 2719_10.txt 496_10.txt 7219_7.txt 946_8.txt 1171_10.txt 271_10.txt 4970_8.txt 721_10.txt 9470_8.txt 11720_8.txt 2720_7.txt 4971_7.txt 7220_7.txt 9471_7.txt 11721_7.txt 2721_9.txt 4972_9.txt 7221_8.txt 9472_8.txt 11722_8.txt 2722_8.txt 4973_9.txt 7222_8.txt 9473_10.txt 11723_7.txt 2723_7.txt 4974_10.txt 7223_10.txt 9474_10.txt 11724_8.txt 2724_9.txt 4975_8.txt 7224_8.txt 9475_9.txt 11725_10.txt 2725_8.txt 4976_10.txt 7225_10.txt 9476_9.txt 11726_9.txt 2726_9.txt 4977_7.txt 7226_8.txt 9477_9.txt 11727_7.txt 2727_9.txt 4978_7.txt 7227_7.txt 9478_8.txt 11728_9.txt 2728_10.txt 4979_10.txt 7228_9.txt 9479_10.txt 11729_10.txt 2729_10.txt 497_10.txt 7229_7.txt 947_10.txt 1172_8.txt 272_10.txt 4980_8.txt 722_7.txt 9480_10.txt 11730_10.txt 2730_9.txt 4981_10.txt 7230_8.txt 9481_9.txt 11731_10.txt 2731_10.txt 4982_7.txt 7231_8.txt 9482_7.txt 11732_7.txt 2732_8.txt 4983_7.txt 7232_8.txt 9483_10.txt 11733_10.txt 2733_10.txt 4984_8.txt 7233_7.txt 9484_10.txt 11734_10.txt 2734_9.txt 4985_10.txt 7234_8.txt 9485_10.txt 11735_10.txt 2735_10.txt 4986_8.txt 7235_8.txt 9486_10.txt 11736_9.txt 2736_9.txt 4987_10.txt 7236_8.txt 9487_8.txt 11737_8.txt 2737_9.txt 4988_7.txt 7237_10.txt 9488_10.txt 11738_9.txt 2738_10.txt 4989_8.txt 7238_9.txt 9489_9.txt 11739_8.txt 2739_9.txt 498_10.txt 7239_7.txt 948_10.txt 1173_8.txt 273_9.txt 4990_8.txt 723_8.txt 9490_10.txt 11740_10.txt 2740_8.txt 4991_10.txt 7240_8.txt 9491_10.txt 11741_10.txt 2741_10.txt 4992_7.txt 7241_8.txt 9492_7.txt 11742_9.txt 2742_9.txt 4993_10.txt 7242_7.txt 9493_8.txt 11743_7.txt 2743_9.txt 4994_9.txt 7243_10.txt 9494_10.txt 11744_9.txt 2744_10.txt 4995_9.txt 7244_10.txt 9495_8.txt 11745_10.txt 2745_10.txt 4996_9.txt 7245_10.txt 9496_10.txt 11746_9.txt 2746_10.txt 4997_9.txt 7246_10.txt 9497_8.txt 11747_8.txt 2747_9.txt 4998_9.txt 7247_10.txt 9498_8.txt 11748_8.txt 2748_8.txt 4999_9.txt 7248_9.txt 9499_7.txt 11749_10.txt 2749_10.txt 499_8.txt 7249_8.txt 949_8.txt 1174_10.txt 274_7.txt 49_10.txt 724_10.txt 94_10.txt 11750_10.txt 2750_10.txt 4_8.txt 7250_8.txt 9500_7.txt 11751_10.txt 2751_10.txt 5000_10.txt 7251_10.txt 9501_10.txt 11752_10.txt 2752_7.txt 5001_7.txt 7252_8.txt 9502_8.txt 11753_10.txt 2753_10.txt 5002_8.txt 7253_10.txt 9503_7.txt 11754_7.txt 2754_10.txt 5003_10.txt 7254_10.txt 9504_10.txt 11755_7.txt 2755_10.txt 5004_9.txt 7255_9.txt 9505_10.txt 11756_7.txt 2756_9.txt 5005_8.txt 7256_9.txt 9506_8.txt 11757_9.txt 2757_10.txt 5006_10.txt 7257_10.txt 9507_10.txt 11758_7.txt 2758_7.txt 5007_10.txt 7258_10.txt 9508_8.txt 11759_10.txt 2759_10.txt 5008_10.txt 7259_7.txt 9509_9.txt 1175_9.txt 275_10.txt 5009_9.txt 725_10.txt 950_9.txt 11760_8.txt 2760_8.txt 500_9.txt 7260_10.txt 9510_8.txt 11761_10.txt 2761_10.txt 5010_10.txt 7261_8.txt 9511_7.txt 11762_8.txt 2762_9.txt 5011_10.txt 7262_10.txt 9512_8.txt 11763_7.txt 2763_8.txt 5012_10.txt 7263_10.txt 9513_10.txt 11764_10.txt 2764_10.txt 5013_10.txt 7264_8.txt 9514_10.txt 11765_8.txt 2765_9.txt 5014_9.txt 7265_8.txt 9515_10.txt 11766_8.txt 2766_9.txt 5015_7.txt 7266_7.txt 9516_8.txt 11767_8.txt 2767_8.txt 5016_10.txt 7267_7.txt 9517_10.txt 11768_7.txt 2768_8.txt 5017_8.txt 7268_9.txt 9518_10.txt 11769_7.txt 2769_10.txt 5018_8.txt 7269_7.txt 9519_10.txt 1176_10.txt 276_10.txt 5019_10.txt 726_7.txt 951_10.txt 11770_10.txt 2770_8.txt 501_10.txt 7270_8.txt 9520_10.txt 11771_10.txt 2771_7.txt 5020_10.txt 7271_7.txt 9521_10.txt 11772_10.txt 2772_7.txt 5021_10.txt 7272_10.txt 9522_10.txt 11773_8.txt 2773_7.txt 5022_8.txt 7273_8.txt 9523_10.txt 11774_10.txt 2774_10.txt 5023_10.txt 7274_10.txt 9524_9.txt 11775_10.txt 2775_7.txt 5024_8.txt 7275_8.txt 9525_8.txt 11776_9.txt 2776_8.txt 5025_10.txt 7276_8.txt 9526_10.txt 11777_9.txt 2777_7.txt 5026_8.txt 7277_7.txt 9527_10.txt 11778_10.txt 2778_8.txt 5027_10.txt 7278_7.txt 9528_9.txt 11779_7.txt 2779_7.txt 5028_10.txt 7279_9.txt 9529_9.txt 1177_9.txt 277_8.txt 5029_8.txt 727_9.txt 952_10.txt 11780_8.txt 2780_9.txt 502_10.txt 7280_10.txt 9530_9.txt 11781_8.txt 2781_8.txt 5030_9.txt 7281_10.txt 9531_7.txt 11782_8.txt 2782_10.txt 5031_10.txt 7282_9.txt 9532_10.txt 11783_10.txt 2783_8.txt 5032_10.txt 7283_8.txt 9533_9.txt 11784_10.txt 2784_8.txt 5033_10.txt 7284_8.txt 9534_10.txt 11785_10.txt 2785_7.txt 5034_7.txt 7285_9.txt 9535_10.txt 11786_8.txt 2786_7.txt 5035_7.txt 7286_8.txt 9536_7.txt 11787_9.txt 2787_8.txt 5036_9.txt 7287_7.txt 9537_8.txt 11788_8.txt 2788_9.txt 5037_10.txt 7288_7.txt 9538_10.txt 11789_10.txt 2789_7.txt 5038_10.txt 7289_10.txt 9539_7.txt 1178_10.txt 278_9.txt 5039_10.txt 728_10.txt 953_10.txt 11790_8.txt 2790_10.txt 503_10.txt 7290_10.txt 9540_8.txt 11791_8.txt 2791_10.txt 5040_9.txt 7291_10.txt 9541_8.txt 11792_8.txt 2792_10.txt 5041_8.txt 7292_10.txt 9542_10.txt 11793_8.txt 2793_9.txt 5042_9.txt 7293_10.txt 9543_7.txt 11794_7.txt 2794_8.txt 5043_7.txt 7294_9.txt 9544_10.txt 11795_10.txt 2795_7.txt 5044_9.txt 7295_10.txt 9545_7.txt 11796_9.txt 2796_10.txt 5045_8.txt 7296_10.txt 9546_8.txt 11797_8.txt 2797_10.txt 5046_10.txt 7297_10.txt 9547_9.txt 11798_10.txt 2798_8.txt 5047_8.txt 7298_7.txt 9548_10.txt 11799_8.txt 2799_10.txt 5048_9.txt 7299_8.txt 9549_8.txt 1179_9.txt 279_9.txt 5049_9.txt 729_10.txt 954_10.txt 117_10.txt 27_10.txt 504_8.txt 72_7.txt 9550_7.txt 11800_8.txt 2800_10.txt 5050_7.txt 7300_8.txt 9551_10.txt 11801_8.txt 2801_9.txt 5051_7.txt 7301_7.txt 9552_8.txt 11802_7.txt 2802_10.txt 5052_10.txt 7302_10.txt 9553_9.txt 11803_7.txt 2803_10.txt 5053_8.txt 7303_10.txt 9554_8.txt 11804_10.txt 2804_8.txt 5054_7.txt 7304_8.txt 9555_10.txt 11805_10.txt 2805_9.txt 5055_8.txt 7305_10.txt 9556_10.txt 11806_10.txt 2806_10.txt 5056_8.txt 7306_10.txt 9557_7.txt 11807_7.txt 2807_7.txt 5057_10.txt 7307_10.txt 9558_10.txt 11808_9.txt 2808_10.txt 5058_10.txt 7308_8.txt 9559_8.txt 11809_10.txt 2809_8.txt 5059_7.txt 7309_9.txt 955_7.txt 1180_9.txt 280_8.txt 505_9.txt 730_7.txt 9560_9.txt 11810_7.txt 2810_10.txt 5060_8.txt 7310_9.txt 9561_8.txt 11811_8.txt 2811_10.txt 5061_8.txt 7311_9.txt 9562_8.txt 11812_10.txt 2812_8.txt 5062_9.txt 7312_10.txt 9563_9.txt 11813_8.txt 2813_8.txt 5063_7.txt 7313_10.txt 9564_10.txt 11814_7.txt 2814_10.txt 5064_8.txt 7314_10.txt 9565_8.txt 11815_10.txt 2815_10.txt 5065_7.txt 7315_10.txt 9566_7.txt 11816_8.txt 2816_7.txt 5066_8.txt 7316_10.txt 9567_7.txt 11817_10.txt 2817_9.txt 5067_10.txt 7317_10.txt 9568_7.txt 11818_10.txt 2818_7.txt 5068_9.txt 7318_10.txt 9569_9.txt 11819_8.txt 2819_10.txt 5069_7.txt 7319_10.txt 956_9.txt 1181_9.txt 281_10.txt 506_7.txt 731_9.txt 9570_8.txt 11820_8.txt 2820_7.txt 5070_9.txt 7320_10.txt 9571_9.txt 11821_10.txt 2821_9.txt 5071_10.txt 7321_10.txt 9572_8.txt 11822_10.txt 2822_7.txt 5072_8.txt 7322_10.txt 9573_8.txt 11823_9.txt 2823_7.txt 5073_7.txt 7323_9.txt 9574_9.txt 11824_10.txt 2824_8.txt 5074_10.txt 7324_10.txt 9575_9.txt 11825_8.txt 2825_9.txt 5075_7.txt 7325_9.txt 9576_9.txt 11826_10.txt 2826_10.txt 5076_10.txt 7326_10.txt 9577_8.txt 11827_9.txt 2827_10.txt 5077_9.txt 7327_10.txt 9578_10.txt 11828_8.txt 2828_9.txt 5078_9.txt 7328_8.txt 9579_8.txt 11829_10.txt 2829_10.txt 5079_8.txt 7329_8.txt 957_10.txt 1182_8.txt 282_9.txt 507_10.txt 732_7.txt 9580_7.txt 11830_9.txt 2830_9.txt 5080_10.txt 7330_8.txt 9581_10.txt 11831_7.txt 2831_7.txt 5081_8.txt 7331_10.txt 9582_10.txt 11832_7.txt 2832_8.txt 5082_7.txt 7332_7.txt 9583_8.txt 11833_7.txt 2833_7.txt 5083_8.txt 7333_10.txt 9584_10.txt 11834_7.txt 2834_9.txt 5084_7.txt 7334_7.txt 9585_10.txt 11835_8.txt 2835_7.txt 5085_7.txt 7335_7.txt 9586_10.txt 11836_10.txt 2836_7.txt 5086_7.txt 7336_10.txt 9587_10.txt 11837_7.txt 2837_8.txt 5087_7.txt 7337_10.txt 9588_10.txt 11838_9.txt 2838_10.txt 5088_8.txt 7338_7.txt 9589_10.txt 11839_9.txt 2839_10.txt 5089_9.txt 7339_10.txt 958_9.txt 1183_8.txt 283_8.txt 508_9.txt 733_9.txt 9590_10.txt 11840_10.txt 2840_9.txt 5090_10.txt 7340_8.txt 9591_10.txt 11841_9.txt 2841_10.txt 5091_8.txt 7341_8.txt 9592_8.txt 11842_10.txt 2842_8.txt 5092_8.txt 7342_7.txt 9593_10.txt 11843_10.txt 2843_7.txt 5093_10.txt 7343_10.txt 9594_10.txt 11844_10.txt 2844_9.txt 5094_8.txt 7344_10.txt 9595_10.txt 11845_10.txt 2845_8.txt 5095_7.txt 7345_7.txt 9596_10.txt 11846_10.txt 2846_10.txt 5096_7.txt 7346_8.txt 9597_10.txt 11847_9.txt 2847_8.txt 5097_8.txt 7347_10.txt 9598_10.txt 11848_10.txt 2848_7.txt 5098_9.txt 7348_10.txt 9599_9.txt 11849_8.txt 2849_7.txt 5099_7.txt 7349_7.txt 959_7.txt 1184_7.txt 284_10.txt 509_10.txt 734_10.txt 95_10.txt 11850_10.txt 2850_9.txt 50_10.txt 7350_8.txt 9600_8.txt 11851_10.txt 2851_10.txt 5100_10.txt 7351_10.txt 9601_10.txt 11852_10.txt 2852_7.txt 5101_7.txt 7352_10.txt 9602_10.txt 11853_7.txt 2853_8.txt 5102_7.txt 7353_7.txt 9603_10.txt 11854_7.txt 2854_10.txt 5103_8.txt 7354_7.txt 9604_10.txt 11855_7.txt 2855_8.txt 5104_10.txt 7355_10.txt 9605_10.txt 11856_10.txt 2856_10.txt 5105_8.txt 7356_10.txt 9606_10.txt 11857_10.txt 2857_9.txt 5106_8.txt 7357_10.txt 9607_9.txt 11858_9.txt 2858_10.txt 5107_10.txt 7358_10.txt 9608_10.txt 11859_10.txt 2859_10.txt 5108_10.txt 7359_10.txt 9609_7.txt 1185_8.txt 285_10.txt 5109_10.txt 735_8.txt 960_7.txt 11860_10.txt 2860_9.txt 510_8.txt 7360_10.txt 9610_10.txt 11861_10.txt 2861_10.txt 5110_10.txt 7361_10.txt 9611_8.txt 11862_10.txt 2862_10.txt 5111_10.txt 7362_10.txt 9612_8.txt 11863_10.txt 2863_8.txt 5112_9.txt 7363_9.txt 9613_10.txt 11864_8.txt 2864_9.txt 5113_10.txt 7364_7.txt 9614_10.txt 11865_10.txt 2865_10.txt 5114_10.txt 7365_7.txt 9615_9.txt 11866_8.txt 2866_9.txt 5115_10.txt 7366_7.txt 9616_8.txt 11867_8.txt 2867_10.txt 5116_10.txt 7367_7.txt 9617_10.txt 11868_10.txt 2868_7.txt 5117_10.txt 7368_8.txt 9618_8.txt 11869_10.txt 2869_10.txt 5118_10.txt 7369_7.txt 9619_10.txt 1186_8.txt 286_10.txt 5119_10.txt 736_10.txt 961_9.txt 11870_10.txt 2870_10.txt 511_10.txt 7370_7.txt 9620_7.txt 11871_7.txt 2871_10.txt 5120_10.txt 7371_7.txt 9621_8.txt 11872_10.txt 2872_10.txt 5121_10.txt 7372_9.txt 9622_10.txt 11873_8.txt 2873_10.txt 5122_9.txt 7373_8.txt 9623_7.txt 11874_10.txt 2874_10.txt 5123_8.txt 7374_10.txt 9624_10.txt 11875_7.txt 2875_10.txt 5124_10.txt 7375_7.txt 9625_8.txt 11876_10.txt 2876_10.txt 5125_7.txt 7376_8.txt 9626_8.txt 11877_10.txt 2877_9.txt 5126_7.txt 7377_9.txt 9627_10.txt 11878_10.txt 2878_8.txt 5127_10.txt 7378_8.txt 9628_8.txt 11879_10.txt 2879_9.txt 5128_9.txt 7379_9.txt 9629_8.txt 1187_10.txt 287_9.txt 5129_9.txt 737_8.txt 962_8.txt 11880_10.txt 2880_8.txt 512_10.txt 7380_9.txt 9630_10.txt 11881_9.txt 2881_8.txt 5130_10.txt 7381_8.txt 9631_10.txt 11882_8.txt 2882_9.txt 5131_10.txt 7382_8.txt 9632_8.txt 11883_8.txt 2883_9.txt 5132_8.txt 7383_9.txt 9633_7.txt 11884_8.txt 2884_7.txt 5133_10.txt 7384_10.txt 9634_8.txt 11885_7.txt 2885_9.txt 5134_10.txt 7385_10.txt 9635_7.txt 11886_10.txt 2886_8.txt 5135_10.txt 7386_7.txt 9636_7.txt 11887_8.txt 2887_9.txt 5136_10.txt 7387_7.txt 9637_8.txt 11888_9.txt 2888_10.txt 5137_10.txt 7388_10.txt 9638_8.txt 11889_7.txt 2889_10.txt 5138_10.txt 7389_10.txt 9639_10.txt 1188_8.txt 288_10.txt 5139_9.txt 738_8.txt 963_8.txt 11890_7.txt 2890_7.txt 513_10.txt 7390_10.txt 9640_9.txt 11891_9.txt 2891_8.txt 5140_10.txt 7391_9.txt 9641_9.txt 11892_9.txt 2892_10.txt 5141_9.txt 7392_8.txt 9642_10.txt 11893_10.txt 2893_10.txt 5142_10.txt 7393_7.txt 9643_10.txt 11894_10.txt 2894_10.txt 5143_8.txt 7394_9.txt 9644_10.txt 11895_8.txt 2895_9.txt 5144_10.txt 7395_8.txt 9645_8.txt 11896_9.txt 2896_9.txt 5145_10.txt 7396_9.txt 9646_7.txt 11897_9.txt 2897_10.txt 5146_9.txt 7397_8.txt 9647_10.txt 11898_8.txt 2898_9.txt 5147_8.txt 7398_10.txt 9648_8.txt 11899_7.txt 2899_10.txt 5148_7.txt 7399_10.txt 9649_9.txt 1189_9.txt 289_10.txt 5149_8.txt 739_7.txt 964_8.txt 118_8.txt 28_10.txt 514_10.txt 73_7.txt 9650_10.txt 11900_8.txt 2900_9.txt 5150_7.txt 7400_9.txt 9651_9.txt 11901_9.txt 2901_7.txt 5151_8.txt 7401_9.txt 9652_7.txt 11902_9.txt 2902_9.txt 5152_9.txt 7402_8.txt 9653_9.txt 11903_10.txt 2903_9.txt 5153_8.txt 7403_8.txt 9654_10.txt 11904_10.txt 2904_9.txt 5154_8.txt 7404_9.txt 9655_10.txt 11905_8.txt 2905_7.txt 5155_9.txt 7405_10.txt 9656_10.txt 11906_7.txt 2906_9.txt 5156_9.txt 7406_9.txt 9657_10.txt 11907_7.txt 2907_7.txt 5157_7.txt 7407_10.txt 9658_7.txt 11908_8.txt 2908_8.txt 5158_10.txt 7408_8.txt 9659_8.txt 11909_7.txt 2909_10.txt 5159_10.txt 7409_10.txt 965_10.txt 1190_7.txt 290_9.txt 515_10.txt 740_7.txt 9660_9.txt 11910_10.txt 2910_7.txt 5160_8.txt 7410_7.txt 9661_8.txt 11911_10.txt 2911_10.txt 5161_8.txt 7411_10.txt 9662_10.txt 11912_10.txt 2912_9.txt 5162_9.txt 7412_9.txt 9663_10.txt 11913_10.txt 2913_7.txt 5163_7.txt 7413_9.txt 9664_10.txt 11914_7.txt 2914_8.txt 5164_10.txt 7414_8.txt 9665_10.txt 11915_10.txt 2915_7.txt 5165_9.txt 7415_8.txt 9666_10.txt 11916_7.txt 2916_9.txt 5166_10.txt 7416_9.txt 9667_9.txt 11917_8.txt 2917_7.txt 5167_7.txt 7417_10.txt 9668_9.txt 11918_10.txt 2918_9.txt 5168_10.txt 7418_8.txt 9669_9.txt 11919_7.txt 2919_10.txt 5169_7.txt 7419_10.txt 966_7.txt 1191_9.txt 291_10.txt 516_10.txt 741_7.txt 9670_9.txt 11920_9.txt 2920_8.txt 5170_9.txt 7420_7.txt 9671_10.txt 11921_10.txt 2921_10.txt 5171_10.txt 7421_7.txt 9672_7.txt 11922_10.txt 2922_10.txt 5172_9.txt 7422_10.txt 9673_7.txt 11923_10.txt 2923_10.txt 5173_9.txt 7423_9.txt 9674_9.txt 11924_10.txt 2924_10.txt 5174_10.txt 7424_10.txt 9675_8.txt 11925_7.txt 2925_10.txt 5175_10.txt 7425_9.txt 9676_10.txt 11926_10.txt 2926_8.txt 5176_10.txt 7426_10.txt 9677_9.txt 11927_10.txt 2927_10.txt 5177_8.txt 7427_9.txt 9678_8.txt 11928_8.txt 2928_10.txt 5178_10.txt 7428_10.txt 9679_10.txt 11929_9.txt 2929_10.txt 5179_7.txt 7429_9.txt 967_7.txt 1192_8.txt 292_10.txt 517_10.txt 742_9.txt 9680_8.txt 11930_10.txt 2930_10.txt 5180_9.txt 7430_10.txt 9681_9.txt 11931_9.txt 2931_10.txt 5181_10.txt 7431_10.txt 9682_10.txt 11932_7.txt 2932_10.txt 5182_8.txt 7432_10.txt 9683_9.txt 11933_8.txt 2933_10.txt 5183_9.txt 7433_10.txt 9684_10.txt 11934_8.txt 2934_10.txt 5184_9.txt 7434_7.txt 9685_10.txt 11935_7.txt 2935_10.txt 5185_10.txt 7435_9.txt 9686_7.txt 11936_7.txt 2936_10.txt 5186_10.txt 7436_10.txt 9687_9.txt 11937_8.txt 2937_10.txt 5187_7.txt 7437_10.txt 9688_9.txt 11938_7.txt 2938_10.txt 5188_8.txt 7438_10.txt 9689_9.txt 11939_10.txt 2939_10.txt 5189_10.txt 7439_9.txt 968_10.txt 1193_9.txt 293_7.txt 518_10.txt 743_7.txt 9690_10.txt 11940_7.txt 2940_10.txt 5190_8.txt 7440_7.txt 9691_10.txt 11941_8.txt 2941_10.txt 5191_8.txt 7441_7.txt 9692_10.txt 11942_8.txt 2942_10.txt 5192_7.txt 7442_7.txt 9693_8.txt 11943_7.txt 2943_10.txt 5193_9.txt 7443_7.txt 9694_9.txt 11944_10.txt 2944_10.txt 5194_10.txt 7444_10.txt 9695_10.txt 11945_9.txt 2945_10.txt 5195_7.txt 7445_7.txt 9696_8.txt 11946_9.txt 2946_10.txt 5196_9.txt 7446_8.txt 9697_8.txt 11947_9.txt 2947_10.txt 5197_10.txt 7447_8.txt 9698_10.txt 11948_10.txt 2948_10.txt 5198_7.txt 7448_8.txt 9699_10.txt 11949_8.txt 2949_10.txt 5199_7.txt 7449_10.txt 969_7.txt 1194_7.txt 294_10.txt 519_10.txt 744_7.txt 96_10.txt 11950_8.txt 2950_10.txt 51_10.txt 7450_9.txt 9700_10.txt 11951_8.txt 2951_8.txt 5200_10.txt 7451_9.txt 9701_10.txt 11952_8.txt 2952_8.txt 5201_10.txt 7452_10.txt 9702_10.txt 11953_8.txt 2953_8.txt 5202_8.txt 7453_10.txt 9703_9.txt 11954_9.txt 2954_10.txt 5203_7.txt 7454_10.txt 9704_10.txt 11955_7.txt 2955_10.txt 5204_8.txt 7455_8.txt 9705_10.txt 11956_9.txt 2956_8.txt 5205_7.txt 7456_10.txt 9706_10.txt 11957_8.txt 2957_10.txt 5206_7.txt 7457_9.txt 9707_10.txt 11958_9.txt 2958_10.txt 5207_7.txt 7458_10.txt 9708_10.txt 11959_8.txt 2959_10.txt 5208_7.txt 7459_10.txt 9709_10.txt 1195_8.txt 295_10.txt 5209_8.txt 745_10.txt 970_10.txt 11960_8.txt 2960_10.txt 520_8.txt 7460_10.txt 9710_10.txt 11961_8.txt 2961_10.txt 5210_10.txt 7461_10.txt 9711_9.txt 11962_7.txt 2962_7.txt 5211_8.txt 7462_10.txt 9712_10.txt 11963_10.txt 2963_8.txt 5212_7.txt 7463_10.txt 9713_10.txt 11964_7.txt 2964_8.txt 5213_8.txt 7464_10.txt 9714_10.txt 11965_8.txt 2965_9.txt 5214_7.txt 7465_10.txt 9715_7.txt 11966_10.txt 2966_10.txt 5215_10.txt 7466_10.txt 9716_10.txt 11967_8.txt 2967_9.txt 5216_8.txt 7467_10.txt 9717_8.txt 11968_10.txt 2968_10.txt 5217_8.txt 7468_10.txt 9718_7.txt 11969_10.txt 2969_10.txt 5218_7.txt 7469_10.txt 9719_7.txt 1196_8.txt 296_10.txt 5219_7.txt 746_10.txt 971_8.txt 11970_10.txt 2970_10.txt 521_10.txt 7470_10.txt 9720_8.txt 11971_10.txt 2971_8.txt 5220_8.txt 7471_10.txt 9721_7.txt 11972_10.txt 2972_7.txt 5221_7.txt 7472_9.txt 9722_7.txt 11973_10.txt 2973_9.txt 5222_8.txt 7473_9.txt 9723_9.txt 11974_10.txt 2974_8.txt 5223_7.txt 7474_9.txt 9724_7.txt 11975_7.txt 2975_7.txt 5224_10.txt 7475_7.txt 9725_7.txt 11976_10.txt 2976_10.txt 5225_9.txt 7476_8.txt 9726_7.txt 11977_8.txt 2977_10.txt 5226_10.txt 7477_9.txt 9727_7.txt 11978_10.txt 2978_10.txt 5227_10.txt 7478_10.txt 9728_7.txt 11979_10.txt 2979_10.txt 5228_8.txt 7479_8.txt 9729_9.txt 1197_10.txt 297_10.txt 5229_10.txt 747_10.txt 972_9.txt 11980_10.txt 2980_7.txt 522_8.txt 7480_10.txt 9730_8.txt 11981_9.txt 2981_10.txt 5230_10.txt 7481_10.txt 9731_8.txt 11982_7.txt 2982_8.txt 5231_10.txt 7482_10.txt 9732_10.txt 11983_10.txt 2983_7.txt 5232_8.txt 7483_10.txt 9733_8.txt 11984_10.txt 2984_8.txt 5233_10.txt 7484_10.txt 9734_7.txt 11985_10.txt 2985_8.txt 5234_10.txt 7485_8.txt 9735_8.txt 11986_8.txt 2986_8.txt 5235_8.txt 7486_8.txt 9736_8.txt 11987_7.txt 2987_10.txt 5236_10.txt 7487_8.txt 9737_9.txt 11988_8.txt 2988_7.txt 5237_10.txt 7488_7.txt 9738_8.txt 11989_10.txt 2989_10.txt 5238_7.txt 7489_7.txt 9739_7.txt 1198_10.txt 298_8.txt 5239_7.txt 748_9.txt 973_9.txt 11990_10.txt 2990_10.txt 523_10.txt 7490_8.txt 9740_7.txt 11991_10.txt 2991_7.txt 5240_7.txt 7491_7.txt 9741_8.txt 11992_10.txt 2992_7.txt 5241_7.txt 7492_7.txt 9742_10.txt 11993_9.txt 2993_10.txt 5242_7.txt 7493_7.txt 9743_8.txt 11994_10.txt 2994_8.txt 5243_9.txt 7494_10.txt 9744_7.txt 11995_10.txt 2995_7.txt 5244_7.txt 7495_7.txt 9745_7.txt 11996_10.txt 2996_7.txt 5245_8.txt 7496_8.txt 9746_9.txt 11997_10.txt 2997_7.txt 5246_10.txt 7497_10.txt 9747_10.txt 11998_9.txt 2998_7.txt 5247_10.txt 7498_10.txt 9748_10.txt 11999_10.txt 2999_7.txt 5248_8.txt 7499_7.txt 9749_9.txt 1199_10.txt 299_10.txt 5249_9.txt 749_10.txt 974_10.txt 119_10.txt 29_10.txt 524_10.txt 74_8.txt 9750_10.txt 11_9.txt 2_9.txt 5250_10.txt 7500_8.txt 9751_7.txt 12000_8.txt 3000_8.txt 5251_7.txt 7501_8.txt 9752_10.txt 12001_8.txt 3001_10.txt 5252_9.txt 7502_7.txt 9753_10.txt 12002_10.txt 3002_8.txt 5253_10.txt 7503_7.txt 9754_10.txt 12003_10.txt 3003_9.txt 5254_9.txt 7504_7.txt 9755_7.txt 12004_10.txt 3004_10.txt 5255_10.txt 7505_9.txt 9756_10.txt 12005_10.txt 3005_8.txt 5256_10.txt 7506_7.txt 9757_8.txt 12006_10.txt 3006_9.txt 5257_10.txt 7507_8.txt 9758_10.txt 12007_10.txt 3007_7.txt 5258_10.txt 7508_7.txt 9759_10.txt 12008_8.txt 3008_7.txt 5259_9.txt 7509_8.txt 975_9.txt 12009_10.txt 3009_8.txt 525_10.txt 750_8.txt 9760_8.txt 1200_10.txt 300_9.txt 5260_10.txt 7510_8.txt 9761_10.txt 12010_10.txt 3010_8.txt 5261_10.txt 7511_9.txt 9762_8.txt 12011_10.txt 3011_7.txt 5262_7.txt 7512_8.txt 9763_8.txt 12012_10.txt 3012_8.txt 5263_8.txt 7513_9.txt 9764_9.txt 12013_10.txt 3013_8.txt 5264_7.txt 7514_8.txt 9765_10.txt 12014_10.txt 3014_8.txt 5265_9.txt 7515_8.txt 9766_9.txt 12015_10.txt 3015_9.txt 5266_10.txt 7516_7.txt 9767_7.txt 12016_7.txt 3016_10.txt 5267_7.txt 7517_7.txt 9768_10.txt 12017_7.txt 3017_7.txt 5268_10.txt 7518_9.txt 9769_8.txt 12018_7.txt 3018_10.txt 5269_10.txt 7519_9.txt 976_8.txt 12019_8.txt 3019_8.txt 526_10.txt 751_9.txt 9770_10.txt 1201_8.txt 301_10.txt 5270_7.txt 7520_8.txt 9771_10.txt 12020_7.txt 3020_9.txt 5271_7.txt 7521_7.txt 9772_10.txt 12021_9.txt 3021_8.txt 5272_7.txt 7522_8.txt 9773_7.txt 12022_10.txt 3022_7.txt 5273_7.txt 7523_7.txt 9774_8.txt 12023_9.txt 3023_7.txt 5274_7.txt 7524_9.txt 9775_8.txt 12024_7.txt 3024_9.txt 5275_10.txt 7525_7.txt 9776_8.txt 12025_7.txt 3025_7.txt 5276_10.txt 7526_8.txt 9777_8.txt 12026_8.txt 3026_7.txt 5277_10.txt 7527_10.txt 9778_8.txt 12027_10.txt 3027_8.txt 5278_7.txt 7528_10.txt 9779_9.txt 12028_10.txt 3028_7.txt 5279_8.txt 7529_10.txt 977_8.txt 12029_10.txt 3029_8.txt 527_9.txt 752_7.txt 9780_10.txt 1202_9.txt 302_10.txt 5280_8.txt 7530_9.txt 9781_8.txt 12030_9.txt 3030_9.txt 5281_10.txt 7531_8.txt 9782_10.txt 12031_7.txt 3031_8.txt 5282_10.txt 7532_7.txt 9783_9.txt 12032_8.txt 3032_7.txt 5283_7.txt 7533_10.txt 9784_8.txt 12033_8.txt 3033_8.txt 5284_9.txt 7534_10.txt 9785_7.txt 12034_10.txt 3034_7.txt 5285_7.txt 7535_10.txt 9786_8.txt 12035_8.txt 3035_9.txt 5286_10.txt 7536_10.txt 9787_7.txt 12036_7.txt 3036_9.txt 5287_8.txt 7537_10.txt 9788_9.txt 12037_10.txt 3037_7.txt 5288_7.txt 7538_7.txt 9789_10.txt 12038_9.txt 3038_8.txt 5289_10.txt 7539_10.txt 978_9.txt 12039_10.txt 3039_8.txt 528_9.txt 753_8.txt 9790_9.txt 1203_8.txt 303_10.txt 5290_10.txt 7540_9.txt 9791_9.txt 12040_7.txt 3040_8.txt 5291_8.txt 7541_10.txt 9792_8.txt 12041_9.txt 3041_7.txt 5292_7.txt 7542_10.txt 9793_7.txt 12042_10.txt 3042_8.txt 5293_8.txt 7543_8.txt 9794_7.txt 12043_10.txt 3043_8.txt 5294_8.txt 7544_7.txt 9795_7.txt 12044_7.txt 3044_7.txt 5295_8.txt 7545_8.txt 9796_9.txt 12045_10.txt 3045_10.txt 5296_10.txt 7546_7.txt 9797_7.txt 12046_10.txt 3046_10.txt 5297_8.txt 7547_10.txt 9798_7.txt 12047_10.txt 3047_7.txt 5298_8.txt 7548_8.txt 9799_7.txt 12048_10.txt 3048_10.txt 5299_8.txt 7549_7.txt 979_8.txt 12049_10.txt 3049_9.txt 529_10.txt 754_9.txt 97_9.txt 1204_10.txt 304_10.txt 52_10.txt 7550_9.txt 9800_9.txt 12050_10.txt 3050_9.txt 5300_8.txt 7551_10.txt 9801_10.txt 12051_10.txt 3051_9.txt 5301_8.txt 7552_10.txt 9802_10.txt 12052_9.txt 3052_10.txt 5302_9.txt 7553_10.txt 9803_7.txt 12053_9.txt 3053_8.txt 5303_10.txt 7554_10.txt 9804_10.txt 12054_10.txt 3054_7.txt 5304_10.txt 7555_7.txt 9805_10.txt 12055_10.txt 3055_9.txt 5305_7.txt 7556_8.txt 9806_8.txt 12056_10.txt 3056_8.txt 5306_7.txt 7557_9.txt 9807_10.txt 12057_10.txt 3057_8.txt 5307_7.txt 7558_8.txt 9808_9.txt 12058_9.txt 3058_9.txt 5308_10.txt 7559_10.txt 9809_7.txt 12059_9.txt 3059_8.txt 5309_7.txt 755_10.txt 980_7.txt 1205_8.txt 305_8.txt 530_10.txt 7560_9.txt 9810_7.txt 12060_9.txt 3060_10.txt 5310_10.txt 7561_9.txt 9811_7.txt 12061_10.txt 3061_10.txt 5311_7.txt 7562_10.txt 9812_7.txt 12062_10.txt 3062_10.txt 5312_7.txt 7563_10.txt 9813_8.txt 12063_10.txt 3063_10.txt 5313_7.txt 7564_10.txt 9814_10.txt 12064_10.txt 3064_8.txt 5314_10.txt 7565_10.txt 9815_7.txt 12065_7.txt 3065_10.txt 5315_9.txt 7566_10.txt 9816_10.txt 12066_8.txt 3066_10.txt 5316_7.txt 7567_10.txt 9817_8.txt 12067_9.txt 3067_9.txt 5317_7.txt 7568_10.txt 9818_7.txt 12068_9.txt 3068_9.txt 5318_7.txt 7569_10.txt 9819_10.txt 12069_10.txt 3069_9.txt 5319_8.txt 756_10.txt 981_7.txt 1206_10.txt 306_10.txt 531_10.txt 7570_10.txt 9820_10.txt 12070_7.txt 3070_7.txt 5320_10.txt 7571_8.txt 9821_8.txt 12071_8.txt 3071_9.txt 5321_10.txt 7572_10.txt 9822_10.txt 12072_10.txt 3072_8.txt 5322_10.txt 7573_9.txt 9823_7.txt 12073_9.txt 3073_8.txt 5323_7.txt 7574_10.txt 9824_10.txt 12074_10.txt 3074_8.txt 5324_10.txt 7575_10.txt 9825_10.txt 12075_7.txt 3075_9.txt 5325_7.txt 7576_10.txt 9826_10.txt 12076_8.txt 3076_8.txt 5326_10.txt 7577_8.txt 9827_10.txt 12077_8.txt 3077_10.txt 5327_10.txt 7578_8.txt 9828_10.txt 12078_8.txt 3078_10.txt 5328_8.txt 7579_8.txt 9829_7.txt 12079_7.txt 3079_10.txt 5329_9.txt 757_8.txt 982_8.txt 1207_10.txt 307_8.txt 532_9.txt 7580_7.txt 9830_7.txt 12080_8.txt 3080_10.txt 5330_7.txt 7581_8.txt 9831_8.txt 12081_8.txt 3081_9.txt 5331_10.txt 7582_8.txt 9832_10.txt 12082_8.txt 3082_10.txt 5332_7.txt 7583_7.txt 9833_8.txt 12083_8.txt 3083_10.txt 5333_10.txt 7584_7.txt 9834_10.txt 12084_8.txt 3084_10.txt 5334_7.txt 7585_8.txt 9835_9.txt 12085_8.txt 3085_10.txt 5335_10.txt 7586_10.txt 9836_9.txt 12086_10.txt 3086_10.txt 5336_8.txt 7587_9.txt 9837_7.txt 12087_10.txt 3087_10.txt 5337_9.txt 7588_8.txt 9838_7.txt 12088_9.txt 3088_8.txt 5338_10.txt 7589_8.txt 9839_7.txt 12089_10.txt 3089_9.txt 5339_8.txt 758_9.txt 983_7.txt 1208_9.txt 308_8.txt 533_10.txt 7590_10.txt 9840_10.txt 12090_9.txt 3090_10.txt 5340_8.txt 7591_8.txt 9841_7.txt 12091_8.txt 3091_10.txt 5341_10.txt 7592_9.txt 9842_7.txt 12092_7.txt 3092_10.txt 5342_10.txt 7593_7.txt 9843_7.txt 12093_8.txt 3093_10.txt 5343_8.txt 7594_8.txt 9844_8.txt 12094_7.txt 3094_10.txt 5344_7.txt 7595_10.txt 9845_8.txt 12095_8.txt 3095_10.txt 5345_9.txt 7596_9.txt 9846_7.txt 12096_8.txt 3096_10.txt 5346_7.txt 7597_9.txt 9847_7.txt 12097_7.txt 3097_9.txt 5347_7.txt 7598_10.txt 9848_7.txt 12098_8.txt 3098_10.txt 5348_8.txt 7599_10.txt 9849_9.txt 12099_9.txt 3099_10.txt 5349_7.txt 759_10.txt 984_7.txt 1209_10.txt 309_9.txt 534_10.txt 75_8.txt 9850_7.txt 120_8.txt 30_7.txt 5350_9.txt 7600_10.txt 9851_7.txt 12100_8.txt 3100_10.txt 5351_7.txt 7601_10.txt 9852_7.txt 12101_7.txt 3101_9.txt 5352_7.txt 7602_10.txt 9853_8.txt 12102_8.txt 3102_9.txt 5353_7.txt 7603_10.txt 9854_7.txt 12103_7.txt 3103_7.txt 5354_10.txt 7604_9.txt 9855_7.txt 12104_7.txt 3104_10.txt 5355_8.txt 7605_8.txt 9856_8.txt 12105_10.txt 3105_8.txt 5356_8.txt 7606_9.txt 9857_7.txt 12106_10.txt 3106_8.txt 5357_8.txt 7607_8.txt 9858_7.txt 12107_10.txt 3107_8.txt 5358_10.txt 7608_10.txt 9859_10.txt 12108_9.txt 3108_8.txt 5359_10.txt 7609_10.txt 985_7.txt 12109_10.txt 3109_10.txt 535_10.txt 760_7.txt 9860_7.txt 1210_8.txt 310_7.txt 5360_10.txt 7610_9.txt 9861_10.txt 12110_8.txt 3110_8.txt 5361_10.txt 7611_9.txt 9862_9.txt 12111_7.txt 3111_7.txt 5362_8.txt 7612_9.txt 9863_10.txt 12112_10.txt 3112_7.txt 5363_9.txt 7613_9.txt 9864_8.txt 12113_8.txt 3113_9.txt 5364_10.txt 7614_10.txt 9865_8.txt 12114_7.txt 3114_9.txt 5365_10.txt 7615_10.txt 9866_7.txt 12115_10.txt 3115_8.txt 5366_10.txt 7616_10.txt 9867_10.txt 12116_9.txt 3116_10.txt 5367_9.txt 7617_9.txt 9868_9.txt 12117_7.txt 3117_8.txt 5368_9.txt 7618_10.txt 9869_10.txt 12118_7.txt 3118_9.txt 5369_9.txt 7619_10.txt 986_10.txt 12119_7.txt 3119_8.txt 536_10.txt 761_10.txt 9870_9.txt 1211_7.txt 311_9.txt 5370_7.txt 7620_10.txt 9871_7.txt 12120_10.txt 3120_8.txt 5371_8.txt 7621_10.txt 9872_10.txt 12121_7.txt 3121_10.txt 5372_7.txt 7622_10.txt 9873_7.txt 12122_7.txt 3122_8.txt 5373_7.txt 7623_9.txt 9874_8.txt 12123_8.txt 3123_10.txt 5374_8.txt 7624_7.txt 9875_7.txt 12124_8.txt 3124_9.txt 5375_8.txt 7625_10.txt 9876_8.txt 12125_8.txt 3125_10.txt 5376_7.txt 7626_9.txt 9877_8.txt 12126_8.txt 3126_10.txt 5377_7.txt 7627_10.txt 9878_7.txt 12127_8.txt 3127_9.txt 5378_7.txt 7628_8.txt 9879_8.txt 12128_8.txt 3128_9.txt 5379_7.txt 7629_10.txt 987_8.txt 12129_7.txt 3129_10.txt 537_10.txt 762_9.txt 9880_10.txt 1212_8.txt 312_10.txt 5380_7.txt 7630_8.txt 9881_8.txt 12130_7.txt 3130_9.txt 5381_7.txt 7631_10.txt 9882_8.txt 12131_7.txt 3131_7.txt 5382_7.txt 7632_10.txt 9883_8.txt 12132_10.txt 3132_9.txt 5383_10.txt 7633_8.txt 9884_10.txt 12133_7.txt 3133_9.txt 5384_8.txt 7634_7.txt 9885_10.txt 12134_8.txt 3134_8.txt 5385_7.txt 7635_10.txt 9886_10.txt 12135_10.txt 3135_9.txt 5386_7.txt 7636_8.txt 9887_9.txt 12136_9.txt 3136_8.txt 5387_8.txt 7637_7.txt 9888_10.txt 12137_10.txt 3137_8.txt 5388_8.txt 7638_7.txt 9889_9.txt 12138_7.txt 3138_9.txt 5389_7.txt 7639_8.txt 988_8.txt 12139_10.txt 3139_10.txt 538_10.txt 763_10.txt 9890_8.txt 1213_10.txt 313_10.txt 5390_9.txt 7640_10.txt 9891_10.txt 12140_8.txt 3140_7.txt 5391_9.txt 7641_10.txt 9892_8.txt 12141_8.txt 3141_10.txt 5392_10.txt 7642_9.txt 9893_7.txt 12142_9.txt 3142_8.txt 5393_10.txt 7643_7.txt 9894_8.txt 12143_8.txt 3143_10.txt 5394_10.txt 7644_7.txt 9895_8.txt 12144_8.txt 3144_10.txt 5395_10.txt 7645_7.txt 9896_8.txt 12145_10.txt 3145_10.txt 5396_9.txt 7646_8.txt 9897_10.txt 12146_7.txt 3146_10.txt 5397_9.txt 7647_7.txt 9898_10.txt 12147_10.txt 3147_10.txt 5398_8.txt 7648_7.txt 9899_7.txt 12148_10.txt 3148_9.txt 5399_8.txt 7649_9.txt 989_9.txt 12149_10.txt 3149_8.txt 539_10.txt 764_10.txt 98_10.txt 1214_10.txt 314_10.txt 53_10.txt 7650_9.txt 9900_10.txt 12150_7.txt 3150_10.txt 5400_10.txt 7651_8.txt 9901_8.txt 12151_10.txt 3151_9.txt 5401_9.txt 7652_8.txt 9902_7.txt 12152_9.txt 3152_9.txt 5402_9.txt 7653_10.txt 9903_9.txt 12153_10.txt 3153_10.txt 5403_10.txt 7654_10.txt 9904_8.txt 12154_7.txt 3154_10.txt 5404_10.txt 7655_10.txt 9905_10.txt 12155_7.txt 3155_10.txt 5405_10.txt 7656_10.txt 9906_10.txt 12156_8.txt 3156_9.txt 5406_7.txt 7657_10.txt 9907_7.txt 12157_9.txt 3157_10.txt 5407_7.txt 7658_10.txt 9908_8.txt 12158_7.txt 3158_8.txt 5408_7.txt 7659_10.txt 9909_7.txt 12159_7.txt 3159_9.txt 5409_10.txt 765_9.txt 990_9.txt 1215_10.txt 315_10.txt 540_8.txt 7660_10.txt 9910_10.txt 12160_10.txt 3160_7.txt 5410_7.txt 7661_10.txt 9911_10.txt 12161_7.txt 3161_10.txt 5411_10.txt 7662_10.txt 9912_10.txt 12162_8.txt 3162_8.txt 5412_9.txt 7663_7.txt 9913_10.txt 12163_8.txt 3163_10.txt 5413_10.txt 7664_10.txt 9914_7.txt 12164_9.txt 3164_10.txt 5414_10.txt 7665_7.txt 9915_8.txt 12165_10.txt 3165_10.txt 5415_10.txt 7666_7.txt 9916_7.txt 12166_10.txt 3166_9.txt 5416_9.txt 7667_7.txt 9917_10.txt 12167_10.txt 3167_7.txt 5417_10.txt 7668_10.txt 9918_10.txt 12168_10.txt 3168_7.txt 5418_10.txt 7669_8.txt 9919_9.txt 12169_7.txt 3169_8.txt 5419_10.txt 766_10.txt 991_7.txt 1216_10.txt 316_10.txt 541_7.txt 7670_8.txt 9920_7.txt 12170_8.txt 3170_9.txt 5420_7.txt 7671_10.txt 9921_8.txt 12171_7.txt 3171_9.txt 5421_7.txt 7672_10.txt 9922_10.txt 12172_7.txt 3172_9.txt 5422_9.txt 7673_7.txt 9923_7.txt 12173_8.txt 3173_8.txt 5423_9.txt 7674_10.txt 9924_9.txt 12174_7.txt 3174_9.txt 5424_10.txt 7675_8.txt 9925_7.txt 12175_7.txt 3175_8.txt 5425_10.txt 7676_9.txt 9926_7.txt 12176_8.txt 3176_9.txt 5426_10.txt 7677_10.txt 9927_9.txt 12177_7.txt 3177_9.txt 5427_10.txt 7678_10.txt 9928_7.txt 12178_7.txt 3178_8.txt 5428_10.txt 7679_10.txt 9929_7.txt 12179_8.txt 3179_8.txt 5429_10.txt 767_9.txt 992_7.txt 1217_10.txt 317_10.txt 542_9.txt 7680_8.txt 9930_8.txt 12180_10.txt 3180_8.txt 5430_10.txt 7681_9.txt 9931_9.txt 12181_8.txt 3181_10.txt 5431_10.txt 7682_8.txt 9932_8.txt 12182_8.txt 3182_8.txt 5432_7.txt 7683_10.txt 9933_8.txt 12183_10.txt 3183_9.txt 5433_10.txt 7684_7.txt 9934_8.txt 12184_10.txt 3184_8.txt 5434_9.txt 7685_10.txt 9935_7.txt 12185_9.txt 3185_10.txt 5435_10.txt 7686_10.txt 9936_10.txt 12186_9.txt 3186_8.txt 5436_7.txt 7687_8.txt 9937_10.txt 12187_9.txt 3187_7.txt 5437_10.txt 7688_10.txt 9938_9.txt 12188_10.txt 3188_8.txt 5438_10.txt 7689_10.txt 9939_10.txt 12189_7.txt 3189_8.txt 5439_10.txt 768_9.txt 993_8.txt 1218_8.txt 318_10.txt 543_10.txt 7690_9.txt 9940_9.txt 12190_7.txt 3190_7.txt 5440_10.txt 7691_9.txt 9941_7.txt 12191_7.txt 3191_8.txt 5441_10.txt 7692_7.txt 9942_7.txt 12192_7.txt 3192_10.txt 5442_8.txt 7693_9.txt 9943_7.txt 12193_10.txt 3193_9.txt 5443_8.txt 7694_10.txt 9944_9.txt 12194_10.txt 3194_9.txt 5444_8.txt 7695_9.txt 9945_8.txt 12195_8.txt 3195_10.txt 5445_10.txt 7696_8.txt 9946_9.txt 12196_10.txt 3196_10.txt 5446_9.txt 7697_9.txt 9947_10.txt 12197_9.txt 3197_10.txt 5447_10.txt 7698_10.txt 9948_8.txt 12198_10.txt 3198_10.txt 5448_10.txt 7699_10.txt 9949_9.txt 12199_9.txt 3199_10.txt 5449_8.txt 769_8.txt 994_7.txt 1219_10.txt 319_9.txt 544_8.txt 76_7.txt 9950_8.txt 121_10.txt 31_8.txt 5450_10.txt 7700_10.txt 9951_7.txt 12200_8.txt 3200_10.txt 5451_8.txt 7701_10.txt 9952_8.txt 12201_8.txt 3201_10.txt 5452_8.txt 7702_10.txt 9953_10.txt 12202_9.txt 3202_10.txt 5453_8.txt 7703_8.txt 9954_8.txt 12203_10.txt 3203_10.txt 5454_7.txt 7704_10.txt 9955_9.txt 12204_8.txt 3204_10.txt 5455_7.txt 7705_7.txt 9956_9.txt 12205_8.txt 3205_8.txt 5456_10.txt 7706_7.txt 9957_7.txt 12206_9.txt 3206_8.txt 5457_8.txt 7707_9.txt 9958_7.txt 12207_8.txt 3207_7.txt 5458_10.txt 7708_10.txt 9959_7.txt 12208_8.txt 3208_7.txt 5459_8.txt 7709_7.txt 995_9.txt 12209_7.txt 3209_8.txt 545_10.txt 770_10.txt 9960_7.txt 1220_9.txt 320_8.txt 5460_8.txt 7710_8.txt 9961_9.txt 12210_9.txt 3210_8.txt 5461_7.txt 7711_10.txt 9962_9.txt 12211_10.txt 3211_7.txt 5462_8.txt 7712_10.txt 9963_10.txt 12212_10.txt 3212_7.txt 5463_9.txt 7713_10.txt 9964_10.txt 12213_10.txt 3213_10.txt 5464_7.txt 7714_10.txt 9965_10.txt 12214_10.txt 3214_10.txt 5465_8.txt 7715_8.txt 9966_10.txt 12215_10.txt 3215_10.txt 5466_8.txt 7716_9.txt 9967_10.txt 12216_8.txt 3216_8.txt 5467_9.txt 7717_10.txt 9968_9.txt 12217_10.txt 3217_8.txt 5468_7.txt 7718_10.txt 9969_10.txt 12218_7.txt 3218_9.txt 5469_7.txt 7719_9.txt 996_9.txt 12219_10.txt 3219_10.txt 546_10.txt 771_7.txt 9970_10.txt 1221_7.txt 321_10.txt 5470_10.txt 7720_8.txt 9971_10.txt 12220_8.txt 3220_10.txt 5471_8.txt 7721_7.txt 9972_10.txt 12221_8.txt 3221_10.txt 5472_7.txt 7722_10.txt 9973_8.txt 12222_10.txt 3222_7.txt 5473_8.txt 7723_10.txt 9974_8.txt 12223_10.txt 3223_8.txt 5474_9.txt 7724_10.txt 9975_10.txt 12224_9.txt 3224_7.txt 5475_8.txt 7725_10.txt 9976_7.txt 12225_7.txt 3225_10.txt 5476_8.txt 7726_10.txt 9977_7.txt 12226_8.txt 3226_10.txt 5477_7.txt 7727_9.txt 9978_8.txt 12227_7.txt 3227_7.txt 5478_9.txt 7728_7.txt 9979_7.txt 12228_8.txt 3228_7.txt 5479_10.txt 7729_7.txt 997_7.txt 12229_9.txt 3229_7.txt 547_10.txt 772_10.txt 9980_8.txt 1222_10.txt 322_10.txt 5480_10.txt 7730_7.txt 9981_7.txt 12230_9.txt 3230_8.txt 5481_10.txt 7731_9.txt 9982_9.txt 12231_10.txt 3231_10.txt 5482_10.txt 7732_8.txt 9983_7.txt 12232_8.txt 3232_9.txt 5483_10.txt 7733_9.txt 9984_9.txt 12233_8.txt 3233_10.txt 5484_10.txt 7734_10.txt 9985_9.txt 12234_8.txt 3234_9.txt 5485_10.txt 7735_10.txt 9986_9.txt 12235_7.txt 3235_9.txt 5486_10.txt 7736_10.txt 9987_9.txt 12236_10.txt 3236_7.txt 5487_10.txt 7737_10.txt 9988_8.txt 12237_10.txt 3237_8.txt 5488_10.txt 7738_8.txt 9989_9.txt 12238_8.txt 3238_9.txt 5489_10.txt 7739_10.txt 998_7.txt 12239_10.txt 3239_10.txt 548_7.txt 773_7.txt 9990_8.txt 1223_7.txt 323_10.txt 5490_9.txt 7740_10.txt 9991_10.txt 12240_8.txt 3240_10.txt 5491_10.txt 7741_9.txt 9992_10.txt 12241_10.txt 3241_8.txt 5492_10.txt 7742_8.txt 9993_10.txt 12242_10.txt 3242_8.txt 5493_10.txt 7743_8.txt 9994_10.txt 12243_8.txt 3243_8.txt 5494_10.txt 7744_10.txt 9995_10.txt 12244_7.txt 3244_10.txt 5495_8.txt 7745_8.txt 9996_9.txt 12245_10.txt 3245_10.txt 5496_9.txt 7746_10.txt 9997_7.txt 12246_9.txt 3246_9.txt 5497_9.txt 7747_10.txt 9998_9.txt 12247_8.txt 3247_10.txt 5498_7.txt 7748_9.txt 9999_8.txt 12248_7.txt 3248_10.txt 5499_10.txt 7749_8.txt 999_10.txt 12249_8.txt 3249_9.txt 549_9.txt 774_8.txt 99_8.txt 1224_9.txt 324_8.txt 54_10.txt 7750_8.txt 9_7.txt cat 0_9.txt Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't! # pip install pyprind import pyprind # module for Python Progress Indicator import pandas as pd import os labels = {'pos':1, 'neg':0} # 1 = positive and 0 = negative pbar = pyprind.ProgBar(50000) df = pd.DataFrame() for s in ('test', 'train'): for l in ('pos', 'neg'): path ='data/aclImdb/%s/%s' % (s, l) for file in os.listdir(path): with open(os.path.join(path, file), 'r') as infile: txt = infile.read() df = df.append([[txt, labels[l]]], ignore_index=True) pbar.update() df.columns = ['review', 'sentiment'] 0% 100% [##############################] | ETA: 00:00:00 Total time elapsed: 00:01:41 df.head(3) review sentiment 0 I went and saw this movie last night after bei... 1 1 Actor turned director Bill Paxton follows up h... 1 2 As a recreational golfer with some knowledge o... 1 Shuffling the DataFrame: import numpy as np np.random.seed(0) df = df.reindex(np.random.permutation(df.index)) df.head(3) review sentiment 11841 In 1974, the teenager Martha Moxley (Maggie Gr... 1 19602 OK... so... I really like Kris Kristofferson a... 0 45519 ***SPOILER*** Do not read this, if you think a... 0 Optional: Saving the assembled data as CSV file: df.to_csv('./data/movie_data.csv', index=False, encoding='utf-8') import pandas as pd df = pd.read_csv('./data/movie_data.csv', encoding='utf-8') df.head(3) review sentiment 0 In 1974, the teenager Martha Moxley (Maggie Gr... 1 1 OK... so... I really like Kris Kristofferson a... 0 2 ***SPOILER*** Do not read this, if you think a... 0","title":"Obtaining the IMDb movie review dataset"},{"location":"w7-text-mining/7w/#text-feature-extraction","text":"[ back to top ]","title":"Text feature extraction"},{"location":"w7-text-mining/7w/#bag-of-words-model","text":"[ back to top ] Free text with variables length is very far from the fixed length numeric representation that we need to do machine learning with scikit-learn. However, there is an easy and effective way to go from text data to a numeric representation using the so-called bag-of-words model , which provides a data structure that is compatible with the machine learning aglorithms in scikit-learn. import numpy as np from sklearn.feature_extraction.text import CountVectorizer count = CountVectorizer() docs = np.array([ 'The sun is shining', 'The weather is sweet', 'The sun is shining, the weather is sweet, and one and one is two']) bag = count.fit_transform(docs) count.vocabulary_ {u'and': 0, u'is': 1, u'one': 2, u'shining': 3, u'sun': 4, u'sweet': 5, u'the': 6, u'two': 7, u'weather': 8} count.get_feature_names() [u'and', u'is', u'one', u'shining', u'sun', u'sweet', u'the', u'two', u'weather'] As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary, which maps the unique words that are mapped to integer indices. Next let us print the feature vectors that we just created: bag.toarray() # \u6bcf\u884c\u5bf9\u5e94\u7740\u4e00\u4e2a document\uff0c \u6bcf\u5217\u5bf9\u5e94\u4e00\u4e2a word\uff0c \u503c\u662f word \u5bf9\u5e94\u7684\u8ba1\u6570 array([[0, 1, 0, 1, 1, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 1], [2, 3, 2, 1, 1, 1, 2, 1, 1]]) count.inverse_transform(bag) [array([u'the', u'sun', u'is', u'shining'], dtype=' U7'), array([u'the', u'is', u'weather', u'sweet'], dtype=' U7'), array([u'the', u'sun', u'is', u'shining', u'weather', u'sweet', u'and', u'one', u'two'], dtype=' U7')]","title":"Bag-of-words model"},{"location":"w7-text-mining/7w/#bigrams-and-n-grams","text":"[ back to top ] In last section, we used the so-called 1-gram (unigram) tokenization: Each token represents a single element with regard to the splittling criterion. Entirely discarding word order is not always a good idea, as composite phrases often have specific meaning, and modifiers like \"not\" can invert the meaning of words. A simple way to include some word order are n-grams, which don't only look at a single token, but at all pairs of neighborhing tokens. For example, in 2-gram (bigram) tokenization, we would group words together with an overlap of one word; in 3-gram (trigram) splits we would create an overlap two words, and so forth: original text: \"this is how you get ants\" 1-gram: \"this\", \"is\", \"how\", \"you\", \"get\", \"ants\" 2-gram: \"this is\", \"is how\", \"how you\", \"you get\", \"get ants\" 3-gram: \"this is how\", \"is how you\", \"how you get\", \"you get ants\" Which \"n\" we choose for \"n-gram\" tokenization to obtain the optimal performance in our predictive model depends on the learning algorithm, dataset, and task. Or in other words, we have consider \"n\" in \"n-grams\" as a tuning parameters. The CountVectorizer class in scikit-learn allows us to use different n-gram models via its ngram_range parameter. While a 1-gram representation is used by default, we could switch to a 2-gram representation by initializing a new CountVectorizer instance with ngram_range=(2,2) bigram_vectorizer = CountVectorizer(ngram_range=(2,2)) bigram_vectorizer.fit_transform(docs).toarray() array([[0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) bigram_vectorizer.vocabulary_ {u'and one': 0, u'is shining': 1, u'is sweet': 2, u'is two': 3, u'one and': 4, u'one is': 5, u'shining the': 6, u'sun is': 7, u'sweet and': 8, u'the sun': 9, u'the weather': 10, u'weather is': 11}","title":"Bigrams and N-Grams"},{"location":"w7-text-mining/7w/#character-n-grams","text":"[ back to top ] Sometimes it is also helpful not only to look at words, but to consider single characters instead. That is particularly useful if we have very noisy data and want to identify the language, or if we want to predict something about a single word. We can simply look at characters instead of words by setting analyzer=\"char\" . X = ['Some say the world will end in fire,', 'Some say in ice.'] char_vectorizer = CountVectorizer(analyzer= char ) char_vectorizer.fit(X) CountVectorizer(analyzer='char', binary=False, decode_error=u'strict', dtype= type 'numpy.int64' , encoding=u'utf-8', input=u'content', lowercase=True, max_df=1.0, max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None, stop_words=None, strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None) print(char_vectorizer.get_feature_names()) [u' ', u',', u'.', u'a', u'c', u'd', u'e', u'f', u'h', u'i', u'l', u'm', u'n', u'o', u'r', u's', u't', u'w', u'y']","title":"Character n-grams"},{"location":"w7-text-mining/7w/#tfidf-encoding","text":"[ back to top ] np.set_printoptions(precision=2) When we are analyzing text data, we often encounter words that occur across multiple documents from both classes. Those frequently occurring words typically don't contain useful or discriminatory information. In this subsection, we will learn about a useful technique called term frequency-inverse document frequency (tf-idf) that can be used to downweight those frequently occurring words in the feature vectors. The tf-idf can be de ned as the product of the term frequency and the inverse document frequency: $$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$ Here the tf(t, d) is the term frequency that we introduced in the previous section, and the inverse document frequency idf(t, d) can be calculated as: $$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$ where $$n_d$$ is the total number of documents, and df(d, t) is the number of documents d that contain the term t . Note that adding the constant 1 to the denominator is optional and serves the purpose of assigning a non-zero value to terms that occur in all training samples; the log is used to ensure that low document frequencies are not given too much weight. Scikit-learn implements yet another transformer, the TfidfTransformer , that takes the raw term frequencies from CountVectorizer as input and transforms them into tf-idfs: from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True) print(tfidf.fit_transform(count.fit_transform(docs)).toarray()) [[ 0. 0.43 0. 0.56 0.56 0. 0.43 0. 0. ] [ 0. 0.43 0. 0. 0. 0.56 0.43 0. 0.56] [ 0.5 0.45 0.5 0.19 0.19 0.19 0.3 0.25 0.19]] As we saw in the previous subsection, the word \"is\" (column 2) had the largest term frequency in the 3rd document, being the most frequently occurring word. However, after transforming the same feature vector into tf-idfs, we see that the word \"is\" is now associated with a relatively small tf-idf (0.45) in document 3 since it is also contained in documents 1 and 2 and thus is unlikely to contain any useful, discriminatory information. However, if we'd manually calculated the tf-idfs of the individual terms in our feature vectors, we'd have noticed that the TfidfTransformer calculates the tf-idfs slightly differently compared to the standard textbook equations that we see earlier. The equations for the idf and tf-idf that were implemented in scikit-learn are: $$\\text{idf} (t,d) = log\\frac{1 + n_d}{1 + \\text{df}(d, t)}$$ The tf-idf equation that was implemented in scikit-learn is as follows: $$\\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times (\\text{idf}(t,d)+1)$$ While it is also more typical to normalize the raw term frequencies before calculating the tf-idfs, the TfidfTransformer normalizes the tf-idfs directly. By default ( norm='l2' ), scikit-learn's TfidfTransformer applies the L2-normalization, which returns a vector of length 1 by dividing an un-normalized feature vector v by its L2-norm: $$v_{\\text{norm}} = \\frac{v}{||v|| 2} = \\frac{v}{\\sqrt{v {1}^{2} + v_{2}^{2} + \\dots + v_{n}^{2}}} = \\frac{v}{\\big (\\sum_{i=1}^{n} v_{i}^{2}\\big)^\\frac{1}{2}}$$ To make sure that we understand how TfidfTransformer works, let us walk through an example and calculate the tf-idf of the word is in the 3rd document. The word is has a term frequency of 3 (tf = 3) in document 3, and the document frequency of this term is 3 since the term is occurs in all three documents (df = 3). Thus, we can calculate the idf as follows: $$\\text{idf}(\"is\", d3) = log \\frac{1+3}{1+3} = 0$$ Now in order to calculate the tf-idf, we simply need to add 1 to the inverse document frequency and multiply it by the term frequency: $$\\text{tf-idf}(\"is\",d3)= 3 \\times (0+1) = 3$$ tf_is = 3 n_docs = 3 idf_is = np.log((n_docs+1) / (3+1)) tfidf_is = tf_is * (idf_is + 1) print('tf-idf of term is = %.2f' % tfidf_is) tf-idf of term \"is\" = 3.00 If we repeated these calculations for all terms in the 3rd document, we'd obtain the following tf-idf vectors: [3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]. However, we notice that the values in this feature vector are different from the values that we obtained from the TfidfTransformer that we used previously. The step that we are missing in this tf-idf calculation is the L2-normalization, which can be applied as follows: $$\\text{tfi-df}_{norm} = \\frac{[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0 , 1.69, 1.29]}{\\sqrt{[3.39^2, 3.0^2, 3.39^2, 1.29^2, 1.29^2, 1.29^2, 2.0^2 , 1.69^2, 1.29^2]}}$$ $$=[0.5, 0.45, 0.5, 0.19, 0.19, 0.19, 0.3, 0.25, 0.19]$$ $$\\Rightarrow \\text{tfi-df}_{norm}(\"is\", d3) = 0.45$$ tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True) raw_tfidf = tfidf.fit_transform(count.fit_transform(docs)).toarray()[-1] raw_tfidf array([ 3.39, 3. , 3.39, 1.29, 1.29, 1.29, 2. , 1.69, 1.29]) l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2)) l2_tfidf array([ 0.5 , 0.45, 0.5 , 0.19, 0.19, 0.19, 0.3 , 0.25, 0.19]) As we can see, the results match the results returned by scikit-learn's TfidfTransformer (below). Since we now understand how tf-idfs are calculated, let us proceed to the next sections and apply those concepts to the movie review dataset.","title":"Tfidf encoding"},{"location":"w7-text-mining/7w/#cleaning-text-data","text":"[ back to top ] df.loc[0, 'review'][-50:] # last 50 characters from the first document u'is seven. br / br / Title (Brazil): Not Available' text contains html markup tags, we need clean them. we will now remove all punctuation marks but only keep emoticon characters such as \":)\" import re # Python \u4e2d\u5904\u7406\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u6a21\u5757 def preprocessor(text): text = re.sub(r' [^ ]* ', '', text) # \u53bb\u9664 HTML \u6807\u7b7e emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text) text = re.sub(r'\\W+', ' ', text.lower()) + \\ ' '.join(emoticons).replace('-', '') # \u5c06\u8868\u60c5\u7b26\u53f7\u62fc\u63a5\u5728\u6b63\u6587\u540e\u9762 return text preprocessor(df.loc[0, 'review'][-50:]) u'is seven title brazil not available' preprocessor( /a This :) is :( a test :-)! ) 'this is a test :) :( :)' df['review'] = df['review'].apply(preprocessor)","title":"Cleaning text data"},{"location":"w7-text-mining/7w/#processing-documents-into-tokens","text":"[ back to top ] split the text corpora into individual elements. tokenize \u662f\u628a\u957f\u6587\u672c\u5207\u6210\u4e00\u7cfb\u5217\u5355\u8bcd\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u4ee5 whitespace \u5207\u5206. word stemming \u662f\u5c06\u8bcd\u8f6c\u4e3a\u6700\u539f\u59cb\u7684\u5f62\u5f0f, root form, (\u4f8b\u5982 running - run), \u4e00\u79cd\u7b97\u6cd5\u662f Porter stemmer algorithm \u4ee5\u4e0b\u9700\u8981\u4f7f\u7528 nltk, \u9700\u8981\u5148\u5b89\u88c5: pip install nltk from nltk.stem.porter import PorterStemmer porter = PorterStemmer() def tokenizer(text): return text.split() # \u9ed8\u8ba4\u4ee5\u7a7a\u683c\u5207\u5206 def tokenizer_porter(text): return [porter.stem(word) for word in text.split()] tokenizer('runners like running and thus they run') ['runners', 'like', 'running', 'and', 'thus', 'they', 'run'] tokenizer_porter('runners like running and thus they run') [u'runner', u'like', u'run', u'and', u'thu', u'they', u'run'] Stop-words (\u505c\u8bcd) \u662f \u6700\u5e38\u89c1\u7684\u4e00\u4e9b\u5355\u8bcd, \u5b83\u4eec\u7684\u5b9e\u9645\u610f\u4e49\u5e76\u4e0d\u662f\u5f88\u5927, \u5927\u591a\u662f\u8d77\u8f85\u52a9\u4f5c\u7528\u7684, \u4f46\u662f\u5b83\u4eec\u7684\u9891\u6b21\u975e\u5e38\u9ad8, \u6240\u4ee5\u9700\u8981\u53bb\u9664, \u4f8b\u5982 is, and, has import nltk nltk.download('stopwords') # \u4e0b\u8f7d\u505c\u8bcd [nltk_data] Downloading package stopwords to /Users/alan/nltk_data... [nltk_data] Package stopwords is already up-to-date! True from nltk.corpus import stopwords stop = stopwords.words('english') [w for w in tokenizer_porter('a runner likes running and runs a lot')[-10:] if w not in stop] [u'runner', u'like', u'run', u'run', u'lot']","title":"Processing documents into tokens"},{"location":"w7-text-mining/7w/#training-a-logistic-regression-model-for-sentiment-classification","text":"[ back to top ] Strip HTML and punctuation to speed up the GridSearch later: # 25,000 documents for training and 25,000 documents for testing, \u9700\u8981\u5927\u7ea640\u5206\u949f # \u6240\u4ee5\u5148\u4f7f\u75285000 documents X_train = df.loc[:5000, 'review'].values y_train = df.loc[:5000, 'sentiment'].values X_test = df.loc[5000:, 'review'].values y_test = df.loc[5000:, 'sentiment'].values from sklearn.grid_search import GridSearchCV from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import TfidfVectorizer # TfidfVectorizer \u7b49\u540c\u4e8e CountVectorizer + TfidfTransformer tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None) # grig search param_grid = [{'vect__ngram_range': [(1,1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, {'vect__ngram_range': [(1,1)], 'vect__stop_words': [stop, None], 'vect__tokenizer': [tokenizer, tokenizer_porter], 'vect__use_idf':[False], 'vect__norm':[None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}, ] # \u5148\u8f6c\u5316\u4e3a tfidf matrix lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))]) gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1) # \u6570\u636e\u91cf\u51cf\u5c11\u4e3a5000 \u540e\u9700\u8981\u65f6\u95f4\u5927\u7ea6\u4e3a8\u5206\u949f gs_lr_tfidf.fit(X_train, y_train) Fitting 5 folds for each of 48 candidates, totalling 240 fits [Parallel(n_jobs=-1)]: Done 34 tasks | elapsed: 1.4min [Parallel(n_jobs=-1)]: Done 184 tasks | elapsed: 8.0min [Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 10.5min finished GridSearchCV(cv=5, error_score='raise', estimator=Pipeline(steps=[('vect', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict', dtype= type 'numpy.int64' , encoding=u'utf-8', input=u'content', lowercase=False, max_df=1.0, max_features=None, min_df=1, ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=Tru...nalty='l2', random_state=0, solver='liblinear', tol=0.0001, verbose=0, warm_start=False))]), fit_params={}, iid=True, n_jobs=-1, param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__tokenizer': [ function tokenizer at 0x130dca410 , function tokenizer_porter at 0x130dca488 ], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0], 'vect__stop_words': [[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'y...x130dca488 ], 'vect__use_idf': [False], 'clf__C': [1.0, 10.0, 100.0], 'clf__penalty': ['l1', 'l2']}], pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=1) print('Best parameter set: %s ' % gs_lr_tfidf.best_params_) print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_) Best parameter set: {'vect__ngram_range': (1, 1), 'vect__tokenizer': function tokenizer at 0x130dca410 , 'clf__penalty': 'l2', 'clf__C': 10.0, 'vect__stop_words': [u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']} CV Accuracy: 0.862 clf = gs_lr_tfidf.best_estimator_ print('Test Accuracy: %.3f' % clf.score(X_test, y_test)) Test Accuracy: 0.873","title":"Training a logistic regression model for sentiment classification"},{"location":"w7-text-mining/7w/#working-with-bigger-data-online-algorithms-and-out-of-core-learning","text":"[ back to top ] Out-of-Core learning is the task of training a machine learning model on a dataset that does not fit into memory or RAM. This requires the following conditions: a feature extraction layer with fixed output dimensionality knowing the list of all classes in advance (in this case we only have positive and negative tweets) a machine learning algorithm that supports incremental learning (the partial_fit method in scikit-learn). import numpy as np import re from nltk.corpus import stopwords stop = stopwords.words('english') def tokenizer(text): text = re.sub(' [^ ]* ', '', text) emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()) text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '') tokenized = [w for w in text.split() if w not in stop] return tokenized def stream_docs(path): # \u751f\u6210\u5668 # generator funciton, reads in and returns one document at a time with open(path, 'r') as csv: next(csv) # skip header for line in csv: text, label = line[:-3], int(line[-2]) yield text, label # To verify that our stream_docs function works correctly gen = stream_docs(path='./data/movie_data.csv') next(gen) ('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder. br / br / \"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven. br / br / Title (Brazil): Not Available\"', 1) def get_minibatch(doc_stream, size): ''' take a document stream from the stream_docs function and return a particular number of documents ''' docs, y = [], [] try: for _ in range(size): text, label = next(doc_stream) docs.append(text) y.append(label) except StopIteration: return None, None return docs, y from sklearn.feature_extraction.text import HashingVectorizer # makes use of the Hashing trick from sklearn.linear_model import SGDClassifier # train a logistic regression model using small minibatches of documents vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer) clf = SGDClassifier(loss='log', random_state=1, n_iter=1) doc_stream = stream_docs(path='./data/movie_data.csv') # iterated over 45 minibatches of documents # where each minibatch consists of 1,000 documents each import pyprind pbar = pyprind.ProgBar(45) classes = np.array([0, 1]) for _ in range(45): X_train, y_train = get_minibatch(doc_stream, size=1000) if not X_train: break X_train = vect.transform(X_train) clf.partial_fit(X_train, y_train, classes=classes) pbar.update() 0% 100% [##############################] | ETA: 00:00:00 Total time elapsed: 00:00:40 #use the last 5,000 documents to evaluate the performance X_test, y_test = get_minibatch(doc_stream, size=5000) X_test = vect.transform(X_test) print('Accuracy: %.3f' % clf.score(X_test, y_test)) Accuracy: 0.867 \u867d\u7136\u51c6\u786e\u7387\u7565\u4f4e\u4e8e\u524d\u9762\uff0c\u4f46\u8bad\u7ec3\u901f\u5ea6\u5feb\u4e86\u5f88\u591a\uff0c\u800c\u4e14\u4f7f\u7528\u7684\u5185\u5b58\u66f4\u5c11 # use the last 5,000 documents to update our model # \u53ef\u4ee5\u4f7f\u7528 partial_fit \u7ee7\u7eed\u8bad\u7ec3 clf = clf.partial_fit(X_test, y_test)","title":"Working with bigger data - online algorithms and out-of-core learning"},{"location":"w7-text-mining/7w/#model-persistence","text":"\u8bad\u7ec3\u6a21\u578b\u662f expensive \u5e76\u4e14\u8017\u8d39\u65f6\u95f4\u7684, \u6211\u4eec\u4e0d\u5e0c\u671b\u5728\u5e94\u7528\u4e2d\u6bcf\u6b21\u90fd\u8981\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b, \u6240\u4ee5\u6211\u4eec\u9700\u8981\u4fdd\u5b58\u6a21\u578b, \u5e76\u4e14\u80fd\u8fdb\u884c\u65b0\u7684\u9884\u6d4b\u4ee5\u53ca\u66f4\u65b0\u3002 \u53ef\u4ee5\u7528\u5230 pickle \u6a21\u5757\u6765\u50a8\u5b58\u6a21\u578b, \u5c06 python object \u50a8\u5b58\u4e3abyte code, \u53ef\u4ee5\u8bfb\u53d6\u4e5f\u53ef\u4ee5\u5199\u5165 [ back to top ] After we trained the logistic regression model as shown above, we can save the classifier along with the stop words, Porter Stemmer, and HashingVectorizer as serialized objects to our local disk so that we can use the fitted classifier in our web application later. import pickle import os # created a movieclassifier directory # created a pkl_objects subdirectory to save the serialized Python objects to our local drive dest = os.path.join('movieclassifier', 'pkl_objects') if not os.path.exists(dest): os.makedirs(dest) # \u5199\u5165 pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=2) pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=2) Next, we save the HashingVectorizer as in a separate file so that we can import it later. %%writefile movieclassifier/vectorizer.py from sklearn.feature_extraction.text import HashingVectorizer import re import os import pickle cur_dir = os.path.dirname(__file__) stop = pickle.load(open( os.path.join(cur_dir, 'pkl_objects', 'stopwords.pkl'), 'rb')) def tokenizer(text): text = re.sub(' [^ ]* ', '', text) emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()) text = re.sub('[\\W]+', ' ', text.lower()) \\ + ' '.join(emoticons).replace('-', '') tokenized = [w for w in text.split() if w not in stop] return tokenized vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer) Overwriting movieclassifier/vectorizer.py After executing the preceeding code cells, we can now restart the IPython notebook kernel to check if the objects were serialized correctly. First, change the current Python directory to movieclassifer : import os os.chdir('movieclassifier') import pickle import re import os from vectorizer import vect clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb')) import numpy as np label = {0:'negative', 1:'positive'} example = ['I love this movie'] X = vect.transform(example) print('Prediction: %s\\nProbability: %.2f%%' %\\ (label[clf.predict(X)[0]], np.max(clf.predict_proba(X))*100)) Prediction: positive Probability: 82.53%","title":"Model persistence"},{"location":"w7-text-mining/7w/#word2vec","text":"[ back to top ] word2vec \u662f Mikolov et al. \u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u65b9\u6cd5\u3002\u5b83\u6709 Continuous Bag-of-Words model (CBOW) and the Skip-Gram model \u4e24\u79cd\u53d8\u5f0f\uff0c\u524d\u8005\u662f\u7528\u4e00\u4e2a\u8bcd\u5e8f\u5217\u7a97\u53e3\u4e2d\u7684\u5176\u4ed6\u8bcd\u6765\u9884\u6d4b\u4e2d\u5fc3\u8bcd\uff0c\u540e\u8005\u5219\u662f\u7528\u4e2d\u5fc3\u8bcd\u6765\u9884\u6d4b\u5176\u4ed6\u8bcd\u3002\u5728\u5b9e\u9645\u4f7f\u7528 word2vec \u65f6\uff0c\u4e00\u822c\u4f7f\u7528 Skip-Gram model \u7ed3\u5408 Negative Sampling \u8fdb\u884c\u8bad\u7ec3\u3002 # gensim \u5e93\u4e2d\u5305\u542b\u4e86 word2vec \u6a21\u5757 from gensim.models.word2vec import Word2Vec \u8bad\u7ec3\u8bcd\u5411\u91cf\uff0c gensim \u8bad\u7ec3\u8bcd\u5411\u91cf\u65f6\u53ef\u4ee5\u5582\u5165\u4e00\u4e2a iterator class MySentence: def __init__(self, data): self.data = data def __iter__(self): for line in self.data: yield line.lower().split() train_corpus = MySentence(df['review']) # \u6216\u8005 train_corpus = [s.split() for s in df['review']] model = Word2Vec(train_corpus, size=200, # \u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6 iter=20, # \u6570\u636e\u5728\u8bad\u7ec3\u4e2d\u7528\u5230\u7684\u6b21\u6570, \u5373 epoch \u6570 workers=4) # \u8c03\u7528\u7684\u8fdb\u7a0b\u6570 \u7b80\u5355\u67e5\u770b\u8bcd\u5411\u91cf\u7684\u7ed3\u679c model.most_similar('good') [(u'decent', 0.7367605566978455), (u'bad', 0.7155213356018066), (u'great', 0.7067341804504395), (u'nice', 0.6252745389938354), (u'cool', 0.5865251421928406), (u'passable', 0.5830472707748413), (u'funny', 0.5790721774101257), (u'fine', 0.5767843127250671), (u'lousy', 0.5747220516204834), (u'terrible', 0.5679782629013062)] \u83b7\u5f97\u8bcd\u5bf9\u5e94\u7684\u8bcd\u5411\u91cf model['good'] array([-1.80578566, 2.59199047, 1.00538623, 0.00376823, -1.63011074, 1.19826996, -0.05748612, 0.34942579, -3.64253092, 0.77269369, -0.12568648, 0.63311213, -0.29951221, 1.00349665, 0.57139468, 0.26391807, -2.15211248, 1.37436974, 0.72486091, 1.39909697, 1.39214003, -0.55878437, 1.28609359, 1.4021709 , -0.61972415, 1.40909946, 1.23801959, 0.12476239, -2.10969472, -0.16599979, 1.07056391, -0.9283669 , -1.24553549, -1.00224996, 0.85817921, -2.98209238, 2.52684212, -0.84701031, -1.30793285, -3.52102351, 2.51759624, 1.55442834, -0.81624299, 0.80133152, 0.76279849, -1.61152196, 0.84180117, 1.95387816, 0.39838204, -2.50793004, -1.3055681 , 2.4016645 , 2.7259481 , 1.65427065, 2.08329916, -0.67130673, -2.77032781, -0.86488032, -0.83239168, 0.70329827, 0.1054525 , -0.48466596, 1.93771338, 0.57588094, -0.16703451, 2.06361747, 0.90677142, 1.05519104, 1.84931016, 0.77389133, 2.08386183, -2.16565847, 1.20824552, 0.05443871, 3.20145893, -2.12376046, -0.46623436, 1.19470978, -0.60041159, -2.01585841, -1.14179373, 0.25336841, -2.7429111 , 0.19020027, 0.77245057, 1.07972777, 3.45747972, -1.17416704, 0.14808762, -0.04948059, 2.25656533, -1.87219381, -2.38383865, -2.15413165, -0.18855318, -1.33702457, -0.72171617, -0.28952792, 0.9959411 , 2.43245673, -0.94892198, 1.32803464, 3.27445745, 0.44902089, 1.76025999, -0.15520753, -0.42482886, 0.22728981, -0.93469167, 0.87159711, 1.20421445, 1.03957427, 0.56455994, 0.19218144, -1.84992611, 1.75870144, -2.13160086, 0.99597716, -1.92741001, 1.47426069, -2.23345065, -0.38542071, -0.33888757, -0.50440568, -1.29633522, 1.55562842, 1.08055615, 1.2215786 , -1.75132465, -0.11741698, 0.77109945, -0.68743432, -0.31800482, 0.23696584, 0.77463788, -1.75956166, 1.77490389, 0.07634074, 2.27028704, 0.66218233, 3.3049078 , 3.30950427, -1.33130598, -0.92713892, 0.15286015, 0.287049 , 0.42833641, -1.46425474, 0.9658314 , 0.40959406, 0.47199422, -0.95312113, -3.23242712, -0.08304592, -1.2138108 , 0.30118775, -0.96748334, -0.68403792, -2.61612749, -0.97091049, 0.03540362, -1.22498131, 0.23248808, 2.79292464, 1.39140284, 0.35824764, 0.53603202, 2.08097696, 1.61062503, -1.45209515, -0.87486774, 0.6024124 , 1.00253165, -0.25757971, 3.5850575 , 0.37494364, 1.26284862, -1.56030715, -2.38174224, -2.99708772, 0.75272441, 0.95936358, 0.24002089, 1.7718761 , 1.28533518, -2.32952619, 2.59281707, -3.69943428, 0.34991279, -0.52845728, 1.35607052, 0.67054856, -1.21081388, 1.81272793, 0.64519709, -0.4952068 , 2.65413833, 1.53806341, -2.95830393, 2.38203692], dtype=float32) \u5b58\u50a8/\u8bfb\u53d6\u6a21\u578b model.save('data/imdb.d2v') model = Word2Vec.load('data/imdb.d2v') \u5229\u7528\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\u6765\u8fdb\u884c\u60c5\u611f\u5206\u6790 # \u7528 word vec \u7684\u5747\u503c\u4f5c\u4e3a doc vec def get_doc_vec(sentence, model): scores = [model[word] for word in sentence.split() if word in model] # \u5982\u679c\u8bcd\u9891\u5c0f\u4e8e min_count, word2vec \u4e0d\u4f1a\u628a\u8fd9\u4e2a\u8bcd\u653e\u5165 vocab \u91cc return np.mean(scores, axis=0) X_word2vec_train = np.array([get_doc_vec(sentence, model) for sentence in X_train]) X_word2vec_test = np.array([get_doc_vec(sentence, model) for sentence in X_test]) from sklearn.grid_search import GridSearchCV from sklearn.linear_model import LogisticRegression lr = LogisticRegression(random_state=0) param_grid = [{'penalty': ['l1', 'l2'], 'C': [1.0, 10.0, 100.0]}] gs = GridSearchCV(lr, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1) gs.fit(X_word2vec_train, y_train); Fitting 5 folds for each of 6 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 14.5s finished gs.best_params_ {'C': 1.0, 'penalty': 'l2'} gs.best_score_ 0.85642871425714862 clf = gs.best_estimator_ print('Test Accuracy: %.3f' % clf.score(X_word2vec_test, y_test)) Test Accuracy: 0.869","title":"word2vec"},{"location":"w7-text-mining/7w/#_1","text":"from os import path import os import re import pandas as pd import numpy as np rootdir = 'data/SogouC.reduced/Reduced' dirs = os.listdir(rootdir) dirs = [path.join(rootdir,f) for f in dirs if f.startswith('C')] dirs ['data/SogouC.reduced/Reduced/C000008', 'data/SogouC.reduced/Reduced/C000010', 'data/SogouC.reduced/Reduced/C000013', 'data/SogouC.reduced/Reduced/C000014', 'data/SogouC.reduced/Reduced/C000016', 'data/SogouC.reduced/Reduced/C000020', 'data/SogouC.reduced/Reduced/C000022', 'data/SogouC.reduced/Reduced/C000023', 'data/SogouC.reduced/Reduced/C000024'] def load_txt(x): with open(x) as f: res = [t.decode('gbk','ignore') for t in f] return ''.join(res) text_t = {} for i, d in enumerate(dirs): files = os.listdir(d) files = [path.join(d, x) for x in files if x.endswith('txt') and not x.startswith('.')] text_t[i] = [load_txt(f) for f in files] print(text_t[0][0][:100]) \u3000\u3000\u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77 \u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f\u6210\u4e3a\u5e02\u503c100\u4ebf\u7f8e\u5143\u7684\u4e0a\u5e02\u516c\u53f8\u3002 \u3000\u3000\u8fd9\u5bb6\u516c\u53f8\u53eb\u505a\u5feb\u94b1\uff0c\u8bf4\u8fd9\u53e5\u8bdd\u7684\u662f\u5feb\u94b1\u7684CEO\u5173\u56fd\u5149\u3002\u4ed6\u4e4b\u524d\u66fe\u4efb\u7f51\u6613\u7684\u9ad8\u7ea7\u526f flen = [len(t) for t in text_t.values()] labels = np.repeat(text_t.keys(),flen) # flatter nested list import itertools merged = list(itertools.chain.from_iterable(text_t.values())) df = pd.DataFrame({'label': labels, 'txt': merged}) df.head() label txt 0 0 \u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77\\r\\n\u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f... 1 0 \u8bc1\u5238\u901a\uff1a\u767e\u8054\u80a1\u4efd\u672a\u67655\u5e74\u6709\u80fd\u529b\u4fdd\u6301\u9ad8\u901f\u589e\u957f\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2... 2 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 3 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 4 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... # cut word import jieba jieba.enable_parallel(4) def cutword_1(x): words = jieba.cut(x) return ' '.join(words) Building prefix dict from the default dictionary ... Loading model from cache /var/folders/y8/z6ws1f2907vbb33mcp8c63300000gn/T/jieba.cache Loading model cost 1.119 seconds. Prefix dict has been built succesfully. df['seg_word'] = df.txt.map(cutword_1) df.head() label txt seg_word 0 0 \u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77\\r\\n\u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f... \u672c\u62a5\u8bb0\u8005 \u9648\u96ea\u9891 \u5b9e\u4e60 \u8bb0\u8005 \u5510\u7fd4 \u53d1\u81ea \u4e0a\u6d77 \\r\\n \u3000 \u3000 \u4e00\u5bb6 \u521a\u521a \u6210\u7acb ... 1 0 \u8bc1\u5238\u901a\uff1a\u767e\u8054\u80a1\u4efd\u672a\u67655\u5e74\u6709\u80fd\u529b\u4fdd\u6301\u9ad8\u901f\u589e\u957f\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2... \u8bc1\u5238 \u901a \uff1a \u767e\u8054 \u80a1\u4efd \u672a\u6765 5 \u5e74 \u6709 \u80fd\u529b \u4fdd\u6301\u9ad8\u901f \u589e\u957f \\r\\n ... 2 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 3 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 4 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... from cPickle import dump,load dump(df, open('data/tmdf.pickle', 'wb')) # df = load(open('df.pickle','rb')) from sklearn.feature_extraction.text import TfidfVectorizer vect = TfidfVectorizer(ngram_range=(1,1), min_df = 2, max_features = 10000) xvec = vect.fit_transform(df.seg_word) xvec.shape (17910, 10000) y = df.label from sklearn.cross_validation import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import MultinomialNB from sklearn.ensemble import RandomForestClassifier train_X, test_X, train_y, test_y = train_test_split(xvec, y , train_size=0.7, random_state=1) clf = MultinomialNB() clf.fit(train_X, train_y) MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) from sklearn import metrics pre = clf.predict(test_X) print metrics.classification_report(test_y, pre) precision recall f1-score support 0 0.90 0.88 0.89 577 1 0.89 0.81 0.85 603 2 0.88 0.82 0.85 619 3 0.98 0.97 0.98 584 4 0.86 0.88 0.87 570 5 0.88 0.79 0.83 600 6 0.77 0.90 0.83 600 7 0.76 0.83 0.80 615 8 0.92 0.93 0.93 605 avg / total 0.87 0.87 0.87 5373 # word2vec txt = df.seg_word.values txtlist = [] for sent in txt: temp = [w for w in sent.split()] txtlist.append(temp) num_features = 100 min_word_count = 10 num_workers = 4 context = 5 epoch = 20 sample = 1e-5 from gensim.models import word2vec model = word2vec.Word2Vec(txtlist, workers = num_workers, sample = sample, size = num_features, min_count=min_word_count, window = context, iter = epoch) model.syn0.shape (57675, 100) for w in model.most_similar(u'\u4e92\u8054\u7f51'): print w[0], w[1] \u7f51\u7edc 0.787674069405 \u95e8\u6237\u7f51\u7ad9 0.747487425804 \u641c\u7d22\u5f15\u64ce 0.744884610176 \u65e0\u7ebf 0.732329308987 B2B 0.713720798492 \u7f51\u7edc\u5e7f\u544a 0.712735056877 \u817e\u8baf 0.702631175518 MSN 0.701346695423 \u5927\u65d7\u7f51 0.69608104229 Google 0.68867880106 #model.save('sogo_wv') #model = word2vec.Word2Vec.load('sogo_wv') # \u5c06\u8bcd\u5411\u91cf\u5e73\u5747\u5316\u4e3a\u6587\u6863\u5411\u91cf def sentvec_1(sent,m=num_features,model=model): res = np.zeros(m) words = sent.split() num = 0 for w in words: if w in model.index2word: res += model[w] num += 1.0 if num == 0: return np.zeros(m) else: return res/num n = df.shape[0] sent_matrix = np.zeros([n,num_features],float) for i ,sent in enumerate(df.seg_word.values): sent_matrix[i,:] = sentvec_1(sent) sent_matrix.shape (17910, 100) from sklearn.ensemble import GradientBoostingClassifier from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(sent_matrix, y , train_size=0.7, random_state=1) clf = GradientBoostingClassifier() clf.fit(train_X, train_y) from sklearn import metrics pre = clf.predict(test_X) print metrics.classification_report(test_y, pre) precision recall f1-score support 0 0.91 0.85 0.88 577 1 0.84 0.82 0.83 603 2 0.86 0.88 0.87 619 3 0.98 0.97 0.98 584 4 0.84 0.86 0.85 570 5 0.84 0.80 0.82 600 6 0.86 0.87 0.87 600 7 0.77 0.83 0.80 615 8 0.93 0.93 0.93 605 avg / total 0.87 0.87 0.87 5373","title":"\u4e2d\u6587\u65b0\u95fb\u5206\u7c7b"},{"location":"w8-neural-network/8w/","text":"Sections Modeling complex functions with artificial neural networks Single-layer neural network recap Introducing the multi-layer neural network architecture MLP learning procedure Activating a neural network via forward propagation Loss functions Training neural networks via backpropagation Optimizaiton methods Classifying handwritten digits Obtaining the MNIST dataset Implementing a multi-layer perceptron Training an artificial neural network Debugging neural networks with gradient checking Other neural network architectures Convolutional Neural Networks Recurrent Neural Networks Modeling complex functions with artificial neural networks [ back to top ] Single-layer neural network recap [ back to top ] linear combination Linear combination$$z=\\sum_{i=1}^m x_i\\times w_i + w_0 $$\uff0c\u5176\u4e2d$$w_0$$\u662fbias \u9879\u3002 activation functions Introducing the multi-layer neural network architecture [ back to top ] Multi-layer perceptron (MLP) MLP \u53ef\u4ee5\u770b\u6210\u591a\u6b21\u7ebf\u6027\u8f6c\u6362\u548c\u6fc0\u6d3b\u51fd\u6570\u7684\u5806\u53e0, \u4e0b\u56fe\u7ed3\u6784\u53ef\u4ee5\u5199\u6210 $$y = \\sigma({\\sigma (XW^{(1)} + b^{(1)})}W^{(2)} + b^{(2)})$$ MLP learning procedure [ back to top ] Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output. Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later. We backpropagate the error, find its derivative with respect to each weight in the network, and update the model. Activating a neural network via forward propagation [ back to top ] Loss functions [ back to top ] Training neural networks via backpropagation [ back to top ] Optimizaiton methods [ back to top ] Classifying handwritten digits Obtaining the MNIST dataset [ back to top ] The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts: Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 samples) Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels) Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 samples) Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels) In this section, we will only be working with a subset of MNIST, thus, we only need to download the training set images and training set labels. After downloading the files, I recommend unzipping the files using the Unix/Linux gzip tool from the terminal for efficiency, e.g., using the command gzip *ubyte.gz -d in your local MNIST download directory, or, using your favorite unzipping tool if you are working with a machine running on Microsoft Windows. The images are stored in byte form, and using the following function, we will read them into NumPy arrays that we will use to train our MLP. import os import struct import numpy as np def load_mnist(path, kind='train'): Load MNIST data from `path` labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind) images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind) with open(labels_path, 'rb') as lbpath: magic, n = struct.unpack(' II', lbpath.read(8)) labels = np.fromfile(lbpath, dtype=np.uint8) with open(images_path, 'rb') as imgpath: magic, num, rows, cols = struct.unpack( IIII , imgpath.read(16)) images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) return images, labels X_train, y_train = load_mnist('data/mnist', kind='train') print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1])) Rows: 60000, columns: 784 X_test, y_test = load_mnist('data/mnist', kind='t10k') print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1])) Rows: 10000, columns: 784 Visualize the first digit of each class: # import pandas as pd # train = pd.read_csv('data/digit_recognizer/train.csv') # train.shape import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,) ax = ax.flatten() for i in range(10): img = X_train[y_train == i][0].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest') ax[0].set_xticks([]) ax[0].set_yticks([]) plt.tight_layout() # plt.savefig('./figures/mnist_all.png', dpi=300) Visualize 25 different versions of \"7\": fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,) ax = ax.flatten() for i in range(25): img = X_train[y_train == 7][i].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest') ax[0].set_xticks([]) ax[0].set_yticks([]) plt.tight_layout() # plt.savefig('./figures/mnist_7.png', dpi=300) Uncomment the following lines to optionally save the data in CSV format. However, note that those CSV files will take up a substantial amount of storage space: train_img.csv 1.1 GB (gigabytes) train_labels.csv 1.4 MB (megabytes) test_img.csv 187.0 MB test_labels 144 KB (kilobytes) # np.savetxt('train_img.csv', X_train, fmt='%i', delimiter=',') # np.savetxt('train_labels.csv', y_train, fmt='%i', delimiter=',') # X_train = np.genfromtxt('train_img.csv', dtype=int, delimiter=',') # y_train = np.genfromtxt('train_labels.csv', dtype=int, delimiter=',') # np.savetxt('test_img.csv', X_test, fmt='%i', delimiter=',') # np.savetxt('test_labels.csv', y_test, fmt='%i', delimiter=',') # X_test = np.genfromtxt('test_img.csv', dtype=int, delimiter=',') # y_test = np.genfromtxt('test_labels.csv', dtype=int, delimiter=',') Implementing a multi-layer perceptron [ back to top ] import numpy as np from scipy.special import expit import sys class NeuralNetMLP(object): Feedforward neural network / Multi-layer perceptron classifier. Parameters ------------ n_output : int Number of output units, should be equal to the number of unique class labels. n_features : int Number of features (dimensions) in the target dataset. Should be equal to the number of columns in the X array. n_hidden : int (default: 30) Number of hidden units. l1 : float (default: 0.0) Lambda value for L1-regularization. No regularization if l1=0.0 (default) l2 : float (default: 0.0) Lambda value for L2-regularization. No regularization if l2=0.0 (default) epochs : int (default: 500) Number of passes over the training set. eta : float (default: 0.001) Learning rate. alpha : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + alpha*grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) shuffle : bool (default: False) Shuffles training data every epoch if True to prevent circles. minibatches : int (default: 1) Divides training data into k minibatches for efficiency. Normal gradient descent learning if k=1 (default). random_state : int (default: None) Set random state for shuffling and initializing the weights. Attributes ----------- cost_ : list Sum of squared errors after each epoch. def __init__(self, n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1, random_state=None): np.random.seed(random_state) self.n_output = n_output self.n_features = n_features self.n_hidden = n_hidden self.w1, self.w2 = self._initialize_weights() self.l1 = l1 self.l2 = l2 self.epochs = epochs self.eta = eta self.alpha = alpha self.decrease_const = decrease_const self.shuffle = shuffle self.minibatches = minibatches def _encode_labels(self, y, k): Encode labels into one-hot representation Parameters ------------ y : array, shape = [n_samples] Target values. Returns ----------- onehot : array, shape = (n_labels, n_samples) onehot = np.zeros((k, y.shape[0])) for idx, val in enumerate(y): onehot[val, idx] = 1.0 return onehot def _initialize_weights(self): Initialize weights with small random numbers. w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1)) w1 = w1.reshape(self.n_hidden, self.n_features + 1) w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1)) w2 = w2.reshape(self.n_output, self.n_hidden + 1) return w1, w2 def _sigmoid(self, z): Compute logistic function (sigmoid) Uses scipy.special.expit to avoid overflow error for very small input values z. # return 1.0 / (1.0 + np.exp(-z)) return expit(z) def _sigmoid_gradient(self, z): Compute gradient of the logistic function sg = self._sigmoid(z) return sg * (1 - sg) # sigmoid \u51fd\u6570\u7684\u5bfc\u6570\u6bd4\u8f83\u7b80\u5355\uff0c\u5c31\u662f sg * (1-sg) def _add_bias_unit(self, X, how='column'): Add bias unit (column or row of 1s) to array at index 0 if how == 'column': X_new = np.ones((X.shape[0], X.shape[1]+1)) X_new[:, 1:] = X elif how == 'row': X_new = np.ones((X.shape[0]+1, X.shape[1])) X_new[1:, :] = X else: raise AttributeError('`how` must be `column` or `row`') return X_new def _feedforward(self, X, w1, w2): Compute feedforward step Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns ---------- a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. z3 : array, shape = [n_output_units, n_samples] Net input of output layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. a1 = self._add_bias_unit(X, how='column') z2 = w1.dot(a1.T) a2 = self._sigmoid(z2) a2 = self._add_bias_unit(a2, how='row') z3 = w2.dot(a2) a3 = self._sigmoid(z3) return a1, z2, a2, z3, a3 def _L2_reg(self, lambda_, w1, w2): Compute L2-regularization cost return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2)) def _L1_reg(self, lambda_, w1, w2): Compute L1-regularization cost return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum()) def _get_cost(self, y_enc, output, w1, w2): Compute cost function. y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. output : array, shape = [n_output_units, n_samples] Activation of the output layer (feedforward) w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- cost : float Regularized cost. term1 = -y_enc * (np.log(output)) term2 = (1 - y_enc) * np.log(1 - output) cost = np.sum(term1 - term2) # \u4ea4\u4e92\u71b5 L1_term = self._L1_reg(self.l1, w1, w2) L2_term = self._L2_reg(self.l2, w1, w2) cost = cost + L1_term + L2_term return cost def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2): Compute gradient step using backpropagation. Parameters ------------ a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- grad1 : array, shape = [n_hidden_units, n_features] Gradient of the weight matrix w1. grad2 : array, shape = [n_output_units, n_hidden_units] Gradient of the weight matrix w2. # backpropagation sigma3 = a3 - y_enc z2 = self._add_bias_unit(z2, how='row') sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2) sigma2 = sigma2[1:, :] grad1 = sigma2.dot(a1) grad2 = sigma3.dot(a2.T) # regularize grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2)) grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2)) return grad1, grad2 def predict(self, X): Predict class labels Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. Returns: ---------- y_pred : array, shape = [n_samples] Predicted class labels. if len(X.shape) != 2: raise AttributeError('X must be a [n_samples, n_features] array.\\n' 'Use X[:,None] for 1-feature classification,' '\\nor X[[i]] for 1-sample classification') a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2) y_pred = np.argmax(z3, axis=0) return y_pred def fit(self, X, y, print_progress=False): Learn weights from training data. Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. y : array, shape = [n_samples] Target class labels. print_progress : bool (default: False) Prints progress as the number of epochs to stderr. Returns: ---------- self self.cost_ = [] X_data, y_data = X.copy(), y.copy() y_enc = self._encode_labels(y, self.n_output) delta_w1_prev = np.zeros(self.w1.shape) delta_w2_prev = np.zeros(self.w2.shape) for i in range(self.epochs): # adaptive learning rate self.eta /= (1 + self.decrease_const*i) if print_progress: sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs)) sys.stderr.flush() if self.shuffle: idx = np.random.permutation(y_data.shape[0]) X_data, y_data = X_data[idx], y_data[idx] mini = np.array_split(range(y_data.shape[0]), self.minibatches) for idx in mini: # feedforward a1, z2, a2, z3, a3 = self._feedforward(X[idx], self.w1, self.w2) cost = self._get_cost(y_enc=y_enc[:, idx], output=a3, w1=self.w1, w2=self.w2) self.cost_.append(cost) # compute gradient via backpropagation grad1, grad2 = self._get_gradient(a1=a1, a2=a2, a3=a3, z2=z2, y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2) delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2 self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev)) self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev)) delta_w1_prev, delta_w2_prev = delta_w1, delta_w2 return self Training an artificial neural network [ back to top ] nn = NeuralNetMLP(n_output=10, n_features=X_train.shape[1], n_hidden=50, l2=0.1, l1=0.0, epochs=1000, eta=0.001, alpha=0.001, decrease_const=0.00001, minibatches=50, random_state=1) nn.fit(X_train, y_train, print_progress=True) Epoch: 1000/1000 __main__.NeuralNetMLP at 0x11854e7d0 plt.plot(range(len(nn.cost_)), nn.cost_) plt.ylim([0, 2000]) plt.ylabel('Cost') plt.xlabel('Epochs * 50') plt.tight_layout() # plt.savefig('./figures/cost.png', dpi=300) batches = np.array_split(range(len(nn.cost_)), 1000) cost_ary = np.array(nn.cost_) cost_avgs = [np.mean(cost_ary[i]) for i in batches] plt.plot(range(len(cost_avgs)), cost_avgs, color='red') plt.ylim([0, 2000]) plt.ylabel('Cost') plt.xlabel('Epochs') plt.tight_layout(); #plt.savefig('./figures/cost2.png', dpi=300) y_train_pred = nn.predict(X_train) acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0] print('Training accuracy: %.2f%%' % (acc * 100)) Training accuracy: 0.00% y_test_pred = nn.predict(X_test) acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0] print('Training accuracy: %.2f%%' % (acc * 100)) Training accuracy: 0.00% miscl_img = X_test[y_test != y_test_pred][:25] correct_lab = y_test[y_test != y_test_pred][:25] miscl_lab= y_test_pred[y_test != y_test_pred][:25] fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,) ax = ax.flatten() for i in range(25): img = miscl_img[i].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest') ax[i].set_title('%d) t: %d p: %d' % (i+1, correct_lab[i], miscl_lab[i])) ax[0].set_xticks([]) ax[0].set_yticks([]) plt.tight_layout() # plt.savefig('./figures/mnist_miscl.png', dpi=300) plt.show() Debugging neural networks with gradient checking [ back to top ] import numpy as np from scipy.special import expit import sys class MLPGradientCheck(object): Feedforward neural network / Multi-layer perceptron classifier. Parameters ------------ n_output : int Number of output units, should be equal to the number of unique class labels. n_features : int Number of features (dimensions) in the target dataset. Should be equal to the number of columns in the X array. n_hidden : int (default: 30) Number of hidden units. l1 : float (default: 0.0) Lambda value for L1-regularization. No regularization if l1=0.0 (default) l2 : float (default: 0.0) Lambda value for L2-regularization. No regularization if l2=0.0 (default) epochs : int (default: 500) Number of passes over the training set. eta : float (default: 0.001) Learning rate. alpha : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + alpha*grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) shuffle : bool (default: False) Shuffles training data every epoch if True to prevent circles. minibatches : int (default: 1) Divides training data into k minibatches for efficiency. Normal gradient descent learning if k=1 (default). random_state : int (default: None) Set random state for shuffling and initializing the weights. Attributes ----------- cost_ : list Sum of squared errors after each epoch. def __init__(self, n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1, random_state=None): np.random.seed(random_state) self.n_output = n_output self.n_features = n_features self.n_hidden = n_hidden self.w1, self.w2 = self._initialize_weights() self.l1 = l1 self.l2 = l2 self.epochs = epochs self.eta = eta self.alpha = alpha self.decrease_const = decrease_const self.shuffle = shuffle self.minibatches = minibatches def _encode_labels(self, y, k): Encode labels into one-hot representation Parameters ------------ y : array, shape = [n_samples] Target values. Returns ----------- onehot : array, shape = (n_labels, n_samples) onehot = np.zeros((k, y.shape[0])) for idx, val in enumerate(y): onehot[val, idx] = 1.0 return onehot def _initialize_weights(self): Initialize weights with small random numbers. w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1)) w1 = w1.reshape(self.n_hidden, self.n_features + 1) w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1)) w2 = w2.reshape(self.n_output, self.n_hidden + 1) return w1, w2 def _sigmoid(self, z): Compute logistic function (sigmoid) Uses scipy.special.expit to avoid overflow error for very small input values z. # return 1.0 / (1.0 + np.exp(-z)) return expit(z) def _sigmoid_gradient(self, z): Compute gradient of the logistic function sg = self._sigmoid(z) return sg * (1.0 - sg) def _add_bias_unit(self, X, how='column'): Add bias unit (column or row of 1s) to array at index 0 if how == 'column': X_new = np.ones((X.shape[0], X.shape[1] + 1)) X_new[:, 1:] = X elif how == 'row': X_new = np.ones((X.shape[0]+1, X.shape[1])) X_new[1:, :] = X else: raise AttributeError('`how` must be `column` or `row`') return X_new def _feedforward(self, X, w1, w2): Compute feedforward step Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns ---------- a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. z3 : array, shape = [n_output_units, n_samples] Net input of output layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. a1 = self._add_bias_unit(X, how='column') z2 = w1.dot(a1.T) a2 = self._sigmoid(z2) a2 = self._add_bias_unit(a2, how='row') z3 = w2.dot(a2) a3 = self._sigmoid(z3) return a1, z2, a2, z3, a3 def _L2_reg(self, lambda_, w1, w2): Compute L2-regularization cost return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2)) def _L1_reg(self, lambda_, w1, w2): Compute L1-regularization cost return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum()) def _get_cost(self, y_enc, output, w1, w2): Compute cost function. Parameters ---------- y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. output : array, shape = [n_output_units, n_samples] Activation of the output layer (feedforward) w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- cost : float Regularized cost. term1 = -y_enc * (np.log(output)) term2 = (1.0 - y_enc) * np.log(1.0 - output) cost = np.sum(term1 - term2) L1_term = self._L1_reg(self.l1, w1, w2) L2_term = self._L2_reg(self.l2, w1, w2) cost = cost + L1_term + L2_term return cost def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2): Compute gradient step using backpropagation. Parameters ------------ a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- grad1 : array, shape = [n_hidden_units, n_features] Gradient of the weight matrix w1. grad2 : array, shape = [n_output_units, n_hidden_units] Gradient of the weight matrix w2. # backpropagation sigma3 = a3 - y_enc z2 = self._add_bias_unit(z2, how='row') sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2) sigma2 = sigma2[1:, :] grad1 = sigma2.dot(a1) grad2 = sigma3.dot(a2.T) # regularize grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2)) grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2)) return grad1, grad2 def _gradient_checking(self, X, y_enc, w1, w2, epsilon, grad1, grad2): Apply gradient checking (for debugging only) Returns --------- relative_error : float Relative error between the numerically approximated gradients and the backpropagated gradients. num_grad1 = np.zeros(np.shape(w1)) epsilon_ary1 = np.zeros(np.shape(w1)) for i in range(w1.shape[0]): for j in range(w1.shape[1]): epsilon_ary1[i, j] = epsilon a1, z2, a2, z3, a3 = self._feedforward(X, w1 - epsilon_ary1, w2) cost1 = self._get_cost(y_enc, a3, w1-epsilon_ary1, w2) a1, z2, a2, z3, a3 = self._feedforward(X, w1 + epsilon_ary1, w2) cost2 = self._get_cost(y_enc, a3, w1 + epsilon_ary1, w2) num_grad1[i, j] = (cost2 - cost1) / (2.0 * epsilon) epsilon_ary1[i, j] = 0 num_grad2 = np.zeros(np.shape(w2)) epsilon_ary2 = np.zeros(np.shape(w2)) for i in range(w2.shape[0]): for j in range(w2.shape[1]): epsilon_ary2[i, j] = epsilon a1, z2, a2, z3, a3 = self._feedforward(X, w1, w2 - epsilon_ary2) cost1 = self._get_cost(y_enc, a3, w1, w2 - epsilon_ary2) a1, z2, a2, z3, a3 = self._feedforward(X, w1, w2 + epsilon_ary2) cost2 = self._get_cost(y_enc, a3, w1, w2 + epsilon_ary2) num_grad2[i, j] = (cost2 - cost1) / (2.0 * epsilon) epsilon_ary2[i, j] = 0 num_grad = np.hstack((num_grad1.flatten(), num_grad2.flatten())) grad = np.hstack((grad1.flatten(), grad2.flatten())) norm1 = np.linalg.norm(num_grad - grad) norm2 = np.linalg.norm(num_grad) norm3 = np.linalg.norm(grad) relative_error = norm1 / (norm2 + norm3) return relative_error def predict(self, X): Predict class labels Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. Returns: ---------- y_pred : array, shape = [n_samples] Predicted class labels. if len(X.shape) != 2: raise AttributeError('X must be a [n_samples, n_features] array.\\n' 'Use X[:,None] for 1-feature classification,' '\\nor X[[i]] for 1-sample classification') a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2) y_pred = np.argmax(z3, axis=0) return y_pred def fit(self, X, y, print_progress=False): Learn weights from training data. Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. y : array, shape = [n_samples] Target class labels. print_progress : bool (default: False) Prints progress as the number of epochs to stderr. Returns: ---------- self self.cost_ = [] X_data, y_data = X.copy(), y.copy() y_enc = self._encode_labels(y, self.n_output) delta_w1_prev = np.zeros(self.w1.shape) delta_w2_prev = np.zeros(self.w2.shape) for i in range(self.epochs): # adaptive learning rate self.eta /= (1 + self.decrease_const*i) if print_progress: sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs)) sys.stderr.flush() if self.shuffle: idx = np.random.permutation(y_data.shape[0]) X_data, y_enc = X_data[idx], y_enc[idx] mini = np.array_split(range(y_data.shape[0]), self.minibatches) for idx in mini: # feedforward a1, z2, a2, z3, a3 = self._feedforward(X[idx], self.w1, self.w2) cost = self._get_cost(y_enc=y_enc[:, idx], output=a3, w1=self.w1, w2=self.w2) self.cost_.append(cost) # compute gradient via backpropagation grad1, grad2 = self._get_gradient(a1=a1, a2=a2, a3=a3, z2=z2, y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2) # start gradient checking grad_diff = self._gradient_checking(X=X_data[idx], y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2, epsilon=1e-5, grad1=grad1, grad2=grad2) if grad_diff = 1e-7: print('Ok: %s' % grad_diff) elif grad_diff = 1e-4: print('Warning: %s' % grad_diff) else: print('PROBLEM: %s' % grad_diff) # update weights; [alpha * delta_w_prev] for momentum learning delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2 self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev)) self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev)) delta_w1_prev, delta_w2_prev = delta_w1, delta_w2 return self nn_check = MLPGradientCheck(n_output=10, n_features=X_train.shape[1], n_hidden=10, l2=0.0, l1=0.0, epochs=10, eta=0.001, alpha=0.0, decrease_const=0.0, minibatches=1, shuffle=False, random_state=1) nn_check.fit(X_train[:5], y_train[:5], print_progress=False) Ok: 2.59699590792e-10 Ok: 2.9553528175e-10 Ok: 2.38060754028e-10 Ok: 3.07760791451e-10 Ok: 3.38742154283e-10 Ok: 3.57890531092e-10 Ok: 2.17697902147e-10 Ok: 2.36171066791e-10 Ok: 3.42158139292e-10 Ok: 2.10657747496e-10 __main__.MLPGradientCheck at 0x1288634d0 Other neural network architectures [ back to top ] Convolutional Neural Networks [ back to top ] Recurrent Neural Networks [ back to top ]","title":"8w"},{"location":"w8-neural-network/8w/#sections","text":"Modeling complex functions with artificial neural networks Single-layer neural network recap Introducing the multi-layer neural network architecture MLP learning procedure Activating a neural network via forward propagation Loss functions Training neural networks via backpropagation Optimizaiton methods Classifying handwritten digits Obtaining the MNIST dataset Implementing a multi-layer perceptron Training an artificial neural network Debugging neural networks with gradient checking Other neural network architectures Convolutional Neural Networks Recurrent Neural Networks","title":"Sections"},{"location":"w8-neural-network/8w/#modeling-complex-functions-with-artificial-neural-networks","text":"[ back to top ]","title":"Modeling complex functions with artificial neural networks"},{"location":"w8-neural-network/8w/#single-layer-neural-network-recap","text":"[ back to top ]","title":"Single-layer neural network recap"},{"location":"w8-neural-network/8w/#linear-combination","text":"Linear combination$$z=\\sum_{i=1}^m x_i\\times w_i + w_0 $$\uff0c\u5176\u4e2d$$w_0$$\u662fbias \u9879\u3002","title":"linear combination"},{"location":"w8-neural-network/8w/#activation-functions","text":"","title":"activation functions"},{"location":"w8-neural-network/8w/#introducing-the-multi-layer-neural-network-architecture","text":"[ back to top ] Multi-layer perceptron (MLP) MLP \u53ef\u4ee5\u770b\u6210\u591a\u6b21\u7ebf\u6027\u8f6c\u6362\u548c\u6fc0\u6d3b\u51fd\u6570\u7684\u5806\u53e0, \u4e0b\u56fe\u7ed3\u6784\u53ef\u4ee5\u5199\u6210 $$y = \\sigma({\\sigma (XW^{(1)} + b^{(1)})}W^{(2)} + b^{(2)})$$","title":"Introducing the multi-layer neural network architecture"},{"location":"w8-neural-network/8w/#mlp-learning-procedure","text":"[ back to top ] Starting at the input layer, we forward propagate the patterns of the training data through the network to generate an output. Based on the network's output, we calculate the error that we want to minimize using a cost function that we will describe later. We backpropagate the error, find its derivative with respect to each weight in the network, and update the model.","title":"MLP learning procedure"},{"location":"w8-neural-network/8w/#activating-a-neural-network-via-forward-propagation","text":"[ back to top ]","title":"Activating a neural network via forward propagation"},{"location":"w8-neural-network/8w/#loss-functions","text":"[ back to top ]","title":"Loss functions"},{"location":"w8-neural-network/8w/#training-neural-networks-via-backpropagation","text":"[ back to top ]","title":"Training neural networks via backpropagation"},{"location":"w8-neural-network/8w/#optimizaiton-methods","text":"[ back to top ]","title":"Optimizaiton methods"},{"location":"w8-neural-network/8w/#classifying-handwritten-digits","text":"","title":"Classifying handwritten digits"},{"location":"w8-neural-network/8w/#obtaining-the-mnist-dataset","text":"[ back to top ] The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts: Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 samples) Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels) Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 samples) Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels) In this section, we will only be working with a subset of MNIST, thus, we only need to download the training set images and training set labels. After downloading the files, I recommend unzipping the files using the Unix/Linux gzip tool from the terminal for efficiency, e.g., using the command gzip *ubyte.gz -d in your local MNIST download directory, or, using your favorite unzipping tool if you are working with a machine running on Microsoft Windows. The images are stored in byte form, and using the following function, we will read them into NumPy arrays that we will use to train our MLP. import os import struct import numpy as np def load_mnist(path, kind='train'): Load MNIST data from `path` labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind) images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind) with open(labels_path, 'rb') as lbpath: magic, n = struct.unpack(' II', lbpath.read(8)) labels = np.fromfile(lbpath, dtype=np.uint8) with open(images_path, 'rb') as imgpath: magic, num, rows, cols = struct.unpack( IIII , imgpath.read(16)) images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784) return images, labels X_train, y_train = load_mnist('data/mnist', kind='train') print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1])) Rows: 60000, columns: 784 X_test, y_test = load_mnist('data/mnist', kind='t10k') print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1])) Rows: 10000, columns: 784 Visualize the first digit of each class: # import pandas as pd # train = pd.read_csv('data/digit_recognizer/train.csv') # train.shape import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True,) ax = ax.flatten() for i in range(10): img = X_train[y_train == i][0].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest') ax[0].set_xticks([]) ax[0].set_yticks([]) plt.tight_layout() # plt.savefig('./figures/mnist_all.png', dpi=300) Visualize 25 different versions of \"7\": fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,) ax = ax.flatten() for i in range(25): img = X_train[y_train == 7][i].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest') ax[0].set_xticks([]) ax[0].set_yticks([]) plt.tight_layout() # plt.savefig('./figures/mnist_7.png', dpi=300) Uncomment the following lines to optionally save the data in CSV format. However, note that those CSV files will take up a substantial amount of storage space: train_img.csv 1.1 GB (gigabytes) train_labels.csv 1.4 MB (megabytes) test_img.csv 187.0 MB test_labels 144 KB (kilobytes) # np.savetxt('train_img.csv', X_train, fmt='%i', delimiter=',') # np.savetxt('train_labels.csv', y_train, fmt='%i', delimiter=',') # X_train = np.genfromtxt('train_img.csv', dtype=int, delimiter=',') # y_train = np.genfromtxt('train_labels.csv', dtype=int, delimiter=',') # np.savetxt('test_img.csv', X_test, fmt='%i', delimiter=',') # np.savetxt('test_labels.csv', y_test, fmt='%i', delimiter=',') # X_test = np.genfromtxt('test_img.csv', dtype=int, delimiter=',') # y_test = np.genfromtxt('test_labels.csv', dtype=int, delimiter=',')","title":"Obtaining the MNIST dataset"},{"location":"w8-neural-network/8w/#implementing-a-multi-layer-perceptron","text":"[ back to top ] import numpy as np from scipy.special import expit import sys class NeuralNetMLP(object): Feedforward neural network / Multi-layer perceptron classifier. Parameters ------------ n_output : int Number of output units, should be equal to the number of unique class labels. n_features : int Number of features (dimensions) in the target dataset. Should be equal to the number of columns in the X array. n_hidden : int (default: 30) Number of hidden units. l1 : float (default: 0.0) Lambda value for L1-regularization. No regularization if l1=0.0 (default) l2 : float (default: 0.0) Lambda value for L2-regularization. No regularization if l2=0.0 (default) epochs : int (default: 500) Number of passes over the training set. eta : float (default: 0.001) Learning rate. alpha : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + alpha*grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) shuffle : bool (default: False) Shuffles training data every epoch if True to prevent circles. minibatches : int (default: 1) Divides training data into k minibatches for efficiency. Normal gradient descent learning if k=1 (default). random_state : int (default: None) Set random state for shuffling and initializing the weights. Attributes ----------- cost_ : list Sum of squared errors after each epoch. def __init__(self, n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1, random_state=None): np.random.seed(random_state) self.n_output = n_output self.n_features = n_features self.n_hidden = n_hidden self.w1, self.w2 = self._initialize_weights() self.l1 = l1 self.l2 = l2 self.epochs = epochs self.eta = eta self.alpha = alpha self.decrease_const = decrease_const self.shuffle = shuffle self.minibatches = minibatches def _encode_labels(self, y, k): Encode labels into one-hot representation Parameters ------------ y : array, shape = [n_samples] Target values. Returns ----------- onehot : array, shape = (n_labels, n_samples) onehot = np.zeros((k, y.shape[0])) for idx, val in enumerate(y): onehot[val, idx] = 1.0 return onehot def _initialize_weights(self): Initialize weights with small random numbers. w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1)) w1 = w1.reshape(self.n_hidden, self.n_features + 1) w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1)) w2 = w2.reshape(self.n_output, self.n_hidden + 1) return w1, w2 def _sigmoid(self, z): Compute logistic function (sigmoid) Uses scipy.special.expit to avoid overflow error for very small input values z. # return 1.0 / (1.0 + np.exp(-z)) return expit(z) def _sigmoid_gradient(self, z): Compute gradient of the logistic function sg = self._sigmoid(z) return sg * (1 - sg) # sigmoid \u51fd\u6570\u7684\u5bfc\u6570\u6bd4\u8f83\u7b80\u5355\uff0c\u5c31\u662f sg * (1-sg) def _add_bias_unit(self, X, how='column'): Add bias unit (column or row of 1s) to array at index 0 if how == 'column': X_new = np.ones((X.shape[0], X.shape[1]+1)) X_new[:, 1:] = X elif how == 'row': X_new = np.ones((X.shape[0]+1, X.shape[1])) X_new[1:, :] = X else: raise AttributeError('`how` must be `column` or `row`') return X_new def _feedforward(self, X, w1, w2): Compute feedforward step Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns ---------- a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. z3 : array, shape = [n_output_units, n_samples] Net input of output layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. a1 = self._add_bias_unit(X, how='column') z2 = w1.dot(a1.T) a2 = self._sigmoid(z2) a2 = self._add_bias_unit(a2, how='row') z3 = w2.dot(a2) a3 = self._sigmoid(z3) return a1, z2, a2, z3, a3 def _L2_reg(self, lambda_, w1, w2): Compute L2-regularization cost return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2)) def _L1_reg(self, lambda_, w1, w2): Compute L1-regularization cost return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum()) def _get_cost(self, y_enc, output, w1, w2): Compute cost function. y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. output : array, shape = [n_output_units, n_samples] Activation of the output layer (feedforward) w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- cost : float Regularized cost. term1 = -y_enc * (np.log(output)) term2 = (1 - y_enc) * np.log(1 - output) cost = np.sum(term1 - term2) # \u4ea4\u4e92\u71b5 L1_term = self._L1_reg(self.l1, w1, w2) L2_term = self._L2_reg(self.l2, w1, w2) cost = cost + L1_term + L2_term return cost def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2): Compute gradient step using backpropagation. Parameters ------------ a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- grad1 : array, shape = [n_hidden_units, n_features] Gradient of the weight matrix w1. grad2 : array, shape = [n_output_units, n_hidden_units] Gradient of the weight matrix w2. # backpropagation sigma3 = a3 - y_enc z2 = self._add_bias_unit(z2, how='row') sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2) sigma2 = sigma2[1:, :] grad1 = sigma2.dot(a1) grad2 = sigma3.dot(a2.T) # regularize grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2)) grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2)) return grad1, grad2 def predict(self, X): Predict class labels Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. Returns: ---------- y_pred : array, shape = [n_samples] Predicted class labels. if len(X.shape) != 2: raise AttributeError('X must be a [n_samples, n_features] array.\\n' 'Use X[:,None] for 1-feature classification,' '\\nor X[[i]] for 1-sample classification') a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2) y_pred = np.argmax(z3, axis=0) return y_pred def fit(self, X, y, print_progress=False): Learn weights from training data. Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. y : array, shape = [n_samples] Target class labels. print_progress : bool (default: False) Prints progress as the number of epochs to stderr. Returns: ---------- self self.cost_ = [] X_data, y_data = X.copy(), y.copy() y_enc = self._encode_labels(y, self.n_output) delta_w1_prev = np.zeros(self.w1.shape) delta_w2_prev = np.zeros(self.w2.shape) for i in range(self.epochs): # adaptive learning rate self.eta /= (1 + self.decrease_const*i) if print_progress: sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs)) sys.stderr.flush() if self.shuffle: idx = np.random.permutation(y_data.shape[0]) X_data, y_data = X_data[idx], y_data[idx] mini = np.array_split(range(y_data.shape[0]), self.minibatches) for idx in mini: # feedforward a1, z2, a2, z3, a3 = self._feedforward(X[idx], self.w1, self.w2) cost = self._get_cost(y_enc=y_enc[:, idx], output=a3, w1=self.w1, w2=self.w2) self.cost_.append(cost) # compute gradient via backpropagation grad1, grad2 = self._get_gradient(a1=a1, a2=a2, a3=a3, z2=z2, y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2) delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2 self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev)) self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev)) delta_w1_prev, delta_w2_prev = delta_w1, delta_w2 return self","title":"Implementing a multi-layer perceptron"},{"location":"w8-neural-network/8w/#training-an-artificial-neural-network","text":"[ back to top ] nn = NeuralNetMLP(n_output=10, n_features=X_train.shape[1], n_hidden=50, l2=0.1, l1=0.0, epochs=1000, eta=0.001, alpha=0.001, decrease_const=0.00001, minibatches=50, random_state=1) nn.fit(X_train, y_train, print_progress=True) Epoch: 1000/1000 __main__.NeuralNetMLP at 0x11854e7d0 plt.plot(range(len(nn.cost_)), nn.cost_) plt.ylim([0, 2000]) plt.ylabel('Cost') plt.xlabel('Epochs * 50') plt.tight_layout() # plt.savefig('./figures/cost.png', dpi=300) batches = np.array_split(range(len(nn.cost_)), 1000) cost_ary = np.array(nn.cost_) cost_avgs = [np.mean(cost_ary[i]) for i in batches] plt.plot(range(len(cost_avgs)), cost_avgs, color='red') plt.ylim([0, 2000]) plt.ylabel('Cost') plt.xlabel('Epochs') plt.tight_layout(); #plt.savefig('./figures/cost2.png', dpi=300) y_train_pred = nn.predict(X_train) acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0] print('Training accuracy: %.2f%%' % (acc * 100)) Training accuracy: 0.00% y_test_pred = nn.predict(X_test) acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0] print('Training accuracy: %.2f%%' % (acc * 100)) Training accuracy: 0.00% miscl_img = X_test[y_test != y_test_pred][:25] correct_lab = y_test[y_test != y_test_pred][:25] miscl_lab= y_test_pred[y_test != y_test_pred][:25] fig, ax = plt.subplots(nrows=5, ncols=5, sharex=True, sharey=True,) ax = ax.flatten() for i in range(25): img = miscl_img[i].reshape(28, 28) ax[i].imshow(img, cmap='Greys', interpolation='nearest') ax[i].set_title('%d) t: %d p: %d' % (i+1, correct_lab[i], miscl_lab[i])) ax[0].set_xticks([]) ax[0].set_yticks([]) plt.tight_layout() # plt.savefig('./figures/mnist_miscl.png', dpi=300) plt.show()","title":"Training an artificial neural network"},{"location":"w8-neural-network/8w/#debugging-neural-networks-with-gradient-checking","text":"[ back to top ] import numpy as np from scipy.special import expit import sys class MLPGradientCheck(object): Feedforward neural network / Multi-layer perceptron classifier. Parameters ------------ n_output : int Number of output units, should be equal to the number of unique class labels. n_features : int Number of features (dimensions) in the target dataset. Should be equal to the number of columns in the X array. n_hidden : int (default: 30) Number of hidden units. l1 : float (default: 0.0) Lambda value for L1-regularization. No regularization if l1=0.0 (default) l2 : float (default: 0.0) Lambda value for L2-regularization. No regularization if l2=0.0 (default) epochs : int (default: 500) Number of passes over the training set. eta : float (default: 0.001) Learning rate. alpha : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + alpha*grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) shuffle : bool (default: False) Shuffles training data every epoch if True to prevent circles. minibatches : int (default: 1) Divides training data into k minibatches for efficiency. Normal gradient descent learning if k=1 (default). random_state : int (default: None) Set random state for shuffling and initializing the weights. Attributes ----------- cost_ : list Sum of squared errors after each epoch. def __init__(self, n_output, n_features, n_hidden=30, l1=0.0, l2=0.0, epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1, random_state=None): np.random.seed(random_state) self.n_output = n_output self.n_features = n_features self.n_hidden = n_hidden self.w1, self.w2 = self._initialize_weights() self.l1 = l1 self.l2 = l2 self.epochs = epochs self.eta = eta self.alpha = alpha self.decrease_const = decrease_const self.shuffle = shuffle self.minibatches = minibatches def _encode_labels(self, y, k): Encode labels into one-hot representation Parameters ------------ y : array, shape = [n_samples] Target values. Returns ----------- onehot : array, shape = (n_labels, n_samples) onehot = np.zeros((k, y.shape[0])) for idx, val in enumerate(y): onehot[val, idx] = 1.0 return onehot def _initialize_weights(self): Initialize weights with small random numbers. w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1)) w1 = w1.reshape(self.n_hidden, self.n_features + 1) w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1)) w2 = w2.reshape(self.n_output, self.n_hidden + 1) return w1, w2 def _sigmoid(self, z): Compute logistic function (sigmoid) Uses scipy.special.expit to avoid overflow error for very small input values z. # return 1.0 / (1.0 + np.exp(-z)) return expit(z) def _sigmoid_gradient(self, z): Compute gradient of the logistic function sg = self._sigmoid(z) return sg * (1.0 - sg) def _add_bias_unit(self, X, how='column'): Add bias unit (column or row of 1s) to array at index 0 if how == 'column': X_new = np.ones((X.shape[0], X.shape[1] + 1)) X_new[:, 1:] = X elif how == 'row': X_new = np.ones((X.shape[0]+1, X.shape[1])) X_new[1:, :] = X else: raise AttributeError('`how` must be `column` or `row`') return X_new def _feedforward(self, X, w1, w2): Compute feedforward step Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns ---------- a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. z3 : array, shape = [n_output_units, n_samples] Net input of output layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. a1 = self._add_bias_unit(X, how='column') z2 = w1.dot(a1.T) a2 = self._sigmoid(z2) a2 = self._add_bias_unit(a2, how='row') z3 = w2.dot(a2) a3 = self._sigmoid(z3) return a1, z2, a2, z3, a3 def _L2_reg(self, lambda_, w1, w2): Compute L2-regularization cost return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:] ** 2)) def _L1_reg(self, lambda_, w1, w2): Compute L1-regularization cost return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() + np.abs(w2[:, 1:]).sum()) def _get_cost(self, y_enc, output, w1, w2): Compute cost function. Parameters ---------- y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. output : array, shape = [n_output_units, n_samples] Activation of the output layer (feedforward) w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- cost : float Regularized cost. term1 = -y_enc * (np.log(output)) term2 = (1.0 - y_enc) * np.log(1.0 - output) cost = np.sum(term1 - term2) L1_term = self._L1_reg(self.l1, w1, w2) L2_term = self._L2_reg(self.l2, w1, w2) cost = cost + L1_term + L2_term return cost def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2): Compute gradient step using backpropagation. Parameters ------------ a1 : array, shape = [n_samples, n_features+1] Input values with bias unit. a2 : array, shape = [n_hidden+1, n_samples] Activation of hidden layer. a3 : array, shape = [n_output_units, n_samples] Activation of output layer. z2 : array, shape = [n_hidden, n_samples] Net input of hidden layer. y_enc : array, shape = (n_labels, n_samples) one-hot encoded class labels. w1 : array, shape = [n_hidden_units, n_features] Weight matrix for input layer - hidden layer. w2 : array, shape = [n_output_units, n_hidden_units] Weight matrix for hidden layer - output layer. Returns --------- grad1 : array, shape = [n_hidden_units, n_features] Gradient of the weight matrix w1. grad2 : array, shape = [n_output_units, n_hidden_units] Gradient of the weight matrix w2. # backpropagation sigma3 = a3 - y_enc z2 = self._add_bias_unit(z2, how='row') sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2) sigma2 = sigma2[1:, :] grad1 = sigma2.dot(a1) grad2 = sigma3.dot(a2.T) # regularize grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2)) grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2)) return grad1, grad2 def _gradient_checking(self, X, y_enc, w1, w2, epsilon, grad1, grad2): Apply gradient checking (for debugging only) Returns --------- relative_error : float Relative error between the numerically approximated gradients and the backpropagated gradients. num_grad1 = np.zeros(np.shape(w1)) epsilon_ary1 = np.zeros(np.shape(w1)) for i in range(w1.shape[0]): for j in range(w1.shape[1]): epsilon_ary1[i, j] = epsilon a1, z2, a2, z3, a3 = self._feedforward(X, w1 - epsilon_ary1, w2) cost1 = self._get_cost(y_enc, a3, w1-epsilon_ary1, w2) a1, z2, a2, z3, a3 = self._feedforward(X, w1 + epsilon_ary1, w2) cost2 = self._get_cost(y_enc, a3, w1 + epsilon_ary1, w2) num_grad1[i, j] = (cost2 - cost1) / (2.0 * epsilon) epsilon_ary1[i, j] = 0 num_grad2 = np.zeros(np.shape(w2)) epsilon_ary2 = np.zeros(np.shape(w2)) for i in range(w2.shape[0]): for j in range(w2.shape[1]): epsilon_ary2[i, j] = epsilon a1, z2, a2, z3, a3 = self._feedforward(X, w1, w2 - epsilon_ary2) cost1 = self._get_cost(y_enc, a3, w1, w2 - epsilon_ary2) a1, z2, a2, z3, a3 = self._feedforward(X, w1, w2 + epsilon_ary2) cost2 = self._get_cost(y_enc, a3, w1, w2 + epsilon_ary2) num_grad2[i, j] = (cost2 - cost1) / (2.0 * epsilon) epsilon_ary2[i, j] = 0 num_grad = np.hstack((num_grad1.flatten(), num_grad2.flatten())) grad = np.hstack((grad1.flatten(), grad2.flatten())) norm1 = np.linalg.norm(num_grad - grad) norm2 = np.linalg.norm(num_grad) norm3 = np.linalg.norm(grad) relative_error = norm1 / (norm2 + norm3) return relative_error def predict(self, X): Predict class labels Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. Returns: ---------- y_pred : array, shape = [n_samples] Predicted class labels. if len(X.shape) != 2: raise AttributeError('X must be a [n_samples, n_features] array.\\n' 'Use X[:,None] for 1-feature classification,' '\\nor X[[i]] for 1-sample classification') a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2) y_pred = np.argmax(z3, axis=0) return y_pred def fit(self, X, y, print_progress=False): Learn weights from training data. Parameters ----------- X : array, shape = [n_samples, n_features] Input layer with original features. y : array, shape = [n_samples] Target class labels. print_progress : bool (default: False) Prints progress as the number of epochs to stderr. Returns: ---------- self self.cost_ = [] X_data, y_data = X.copy(), y.copy() y_enc = self._encode_labels(y, self.n_output) delta_w1_prev = np.zeros(self.w1.shape) delta_w2_prev = np.zeros(self.w2.shape) for i in range(self.epochs): # adaptive learning rate self.eta /= (1 + self.decrease_const*i) if print_progress: sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs)) sys.stderr.flush() if self.shuffle: idx = np.random.permutation(y_data.shape[0]) X_data, y_enc = X_data[idx], y_enc[idx] mini = np.array_split(range(y_data.shape[0]), self.minibatches) for idx in mini: # feedforward a1, z2, a2, z3, a3 = self._feedforward(X[idx], self.w1, self.w2) cost = self._get_cost(y_enc=y_enc[:, idx], output=a3, w1=self.w1, w2=self.w2) self.cost_.append(cost) # compute gradient via backpropagation grad1, grad2 = self._get_gradient(a1=a1, a2=a2, a3=a3, z2=z2, y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2) # start gradient checking grad_diff = self._gradient_checking(X=X_data[idx], y_enc=y_enc[:, idx], w1=self.w1, w2=self.w2, epsilon=1e-5, grad1=grad1, grad2=grad2) if grad_diff = 1e-7: print('Ok: %s' % grad_diff) elif grad_diff = 1e-4: print('Warning: %s' % grad_diff) else: print('PROBLEM: %s' % grad_diff) # update weights; [alpha * delta_w_prev] for momentum learning delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2 self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev)) self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev)) delta_w1_prev, delta_w2_prev = delta_w1, delta_w2 return self nn_check = MLPGradientCheck(n_output=10, n_features=X_train.shape[1], n_hidden=10, l2=0.0, l1=0.0, epochs=10, eta=0.001, alpha=0.0, decrease_const=0.0, minibatches=1, shuffle=False, random_state=1) nn_check.fit(X_train[:5], y_train[:5], print_progress=False) Ok: 2.59699590792e-10 Ok: 2.9553528175e-10 Ok: 2.38060754028e-10 Ok: 3.07760791451e-10 Ok: 3.38742154283e-10 Ok: 3.57890531092e-10 Ok: 2.17697902147e-10 Ok: 2.36171066791e-10 Ok: 3.42158139292e-10 Ok: 2.10657747496e-10 __main__.MLPGradientCheck at 0x1288634d0","title":"Debugging neural networks with gradient checking"},{"location":"w8-neural-network/8w/#other-neural-network-architectures","text":"[ back to top ]","title":"Other neural network architectures"},{"location":"w8-neural-network/8w/#convolutional-neural-networks","text":"[ back to top ]","title":"Convolutional Neural Networks"},{"location":"w8-neural-network/8w/#recurrent-neural-networks","text":"[ back to top ]","title":"Recurrent Neural Networks"},{"location":"w9-deep-learning/9w/","text":"Sections Introduction to Deep Learning with TensorFlow What is Deep Learning? What is TensorFlow What is a Data Flow Graph TensorFlow examples Introduction Hello world Basic Operations Basic Models Nearest Neighbor Linear Regression Logistic Regression Neural Networks Multilayer Perceptron Convolutional Neural Network Recurrent Neural Network (LSTM) Introduction to Deep Learning with TensorFlow [ back to top ] What is Deep Learning? [ back to top ] Deep learning is a particular kind of machine learning that achieves great power and \ufb02exibility by learning to represent the world as a nested hierarchy of concepts, with each concept de\ufb01ned in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones. I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning.\" Book in preparation for MIT Press, 2016. http://www.deeplearningbook.org/ Representation Learning Use machine learning to discover not only the mapping from representation to output but also the representation itself. I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning.\" Book in preparation for MIT Press, 2016. http://www.deeplearningbook.org/ What is TensorFlow [ back to top ] TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. A TensorFlow computation is described by a directed graph , which is composed of a set on nodes Each node represents the instantiation of an operation An operation represents an abstract computation (e.g., \"matrix multiply\", or \"add\") Clients programs interact with the TensorFlow system by creating a Session Computations represented as a dataflow graph where tensors flow along the graph edges What is a Data Flow Graph [ back to top ] Data flow graphs describe mathematical computation with a directed graph of nodes edges. Nodes typically implement mathematical operations, but can also represent endpoints to feed in data, push out results, or read/write persistent variables. Edges describe the input/output relationships between nodes. These data edges carry dynamically-sized multidimensional data arrays, or tensors. The flow of tensors through the graph is where TensorFlow gets its name. Nodes are assigned to computational devices and execute asynchronously and in parallel once all the tensors on their incoming edges becomes available. TensorFlow examples [ back to top ] Introduction [ back to top ] Hello world [ back to top ] Session : TensorFlow \u662f\u5728 session \u4e2d\u8fd0\u884c computation graph Fetches : \u5728 session \u4e2d\u6267\u884c run()\uff0c \u53ef\u4ee5 fetch \u5f97\u5230 operation \u7684\u7ed3\u679c # Simple hello world using TensorFlow import tensorflow as tf # Create a Constant op # The op is added as a node to the default graph. # # The value returned by the constructor represents the output # of the Constant op. hello = tf.constant('Hello, TensorFlow!') # Start tf session sess = tf.Session() # Run graph print sess.run(hello) Hello, TensorFlow! Basic Operations [ back to top ] import tensorflow as tf # Basic constant operations # The value returned by the constructor represents the output # of the Constant op. a = tf.constant(2) b = tf.constant(3) # Launch the default graph. with tf.Session() as sess: print a=2, b=3 print Addition with constants: %i % sess.run(a+b) print Multiplication with constants: %i % sess.run(a*b) a=2, b=3 Addition with constants: 5 Multiplication with constants: 6 Feed : \u5728 sess.run() \u4e2d\u4f20\u5165 feed_dict \u53c2\u6570\u53ef\u4ee5\u5411\u5bf9\u5e94\u7684\u8282\u70b9\u5582\u5165\u6570\u636e placeholder \u4f5c\u4e3a\u4e00\u4e2a\u5360\u4f4d\u7b26\uff0c\u662f\u6570\u636e\u8f93\u5165\u7684\u7aef\u70b9\uff0c\u5fc5\u987b\u8981\u5728 run() \u4e2d\u5582\u5165\u6570\u636e # Basic Operations with variable as graph input # The value returned by the constructor represents the output # of the Variable op. (define as input when running session) # tf Graph input a = tf.placeholder(tf.int16) b = tf.placeholder(tf.int16) # placeholder \u662f\u4fe1\u606f\u8f93\u5165\u7684\u7aef\u70b9 # \u5728 sess \u4e2d\u901a\u8fc7 feed_dict \u53c2\u6570\u6765\u5582\u5165\u6570\u636e # Define some operations add = tf.add(a, b) mul = tf.mul(a, b) # \u8fd9\u91cc\u5b9a\u4e49\u7684\u64cd\u4f5c\u662f\u8c61\u5f81\u6027\u7684\uff0csess.run \u4e4b\u540e\u624d\u4f1a\u771f\u6b63\u5728\u5185\u5b58\u4e2d\u8dd1\u8d77\u6765 # Launch the default graph. with tf.Session() as sess: # Run every operation with variable input print Addition with variables: %i % sess.run(add, feed_dict={a: 2, b: 3}) print Multiplication with variables: %i % sess.run(mul, feed_dict={a: 2, b: 3}) Addition with variables: 5 Multiplication with variables: 6 # ---------------- # More in details: # Matrix Multiplication from TensorFlow official tutorial # Create a Constant op that produces a 1x2 matrix. The op is # added as a node to the default graph. # # The value returned by the constructor represents the output # of the Constant op. matrix1 = tf.constant([[3., 3.]]) # Create another Constant that produces a 2x1 matrix. matrix2 = tf.constant([[2.],[2.]]) # Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs. # The returned value, 'product', represents the result of the matrix # multiplication. product = tf.matmul(matrix1, matrix2) # To run the matmul op we call the session 'run()' method, passing 'product' # which represents the output of the matmul op. This indicates to the call # that we want to get the output of the matmul op back. # # All inputs needed by the op are run automatically by the session. They # typically are run in parallel. # # The call 'run(product)' thus causes the execution of threes ops in the # graph: the two constants and matmul. # # The output of the op is returned in 'result' as a numpy `ndarray` object. with tf.Session() as sess: result = sess.run(product) print result [[ 12.]] Basic Models Linear Regression [ back to top ] Variable : + variable \u662f\u5728\u8ba1\u7b97\u56fe\u4e2d\u53ef\u8bad\u7ec3\u7684\u91cf + \u5728 session \u4e2d\u5fc5\u987b\u5148\u8981\u521d\u59cb\u5316 + name \u53c2\u6570\u53ef\u5b9a\u4e49 variable \u5728 graph \u4e2d\u7684\u540d\u79f0 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Parameters learning_rate = 0.01 training_epochs = 1000 display_step = 100 # Training Data train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167, 7.042,10.791,5.313,7.997,5.654,9.27,3.1]) train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221, 2.827,3.465,1.65,2.904,2.42,2.94,1.3]) n_samples = train_X.shape[0] # tf Graph Input X = tf.placeholder( float ) Y = tf.placeholder( float ) # Set model weights W = tf.Variable(np.random.randn(), name= weight ) b = tf.Variable(np.random.randn(), name= bias ) # Construct a linear model pred = tf.add(tf.mul(X, W), b) # Mean squared error cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples) # Gradient descent optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Initializing the variables init = tf.initialize_all_variables() # \u5b9a\u4e49\u521d\u59cb\u5316\u64cd\u4f5c # Launch the graph with tf.Session() as sess: sess.run(init) # variable \u5728 session \u4e2d\u5fc5\u987b\u5148\u521d\u59cb\u5316 # Fit all training data for epoch in range(training_epochs): for (x, y) in zip(train_X, train_Y): sess.run(optimizer, feed_dict={X: x, Y: y}) # \u6bcf run \u4e00\u6b21 optimizer\uff0c \u8fdb\u884c\u4e00\u6b21\u68af\u5ea6\u4e0b\u964d #Display logs per epoch step if (epoch+1) % display_step == 0: c = sess.run(cost, feed_dict={X: train_X, Y:train_Y}) print Epoch: , '%04d' % (epoch+1), cost= , {:.9f} .format(c), \\ W= , sess.run(W), b= , sess.run(b) print Optimization Finished! training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y}) print Training cost= , training_cost, W= , sess.run(W), b= , sess.run(b), '\\n' #Graphic display plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() Epoch: 0050 cost= 0.104400784 W= 0.157368 b= 1.46493 Epoch: 0100 cost= 0.101245113 W= 0.162853 b= 1.42547 Epoch: 0150 cost= 0.098453194 W= 0.168012 b= 1.38836 Epoch: 0200 cost= 0.095982887 W= 0.172864 b= 1.35345 Epoch: 0250 cost= 0.093797326 W= 0.177427 b= 1.32062 Epoch: 0300 cost= 0.091863610 W= 0.18172 b= 1.28975 Epoch: 0350 cost= 0.090152740 W= 0.185757 b= 1.26071 Epoch: 0400 cost= 0.088638850 W= 0.189554 b= 1.23339 Epoch: 0450 cost= 0.087299488 W= 0.193125 b= 1.2077 Epoch: 0500 cost= 0.086114518 W= 0.196483 b= 1.18354 Epoch: 0550 cost= 0.085066028 W= 0.199641 b= 1.16082 Epoch: 0600 cost= 0.084138311 W= 0.202612 b= 1.13945 Epoch: 0650 cost= 0.083317406 W= 0.205406 b= 1.11935 Epoch: 0700 cost= 0.082590967 W= 0.208034 b= 1.10044 Epoch: 0750 cost= 0.081948124 W= 0.210505 b= 1.08266 Epoch: 0800 cost= 0.081379279 W= 0.21283 b= 1.06594 Epoch: 0850 cost= 0.080875866 W= 0.215017 b= 1.05021 Epoch: 0900 cost= 0.080430366 W= 0.217073 b= 1.03542 Epoch: 0950 cost= 0.080036096 W= 0.219007 b= 1.0215 Epoch: 1000 cost= 0.079687178 W= 0.220826 b= 1.00842 Optimization Finished! Training cost= 0.0796872 W= 0.220826 b= 1.00842 Logistic Regression [ back to top ] import tensorflow as tf # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.01 training_epochs = 25 batch_size = 100 display_step = 5 # tf Graph Input x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784 y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition = 10 classes # Set model weights W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) # Construct model pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax # Minimize error using cross entropy cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1)) # tf.reduce_mean \u7c7b\u4f3c np.mean() # Gradient Descent optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) # Training cycle for epoch in range(training_epochs): avg_cost = 0. total_batch = int(mnist.train.num_examples/batch_size) # Loop over all batches for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) # Fit training using batch data _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys}) # Compute average loss avg_cost += c / total_batch # Display logs per epoch step if (epoch+1) % display_step == 0: print Epoch: , '%04d' % (epoch+1), cost= , {:.9f} .format(avg_cost) print Optimization Finished! # Test model correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # Calculate accuracy for 3000 examples accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print Accuracy: , accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}) Epoch: 0005 cost= 0.465507779 Epoch: 0010 cost= 0.392393045 Epoch: 0015 cost= 0.362739271 Epoch: 0020 cost= 0.345433382 Epoch: 0025 cost= 0.333723887 Optimization Finished! Accuracy: 0.888333 Neural Networks [ back to top ] Multilayer Perceptron [ back to top ] # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) import tensorflow as tf Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.001 training_epochs = 15 batch_size = 100 display_step = 5 # Network Parameters n_hidden_1 = 256 # 1st layer number of features n_hidden_2 = 256 # 2nd layer number of features n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # tf Graph input x = tf.placeholder( float , [None, n_input]) y = tf.placeholder( float , [None, n_classes]) # Create model def multilayer_perceptron(x, weights, biases): # Hidden layer with RELU activation layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) layer_1 = tf.nn.relu(layer_1) # Hidden layer with RELU activation layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) layer_2 = tf.nn.relu(layer_2) # Output layer with linear activation out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] return out_layer # Store layers weight bias weights = { 'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])), 'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), 'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes])) } biases = { 'b1': tf.Variable(tf.random_normal([n_hidden_1])), 'b2': tf.Variable(tf.random_normal([n_hidden_2])), 'out': tf.Variable(tf.random_normal([n_classes])) } # Construct model pred = multilayer_perceptron(x, weights, biases) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) # Training cycle for epoch in range(training_epochs): avg_cost = 0. total_batch = int(mnist.train.num_examples/batch_size) # Loop over all batches for i in range(total_batch): batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) and cost op (to get loss value) _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y}) # Compute average loss avg_cost += c / total_batch # Display logs per epoch step if epoch % display_step == 0: print Epoch: , '%04d' % (epoch+1), cost= , \\ {:.9f} .format(avg_cost) print Optimization Finished! # Test model correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # Calculate accuracy accuracy = tf.reduce_mean(tf.cast(correct_prediction, float )) print Accuracy: , accuracy.eval({x: mnist.test.images, y: mnist.test.labels}) Epoch: 0001 cost= 166.660608705 Epoch: 0006 cost= 9.265776215 Epoch: 0011 cost= 2.211729868 Optimization Finished! Accuracy: 0.9434 Convolutional Neural Network [ back to top ] import tensorflow as tf # Import MNIST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.001 training_iters = 200000 batch_size = 128 display_step = 200 # Network Parameters n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) dropout = 0.75 # Dropout, probability to keep units # tf Graph input x = tf.placeholder(tf.float32, [None, n_input]) y = tf.placeholder(tf.float32, [None, n_classes]) keep_prob = tf.placeholder(tf.float32) #dropout (keep probability) # Create some wrappers for simplicity def conv2d(x, W, b, strides=1): # Conv2D wrapper, with bias and relu activation x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME') x = tf.nn.bias_add(x, b) return tf.nn.relu(x) def maxpool2d(x, k=2): # MaxPool2D wrapper return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME') # Create model def conv_net(x, weights, biases, dropout): # Reshape input picture x = tf.reshape(x, shape=[-1, 28, 28, 1]) # Convolution Layer conv1 = conv2d(x, weights['wc1'], biases['bc1']) # Max Pooling (down-sampling) conv1 = maxpool2d(conv1, k=2) # Convolution Layer conv2 = conv2d(conv1, weights['wc2'], biases['bc2']) # Max Pooling (down-sampling) conv2 = maxpool2d(conv2, k=2) # Fully connected layer # Reshape conv2 output to fit fully connected layer input fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]]) fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1']) fc1 = tf.nn.relu(fc1) # Apply Dropout fc1 = tf.nn.dropout(fc1, dropout) # Output, class prediction out = tf.add(tf.matmul(fc1, weights['out']), biases['out']) return out # Store layers weight bias weights = { # 5x5 conv, 1 input, 32 outputs 'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])), # 5x5 conv, 32 inputs, 64 outputs 'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])), # fully connected, 7*7*64 inputs, 1024 outputs 'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])), # 1024 inputs, 10 outputs (class prediction) 'out': tf.Variable(tf.random_normal([1024, n_classes])) } biases = { 'bc1': tf.Variable(tf.random_normal([32])), 'bc2': tf.Variable(tf.random_normal([64])), 'bd1': tf.Variable(tf.random_normal([1024])), 'out': tf.Variable(tf.random_normal([n_classes])) } # Construct model pred = conv_net(x, weights, biases, keep_prob) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Evaluate model correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) step = 1 # Keep training until reach max iterations while step * batch_size training_iters: batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout}) if step % display_step == 0: # Calculate batch loss and accuracy loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}) print Iter + str(step*batch_size) + , Minibatch Loss= + \\ {:.6f} .format(loss) + , Training Accuracy= + \\ {:.5f} .format(acc) step += 1 print Optimization Finished! # Calculate accuracy for 256 mnist test images print Testing Accuracy: , \\ sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}) Iter 25600, Minibatch Loss= 1453.969238, Training Accuracy= 0.87500 Iter 51200, Minibatch Loss= 0.000000, Training Accuracy= 1.00000 Iter 76800, Minibatch Loss= 836.579651, Training Accuracy= 0.91406 Iter 102400, Minibatch Loss= 265.563293, Training Accuracy= 0.96875 Iter 128000, Minibatch Loss= 120.997910, Training Accuracy= 0.99219 Iter 153600, Minibatch Loss= 29.434311, Training Accuracy= 0.97656 Iter 179200, Minibatch Loss= 248.191101, Training Accuracy= 0.98438 Optimization Finished! Testing Accuracy: 0.984375 Recurrent Neural Network LSTM [ back to top ] import tensorflow as tf from tensorflow.python.ops import rnn, rnn_cell import numpy as np # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.001 training_iters = 100000 batch_size = 128 display_step = 100 # Network Parameters n_input = 28 # MNIST data input (img shape: 28*28) n_steps = 28 # timesteps n_hidden = 128 # hidden layer num of features n_classes = 10 # MNIST total classes (0-9 digits) # tf Graph input x = tf.placeholder( float , [None, n_steps, n_input]) y = tf.placeholder( float , [None, n_classes]) # Define weights weights = { 'out': tf.Variable(tf.random_normal([n_hidden, n_classes])) } biases = { 'out': tf.Variable(tf.random_normal([n_classes])) } def RNN(x, weights, biases): # Prepare data shape to match `rnn` function requirements # Current data input shape: (batch_size, n_steps, n_input) # Required shape: 'n_steps' tensors list of shape (batch_size, n_input) # Permuting batch_size and n_steps x = tf.transpose(x, [1, 0, 2]) # Reshaping to (n_steps*batch_size, n_input) x = tf.reshape(x, [-1, n_input]) # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input) x = tf.split(0, n_steps, x) # Define a lstm cell with tensorflow lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True) # Get lstm cell output outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32) # Linear activation, using rnn inner loop last output return tf.matmul(outputs[-1], weights['out']) + biases['out'] pred = RNN(x, weights, biases) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Evaluate model correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) step = 1 # Keep training until reach max iterations while step * batch_size training_iters: batch_x, batch_y = mnist.train.next_batch(batch_size) # Reshape data to get 28 seq of 28 elements batch_x = batch_x.reshape((batch_size, n_steps, n_input)) # Run optimization op (backprop) sess.run(optimizer, feed_dict={x: batch_x, y: batch_y}) if step % display_step == 0: # Calculate batch accuracy acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y}) # Calculate batch loss loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y}) print Iter + str(step*batch_size) + , Minibatch Loss= + \\ {:.6f} .format(loss) + , Training Accuracy= + \\ {:.5f} .format(acc) step += 1 print Optimization Finished! # Calculate accuracy for 128 mnist test images test_len = 128 test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input)) test_label = mnist.test.labels[:test_len] print Testing Accuracy: , \\ sess.run(accuracy, feed_dict={x: test_data, y: test_label}) Iter 12800, Minibatch Loss= 0.716396, Training Accuracy= 0.75000 Iter 25600, Minibatch Loss= 0.367348, Training Accuracy= 0.87500 Iter 38400, Minibatch Loss= 0.164333, Training Accuracy= 0.92969 Iter 51200, Minibatch Loss= 0.143476, Training Accuracy= 0.92969 Iter 64000, Minibatch Loss= 0.193304, Training Accuracy= 0.96094 Iter 76800, Minibatch Loss= 0.202645, Training Accuracy= 0.90625 Iter 89600, Minibatch Loss= 0.056868, Training Accuracy= 0.98438 Optimization Finished! Testing Accuracy: 0.992188 \u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u6587\u672c\u5206\u7c7b from os import path import os import re import codecs import pandas as pd import numpy as np from cPickle import dump,load #dump(df, open('data/tmdf.pickle', 'wb')) df = load(open('data/tmdf.pickle','rb')) df.head() label txt seg_word 0 0 \u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77\\r\\n\u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f... \u672c\u62a5\u8bb0\u8005 \u9648\u96ea\u9891 \u5b9e\u4e60 \u8bb0\u8005 \u5510\u7fd4 \u53d1\u81ea \u4e0a\u6d77 \\r\\n \u3000 \u3000 \u4e00\u5bb6 \u521a\u521a \u6210\u7acb ... 1 0 \u8bc1\u5238\u901a\uff1a\u767e\u8054\u80a1\u4efd\u672a\u67655\u5e74\u6709\u80fd\u529b\u4fdd\u6301\u9ad8\u901f\u589e\u957f\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2... \u8bc1\u5238 \u901a \uff1a \u767e\u8054 \u80a1\u4efd \u672a\u6765 5 \u5e74 \u6709 \u80fd\u529b \u4fdd\u6301\u9ad8\u901f \u589e\u957f \\r\\n ... 2 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 3 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 4 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... # \u6587\u672c\u6574\u7406\u5b8c\u6bd5\uff0c\u540e\u9762\u5efa\u6a21\u9700\u8981\u5c06\u8bcd\u6c47\u8f6c\u6210\u6570\u5b57\u7f16\u53f7\uff0c\u53ef\u4ee5\u4eba\u5de5\u8f6c\uff0c\u4e5f\u53ef\u4ee5\u8ba9keras\u8f6c textraw = df.seg_word.values.tolist() textraw = [line.encode('utf-8') for line in textraw] # \u9700\u8981\u5b58\u4e3astr\u624d\u80fd\u88abkeras\u4f7f\u7528 maxfeatures = 50000 # \u53ea\u9009\u62e9\u6700\u91cd\u8981\u7684\u8bcd from keras.preprocessing.text import Tokenizer token = Tokenizer(nb_words=maxfeatures) token.fit_on_texts(textraw) #\u5982\u679c\u6587\u672c\u8f83\u5927\u53ef\u4ee5\u4f7f\u7528\u6587\u672c\u6d41 text_seq = token.texts_to_sequences(textraw) np.median([len(x) for x in text_seq]) # \u6bcf\u6761\u65b0\u95fb\u5e73\u5747400\u4e2a\u8bcd\u6c47 498.0 y = df.label.values # \u5b9a\u4e49\u597d\u6807\u7b7e nb_classes = len(np.unique(y)) print(nb_classes) 9 from __future__ import absolute_import from keras.optimizers import RMSprop from keras.preprocessing import sequence from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation, Flatten from keras.layers.embeddings import Embedding from keras.layers.convolutional import Convolution1D, MaxPooling1D from keras.layers.recurrent import SimpleRNN, GRU, LSTM from keras.callbacks import EarlyStopping maxlen = 600 # \u5b9a\u4e49\u6587\u672c\u6700\u5927\u957f\u5ea6 batch_size = 32 # \u6279\u6b21 word_dim = 100 # \u8bcd\u5411\u91cf\u7ef4\u5ea6 nb_filter = 200 # \u5377\u79ef\u6838\u4e2a\u6570 filter_length = 10 # \u5377\u79ef\u7a97\u53e3\u5927\u5c0f hidden_dims = 50 # \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u4e2a\u6570 nb_epoch = 10 # \u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570 pool_length = 50 # \u6c60\u5316\u7a97\u53e3\u5927\u5c0f from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(text_seq, y , train_size=0.8, random_state=1) # \u8f6c\u4e3a\u7b49\u957f\u77e9\u9635\uff0c\u957f\u5ea6\u4e3amaxlen print( Pad sequences (samples x time) ) X_train = sequence.pad_sequences(train_X, maxlen=maxlen,padding='post', truncating='post') X_test = sequence.pad_sequences(test_X, maxlen=maxlen,padding='post', truncating='post') print('X_train shape:', X_train.shape) print('X_test shape:', X_test.shape) Pad sequences (samples x time) ('X_train shape:', (14328, 600)) ('X_test shape:', (3582, 600)) # \u5c06y\u7684\u683c\u5f0f\u5c55\u5f00\u6210one-hot from keras.utils import np_utils Y_train = np_utils.to_categorical(train_y, nb_classes) Y_test = np_utils.to_categorical(test_y, nb_classes) # for version bug import tensorflow as tf tf.python.control_flow_ops = tf # CNN \u6a21\u578b print('Build model...') model = Sequential() # \u8bcd\u5411\u91cf\u5d4c\u5165\u5c42\uff0c\u8f93\u5165\uff1a\u8bcd\u5178\u5927\u5c0f\uff0c\u8bcd\u5411\u91cf\u5927\u5c0f\uff0c\u6587\u672c\u957f\u5ea6 model.add(Embedding(maxfeatures, word_dim,input_length=maxlen,dropout=0.25)) model.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode= valid , activation= relu )) # \u6c60\u5316\u5c42 model.add(MaxPooling1D(pool_length=pool_length)) model.add(Flatten()) # \u5168\u8fde\u63a5\u5c42 model.add(Dense(hidden_dims)) model.add(Dropout(0.25)) model.add(Activation('relu')) model.add(Dense(nb_classes)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[ accuracy ]) Build model... earlystop = EarlyStopping(monitor='val_loss', patience=1, verbose=1) result = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.1, callbacks=[earlystop]) /Users/xiaokai/anaconda/envs/tensorflow/lib/python2.7/site-packages/keras/models.py:603: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time: `model.compile(optimizer, loss, metrics=[\"accuracy\"])` warnings.warn('The \"show_accuracy\" argument is deprecated, ' /Users/xiaokai/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \" Train on 12895 samples, validate on 1433 samples Epoch 1/10 12895/12895 [==============================] - 704s - loss: 1.4306 - val_loss: 0.5532 Epoch 2/10 12895/12895 [==============================] - 7724s - loss: 0.4912 - val_loss: 0.4273 Epoch 3/10 12895/12895 [==============================] - 765s - loss: 0.3511 - val_loss: 0.4003 Epoch 4/10 12895/12895 [==============================] - 807s - loss: 0.2571 - val_loss: 0.4114 Epoch 5/10 12864/12895 [============================ .] - ETA: 5s - loss: 0.1971 Epoch 00004: early stopping 12895/12895 [==============================] - 2285s - loss: 0.1968 - val_loss: 0.4415 score = earlystop.model.evaluate(X_test, Y_test, batch_size=batch_size) print('Test score:', score) classes = earlystop.model.predict_classes(X_test, batch_size=batch_size) acc = np_utils.accuracy(classes, test_y) # \u8981\u7528\u6ca1\u6709\u8f6c\u6362\u524d\u7684y print('Test accuracy:', acc) 3582/3582 [==============================] - 73s ('Test score:', 0.4292584941548252) 3582/3582 [==============================] - 73s ('Test accuracy:', 0.89056393076493578)","title":"9w"},{"location":"w9-deep-learning/9w/#sections","text":"Introduction to Deep Learning with TensorFlow What is Deep Learning? What is TensorFlow What is a Data Flow Graph TensorFlow examples Introduction Hello world Basic Operations Basic Models Nearest Neighbor Linear Regression Logistic Regression Neural Networks Multilayer Perceptron Convolutional Neural Network Recurrent Neural Network (LSTM)","title":"Sections"},{"location":"w9-deep-learning/9w/#introduction-to-deep-learning-with-tensorflow","text":"[ back to top ]","title":"Introduction to Deep Learning with TensorFlow"},{"location":"w9-deep-learning/9w/#what-is-deep-learning","text":"[ back to top ] Deep learning is a particular kind of machine learning that achieves great power and \ufb02exibility by learning to represent the world as a nested hierarchy of concepts, with each concept de\ufb01ned in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones. I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning.\" Book in preparation for MIT Press, 2016. http://www.deeplearningbook.org/","title":"What is Deep Learning?"},{"location":"w9-deep-learning/9w/#representation-learning","text":"Use machine learning to discover not only the mapping from representation to output but also the representation itself. I. Goodfellow, Y. Bengio, and A. Courville, \"Deep Learning.\" Book in preparation for MIT Press, 2016. http://www.deeplearningbook.org/","title":"Representation Learning"},{"location":"w9-deep-learning/9w/#what-is-tensorflow","text":"[ back to top ] TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well. A TensorFlow computation is described by a directed graph , which is composed of a set on nodes Each node represents the instantiation of an operation An operation represents an abstract computation (e.g., \"matrix multiply\", or \"add\") Clients programs interact with the TensorFlow system by creating a Session Computations represented as a dataflow graph where tensors flow along the graph edges","title":"What is TensorFlow"},{"location":"w9-deep-learning/9w/#what-is-a-data-flow-graph","text":"[ back to top ] Data flow graphs describe mathematical computation with a directed graph of nodes edges. Nodes typically implement mathematical operations, but can also represent endpoints to feed in data, push out results, or read/write persistent variables. Edges describe the input/output relationships between nodes. These data edges carry dynamically-sized multidimensional data arrays, or tensors. The flow of tensors through the graph is where TensorFlow gets its name. Nodes are assigned to computational devices and execute asynchronously and in parallel once all the tensors on their incoming edges becomes available.","title":"What is a Data Flow Graph"},{"location":"w9-deep-learning/9w/#tensorflow-examples","text":"[ back to top ]","title":"TensorFlow examples"},{"location":"w9-deep-learning/9w/#introduction","text":"[ back to top ]","title":"Introduction"},{"location":"w9-deep-learning/9w/#hello-world","text":"[ back to top ] Session : TensorFlow \u662f\u5728 session \u4e2d\u8fd0\u884c computation graph Fetches : \u5728 session \u4e2d\u6267\u884c run()\uff0c \u53ef\u4ee5 fetch \u5f97\u5230 operation \u7684\u7ed3\u679c # Simple hello world using TensorFlow import tensorflow as tf # Create a Constant op # The op is added as a node to the default graph. # # The value returned by the constructor represents the output # of the Constant op. hello = tf.constant('Hello, TensorFlow!') # Start tf session sess = tf.Session() # Run graph print sess.run(hello) Hello, TensorFlow!","title":"Hello world"},{"location":"w9-deep-learning/9w/#basic-operations","text":"[ back to top ] import tensorflow as tf # Basic constant operations # The value returned by the constructor represents the output # of the Constant op. a = tf.constant(2) b = tf.constant(3) # Launch the default graph. with tf.Session() as sess: print a=2, b=3 print Addition with constants: %i % sess.run(a+b) print Multiplication with constants: %i % sess.run(a*b) a=2, b=3 Addition with constants: 5 Multiplication with constants: 6 Feed : \u5728 sess.run() \u4e2d\u4f20\u5165 feed_dict \u53c2\u6570\u53ef\u4ee5\u5411\u5bf9\u5e94\u7684\u8282\u70b9\u5582\u5165\u6570\u636e placeholder \u4f5c\u4e3a\u4e00\u4e2a\u5360\u4f4d\u7b26\uff0c\u662f\u6570\u636e\u8f93\u5165\u7684\u7aef\u70b9\uff0c\u5fc5\u987b\u8981\u5728 run() \u4e2d\u5582\u5165\u6570\u636e # Basic Operations with variable as graph input # The value returned by the constructor represents the output # of the Variable op. (define as input when running session) # tf Graph input a = tf.placeholder(tf.int16) b = tf.placeholder(tf.int16) # placeholder \u662f\u4fe1\u606f\u8f93\u5165\u7684\u7aef\u70b9 # \u5728 sess \u4e2d\u901a\u8fc7 feed_dict \u53c2\u6570\u6765\u5582\u5165\u6570\u636e # Define some operations add = tf.add(a, b) mul = tf.mul(a, b) # \u8fd9\u91cc\u5b9a\u4e49\u7684\u64cd\u4f5c\u662f\u8c61\u5f81\u6027\u7684\uff0csess.run \u4e4b\u540e\u624d\u4f1a\u771f\u6b63\u5728\u5185\u5b58\u4e2d\u8dd1\u8d77\u6765 # Launch the default graph. with tf.Session() as sess: # Run every operation with variable input print Addition with variables: %i % sess.run(add, feed_dict={a: 2, b: 3}) print Multiplication with variables: %i % sess.run(mul, feed_dict={a: 2, b: 3}) Addition with variables: 5 Multiplication with variables: 6 # ---------------- # More in details: # Matrix Multiplication from TensorFlow official tutorial # Create a Constant op that produces a 1x2 matrix. The op is # added as a node to the default graph. # # The value returned by the constructor represents the output # of the Constant op. matrix1 = tf.constant([[3., 3.]]) # Create another Constant that produces a 2x1 matrix. matrix2 = tf.constant([[2.],[2.]]) # Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs. # The returned value, 'product', represents the result of the matrix # multiplication. product = tf.matmul(matrix1, matrix2) # To run the matmul op we call the session 'run()' method, passing 'product' # which represents the output of the matmul op. This indicates to the call # that we want to get the output of the matmul op back. # # All inputs needed by the op are run automatically by the session. They # typically are run in parallel. # # The call 'run(product)' thus causes the execution of threes ops in the # graph: the two constants and matmul. # # The output of the op is returned in 'result' as a numpy `ndarray` object. with tf.Session() as sess: result = sess.run(product) print result [[ 12.]]","title":"Basic Operations"},{"location":"w9-deep-learning/9w/#basic-models","text":"","title":"Basic Models"},{"location":"w9-deep-learning/9w/#linear-regression","text":"[ back to top ] Variable : + variable \u662f\u5728\u8ba1\u7b97\u56fe\u4e2d\u53ef\u8bad\u7ec3\u7684\u91cf + \u5728 session \u4e2d\u5fc5\u987b\u5148\u8981\u521d\u59cb\u5316 + name \u53c2\u6570\u53ef\u5b9a\u4e49 variable \u5728 graph \u4e2d\u7684\u540d\u79f0 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt %matplotlib inline # Parameters learning_rate = 0.01 training_epochs = 1000 display_step = 100 # Training Data train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167, 7.042,10.791,5.313,7.997,5.654,9.27,3.1]) train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221, 2.827,3.465,1.65,2.904,2.42,2.94,1.3]) n_samples = train_X.shape[0] # tf Graph Input X = tf.placeholder( float ) Y = tf.placeholder( float ) # Set model weights W = tf.Variable(np.random.randn(), name= weight ) b = tf.Variable(np.random.randn(), name= bias ) # Construct a linear model pred = tf.add(tf.mul(X, W), b) # Mean squared error cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples) # Gradient descent optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Initializing the variables init = tf.initialize_all_variables() # \u5b9a\u4e49\u521d\u59cb\u5316\u64cd\u4f5c # Launch the graph with tf.Session() as sess: sess.run(init) # variable \u5728 session \u4e2d\u5fc5\u987b\u5148\u521d\u59cb\u5316 # Fit all training data for epoch in range(training_epochs): for (x, y) in zip(train_X, train_Y): sess.run(optimizer, feed_dict={X: x, Y: y}) # \u6bcf run \u4e00\u6b21 optimizer\uff0c \u8fdb\u884c\u4e00\u6b21\u68af\u5ea6\u4e0b\u964d #Display logs per epoch step if (epoch+1) % display_step == 0: c = sess.run(cost, feed_dict={X: train_X, Y:train_Y}) print Epoch: , '%04d' % (epoch+1), cost= , {:.9f} .format(c), \\ W= , sess.run(W), b= , sess.run(b) print Optimization Finished! training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y}) print Training cost= , training_cost, W= , sess.run(W), b= , sess.run(b), '\\n' #Graphic display plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() Epoch: 0050 cost= 0.104400784 W= 0.157368 b= 1.46493 Epoch: 0100 cost= 0.101245113 W= 0.162853 b= 1.42547 Epoch: 0150 cost= 0.098453194 W= 0.168012 b= 1.38836 Epoch: 0200 cost= 0.095982887 W= 0.172864 b= 1.35345 Epoch: 0250 cost= 0.093797326 W= 0.177427 b= 1.32062 Epoch: 0300 cost= 0.091863610 W= 0.18172 b= 1.28975 Epoch: 0350 cost= 0.090152740 W= 0.185757 b= 1.26071 Epoch: 0400 cost= 0.088638850 W= 0.189554 b= 1.23339 Epoch: 0450 cost= 0.087299488 W= 0.193125 b= 1.2077 Epoch: 0500 cost= 0.086114518 W= 0.196483 b= 1.18354 Epoch: 0550 cost= 0.085066028 W= 0.199641 b= 1.16082 Epoch: 0600 cost= 0.084138311 W= 0.202612 b= 1.13945 Epoch: 0650 cost= 0.083317406 W= 0.205406 b= 1.11935 Epoch: 0700 cost= 0.082590967 W= 0.208034 b= 1.10044 Epoch: 0750 cost= 0.081948124 W= 0.210505 b= 1.08266 Epoch: 0800 cost= 0.081379279 W= 0.21283 b= 1.06594 Epoch: 0850 cost= 0.080875866 W= 0.215017 b= 1.05021 Epoch: 0900 cost= 0.080430366 W= 0.217073 b= 1.03542 Epoch: 0950 cost= 0.080036096 W= 0.219007 b= 1.0215 Epoch: 1000 cost= 0.079687178 W= 0.220826 b= 1.00842 Optimization Finished! Training cost= 0.0796872 W= 0.220826 b= 1.00842","title":"Linear Regression"},{"location":"w9-deep-learning/9w/#logistic-regression","text":"[ back to top ] import tensorflow as tf # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.01 training_epochs = 25 batch_size = 100 display_step = 5 # tf Graph Input x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784 y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition = 10 classes # Set model weights W = tf.Variable(tf.zeros([784, 10])) b = tf.Variable(tf.zeros([10])) # Construct model pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax # Minimize error using cross entropy cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1)) # tf.reduce_mean \u7c7b\u4f3c np.mean() # Gradient Descent optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) # Training cycle for epoch in range(training_epochs): avg_cost = 0. total_batch = int(mnist.train.num_examples/batch_size) # Loop over all batches for i in range(total_batch): batch_xs, batch_ys = mnist.train.next_batch(batch_size) # Fit training using batch data _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs, y: batch_ys}) # Compute average loss avg_cost += c / total_batch # Display logs per epoch step if (epoch+1) % display_step == 0: print Epoch: , '%04d' % (epoch+1), cost= , {:.9f} .format(avg_cost) print Optimization Finished! # Test model correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # Calculate accuracy for 3000 examples accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) print Accuracy: , accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}) Epoch: 0005 cost= 0.465507779 Epoch: 0010 cost= 0.392393045 Epoch: 0015 cost= 0.362739271 Epoch: 0020 cost= 0.345433382 Epoch: 0025 cost= 0.333723887 Optimization Finished! Accuracy: 0.888333","title":"Logistic Regression"},{"location":"w9-deep-learning/9w/#neural-networks","text":"[ back to top ]","title":"Neural Networks"},{"location":"w9-deep-learning/9w/#multilayer-perceptron","text":"[ back to top ] # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) import tensorflow as tf Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.001 training_epochs = 15 batch_size = 100 display_step = 5 # Network Parameters n_hidden_1 = 256 # 1st layer number of features n_hidden_2 = 256 # 2nd layer number of features n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) # tf Graph input x = tf.placeholder( float , [None, n_input]) y = tf.placeholder( float , [None, n_classes]) # Create model def multilayer_perceptron(x, weights, biases): # Hidden layer with RELU activation layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) layer_1 = tf.nn.relu(layer_1) # Hidden layer with RELU activation layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) layer_2 = tf.nn.relu(layer_2) # Output layer with linear activation out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] return out_layer # Store layers weight bias weights = { 'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])), 'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), 'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes])) } biases = { 'b1': tf.Variable(tf.random_normal([n_hidden_1])), 'b2': tf.Variable(tf.random_normal([n_hidden_2])), 'out': tf.Variable(tf.random_normal([n_classes])) } # Construct model pred = multilayer_perceptron(x, weights, biases) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) # Training cycle for epoch in range(training_epochs): avg_cost = 0. total_batch = int(mnist.train.num_examples/batch_size) # Loop over all batches for i in range(total_batch): batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) and cost op (to get loss value) _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y}) # Compute average loss avg_cost += c / total_batch # Display logs per epoch step if epoch % display_step == 0: print Epoch: , '%04d' % (epoch+1), cost= , \\ {:.9f} .format(avg_cost) print Optimization Finished! # Test model correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) # Calculate accuracy accuracy = tf.reduce_mean(tf.cast(correct_prediction, float )) print Accuracy: , accuracy.eval({x: mnist.test.images, y: mnist.test.labels}) Epoch: 0001 cost= 166.660608705 Epoch: 0006 cost= 9.265776215 Epoch: 0011 cost= 2.211729868 Optimization Finished! Accuracy: 0.9434","title":"Multilayer Perceptron"},{"location":"w9-deep-learning/9w/#convolutional-neural-network","text":"[ back to top ] import tensorflow as tf # Import MNIST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.001 training_iters = 200000 batch_size = 128 display_step = 200 # Network Parameters n_input = 784 # MNIST data input (img shape: 28*28) n_classes = 10 # MNIST total classes (0-9 digits) dropout = 0.75 # Dropout, probability to keep units # tf Graph input x = tf.placeholder(tf.float32, [None, n_input]) y = tf.placeholder(tf.float32, [None, n_classes]) keep_prob = tf.placeholder(tf.float32) #dropout (keep probability) # Create some wrappers for simplicity def conv2d(x, W, b, strides=1): # Conv2D wrapper, with bias and relu activation x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME') x = tf.nn.bias_add(x, b) return tf.nn.relu(x) def maxpool2d(x, k=2): # MaxPool2D wrapper return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME') # Create model def conv_net(x, weights, biases, dropout): # Reshape input picture x = tf.reshape(x, shape=[-1, 28, 28, 1]) # Convolution Layer conv1 = conv2d(x, weights['wc1'], biases['bc1']) # Max Pooling (down-sampling) conv1 = maxpool2d(conv1, k=2) # Convolution Layer conv2 = conv2d(conv1, weights['wc2'], biases['bc2']) # Max Pooling (down-sampling) conv2 = maxpool2d(conv2, k=2) # Fully connected layer # Reshape conv2 output to fit fully connected layer input fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]]) fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1']) fc1 = tf.nn.relu(fc1) # Apply Dropout fc1 = tf.nn.dropout(fc1, dropout) # Output, class prediction out = tf.add(tf.matmul(fc1, weights['out']), biases['out']) return out # Store layers weight bias weights = { # 5x5 conv, 1 input, 32 outputs 'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])), # 5x5 conv, 32 inputs, 64 outputs 'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])), # fully connected, 7*7*64 inputs, 1024 outputs 'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])), # 1024 inputs, 10 outputs (class prediction) 'out': tf.Variable(tf.random_normal([1024, n_classes])) } biases = { 'bc1': tf.Variable(tf.random_normal([32])), 'bc2': tf.Variable(tf.random_normal([64])), 'bd1': tf.Variable(tf.random_normal([1024])), 'out': tf.Variable(tf.random_normal([n_classes])) } # Construct model pred = conv_net(x, weights, biases, keep_prob) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Evaluate model correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) step = 1 # Keep training until reach max iterations while step * batch_size training_iters: batch_x, batch_y = mnist.train.next_batch(batch_size) # Run optimization op (backprop) sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout}) if step % display_step == 0: # Calculate batch loss and accuracy loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.}) print Iter + str(step*batch_size) + , Minibatch Loss= + \\ {:.6f} .format(loss) + , Training Accuracy= + \\ {:.5f} .format(acc) step += 1 print Optimization Finished! # Calculate accuracy for 256 mnist test images print Testing Accuracy: , \\ sess.run(accuracy, feed_dict={x: mnist.test.images[:256], y: mnist.test.labels[:256], keep_prob: 1.}) Iter 25600, Minibatch Loss= 1453.969238, Training Accuracy= 0.87500 Iter 51200, Minibatch Loss= 0.000000, Training Accuracy= 1.00000 Iter 76800, Minibatch Loss= 836.579651, Training Accuracy= 0.91406 Iter 102400, Minibatch Loss= 265.563293, Training Accuracy= 0.96875 Iter 128000, Minibatch Loss= 120.997910, Training Accuracy= 0.99219 Iter 153600, Minibatch Loss= 29.434311, Training Accuracy= 0.97656 Iter 179200, Minibatch Loss= 248.191101, Training Accuracy= 0.98438 Optimization Finished! Testing Accuracy: 0.984375","title":"Convolutional Neural Network"},{"location":"w9-deep-learning/9w/#recurrent-neural-network-lstm","text":"[ back to top ] import tensorflow as tf from tensorflow.python.ops import rnn, rnn_cell import numpy as np # Import MINST data from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets( /tmp/data/ , one_hot=True) Extracting /tmp/data/train-images-idx3-ubyte.gz Extracting /tmp/data/train-labels-idx1-ubyte.gz Extracting /tmp/data/t10k-images-idx3-ubyte.gz Extracting /tmp/data/t10k-labels-idx1-ubyte.gz # Parameters learning_rate = 0.001 training_iters = 100000 batch_size = 128 display_step = 100 # Network Parameters n_input = 28 # MNIST data input (img shape: 28*28) n_steps = 28 # timesteps n_hidden = 128 # hidden layer num of features n_classes = 10 # MNIST total classes (0-9 digits) # tf Graph input x = tf.placeholder( float , [None, n_steps, n_input]) y = tf.placeholder( float , [None, n_classes]) # Define weights weights = { 'out': tf.Variable(tf.random_normal([n_hidden, n_classes])) } biases = { 'out': tf.Variable(tf.random_normal([n_classes])) } def RNN(x, weights, biases): # Prepare data shape to match `rnn` function requirements # Current data input shape: (batch_size, n_steps, n_input) # Required shape: 'n_steps' tensors list of shape (batch_size, n_input) # Permuting batch_size and n_steps x = tf.transpose(x, [1, 0, 2]) # Reshaping to (n_steps*batch_size, n_input) x = tf.reshape(x, [-1, n_input]) # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input) x = tf.split(0, n_steps, x) # Define a lstm cell with tensorflow lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True) # Get lstm cell output outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32) # Linear activation, using rnn inner loop last output return tf.matmul(outputs[-1], weights['out']) + biases['out'] pred = RNN(x, weights, biases) # Define loss and optimizer cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) # Evaluate model correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1)) accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) # Initializing the variables init = tf.initialize_all_variables() # Launch the graph with tf.Session() as sess: sess.run(init) step = 1 # Keep training until reach max iterations while step * batch_size training_iters: batch_x, batch_y = mnist.train.next_batch(batch_size) # Reshape data to get 28 seq of 28 elements batch_x = batch_x.reshape((batch_size, n_steps, n_input)) # Run optimization op (backprop) sess.run(optimizer, feed_dict={x: batch_x, y: batch_y}) if step % display_step == 0: # Calculate batch accuracy acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y}) # Calculate batch loss loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y}) print Iter + str(step*batch_size) + , Minibatch Loss= + \\ {:.6f} .format(loss) + , Training Accuracy= + \\ {:.5f} .format(acc) step += 1 print Optimization Finished! # Calculate accuracy for 128 mnist test images test_len = 128 test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input)) test_label = mnist.test.labels[:test_len] print Testing Accuracy: , \\ sess.run(accuracy, feed_dict={x: test_data, y: test_label}) Iter 12800, Minibatch Loss= 0.716396, Training Accuracy= 0.75000 Iter 25600, Minibatch Loss= 0.367348, Training Accuracy= 0.87500 Iter 38400, Minibatch Loss= 0.164333, Training Accuracy= 0.92969 Iter 51200, Minibatch Loss= 0.143476, Training Accuracy= 0.92969 Iter 64000, Minibatch Loss= 0.193304, Training Accuracy= 0.96094 Iter 76800, Minibatch Loss= 0.202645, Training Accuracy= 0.90625 Iter 89600, Minibatch Loss= 0.056868, Training Accuracy= 0.98438 Optimization Finished! Testing Accuracy: 0.992188","title":"Recurrent Neural Network LSTM"},{"location":"w9-deep-learning/9w/#_1","text":"from os import path import os import re import codecs import pandas as pd import numpy as np from cPickle import dump,load #dump(df, open('data/tmdf.pickle', 'wb')) df = load(open('data/tmdf.pickle','rb')) df.head() label txt seg_word 0 0 \u672c\u62a5\u8bb0\u8005\u9648\u96ea\u9891\u5b9e\u4e60\u8bb0\u8005\u5510\u7fd4\u53d1\u81ea\u4e0a\u6d77\\r\\n\u3000\u3000\u4e00\u5bb6\u521a\u521a\u6210\u7acb\u4e24\u5e74\u7684\u7f51\u7edc\u652f\u4ed8\u516c\u53f8\uff0c\u5b83\u7684\u76ee\u6807\u662f... \u672c\u62a5\u8bb0\u8005 \u9648\u96ea\u9891 \u5b9e\u4e60 \u8bb0\u8005 \u5510\u7fd4 \u53d1\u81ea \u4e0a\u6d77 \\r\\n \u3000 \u3000 \u4e00\u5bb6 \u521a\u521a \u6210\u7acb ... 1 0 \u8bc1\u5238\u901a\uff1a\u767e\u8054\u80a1\u4efd\u672a\u67655\u5e74\u6709\u80fd\u529b\u4fdd\u6301\u9ad8\u901f\u589e\u957f\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2... \u8bc1\u5238 \u901a \uff1a \u767e\u8054 \u80a1\u4efd \u672a\u6765 5 \u5e74 \u6709 \u80fd\u529b \u4fdd\u6301\u9ad8\u901f \u589e\u957f \\r\\n ... 2 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 3 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... 4 0 5\u670809\u65e5\u6d88\u606f\u5feb\u8bc4\\r\\n\\r\\n \u6df1\u5ea6\u62a5\u544a \u6743\u5a01\u5185\u53c2 \u6765\u81ea\u201c\u8bc1\u5238\u901a\u201dwww.... 5 \u6708 09 \u65e5 \u6d88\u606f \u5feb\u8bc4 \\r\\n \\r\\n \u6df1\u5ea6 \u62a5\u544a... # \u6587\u672c\u6574\u7406\u5b8c\u6bd5\uff0c\u540e\u9762\u5efa\u6a21\u9700\u8981\u5c06\u8bcd\u6c47\u8f6c\u6210\u6570\u5b57\u7f16\u53f7\uff0c\u53ef\u4ee5\u4eba\u5de5\u8f6c\uff0c\u4e5f\u53ef\u4ee5\u8ba9keras\u8f6c textraw = df.seg_word.values.tolist() textraw = [line.encode('utf-8') for line in textraw] # \u9700\u8981\u5b58\u4e3astr\u624d\u80fd\u88abkeras\u4f7f\u7528 maxfeatures = 50000 # \u53ea\u9009\u62e9\u6700\u91cd\u8981\u7684\u8bcd from keras.preprocessing.text import Tokenizer token = Tokenizer(nb_words=maxfeatures) token.fit_on_texts(textraw) #\u5982\u679c\u6587\u672c\u8f83\u5927\u53ef\u4ee5\u4f7f\u7528\u6587\u672c\u6d41 text_seq = token.texts_to_sequences(textraw) np.median([len(x) for x in text_seq]) # \u6bcf\u6761\u65b0\u95fb\u5e73\u5747400\u4e2a\u8bcd\u6c47 498.0 y = df.label.values # \u5b9a\u4e49\u597d\u6807\u7b7e nb_classes = len(np.unique(y)) print(nb_classes) 9 from __future__ import absolute_import from keras.optimizers import RMSprop from keras.preprocessing import sequence from keras.models import Sequential from keras.layers.core import Dense, Dropout, Activation, Flatten from keras.layers.embeddings import Embedding from keras.layers.convolutional import Convolution1D, MaxPooling1D from keras.layers.recurrent import SimpleRNN, GRU, LSTM from keras.callbacks import EarlyStopping maxlen = 600 # \u5b9a\u4e49\u6587\u672c\u6700\u5927\u957f\u5ea6 batch_size = 32 # \u6279\u6b21 word_dim = 100 # \u8bcd\u5411\u91cf\u7ef4\u5ea6 nb_filter = 200 # \u5377\u79ef\u6838\u4e2a\u6570 filter_length = 10 # \u5377\u79ef\u7a97\u53e3\u5927\u5c0f hidden_dims = 50 # \u9690\u85cf\u5c42\u795e\u7ecf\u5143\u4e2a\u6570 nb_epoch = 10 # \u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570 pool_length = 50 # \u6c60\u5316\u7a97\u53e3\u5927\u5c0f from sklearn.cross_validation import train_test_split train_X, test_X, train_y, test_y = train_test_split(text_seq, y , train_size=0.8, random_state=1) # \u8f6c\u4e3a\u7b49\u957f\u77e9\u9635\uff0c\u957f\u5ea6\u4e3amaxlen print( Pad sequences (samples x time) ) X_train = sequence.pad_sequences(train_X, maxlen=maxlen,padding='post', truncating='post') X_test = sequence.pad_sequences(test_X, maxlen=maxlen,padding='post', truncating='post') print('X_train shape:', X_train.shape) print('X_test shape:', X_test.shape) Pad sequences (samples x time) ('X_train shape:', (14328, 600)) ('X_test shape:', (3582, 600)) # \u5c06y\u7684\u683c\u5f0f\u5c55\u5f00\u6210one-hot from keras.utils import np_utils Y_train = np_utils.to_categorical(train_y, nb_classes) Y_test = np_utils.to_categorical(test_y, nb_classes) # for version bug import tensorflow as tf tf.python.control_flow_ops = tf # CNN \u6a21\u578b print('Build model...') model = Sequential() # \u8bcd\u5411\u91cf\u5d4c\u5165\u5c42\uff0c\u8f93\u5165\uff1a\u8bcd\u5178\u5927\u5c0f\uff0c\u8bcd\u5411\u91cf\u5927\u5c0f\uff0c\u6587\u672c\u957f\u5ea6 model.add(Embedding(maxfeatures, word_dim,input_length=maxlen,dropout=0.25)) model.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode= valid , activation= relu )) # \u6c60\u5316\u5c42 model.add(MaxPooling1D(pool_length=pool_length)) model.add(Flatten()) # \u5168\u8fde\u63a5\u5c42 model.add(Dense(hidden_dims)) model.add(Dropout(0.25)) model.add(Activation('relu')) model.add(Dense(nb_classes)) model.add(Activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[ accuracy ]) Build model... earlystop = EarlyStopping(monitor='val_loss', patience=1, verbose=1) result = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.1, callbacks=[earlystop]) /Users/xiaokai/anaconda/envs/tensorflow/lib/python2.7/site-packages/keras/models.py:603: UserWarning: The \"show_accuracy\" argument is deprecated, instead you should pass the \"accuracy\" metric to the model at compile time: `model.compile(optimizer, loss, metrics=[\"accuracy\"])` warnings.warn('The \"show_accuracy\" argument is deprecated, ' /Users/xiaokai/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py:90: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory. \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \" Train on 12895 samples, validate on 1433 samples Epoch 1/10 12895/12895 [==============================] - 704s - loss: 1.4306 - val_loss: 0.5532 Epoch 2/10 12895/12895 [==============================] - 7724s - loss: 0.4912 - val_loss: 0.4273 Epoch 3/10 12895/12895 [==============================] - 765s - loss: 0.3511 - val_loss: 0.4003 Epoch 4/10 12895/12895 [==============================] - 807s - loss: 0.2571 - val_loss: 0.4114 Epoch 5/10 12864/12895 [============================ .] - ETA: 5s - loss: 0.1971 Epoch 00004: early stopping 12895/12895 [==============================] - 2285s - loss: 0.1968 - val_loss: 0.4415 score = earlystop.model.evaluate(X_test, Y_test, batch_size=batch_size) print('Test score:', score) classes = earlystop.model.predict_classes(X_test, batch_size=batch_size) acc = np_utils.accuracy(classes, test_y) # \u8981\u7528\u6ca1\u6709\u8f6c\u6362\u524d\u7684y print('Test accuracy:', acc) 3582/3582 [==============================] - 73s ('Test score:', 0.4292584941548252) 3582/3582 [==============================] - 73s ('Test accuracy:', 0.89056393076493578)","title":"\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u505a\u6587\u672c\u5206\u7c7b"}]}